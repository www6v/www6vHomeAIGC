<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/www6vHomeHexo/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/www6vHomeHexo/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/www6vHomeHexo/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/www6vHomeHexo/images/logo.svg" color="#222">

<link rel="stylesheet" href="/www6vHomeHexo/css/main.css">


<link rel="stylesheet" href="/www6vHomeHexo/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www6v.github.io","root":"/www6vHomeHexo/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="《Streaming System》-Chapter 2. The What, Where, When, and How of Data Processing[完整]">
<meta property="og:url" content="https://www6v.github.io/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/index.html">
<meta property="og:site_name" content="www6v的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2000-03-17T14:08:13.000Z">
<meta property="article:modified_time" content="2023-04-29T22:57:16.396Z">
<meta property="article:author" content="Wang Wei">
<meta property="article:tag" content="Streaming System">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www6v.github.io/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《Streaming System》-Chapter 2. The What, Where, When, and How of Data Processing[完整] | www6v的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/www6vHomeHexo/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">www6v的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录技术点滴</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/www6vHomeHexo/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/www6vHomeHexo/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/www6vHomeHexo/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/www6vHomeHexo/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www6v.github.io/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/www6vHomeHexo/images/avatar.gif">
      <meta itemprop="name" content="Wang Wei">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="www6v的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Streaming System》-Chapter 2. The What, Where, When, and How of Data Processing[完整]
        </h1>

        <div class="post-meta">





          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2000-03-17 22:08:13" itemprop="dateCreated datePublished" datetime="2000-03-17T22:08:13+08:00">2000-03-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-30 06:57:16" itemprop="dateModified" datetime="2023-04-30T06:57:16+08:00">2023-04-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/www6vHomeHexo/categories/Streaming-System/" itemprop="url" rel="index"><span itemprop="name">Streaming System</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#roadmap"><strong>Roadmap</strong></a></li>
<li><a href="#batch-foundations-what-and-where"><strong>Batch Foundations: *What* and *Where*</strong></a><ul>
<li><a href="#what-transformations">*<strong>What*: Transformations</strong></a></li>
<li><a href="#where-windowing">*<strong>Where*: Windowing</strong></a></li>
</ul>
</li>
<li><a href="#going-streaming-when-and-how"><strong>Going Streaming: <em>When</em> and <em>How</em></strong></a><ul>
<li><a href="#when-the-wonderful-thing-about-triggers-is-triggers"><strong><em>When</em>: The Wonderful Thing About Triggers Is Triggers</strong></a></li>
<li><a href="#are-wonderful-things"><strong>Are Wonderful Things!</strong></a></li>
<li><a href="#when-watermarks">*<strong>When*: Watermarks</strong></a></li>
<li><a href="#when-earlyon-timelate-triggers-ftw"><strong><em>When</em>: Early&#x2F;On-Time&#x2F;Late Triggers FTW!</strong></a></li>
<li><a href="#when-allowed-lateness-ie-garbage-collection">*<strong>When*: Allowed Lateness (i.e., Garbage Collection)</strong></a></li>
</ul>
</li>
<li><a href="#how-accumulation">*<strong>How*: Accumulation</strong></a></li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 42</p>
<details><summary>点击 原文</summary><p>Okay party people, it’s time to get concrete!</p>
<p>Chapter 1 focused on three main areas: <em>terminology</em>, defining precisely what I</p>
<p>mean when I use overloaded terms like “streaming”; <em>batch versus streaming</em>,</p>
<p>comparing the theoretical capabilities of the two types of systems, and</p>
<p>postulating that only two things are necessary to take streaming systems</p>
<p>beyond their batch counterparts: correctness and tools for reasoning about</p>
<p>time; and <em>data processing patterns</em>, looking at the conceptual approaches</p>
<p>taken with both batch and streaming systems when processing bounded and</p>
<p>unbounded data.</p>
<p>In this chapter, we’re now going to focus further on the data processing</p>
<p>patterns from Chapter 1, but in more detail, and within the context of concrete</p>
<p>examples. By the time we’re finished, we’ll have covered what I consider to</p>
<p>be the core set of principles and concepts required for robust out-of-order data</p>
<p>processing; these are the tools for reasoning about time that truly get you</p>
<p>beyond classic batch processing.</p>
<p>To give you a sense of what things look like in action, I use snippets of</p>
<p>Apache Beam code, coupled with time-lapse diagrams to provide a visual</p>
<p>representation of the concepts. Apache Beam is a unified programming model</p>
<p>and portability layer for batch and stream processing, with a set of concrete</p>
<p>SDKs in various languages (e.g., Java and Python). Pipelines written with</p>
<p>Apache Beam can then be portably run on any of the supported execution</p>
<p>engines (Apache Apex, Apache Flink, Apache Spark, Cloud Dataflow, etc.).</p>
<p>I use Apache Beam here for examples not because this is a Beam book (it’s</p>
<p>not), but because it most completely embodies the concepts described in this</p>
<p>book. Back when “Streaming 102” was originally written (back when it was</p>
<p>still the Dataflow Model from Google Cloud Dataflow and not the Beam</p>
<p>Model from Apache Beam), it was literally the only system in existence that</p>
<p>provided the amount of expressiveness necessary for all the examples we’ll</p>
<p>cover here. A year and a half later, I’m happy to say much has changed, and</p>
<p>most of the major systems out there have moved or are moving toward</p>
<p>supporting a model that looks a lot like the one described in this book. So rest</p>
<p>assured that the concepts we cover here, though informed through the Beam</p>
<p>lens, as it were, will apply equally across most other systems you’ll come</p>
<p>across.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="roadmap"><strong>Roadmap</strong></span><a href="#roadmap" class="header-anchor">#</a></h1><p>To help set the stage for this chapter, I want to lay out the five main concepts</p>
<p>that will underpin all of the discussions therein, and really, for most of the rest</p>
<p>of Part I. We’ve already covered two of them.</p>
<p>In Chapter 1, I first established the critical distinction between event time (the</p>
<p>time that events happen) and processing time (the time they are observed</p>
<p>during processing). This provides the foundation for one of the main theses</p>
<p>put forth in this book: if you care about both correctness and the context</p>
<p>within which events actually occurred, you must analyze data relative to their</p>
<p>inherent event times, not the processing time at which they are encountered</p>
<p>during the analysis itself.</p>
<p>I then introduced the concept of <em>windowing</em> (i.e., partitioning a dataset along</p>
<p>temporal boundaries), which is a common approach used to cope with the fact</p>
<p>that unbounded data sources technically might never end. Some simpler</p>
<p>examples of windowing strategies are <em>fixed</em> and <em>sliding</em> windows, but more</p>
<p>sophisticated types of windowing, such as <em>sessions</em> (in which the windows are</p>
<p>defined by features of the data themselves; for example, capturing a session of</p>
<p>activity per user followed by a gap of inactivity) also see broad usage.</p>
<p>In addition to these two concepts, we’re now going to look closely at three</p>
<p>more:</p>
<ul>
<li>Triggers</li>
</ul>
<p>A trigger is a mechanism for declaring when the output for a window</p>
<p>should be materialized relative to some external signal. Triggers provide</p>
<p>flexibility in choosing when outputs should be emitted. In some sense,</p>
<p>you can think of them as a flow control mechanism for dictating when</p>
<p>results should be materialized. Another way of looking at it is that triggers</p>
<p>are like the shutter-release on a camera, allowing you to declare when to</p>
<p>take a snapshots in time of the results being computed.</p>
<p>Triggers also make it possible to observe the output for a window multiple</p>
<p>times as it evolves. This in turn opens up the door to refining results over</p>
<p>time, which allows for providing speculative results as data arrive, as well</p>
<p>as dealing with changes in upstream data (revisions) over time or data that</p>
<p>arrive late (e.g., mobile scenarios, in which someone’s phone records</p>
<p>various actions and their event times while the person is offline and then</p>
<p>proceeds to upload those events for processing upon regaining</p>
<p>connectivity).</p>
<ul>
<li>Watermarks</li>
</ul>
<p>A watermark is a notion of input completeness with respect to event</p>
<p>times. A watermark with value of time <em>X</em> makes the statement: “all input</p>
<p>data with event times less than <em>X</em> have been observed.” As such,</p>
<p>watermarks act as a metric of progress when observing an unbounded data</p>
<p>source with no known end. We touch upon the basics of watermarks in</p>
<p>this chapter, and then Slava goes super deep on the subject in Chapter 3.</p>
<ul>
<li>Accumulation</li>
</ul>
<p>An accumulation mode specifies the relationship between multiple results</p>
<p>that are observed for the same window. Those results might be completely</p>
<p>disjointed; that is, representing independent deltas over time, or there</p>
<p>might be overlap between them. Different accumulation modes have</p>
<p>different semantics and costs associated with them and thus find</p>
<p>applicability across a variety of use cases.</p>
<p>Also, because I think it makes it easier to understand the relationships</p>
<p>between all of these concepts, we revisit the old and explore the new within</p>
<p>the structure of answering four questions, all of which I propose are critical to</p>
<p>every unbounded data processing problem:</p>
<ul>
<li><em>What</em> results are calculated? This question is answered by the types</li>
</ul>
<p>of transformations within the pipeline. This includes things like</p>
<p>computing sums, building histograms, training machine learning</p>
<p>models, and so on. It’s also essentially the question answered by</p>
<p>classic batch processing</p>
<ul>
<li><em>Where</em> in event time are results calculated? This question is answered</li>
</ul>
<p>by the use of event-time windowing within the pipeline. This</p>
<p>includes the common examples of windowing from Chapter 1 (fixed,</p>
<p>sliding, and sessions); use cases that seem to have no notion of</p>
<p>windowing (e.g., time-agnostic processing; classic batch processing</p>
<p>also generally falls into this category); and other, more complex</p>
<p>types of windowing, such as time-limited auctions. Also note that it</p>
<p>can include processing-time windowing, as well, if you assign</p>
<p>ingress times as event times for records as they arrive at the system.</p>
<ul>
<li><em>When</em> in processing time are results materialized? This question is</li>
</ul>
<p>answered by the use of triggers and (optionally) watermarks. There</p>
<p>are infinite variations on this theme, but the most common patterns</p>
<p>are those involving repeated updates (i.e., materialized view</p>
<p>semantics), those that utilize a watermark to provide a single output</p>
<p>per window only after the corresponding input is believed to be</p>
<p>complete (i.e., classic batch processing semantics applied on a per</p>
<p>window basis), or some combination of the two.</p>
<ul>
<li><em>How</em> do refinements of results relate? This question is answered by</li>
</ul>
<p>the type of accumulation used: discarding (in which results are all</p>
<p>independent and distinct), accumulating (in which later results build</p>
<p>upon prior ones), or accumulating and retracting (in which both the</p>
<p>accumulating value plus a retraction for the previously triggered</p>
<p>value(s) are emitted).</p>
<p>We look at each of these questions in much more detail throughout the rest of</p>
<p>the book. And, yes, I’m going to run this color scheme thing into the ground</p>
<p>in an attempt to make it abundantly clear which concepts relate to which</p>
<p>question in the <em>What</em>&#x2F;<em>Where</em>&#x2F;<em>When</em>&#x2F;<em>How</em> idiom. You’re welcome &lt;winky</p>
<p>smiley&#x2F;&gt;.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="batch-foundations-what-and-where"><strong>Batch Foundations: *What* and *Where*</strong></span><a href="#batch-foundations-what-and-where" class="header-anchor">#</a></h1><p>Okay, let’s get this party started. First stop: batch processing.</p>
<h3><span id="what-transformations">*<strong>What*: Transformations</strong></span><a href="#what-transformations" class="header-anchor">#</a></h3><p>The transformations applied in classic batch processing answer the question:</p>
<p>“<em>What</em> results are calculated?” Even though you are likely already familiar</p>
<p>with classic batch processing, we’re going to start there anyway because it’s</p>
<p>the foundation on top of which we add all of the other concepts.</p>
<p>In the rest of this chapter (and indeed, through much of the book), we look at</p>
<p>a single example: computing keyed integer sums over a simple dataset</p>
<p>consisting of nine values. Let’s imagine that we’ve written a team-based</p>
<p>mobile game and we want to build a pipeline that calculates team scores by</p>
<p>summing up the individual scores reported by users’ phones. If we were to</p>
<p>capture our nine example scores in a SQL table named “UserScores,” it might</p>
<p>look something like this:</p>
<hr>
<p>| Name | Team | Score | EventTime | ProcTime |</p>
<hr>
<p>| Julie | TeamX | 5 | 12:00:26 | 12:05:19 |</p>
<p>| Frank | TeamX | 9 | 12:01:26 | 12:08:19 |</p>
<p>| Ed | TeamX | 7 | 12:02:26 | 12:05:39 |</p>
<p>| Julie | TeamX | 8 | 12:03:06 | 12:07:06 |</p>
<p>| Amy | TeamX | 3 | 12:03:39 | 12:06:13 |</p>
<p>| Fred | TeamX | 4 | 12:04:19 | 12:06:39 |</p>
<p>| Naomi | TeamX | 3 | 12:06:39 | 12:07:19 |</p>
<p>| Becky | TeamX | 8 | 12:07:26 | 12:08:39 |</p>
<p>| Naomi | TeamX | 1 | 12:07:46 | 12:09:00 |</p>
<hr>
<p>Note that all the scores in this example are from users on the same team; this</p>
<p>is to keep the example simple, given that we have a limited number of</p>
<p>dimensions in our diagrams that follow. And because we’re grouping by</p>
<p>team, we really just care about the last three columns:</p>
<ul>
<li>Score</li>
</ul>
<p>The individual user score associated with this event</p>
<ul>
<li>EventTime</li>
</ul>
<p>The event time for the score; that is, the time at which the score occurred</p>
<ul>
<li>ProcTime</li>
</ul>
<p>The processing for the score; that is, the time at which the score was</p>
<p>observed by the pipeline</p>
<p>For each example pipeline, we’ll look at a time-lapse diagram that highlights</p>
<p>how the data evolves over time. Those diagrams plot our nine scores in the</p>
<p>two dimensions of time we care about: event time in the x-axis, and</p>
<p>processing time in the y-axis. Figure 2-1 illustrates what a static plot of the</p>
<p>input data looks like.</p>
<p><em>Figure 2-1. Nine input records, plotted in both event time and processing time</em></p>
<p>Subsequent time-lapse diagrams are either animations (Safari) or a sequence</p>
<p>of frames (print and all other digital formats), allowing you to see how the</p>
<p>data are processed over time (more on this shortly after we get to the first</p>
<p>time-lapse diagram).</p>
<p>Preceding each example is a short snippet of Apache Beam Java SDK</p>
<p>pseudocode to make the definition of the pipeline more concrete. It is</p>
<p>pseudocode in the sense that I sometime bend the rules to make the examples</p>
<p>clearer, elide details (like the use of concrete I&#x2F;O sources), or simplify names</p>
<p>(the trigger names in Beam Java 2.x and earlier are painfully verbose; I use</p>
<p>simpler names for clarity). Beyond minor things like those, it’s otherwise</p>
<p>real-world Beam code (and real code is available on GitHub for all examples</p>
<p>in this chapter).</p>
<p>If you’re already familiar with something like Spark or Flink, you should</p>
<p>have a relatively easy time understanding what the Beam code is doing. But</p>
<p>to give you a crash course in things, there are two basic primitives in Beam:</p>
<ul>
<li>PCollections</li>
</ul>
<p>These represent datasets (possibly massive ones) across which parallel</p>
<p>transformations can be performed (hence the “P” at the beginning of the</p>
<p>name).</p>
<ul>
<li>PTransforms</li>
</ul>
<p>These are applied to PCollections to create new PCollections.</p>
<p>PTransforms may perform element-wise transformations, they may</p>
<p>group&#x2F;aggregate multiple elements together, or they may be a composite</p>
<p>combination of other PTransforms, as depicted in Figure 2-2.</p>
<p><em>Figure 2-2. Types of transformations</em></p>
<p>For the purposes of our examples, we typically assume that we start out with</p>
<p>a pre-loaded PCollection&lt;KV&lt;Team, Integer&gt;&gt; named “input” (that is, a</p>
<p>PCollection composed of key&#x2F;value pairs of Teams and Integers, where</p>
<p>the Teams are just something like Strings representing team names, and the</p>
<p>Integers are scores from any individual on the corresponding team). In a</p>
<p>real-world pipeline, we would’ve acquired input by reading in a</p>
<p>PCollection<string> of raw data (e.g., log records) from an I&#x2F;O source and</string></p>
<p>then transforming it into a PCollection&lt;KV&lt;Team, Integer&gt;&gt; by parsing</p>
<p>the log records into appropriate key&#x2F;value pairs. For the sake of clarity in this</p>
<p>first example, I include pseudocode for all of those steps, but in subsequent</p>
<p>examples, I elide the I&#x2F;O and parsing.</p>
<p>Thus, for a pipeline that simply reads in data from an I&#x2F;O source, parses</p>
<p>team&#x2F;score pairs, and calculates per-team sums of scores, we’d have</p>
<p>something like that shown in Example 2-1.</p>
<p><em>Example 2-1. Summation pipeline</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;String&gt; raw = IO.read(...);</span><br><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; input = raw.apply(new ParseFn());</span><br><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals =</span><br><span class="line">input.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Key&#x2F;value data are read from an I&#x2F;O source, with a Team (e.g., String of the</p>
<p>team name) as the key and an Integer (e.g., individual team member scores)</p>
<p>as the value. The values for each key are then summed together to generate</p>
<p>per-key sums (e.g., total team score) in the output collection.</p>
<p>For all the examples to come, after seeing a code snippet describing the</p>
<p>pipeline that we’re analyzing, we’ll then look at a time-lapse diagram</p>
<p>showing the execution of that pipeline over our concrete dataset for a single</p>
<p>key. In a real pipeline, you can imagine that similar operations would be</p>
<p>happening in parallel across multiple machines, but for the sake of our</p>
<p>examples, it will be clearer to keep things simple.</p>
<p>As noted previously, Safari editions present the complete execution as an</p>
<p>animated movie, whereas print and all other digital formats use a static</p>
<p>sequence of key frames that provide a sense of how the pipeline progresses</p>
<p>over time. In both cases, we also provide a URL to a fully animated version</p>
<p>on <em><a target="_blank" rel="noopener" href="http://www.streamingbook.net/">www.streamingbook.net</a></em>.</p>
<p>Each diagram plots the inputs and outputs across two dimensions: event time</p>
<p>(on the x-axis) and processing time (on the y-axis). Thus, real time as</p>
<p>observed by the pipeline progresses from bottom to top, as indicated by the</p>
<p>thick horizontal black line that ascends in the processing-time axis as time</p>
<p>progresses. Inputs are circles, with the number inside the circle representing</p>
<p>the value of that specific record. They start out light gray, and darken as the</p>
<p>pipeline observes them.</p>
<p>As the pipeline observes values, it accumulates them in its intermediate state</p>
<p>and eventually materializes the aggregate results as output. State and output</p>
<p>are represented by rectangles (gray for state, blue for output), with the</p>
<p>aggregate value near the top, and with the area covered by the rectangle</p>
<p>representing the portions of event time and processing time accumulated into</p>
<p>the result. For the pipeline in Example 2-1, it would look something like that</p>
<p>shown in Figure 2-3 when executed on a classic batch engine.</p>
<p><em>Figure 2-3. Classic batch processing</em></p>
<p>Because this is a batch pipeline, it accumulates state until it’s seen all of the</p>
<p>inputs (represented by the dashed green line at the top), at which point it</p>
<p>produces its single output of 48. In this example, we’re calculating a sum over</p>
<p>all of event time because we haven’t applied any specific windowing</p>
<p>transformations; hence the rectangles for state and output cover the entirety of</p>
<p>the x-axis. If we want to process an unbounded data source, however, classic</p>
<p>batch processing won’t be sufficient; we can’t wait for the input to end,</p>
<p>because it effectively never will. One of the concepts we want is windowing,</p>
<p>which we introduced in Chapter 1. Thus, within the context of our second</p>
<p>question—“<em>Where</em> in event time are results calculated?”—we’ll now briefly</p>
<p>revisit windowing.</p>
<h3><span id="where-windowing">*<strong>Where*: Windowing</strong></span><a href="#where-windowing" class="header-anchor">#</a></h3><p>As discussed in Chapter 1, windowing is the process of slicing up a data</p>
<p>source along temporal boundaries. Common windowing strategies include</p>
<p>fixed windows, sliding windows, and sessions windows, as demonstrated in</p>
<p>Figure 2-4.</p>
<p><em>Figure 2-4. Example windowing strategies. Each example is shown for three different keys, highlighting</em></p>
<p><em>the difference between aligned windows (which apply across all the data) and unaligned windows</em></p>
<p><em>(which apply across a subset of the data).</em></p>
<p>To get a better sense of what windowing looks like in practice, let’s take our</p>
<p>integer summation pipeline and window it into fixed, two-minute windows.</p>
<p>With Beam, the change is a simple addition of a Window.into transform,</p>
<p>which you can see highlighted in Example 2-2.</p>
<p><em>Example 2-2. Windowed summation code</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES)))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Recall that Beam provides a unified model that works in both batch and</p>
<p>streaming because semantically batch is really just a subset of streaming. As</p>
<p>such, let’s first execute this pipeline on a batch engine; the mechanics are</p>
<p>more straightforward, and it will give us something to directly compare</p>
<p>against when we switch to a streaming engine. Figure 2-5 presents the result.</p>
<p><em>Figure 2-5. Windowed summation on a batch engine</em></p>
<p>As before, inputs are accumulated in state until they are entirely consumed,</p>
<p>after which output is produced. In this case, however, instead of one output,</p>
<p>we get four: a single output, for each of the four relevant two-minute event</p>
<p>time windows.</p>
<p>At this point we’ve revisited the two main concepts that I introduced in</p>
<p>Chapter 1: the relationship between the event-time and processing-time</p>
<p>domains, and windowing. If we want to go any further, we’ll need to start</p>
<p>adding the new concepts mentioned at the beginning of this section: triggers,</p>
<p>watermarks, and accumulation.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="going-streaming-when-and-how"><strong>Going Streaming: <em>When</em> and <em>How</em></strong></span><a href="#going-streaming-when-and-how" class="header-anchor">#</a></h1><p>We just observed the execution of a windowed pipeline on a batch engine.</p>
<p>But, ideally, we’d like to have lower latency for our results, and we’d also</p>
<p>like to natively handle unbounded data sources. Switching to a streaming</p>
<p>engine is a step in the right direction, but our previous strategy of waiting</p>
<p>until our input has been consumed in its entirety to generate output is no</p>
<p>longer feasible. Enter triggers and watermarks.</p>
<h3><span id="when-the-wonderful-thing-about-triggers-is-triggers"><strong><em>When</em>: The Wonderful Thing About Triggers Is Triggers</strong></span><a href="#when-the-wonderful-thing-about-triggers-is-triggers" class="header-anchor">#</a></h3><h3><span id="are-wonderful-things"><strong>Are Wonderful Things!</strong></span><a href="#are-wonderful-things" class="header-anchor">#</a></h3><p>Triggers provide the answer to the question: “<em>When</em> in processing time are</p>
<p>results materialized?” Triggers declare when output for a window should</p>
<p>happen in processing time (though the triggers themselves might make those</p>
<p>decisions based on things that happen in other time domains, such as</p>
<p>watermarks progressing in the event-time domain, as we’ll see in a few</p>
<p>moments). Each specific output for a window is referred to as a <em>pane</em> of the</p>
<p>window.</p>
<p>Though it’s possible to imagine quite a breadth of possible triggering</p>
<p>semantics, conceptually there are only two generally useful types of triggers,</p>
<p>and practical applications almost always boil down using either one or a</p>
<p>combination of both:</p>
<ul>
<li>Repeated update triggers</li>
</ul>
<p>These periodically generate updated panes for a window as its contents</p>
<p>evolve. These updates can be materialized with every new record, or they</p>
<p>can happen after some processing-time delay, such as once a minute. The</p>
<p>choice of period for a repeated update trigger is primarily an exercise in</p>
<p>balancing latency and cost.</p>
<ul>
<li>Completeness triggers</li>
</ul>
<p>These materialize a pane for a window only after the input for that</p>
<p>window is believed to be complete to some threshold. This type of trigger</p>
<p>is most analogous to what we’re familiar with in batch processing: only</p>
<p>after the input is complete do we provide a result. The difference in the</p>
<p>trigger-based approach is that the notion of completeness is scoped to the</p>
<p>context of a single window, rather than always being bound to the</p>
<p>completeness of the entire input.</p>
<p>Repeated update triggers are the most common type of trigger encountered in</p>
<p>streaming systems. They are simple to implement and simple to understand,</p>
<p>and they provide useful semantics for a specific type of use case: repeated</p>
<p>(and eventually consistent) updates to a materialized dataset, analogous to the</p>
<p>semantics you get with materialized views in the database world.</p>
<p>Completeness triggers are less frequently encountered, but provide streaming</p>
<p>semantics that more closely align with those from the classic batch processing</p>
<p>world. They also provide tools for reasoning about things like missing data</p>
<p>and late data, both of which we discuss shortly (and in the next chapter) as we</p>
<p>explore the underlying primitive that drives completeness triggers:</p>
<p>watermarks.</p>
<p>But first, let’s start simple and look at some basic repeated update triggers in</p>
<p>action. To make the notion of triggers a bit more concrete, let’s go ahead and</p>
<p>add the most straightforward type of trigger to our example pipeline: a trigger</p>
<p>that fires with every new record, as shown in Example 2-3.</p>
<p><em>Example 2-3. Triggering repeatedly with every record</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(AfterCount(1))));</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p>If we were to run this new pipeline on a streaming engine, the results would</p>
<p>look something like that shown in Figure 2-6.</p>
<p><em>Figure 2-6. Per-record triggering on a streaming engine</em></p>
<p>You can see how we now get multiple outputs (panes) for each window: once</p>
<p>per corresponding input. This sort of triggering pattern works well when the</p>
<p>output stream is being written to some sort of table that you can simply poll</p>
<p>for results. Any time you look in the table, you’ll see the most up-to-date</p>
<p>value for a given window, and those values will converge toward correctness</p>
<p>over time.</p>
<p>One downside of per-record triggering is that it’s quite chatty. When</p>
<p>processing large-scale data, aggregations like summation provide a nice</p>
<p>opportunity to reduce the cardinality of the stream without losing information.</p>
<p>This is particularly noticeable for cases in which you have high-volume keys;</p>
<p>for our example, massive teams with lots of active players. Imagine a</p>
<p>massively multiplayer game in which players are split into one of two</p>
<p>factions, and you want to tally stats on a per-faction basis. It’s probably</p>
<p>unnecessary to update your tallies with every new input record for every</p>
<p>player in a given faction. Instead, you might be happy updating them after</p>
<p>some processing-time delay, say every second, or every minute. The nice side</p>
<p>effect of using processing-time delays is that it has an equalizing effect across</p>
<p>high-volume keys or windows: the resulting stream ends up being more</p>
<p>uniform cardinality-wise.</p>
<p>There are two different approaches to processing-time delays in triggers:</p>
<p><em>aligned delays</em> (where the delay slices up processing time into fixed regions</p>
<p>that align across keys and windows) and <em>unaligned delays</em> (where the delay is</p>
<p>relative to the data observed within a given window). A pipeline with</p>
<p>unaligned delays might look like Example 2-4, the results of which are shown</p>
<p>in Figure 2-7.</p>
<p><em>Example 2-4. Triggering on aligned two-minute processing-time boundaries</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(AlignedDelay(TWO_MINUTES)))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-7. Two-minute aligned delay triggers (i.e., microbatching)</em></p>
<p>This sort of aligned delay trigger is effectively what you get from a</p>
<p>microbatch streaming system like Spark Streaming. The nice thing about it is</p>
<p>predictability; you get regular updates across all modified windows at the</p>
<p>same time. That’s also the downside: all updates happen at once, which</p>
<p>results in bursty workloads that often require greater peak provisioning to</p>
<p>properly handle the load. The alternative is to use an unaligned delay. That</p>
<p>would look something Example 2-5 in Beam. Figure 2-8 presents the results.</p>
<p><em>Example 2-5. Triggering on unaligned two-minute processing-time</em></p>
<p><em>boundaries</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(UnalignedDelay(TWO_MINUTES))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-8. Two-minute unaligned delay triggers</em></p>
<p>Contrasting the unaligned delays in Figure 2-8 to the aligned delays in</p>
<p>Figure 2-6, it’s easy to see how the unaligned delays spread the load out more</p>
<p>evenly across time. The actual latencies involved for any given window differ</p>
<p>between the two, sometimes more and sometimes less, but in the end the</p>
<p>average latency will remain essentially the same. From that perspective,</p>
<p>unaligned delays are typically the better choice for large-scale processing</p>
<p>because they result in a more even load distribution over time.</p>
<p>Repeated update triggers are great for use cases in which we simply want</p>
<p>periodic updates to our results over time and are fine with those updates</p>
<p>converging toward correctness with no clear indication of when correctness is</p>
<p>achieved. However, as we discussed in Chapter 1, the vagaries of distributed</p>
<p>systems often lead to a varying level of skew between the time an event</p>
<p>happens and the time it’s actually observed by your pipeline, which means it</p>
<p>can be difficult to reason about when your output presents an accurate and</p>
<p>complete view of your input data. For cases in which input completeness</p>
<p>matters, it’s important to have some way of reasoning about completeness</p>
<p>rather than blindly trusting the results calculated by whichever subset of data</p>
<p>happen to have found their way to your pipeline. Enter watermarks.</p>
</details>





<details><summary>点击 原文</summary><h3><span id="when-watermarks">*<strong>When*: Watermarks</strong></span><a href="#when-watermarks" class="header-anchor">#</a></h3><p>Watermarks are a supporting aspect of the answer to the question: “<em>When</em> in</p>
<p>processing time are results materialized?” Watermarks are temporal notions</p>
<p>of input completeness in the event-time domain. Worded differently, they are</p>
<p>the way the system measures progress and completeness relative to the event</p>
<p>times of the records being processed in a stream of events (either bounded or</p>
<p>unbounded, though their usefulness is more apparent in the unbounded case).</p>
<p>Recall this diagram from Chapter 1, slightly modified in Figure 2-9, in which</p>
<p>I described the skew between event time and processing time as an ever</p>
<p>changing function of time for most real-world distributed data processing</p>
<p>systems.</p>
<p><em>Figure 2-9. Event-time progress, skew, and watermarks</em></p>
<p>That meandering red line that I claimed represented reality is essentially the</p>
<p>watermark; it captures the progress of event-time completeness as processing</p>
<p>time progresses. Conceptually, you can think of the watermark as a function,</p>
<p><em>F</em>(<em>P</em>) → <em>E</em>, which takes a point in processing time and returns a point in event</p>
<p>time. That point in event time, <em>E</em>, is the point up to which the system believes</p>
<p>all inputs with event times less than <em>E</em> have been observed. In other words,</p>
<p>it’s an assertion that no more data with event times less than <em>E</em> will ever be</p>
<p>seen again. Depending upon the type of watermark, perfect or heuristic, that</p>
<p>assertion can be a strict guarantee or an educated guess, respectively:</p>
<ul>
<li>Perfect watermarks</li>
</ul>
<p>For the case in which we have perfect knowledge of all of the input data,</p>
<p>it’s possible to construct a perfect watermark. In such a case, there is no</p>
<p>such thing as late data; all data are early or on time.</p>
<ul>
<li>Heuristic watermarks</li>
</ul>
<p>For many distributed input sources, perfect knowledge of the input data is</p>
<p>impractical, in which case the next best option is to provide a heuristic</p>
<p>watermark. Heuristic watermarks use whatever information is available</p>
<p>about the inputs (partitions, ordering within partitions if any, growth rates</p>
<p>of files, etc.) to provide an estimate of progress that is as accurate as</p>
<p>possible. In many cases, such watermarks can be remarkably accurate in</p>
<p>their predictions. Even so, the use of a heuristic watermark means that it</p>
<p>might sometimes be wrong, which will lead to late data. We show you</p>
<p>about ways to deal with late data soon.</p>
<p>Because they provide a notion of completeness relative to our inputs,</p>
<p>watermarks form the foundation for the second type of trigger mentioned</p>
<p>previously: <em>completeness triggers</em>. Watermarks themselves are a fascinating</p>
<p>and complex topic, as you’ll see when you get to Slava’s watermarks deep</p>
<p>dive in Chapter 3. But for now, let’s look at them in action by updating our</p>
<p>example pipeline to utilize a completeness trigger built upon watermarks, as</p>
<p>demonstrated in Example 2-6.</p>
<p><em>Example 2-6. Watermark completeness trigger</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(AfterWatermark()))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Now, an interesting quality of watermarks is that they are a class of functions,</p>
<p>meaning there are multiple different functions <em>F</em>(<em>P</em>) → <em>E</em> that satisfy the</p>
<p>properties of a watermark, to varying degrees of success. As I noted earlier,</p>
<p>for situations in which you have perfect knowledge of your input data, it</p>
<p>might be possible to build a perfect watermark, which is the ideal situation.</p>
<p>But for cases in which you lack perfect knowledge of the inputs or for which</p>
<p>it’s simply too computationally expensive to calculate the perfect watermark,</p>
<p>you might instead choose to utilize a heuristic for defining your watermark.</p>
<p>The point I want to make here is that the given watermark algorithm in use is</p>
<p>independent from the pipeline itself. We’re not going to discuss in detail what</p>
<p>it means to implement a watermark here (Slava does that in Chapter 3). For</p>
<p>now, to help drive home this idea that a given input set can have different</p>
<p>watermarks applied to it, let’s take a look at our pipeline in Example 2-6</p>
<p>when executed on the same dataset but using two distinct watermark</p>
<p>implementations (Figure 2-10): on the left, a perfect watermark; on the right,</p>
<p>a heuristic watermark.</p>
<p>In both cases, windows are materialized as the watermark passes the end of</p>
<p>the window. The perfect watermark, as you might expect, perfectly captures</p>
<p>the event-time completeness of the pipeline as time progresses. In contrast,</p>
<p>the specific algorithm used for the heuristic watermark on the right fails to</p>
<p>take the value of 9 into account, which drastically changes the shape of the</p>
<p>materialized outputs, both in terms of output latency and correctness (as seen</p>
<p>by the incorrect answer of 5 that’s provided for the [12:00, 12:02) window).</p>
<p>The big difference between the watermark triggers from Figure 2-9 and the</p>
<p>repeated update triggers we saw in Figures 2-5 through 2-7 is that the</p>
<p><em>watermarks give us a way to reason about the completeness of our input</em>.</p>
<p>Until the system materializes an output for a given window, we know that the</p>
<p>system does not yet believe the inputs to be complete. This is especially</p>
<p>important for use cases in which you want to reason about a <em>lack of data</em> in</p>
<p>the input, or <em>missing data</em>.</p>
<p><em>Figure 2-10. Windowed summation on a streaming engine with perfect (left) and heuristic (right)</em></p>
<p><em>watermarks</em></p>
<p>A great example of a missing-data use case is outer joins. Without a notion of</p>
<p>completeness like watermarks, how do you know when to give up and emit a</p>
<p>partial join rather than continue to wait for that join to complete? You don’t.</p>
<p>And basing that decision on a processing-time delay, which is the common</p>
<p>approach in streaming systems that lack true watermark support, is not a safe</p>
<p>way to go, because of the variable nature of event-time skew we spoke about</p>
<p>in Chapter 1: as long as skew remains smaller than the chosen processing</p>
<p>time delay, your missing-data results will be correct, but any time skew grows</p>
<p>beyond that delay, they will suddenly become <em>in</em>correct. From this</p>
<p>perspective, event-time watermarks are a critical piece of the puzzle for many</p>
<p>real-world streaming use cases which must reason about a lack of data in the</p>
<p>input, such as outer joins, anomaly detection, and so on.</p>
<p>Now, with that said, these watermark examples also highlight two</p>
<p><em>shortcomings</em> of watermarks (and any other notion of completeness),</p>
<p>specifically that they can be one of the following:</p>
<ul>
<li>Too slow</li>
</ul>
<p>When a watermark of any type is correctly delayed due to known</p>
<p>unprocessed data (e.g., slowly growing input logs due to network</p>
<p>bandwidth constraints), that translates directly into delays in output if</p>
<p>advancement of the watermark is the only thing you depend on for</p>
<p>stimulating results.</p>
<p>This is most obvious in the left diagram of Figure 2-10, for which the late</p>
<p>arriving 9 holds back the watermark for all the subsequent windows, even</p>
<p>though the input data for those windows become complete earlier. This is</p>
<p>particularly apparent for the second window, [12:02, 12:04), for which it</p>
<p>takes nearly seven minutes from the time the first value in the window</p>
<p>occurs until we see any results for the window whatsoever. The heuristic</p>
<p>watermark in this example doesn’t suffer the same issue quite so badly</p>
<p>(five minutes until output), but don’t take that to mean heuristic</p>
<p>watermarks never suffer from watermark lag; that’s really just a</p>
<p>consequence of the record I chose to omit from the heuristic watermark in</p>
<p>this specific example.</p>
<p>The important point here is the following: Although watermarks provide a</p>
<p>very useful notion of completeness, depending upon completeness for</p>
<p>producing output is often not ideal from a latency perspective. Imagine a</p>
<p>dashboard that contains valuable metrics, windowed by hour or day. It’s</p>
<p>unlikely you’d want to wait a full hour or day to begin seeing results for</p>
<p>the current window; that’s one of the pain points of using classic batch</p>
<p>systems to power such systems. Instead, it would be much nicer to see the</p>
<p>results for those windows refine over time as the inputs evolve and</p>
<p>eventually become complete.</p>
<ul>
<li>Too fast</li>
</ul>
<p>When a heuristic watermark is incorrectly advanced earlier than it should</p>
<p>be, it’s possible for data with event times before the watermark to arrive</p>
<p>some time later, creating late data. This is what happened in the example</p>
<p>on the right: the watermark advanced past the end of the first window</p>
<p>before all the input data for that window had been observed, resulting in</p>
<p>an incorrect output value of 5 instead of 14. This shortcoming is strictly a</p>
<p>problem with heuristic watermarks; their heuristic nature implies they will</p>
<p>sometimes be wrong. As a result, relying on them alone for determining</p>
<p>when to materialize output is insufficient if you care about correctness.</p>
<p>In Chapter 1, I made some rather emphatic statements about notions of</p>
<p>completeness being insufficient for most use cases requiring robust out-of-</p>
<p>order processing of unbounded data streams. These two shortcomings—</p>
<p>watermarks being too slow or too fast—are the foundations for those</p>
<p>arguments. You simply cannot get both low latency and correctness out of a</p>
<p>system that relies solely on notions of completeness. So, for cases for which</p>
<p>you do want the best of both worlds, what’s a person to do? Well, if repeated</p>
<p>update triggers provide low-latency updates but no way to reason about</p>
<p>completeness, and watermarks provide a notion of completeness but variable</p>
<p>and possible high latency, why not combine their powers together?</p>
</details>





<details><summary>点击 原文</summary><h3><span id="when-earlyx2fon-timex2flate-triggers-ftw"><strong><em>When</em>: Early&#x2F;On-Time&#x2F;Late Triggers FTW!</strong></span><a href="#when-earlyx2fon-timex2flate-triggers-ftw" class="header-anchor">#</a></h3><p>We’ve now looked at the two main types of triggers: repeated update triggers</p>
<p>and completeness&#x2F;watermark triggers. In many case, neither of them alone is</p>
<p>sufficient, but the combination of them together is. Beam recognizes this fact</p>
<p>by providing an extension of the standard watermark trigger that also supports</p>
<p>repeated update triggering on either side of the watermark. This is known as</p>
<p>the early&#x2F;on-time&#x2F;late trigger because it partitions the panes that are</p>
<p>materialized by the compound trigger into three categories:</p>
<ul>
<li>Zero or more <em>early panes</em>, which are the result of a repeated update</li>
</ul>
<p>trigger that periodically fires up until the watermark passes the end of</p>
<p>the window. The panes generated by these firings contain speculative</p>
<p>results, but allow us to observe the evolution of the window over</p>
<p>time as new input data arrive. This compensates for the shortcoming</p>
<p>of watermarks sometimes being <em>too slow</em>.</p>
<ul>
<li>A single <em>on-time pane</em>, which is the result of the</li>
</ul>
<p>completeness&#x2F;watermark trigger firing after the watermark passes the</p>
<p>end of the window. This firing is special because it provides an</p>
<p>assertion that the system now believes the input for this window to</p>
<p>be complete. This means that it is now safe to reason about <em>missing</em></p>
<p><em>data</em>; for example, to emit a partial join when performing an outer</p>
<p>join.</p>
<ul>
<li>Zero or more <em>late panes</em>, which are the result of another (possibly</li>
</ul>
<p>different) repeated update trigger that periodically fires any time late</p>
<p>data arrive after the watermark has passed the end of the window. In</p>
<p>the case of a perfect watermark, there will always be zero late panes.</p>
<p>But in the case of a heuristic watermark, any data the watermark</p>
<p>failed to properly account for will result in a late firing. This</p>
<p>compensates for the shortcoming of watermarks being <em>too fast</em>.</p>
<p>Let’s see how this looks in action. We’ll update our pipeline to use a periodic</p>
<p>processing-time trigger with an aligned delay of one minute for the early</p>
<p>firings, and a per-record trigger for the late firings. That way, the early firings</p>
<p>will give us some amount of batching for high-volume windows (thanks to</p>
<p>the fact that the trigger will fire only once per minute, regardless of the</p>
<p>throughput into the window), but we won’t introduce unnecessary latency for</p>
<p>the late firings, which are hopefully somewhat rare if we’re using a</p>
<p>reasonably accurate heuristic watermark. In Beam, that looks Example 2-7</p>
<p>(Figure 2-11 shows the results).</p>
<p><em>Example 2-7. Early, on-time, and late firings via the early&#x2F;on-time&#x2F;late API</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(AfterWatermark()</code></p>
<p><code>.withEarlyFirings(AlignedDelay(ONE_MINUTE))</code></p>
<p><code>.withLateFirings(AfterCount(1))))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-11. Windowed summation on a streaming engine with early, on-time, and late firings</em></p>
<p>This version has two clear improvements over Figure 2-9:</p>
<ul>
<li>For the “watermarks too slow” case in the second window, [12:02,</li>
</ul>
<p>12:04): we now provide periodic early updates once per minute. The</p>
<p>difference is most stark in the perfect watermark case, for which</p>
<p>time-to-first-output is reduced from almost seven minutes down to</p>
<p>three and a half; but it’s also clearly improved in the heuristic case,</p>
<p>as well. Both versions now provide steady refinements over time</p>
<p>(panes with values 7, 10, then 18), with relatively minimal latency</p>
<p>between the input becoming complete and materialization of the final</p>
<p>output pane for the window.</p>
<ul>
<li>For the “heuristic watermarks too fast” case in the first window,</li>
</ul>
<p>[12:00, 12:02): when the value of 9 shows up late, we immediately</p>
<p>incorporate it into a new, corrected pane with value of 14.</p>
<p>One interesting side effect of these new triggers is that they effectively</p>
<p>normalize the output pattern between the perfect and heuristic watermark</p>
<p>versions. Whereas the two versions in Figure 2-10 were starkly different, the</p>
<p>two versions here look quite similar. They also look much more similar to the</p>
<p>various repeated update version from Figures 2-6 through 2-8, with one</p>
<p>important difference: thanks to the use of the watermark trigger, we can also</p>
<p>reason about input completeness in the results we generate with the early&#x2F;on</p>
<p>time&#x2F;late trigger. This allows us to better handle use cases that care about</p>
<p><em>missing data</em>, like outer joins, anomaly detection, and so on.</p>
<p>The biggest remaining difference between the perfect and heuristic early&#x2F;on</p>
<p>time&#x2F;late versions at this point is window lifetime bounds. In the perfect</p>
<p>watermark case, we know we’ll never see any more data for a window after</p>
<p>the watermark has passed the end of it, hence we can drop all of our state for</p>
<p>the window at that time. In the heuristic watermark case, we still need to hold</p>
<p>on to the state for a window for some amount of time to account for late data.</p>
<p>But as of yet, our system doesn’t have any good way of knowing just how</p>
<p>long state needs to be kept around for each window. That’s where <em>allowed</em></p>
<p><em>lateness</em> comes in.</p>
</details>



<details><summary>点击 原文</summary><h3><span id="when-allowed-lateness-ie-garbage-collection">*<strong>When*: Allowed Lateness (i.e., Garbage Collection)</strong></span><a href="#when-allowed-lateness-ie-garbage-collection" class="header-anchor">#</a></h3><p>Before moving on to our last question (“<em>How</em> do refinements of results</p>
<p>relate?”), I’d like to touch on a practical necessity within long-lived, out-of</p>
<p>order stream processing systems: garbage collection. In the heuristic</p>
<p>watermarks example in Figure 2-11, the persistent state for each window</p>
<p>lingers around for the entire lifetime of the example; this is necessary to allow</p>
<p>us to appropriately deal with late data when&#x2F;if they arrive. But while it would</p>
<p>be great to be able to keep around all of our persistent state until the end of</p>
<p>time, in reality, when dealing with an unbounded data source, it’s often not</p>
<p>practical to keep state (including metadata) for a given window indefinitely;</p>
<p>we’ll eventually run out of disk space (or at the very least tire of paying for it,</p>
<p>as the value for older data diminishes over time).</p>
<p>As a result, any real-world out-of-order processing system needs to provide</p>
<p>some way to bound the lifetimes of the windows it’s processing. A clean and</p>
<p>concise way of doing this is by defining a horizon on the allowed lateness</p>
<p>within the system; that is, placing a bound on how late any given <em>record</em> may</p>
<p>be (relative to the watermark) for the system to bother processing it; any data</p>
<p>that arrives after this horizon are simply dropped. After you’ve bounded how</p>
<p>late individual data may be, you’ve also established precisely how long the</p>
<p>state for windows must be kept around: until the watermark exceeds the</p>
<p>lateness horizon for the end of the window. But in addition, you’ve also given</p>
<p>the system the liberty to immediately drop any data later than the horizon as</p>
<p>soon as they’re observed, which means the system doesn’t waste resources</p>
<p>processing data that no one cares about.</p>
<p><strong>MEASURING LATENESS</strong></p>
<p>It might seem a little odd to be specifying a horizon for handling late data</p>
<p>using the very metric that resulted in the late data in the first place (i.e., the</p>
<p>heuristic watermark). And in some sense it is. But of the options available,</p>
<p>it’s arguably the best. The only other practical option would be to specify</p>
<p>the horizon in processing time (e.g., keep windows around for 10 minutes</p>
<p>of processing time after the watermark passes the end of the window), but</p>
<p>using processing time would leave the garbage collection policy</p>
<p>vulnerable to issues within the pipeline itself (e.g., workers crashing,</p>
<p>causing the pipeline to stall for a few minutes), which could lead to</p>
<p>windows that didn’t actually have a chance to handle late data that they</p>
<p>otherwise should have. By specifying the horizon in the event-time</p>
<p>domain, garbage collection is directly tied to the actual progress of the</p>
<p>pipeline, which decreases the likelihood that a window will miss its</p>
<p>opportunity to handle late data appropriately.</p>
<p>Note however, that not all watermarks are created equal. When we speak</p>
<p>of watermarks in this book, we generally refer to <em>low</em> watermarks, which</p>
<p>pessimistically attempt to capture the event time of the <em>oldest</em> unprocessed</p>
<p>record the system is aware of. The nice thing about dealing with lateness</p>
<p>via low watermarks is that they are resilient to changes in event-time</p>
<p>skew; no matter how large the skew in a pipeline may grow, the low</p>
<p>watermark will always track the oldest outstanding event known to the</p>
<p>system, providing the best guarantee of correctness possible.</p>
<p>In contrast, some systems may use the term “watermark” to mean other</p>
<p>things. For example, watermarks in Spark Structured Streaming are <em>high</em></p>
<p>watermarks, which optimistically track the event time of the <em>newest</em> record</p>
<p>the system is aware of. When dealing with lateness, the system is free to</p>
<p>garbage collect any window older than the high watermark adjusted by</p>
<p>some user-specified lateness threshold. In other words, the system allows</p>
<p>you to specify the maximum amount of event-time skew you expect to see</p>
<p>in your pipeline, and then throws away any data outside of that skew</p>
<p>window. This can work well if skew within your pipeline remains within</p>
<p>some constant delta, but is more prone to incorrectly discarding data than</p>
<p>low watermarking schemes.</p>
<p>Because the interaction between allowed lateness and the watermark is a little</p>
<p>subtle, it’s worth looking at an example. Let’s take the heuristic watermark</p>
<p>pipeline from Example 2-7&#x2F;Figure 2-11 and add in Example 2-8 a lateness</p>
<p>horizon of one minute (note that this particular horizon has been chosen</p>
<p>strictly because it fits nicely into the diagram; for real-world use cases, a</p>
<p>larger horizon would likely be much more practical):</p>
<p><em>Example 2-8. Early&#x2F;on-time&#x2F;late firings with allowed lateness</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AfterCount(1)))</span><br><span class="line">.withAllowedLateness(ONE_MINUTE))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>The execution of this pipeline would look something like Figure 2-12, in</p>
<p>which I’ve added the following features to highlight the effects of allowed</p>
<p>lateness:</p>
<ul>
<li>The thick black line denoting the current position in processing time</li>
</ul>
<p>is now annotated with ticks indicating the lateness horizon (in event</p>
<p>time) for all active windows.</p>
<ul>
<li>When the watermark passes the lateness horizon for a window, that</li>
</ul>
<p>window is closed, which means that all state for the window is</p>
<p>discarded. I leave around a dotted rectangle showing the extent of</p>
<p>time (in both domains) that the window covered when it was closed,</p>
<p>with a little tail extending to the right to denote the lateness horizon</p>
<p>for the window (for contrasting against the watermark).</p>
<ul>
<li>For this diagram only, I’ve added an additional late datum for the</li>
</ul>
<p>first window with value 6. The 6 is late, but still within the allowed</p>
<p>lateness horizon and thus is incorporated into an updated result with</p>
<p>value 11. The 9, however, arrives beyond the lateness horizon, so it</p>
<p>is simply dropped.</p>
<p><em>Figure 2-12. Allowed lateness with early&#x2F;on-time&#x2F;late firings</em></p>
<p>Two final side notes about lateness horizons:</p>
<ul>
<li>To be absolutely clear, if you happen to be consuming data from</li>
</ul>
<p>sources for which perfect watermarks are available, there’s no need</p>
<p>to deal with late data, and an allowed lateness horizon of zero</p>
<p>seconds will be optimal. This is what we saw in the perfect</p>
<p>watermark portion of Figure 2-10.</p>
<ul>
<li>One noteworthy exception to the rule of needing to specify lateness</li>
</ul>
<p>horizons, even when heuristic watermarks are in use, would be</p>
<p>something like computing global aggregates over all time for a</p>
<p>tractably finite number of keys (e.g., computing the total number of</p>
<p>visits to your site over all time, grouped by web browser family). In</p>
<p>this case, the number of active windows in the system is bounded by</p>
<p>the limited keyspace in use. As long as the number of keys remains</p>
<p>manageably low, there’s no need to worry about limiting the lifetime</p>
<p>of windows via allowed lateness.</p>
<p>Practicality sated, let’s move on to our fourth and final question.</p>
</details>







<details><summary>点击 原文</summary><h1><span id="how-accumulation">*<strong>How*: Accumulation</strong></span><a href="#how-accumulation" class="header-anchor">#</a></h1><p>When triggers are used to produce multiple panes for a single window over</p>
<p>time, we find ourselves confronted with the last question: “<em>How</em> do</p>
<p>refinements of results relate?” In the examples we’ve seen so far, each</p>
<p>successive pane is built upon the one immediately preceding it. However,</p>
<p>there are actually three different modes of accumulation:</p>
<ul>
<li>Discarding</li>
</ul>
<p>Every time a pane is materialized, any stored state is discarded. This</p>
<p>means that each successive pane is independent from any that came</p>
<p>before. Discarding mode is useful when the downstream consumer is</p>
<p>performing some sort of accumulation itself; for example, when sending</p>
<p>integers into a system that expects to receive deltas that it will sum</p>
<p>together to produce a final count.</p>
<ul>
<li>Accumulating</li>
</ul>
<p>As in Figures 2-6 through 2-11, every time a pane is materialized, any</p>
<p>stored state is retained, and future inputs are accumulated into the existing</p>
<p>state. This means that each successive pane builds upon the previous</p>
<p>panes. Accumulating mode is useful when later results can simply</p>
<p>overwrite previous results, such as when storing output in a key&#x2F;value</p>
<p>store like HBase or Bigtable.</p>
<ul>
<li>Accumulating and retracting</li>
</ul>
<p>This is like accumulating mode, but when producing a new pane, it also</p>
<p>produces independent retractions for the previous pane(s). Retractions</p>
<p>(combined with the new accumulated result) are essentially an explicit</p>
<p>way of saying “I previously told you the result was <em>X</em>, but I was wrong.</p>
<p>Get rid of the <em>X</em> I told you last time, and replace it with <em>Y</em>.” There are two</p>
<p>cases for which retractions are particularly helpful:</p>
<ul>
<li>When consumers downstream are <em>regrouping data by a different</em></li>
</ul>
<p><em>dimension</em>, it’s entirely possible the new value may end up keyed</p>
<p>differently from the previous value and thus end up in a different</p>
<p>group. In that case, the new value can’t just overwrite the old value;</p>
<p>you instead need the retraction to remove the old value</p>
<ul>
<li>When <em>dynamic windows</em> (e.g., sessions, which we look at more</li>
</ul>
<p>closely in a few moments) are in use, the new value might be</p>
<p>replacing more than one previous window, due to window merging.</p>
<p>In this case, it can be difficult to determine from the new window</p>
<p>alone which old windows are being replaced. Having explicit</p>
<p>retractions for the old windows makes the task straightforward. We</p>
<p>see an example of this in detail in Chapter 8.</p>
<p>The different semantics for each group are somewhat clearer when seen side</p>
<p>by-side. Consider the two panes for the second window (the one with event</p>
<p>time range [12:06, 12:08)) in Figure 2-11 (the one with early&#x2F;on-time&#x2F;late</p>
<p>triggers). Table 2-1 shows what the values for each pane would look like</p>
<p>across the three accumulation modes (with <em>accumulating</em> mode being the</p>
<p>specific mode used in Figure 2-11 itself).</p>
<p><em>Table 2-1. Comparing accumulation modes using the second</em></p>
<p><em>window from Figure 2-11</em></p>
<p><strong>Discarding</strong></p>
<p><strong>Accumulating</strong></p>
<p><strong>Accumulating &amp; Retracting</strong></p>
<p><strong>Pane 1: inputs&#x3D;[3]</strong></p>
<p>3</p>
<p>3</p>
<p>3</p>
<p><strong>Pane 2: inputs&#x3D;[8, 1]</strong></p>
<p>9</p>
<p>12</p>
<p>12, –3</p>
<p><strong>Value of final normal pane</strong> 9</p>
<p>12</p>
<p>12</p>
<p><strong>Sum of all panes</strong></p>
<p>12</p>
<p>15</p>
<p>12</p>
<p>Let’s take a closer look at what’s happening:</p>
<ul>
<li>Discarding</li>
</ul>
<p>Each pane incorporates only the values that arrived during that specific</p>
<p>pane. As such, the final value observed does not fully capture the total</p>
<p>sum. However, if you were to sum all of the independent panes</p>
<p>themselves, you would arrive at a correct answer of 12. This is why</p>
<p>discarding mode is useful when the downstream consumer itself is</p>
<p>performing some sort of aggregation on the materialized panes.</p>
<ul>
<li>Accumulating</li>
</ul>
<p>As in Figure 2-11, each pane incorporates the values that arrived during</p>
<p>that specific pane, plus all of the values from previous panes. As such, the</p>
<p>final value observed correctly captures the total sum of 12. If you were to</p>
<p>sum up the individual panes themselves, however, you’d effectively be</p>
<p>double-counting the inputs from pane 1, giving you an incorrect total sum</p>
<p>of 15. This is why accumulating mode is most useful when you can</p>
<p>simply overwrite previous values with new values: the new value already</p>
<p>incorporates all of the data seen thus far.</p>
<ul>
<li>Accumulating and retracting</li>
</ul>
<p>Each pane includes both a new accumulating mode value as well as a</p>
<p>retraction of the previous pane’s value. As such, both the last value</p>
<p>observed (excluding retractions) as well as the total sum of all</p>
<p>materialized panes (including retractions) provide you with the correct</p>
<p>answer of 12. This is why retractions are so powerful.</p>
<p>Example 2-9 demonstrates discarding mode in action, illustrating the changes</p>
<p>we would make to Example 2-7:</p>
<p><em>Example 2-9. Discarding mode version of early&#x2F;on-time&#x2F;late firings</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AtCount(1)))</span><br><span class="line">.discardingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Running again on a streaming engine with a heuristic watermark would</p>
<p>produce output like that shown in Figure 2-13.</p>
<p><em>Figure 2-13. Discarding mode version of early&#x2F;on-time&#x2F;late firings on a streaming engine</em></p>
<p>Even though the overall shape of the output is similar to the accumulating</p>
<p>mode version from Figure 2-11, note how none of the panes in this discarding</p>
<p>version overlap. As a result, each output is independent from the others.</p>
<p>If we want to look at retractions in action, the change would be similar, as</p>
<p>shown in Example 2-10. ??? depicts the results.</p>
<p><em>Example 2-10. Accumulating and retracting mode version of early&#x2F;on</em></p>
<p><em>time&#x2F;late firings</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AtCount(1)))</span><br><span class="line">.accumulatingAndRetractingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Accumulating and retracting mode version of early&#x2F;late firings on a streaming</p>
<p>engine</p>
<p>Because the panes for each window all overlap, it’s a little tricky to see the</p>
<p>retractions clearly. The retractions are indicated in red, which combines with</p>
<p>the overlapping blue panes to yield a slightly purplish color. I’ve also</p>
<p>horizontally shifted the values of the two outputs within a given pane slightly</p>
<p>(and separated them with a comma) to make them easier to differentiate.</p>
<p>Figure 2-14 combines the final frames of Figures 2-9, 2-11 (heuristic only),</p>
<p>and side-by-side, providing a nice visual contrast of the three modes.</p>
<p><em>Figure 2-14. Side-by-side comparison of accumulation modes</em></p>
<p>As you can imagine, the modes in the order presented (discarding,</p>
<p>accumulating, accumulating and retracting) are each successively more</p>
<p>expensive in terms of storage and computation costs. To that end, choice of</p>
<p>accumulation mode provides yet another dimension for making trade-offs</p>
<p>along the axes of correctness, latency, and cost.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>With this chapter complete, you now understand the basics of robust stream</p>
<p>processing and are ready to go forth into the world and do amazing things. Of</p>
<p>course, there are eight more chapters anxiously waiting for your attention, so</p>
<p>hopefully you won’t go forth like right now, this very minute. But regardless,</p>
<p>let’s recap what we’ve just covered, lest you forget any of it in your haste to</p>
<p>amble forward. First, the major concepts we touched upon:</p>
<ul>
<li>Event time versus processing time</li>
</ul>
<p>The all-important distinction between when events occurred and when</p>
<p>they are observed by your data processing system.</p>
<ul>
<li>Windowing</li>
</ul>
<p>The commonly utilized approach to managing unbounded data by slicing</p>
<p>it along temporal boundaries (in either processing time or event time,</p>
<p>though we narrow the definition of windowing in the Beam Model to</p>
<p>mean only within event time).</p>
<ul>
<li>Triggers</li>
</ul>
<p>The declarative mechanism for specifying precisely when materialization</p>
<p>of output makes sense for your particular use case.</p>
<ul>
<li>Watermarks</li>
</ul>
<p>The powerful notion of progress in event time that provides a means of</p>
<p>reasoning about completeness (and thus missing data) in an out-of-order</p>
<p>processing system operating on unbounded data.</p>
<ul>
<li>Accumulation</li>
</ul>
<p>The relationship between refinements of results for a single window for</p>
<p>cases in which it’s materialized multiple times as it evolves.</p>
<ul>
<li>Second, the four questions we used to frame our exploration:</li>
<li><em>What</em> results are calculated? &#x3D; transformations.</li>
<li><em>Where</em> in event time are results calculated? &#x3D; windowing.</li>
<li><em>When</em> in processing time are results materialized? &#x3D; triggers plus</li>
</ul>
<p>watermarks.</p>
<ul>
<li><em>How</em> do refinements of results relate? &#x3D; accumulation.</li>
</ul>
<p>Third, to drive home the flexibility afforded by this model of stream</p>
<p>processing (because in the end, that’s really what this is all about: balancing</p>
<p>competing tensions like correctness, latency, and cost), a recap of the major</p>
<p>variations in output we were able to achieve over the same dataset with only a</p>
<p>minimal amount of code change:</p>
<p>Integer summation</p>
<p>Example 2-1 &#x2F; Figure 2-3</p>
<p>Integer summation</p>
<p>Fixed windows batch</p>
<p>Example 2-2 &#x2F; Figure 2-5</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Repeated per-record trigger</p>
<p>Example 2-3 &#x2F; Figure 2-6</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Repeated aligned-delay trigger</p>
<p>Example 2-4 &#x2F; Figure 2-7</p>
<p>Repeated</p>
<p>unaligned-delay</p>
<p>trigger</p>
<p>Example 2-5 &#x2F; Figure 2-8</p>
<p>Fixed windows streaming</p>
<p>Heuristic watermark trigger</p>
<p>Example 2-6 &#x2F; Figure 2-10</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Discarding</p>
<p>Example 2-9 &#x2F; Figure 2-13</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Accumulating</p>
<p>Example 2-7 &#x2F; Figure 2-11</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Accumulating and Retracting</p>
<p>Example 2-10 &#x2F; ???</p>
<p>All that said, at this point, we’ve really looked at only one type of windowing:</p>
<p>fixed windowing in event time. As we know, there are a number of</p>
<p>dimensions to windowing, and I’d like to touch upon at least two more of</p>
<p>those before we call it day with the Beam Model. First, however, we’re going</p>
<p>to take a slight detour to dive deeper into the world of watermarks, as this</p>
<p>knowledge will help frame future discussions (and be fascinating in and of</p>
<p>itself). Enter Slava, stage right…</p>
</details>







<details><summary>点击 原文</summary><ol>
<li>If you’re fortunate enough to be reading the Safari version of the book, you</li>
</ol>
<p>have full-blown time-lapse animations just like in “Streaming 102”. For print,</p>
<p>Kindle, and other ebook versions, there are static images with a link to</p>
<p>animated versions on the web.</p>
<ol start="2">
<li>Bear with me here. Fine-grained emotional expressions via composite</li>
</ol>
<p>punctuation (i.e., emoticons) are strictly forbidden in O’Reilly publications &lt;</p>
<p>winky-smiley&#x2F;&gt;.</p>
<ol start="3">
<li>And indeed, we did just that with the original triggers feature in Beam. In</li>
</ol>
<p>retrospect, we went a bit overboard. Future iterations will be simpler and</p>
<p>easier to use, and in this book I focus only on the pieces that are likely to</p>
<p>remain in some form or another.</p>
<ol start="4">
<li>More accurately, the input to the function is really the state at time <em>P</em> of</li>
</ol>
<p>everything upstream of the point in the pipeline where the watermark is being</p>
<p>observed: the input source, buffered data, data actively being processed, and</p>
<p>so on; but conceptually it’s simpler to think of it as a mapping from</p>
<p>processing time to event time.</p>
<ol start="5">
<li>Note that I specifically chose to omit the value of 9 from the heuristic</li>
</ol>
<p>watermark because it will help me to make some important points about late</p>
<p>data and watermark lag. In reality, a heuristic watermark might be just as</p>
<p>likely to omit some other value(s) instead, which in turn could have</p>
<p>significantly less drastic effect on the watermark. If winnowing late-arriving</p>
<p>data from the watermark is your goal (which is very valid in some cases, such</p>
<p>as abuse detection, for which you just want to see a significant majority of the</p>
<p>data as quickly as possible), you don’t necessarily want a heuristic watermark</p>
<p>rather than a perfect watermark. What you really want is a percentile</p>
<p>watermark, which explicitly drops some percentile of late-arriving data from</p>
<p>its calculations. See Chapter 3.</p>
<ol start="6">
<li>Which isn’t to say there aren’t use cases that care primarily about</li>
</ol>
<p>correctness and not so much about latency; in those cases, using an accurate</p>
<p>watermark as the sole driver of output from a pipeline is a reasonable</p>
<p>approach.</p>
<ol start="7">
<li>And, as we know from before, this assertion is either guaranteed, in the case</li>
</ol>
<p>of a perfect watermark being used, or an educated guess, in the case of a</p>
<p>heuristic watermark.</p>
<ol start="8">
<li>You might note that there should logically be a fourth mode: discarding and</li>
</ol>
<p>retracting. That mode isn’t terribly useful in most cases, so I don’t discuss it</p>
<p>further here.</p>
<ol start="9">
<li>In retrospect, it probably would have been clearer to choose a different set</li>
</ol>
<p>of names that are more oriented toward the observed nature of data in the</p>
<p>materialized stream (e.g., “output modes”) rather than names describing the</p>
<p>state management semantics that yield those data. Perhaps: discarding mode</p>
<p>→ delta mode, accumulating mode → value mode, accumulating and</p>
<p>retracting mode → value and retraction mode? However, the</p>
<p>discarding&#x2F;accumulating&#x2F;accumulating and retracting names are enshrined in</p>
<p>the 1.x and 2.x lineages of the Beam Model, so I don’t want to introduce</p>
<p>potential confusion in the book by deviating. Also, it’s very likely</p>
<p>accumulating modes will blend into the background more with Beam 3.0 and</p>
<p>the introduction of sink triggers; more on this when we discuss SQL in</p>
<p>Chapter 8.</p>
</details>



    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Wang Wei
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www6v.github.io/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/" title="《Streaming System》-Chapter 2. The What, Where, When, and How of Data Processing[完整]">https://www6v.github.io/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/www6vHomeHexo/tags/Streaming-System/" rel="tag"># Streaming System</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/" rel="prev" title="《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行[完整]">
      <i class="fa fa-chevron-left"></i> 《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行[完整]
    </a></div>
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/" rel="next" title="《Streaming System》- 第一章：流处理入门[完整]">
      《Streaming System》- 第一章：流处理入门[完整] <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wang Wei</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/www6vHomeHexo/archives/">
        
          <span class="site-state-item-count">502</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/www6vHomeHexo/categories/">
          
        <span class="site-state-item-count">300</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/www6vHomeHexo/tags/">
          
        <span class="site-state-item-count">159</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/www6v" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;www6v" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:www6v@126.com" title="E-Mail → mailto:www6v@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/www6vHomeHexo/lib/anime.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.ui.min.js"></script>

<script src="/www6vHomeHexo/js/utils.js"></script>

<script src="/www6vHomeHexo/js/motion.js"></script>


<script src="/www6vHomeHexo/js/schemes/muse.js"></script>


<script src="/www6vHomeHexo/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/www6vHomeHexo/js/local-search.js"></script>













  

  

</body>
</html>
