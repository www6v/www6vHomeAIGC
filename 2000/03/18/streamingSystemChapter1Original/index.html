<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/www6vHomeHexo/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/www6vHomeHexo/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/www6vHomeHexo/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/www6vHomeHexo/images/logo.svg" color="#222">

<link rel="stylesheet" href="/www6vHomeHexo/css/main.css">


<link rel="stylesheet" href="/www6vHomeHexo/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www6v.github.io","root":"/www6vHomeHexo/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="《Streaming System》- Chapter 1. Streaming 101[完整]">
<meta property="og:url" content="https://www6v.github.io/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/index.html">
<meta property="og:site_name" content="www6v的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2000-03-18T11:03:13.000Z">
<meta property="article:modified_time" content="2023-06-03T17:46:01.348Z">
<meta property="article:author" content="Wang Wei">
<meta property="article:tag" content="Streaming System">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www6v.github.io/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《Streaming System》- Chapter 1. Streaming 101[完整] | www6v的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/www6vHomeHexo/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">www6v的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录技术点滴</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/www6vHomeHexo/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/www6vHomeHexo/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/www6vHomeHexo/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/www6vHomeHexo/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www6v.github.io/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/www6vHomeHexo/images/avatar.gif">
      <meta itemprop="name" content="Wang Wei">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="www6v的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Streaming System》- Chapter 1. Streaming 101[完整]
        </h1>

        <div class="post-meta">





          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2000-03-18 19:03:13" itemprop="dateCreated datePublished" datetime="2000-03-18T19:03:13+08:00">2000-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-04 01:46:01" itemprop="dateModified" datetime="2023-06-04T01:46:01+08:00">2023-06-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/www6vHomeHexo/categories/Streaming-System/" itemprop="url" rel="index"><span itemprop="name">Streaming System</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#terminology-what-is-streaming"><strong>Terminology: What Is Streaming?</strong></a><ul>
<li><a href="#on-the-greatly-exaggerated-limitations-of-streaming"><strong>On the Greatly Exaggerated Limitations of Streaming</strong></a></li>
<li><a href="#event-time-versus-processing-time"><strong>Event Time Versus Processing Time</strong></a></li>
</ul>
</li>
<li><a href="#data-processing-patterns"><strong>Data Processing Patterns</strong></a><ul>
<li><a href="#bounded-data"><strong>Bounded Data</strong></a></li>
<li><a href="#unbounded-data-batch"><strong>Unbounded Data: Batch</strong></a></li>
<li><a href="#unbounded-data-streaming"><strong>Unbounded Data: Streaming</strong></a></li>
</ul>
</li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 19</p>
<p>Streaming data processing is a big deal in big data these days, and for good<br>reasons; among them are the following:</p>
<ul>
<li>Businesses crave ever-more timely insights into their data, and<br>switching to streaming is a good way to achieve lower latency</li>
<li>The massive, unbounded datasets that are increasingly common in<br>modern business are more easily tamed using a system designed for<br>such never-ending volumes of data.</li>
<li>Processing data as they arrive spreads workloads out more evenly<br>over time, yielding more consistent and predictable consumption of<br>resources.<br>Despite this business-driven surge of interest in streaming, streaming systems<br>long remained relatively immature compared to their batch brethren. It’s only<br>recently that the tide has swung conclusively in the other direction. In my<br>more bumptious moments, I hope that might be in small part due to the solid<br>dose of goading I originally served up in my “Streaming 101” and “Streaming<br>102” blog posts (on which the first few chapters of this book are rather<br>obviously based). But in reality, there’s also just a lot of industry interest in<br>seeing streaming systems mature and a lot of smart and active folks out there<br>who enjoy building them.<br>Even though the battle for general streaming advocacy has been, in my<br>opinion, effectively won, I’m still going to present my original arguments<br>from “Streaming 101” more or less unaltered. For one, they’re still very<br>applicable today, even if much of industry has begun to heed the battle cry.<br>And for two, there are a lot of folks out there who still haven’t gotten the<br>memo; this book is an extended attempt at getting these points across.<br>To begin, I cover some important background information that will help<br>frame the rest of the topics I want to discuss. I do this in three specific<br>sections:</li>
<li>Terminology<br>To talk precisely about complex topics requires precise definitions of<br>terms. For some terms that have overloaded interpretations in current use,<br>I’ll try to nail down exactly what I mean when I say them.</li>
<li>Capabilities<br>I remark on the oft-perceived shortcomings of streaming systems. I also<br>propose the frame of mind that I believe data processing system builders<br>need to adopt in order to address the needs of modern data consumers<br>going forward.</li>
<li>Time domains<br>I introduce the two primary domains of time that are relevant in data<br>processing, show how they relate, and point out some of the difficulties<br>these two domains impose.</li>
</ul>
<h1><span id="terminology-what-is-streaming"><strong>Terminology: What Is Streaming?</strong></span><a href="#terminology-what-is-streaming" class="header-anchor">#</a></h1><p>Before going any further, I’d like to get one thing out of the way: what is</p>
<p>streaming? The term streaming is used today to mean a variety of different</p>
<p>things (and for simplicity I’ve been using it somewhat loosely up until now),</p>
<p>which can lead to misunderstandings about what streaming really is or what</p>
<p>streaming systems are actually capable of. As a result, I would prefer to</p>
<p>define the term somewhat precisely.</p>
<p>The crux of the problem is that many things that ought to be described by</p>
<p><em>what</em> they are (unbounded data processing, approximate results, etc.), have</p>
<p>come to be described colloquially by <em>how</em> they historically have been</p>
<p>accomplished (i.e., via streaming execution engines). This lack of precision in</p>
<p>terminology clouds what streaming really means, and in some cases it</p>
<p>burdens streaming systems themselves with the implication that their</p>
<p>capabilities are limited to characteristics historically described as “streaming,”</p>
<p>such as approximate or speculative results.</p>
<p>Given that well-designed streaming systems are just as capable (technically</p>
<p>more so) of producing correct, consistent, repeatable results as any existing</p>
<p>batch engine, I prefer to isolate the term “streaming” to a very specific</p>
<p>meaning:</p>
<ul>
<li>Streaming system</li>
</ul>
<p>A type of data processing engine that is designed with infinite datasets in</p>
<p>mind.</p>
<p>If I want to talk about low-latency, approximate, or speculative results, I use</p>
<p>those specific words rather than imprecisely calling them “streaming.”</p>
<p>Precise terms are also useful when discussing the different types of data one</p>
<p>might encounter. From my perspective, there are two important (and</p>
<p>orthogonal) dimensions that define the shape of a given dataset: <em>cardinality</em></p>
<p>and <em>constitution</em>.</p>
<p>The cardinality of a dataset dictates its size, with the most salient aspect of</p>
<p>cardinality being whether a given dataset is finite or infinite. Here are the two</p>
<p>terms I prefer to use for describing the coarse cardinality in a dataset:</p>
<ul>
<li>Bounded data</li>
</ul>
<p>A type of dataset that is finite in size.</p>
<ul>
<li>Unbounded data</li>
</ul>
<p>A type of dataset that is infinite in size (at least theoretically).</p>
<p>Cardinality is important because the unbounded nature of infinite datasets</p>
<p>imposes additional burdens on data processing frameworks that consume</p>
<p>them. More on this in the next section.</p>
<p>The constitution of a dataset, on the other hand, dictates its physical</p>
<p>manifestation. As a result, the constitution defines the ways one can interact</p>
<p>with the data in question. We won’t get around to deeply examining</p>
<p>constitutions until Chapter 6, but to give you a brief sense of things, there are</p>
<p>two primary constitutions of importance:</p>
<ul>
<li>Table</li>
</ul>
<p>A holistic view of a dataset at a specific point in time. SQL systems have</p>
<p>traditionally dealt in tables.</p>
<ul>
<li>Stream</li>
</ul>
<p>An element-by-element view of the evolution of a dataset over time. The</p>
<p>MapReduce lineage of data processing systems have traditionally dealt in</p>
<p>streams.</p>
<p>We look quite deeply at the relationship between streams and tables in</p>
<p>Chapters 6, 8, and 9, and in Chapter 8 we also learn about the unifying</p>
<p>underlying concept of <em>time-varying relations</em> that ties them together. But until</p>
<p>then, we deal primarily in streams because that’s the constitution pipeline</p>
<p>developers directly interact with in most data processing systems today (both</p>
<p>batch and streaming). It’s also the constitution that most naturally embodies</p>
<p>the challenges that are unique to stream processing.</p>
<h3><span id="on-the-greatly-exaggerated-limitations-of-streaming"><strong>On the Greatly Exaggerated Limitations of Streaming</strong></span><a href="#on-the-greatly-exaggerated-limitations-of-streaming" class="header-anchor">#</a></h3><p>On that note, let’s next talk a bit about what streaming systems can and can’t</p>
<p>do, with an emphasis on can. One of the biggest things I want to get across in</p>
<p>this chapter is just how capable a well-designed streaming system can be.</p>
<p>Streaming systems have historically been relegated to a somewhat niche</p>
<p>market of providing low-latency, inaccurate, or speculative results, often in</p>
<p>conjunction with a more capable batch system to provide eventually correct</p>
<p>results; in other words, the Lambda Architecture.</p>
<p>For those of you not already familiar with the Lambda Architecture, the basic</p>
<p>idea is that you run a streaming system alongside a batch system, both</p>
<p>performing essentially the same calculation. The streaming system gives you</p>
<p>low-latency, inaccurate results (either because of the use of an approximation</p>
<p>algorithm, or because the streaming system itself does not provide</p>
<p>correctness), and some time later a batch system rolls along and provides you</p>
<p>with correct output. Originally proposed by Twitter’s Nathan Marz (creator of</p>
<p>Storm), it ended up being quite successful because it was, in fact, a fantastic</p>
<p>idea for the time; streaming engines were a bit of a letdown in the correctness</p>
<p>department, and batch engines were as inherently unwieldy as you’d expect,</p>
<p>so Lambda gave you a way to have your proverbial cake and eat it too.</p>
<p>Unfortunately, maintaining a Lambda system is a hassle: you need to build,</p>
<p>provision, and maintain two independent versions of your pipeline and then</p>
<p>also somehow merge the results from the two pipelines at the end.</p>
<p>As someone who spent years working on a strongly consistent streaming</p>
<p>engine, I also found the entire principle of the Lambda Architecture a bit</p>
<p>unsavory. Unsurprisingly, I was a huge fan of Jay Kreps’ “Questioning the</p>
<p>Lambda Architecture” post when it came out. Here was one of the first highly</p>
<p>visible statements against the necessity of dual-mode execution. Delightful.</p>
<p>Kreps addressed the issue of repeatability in the context of using a replayable</p>
<p>system like Kafka as the streaming interconnect, and went so far as to propose</p>
<p>the Kappa Architecture, which basically means running a single pipeline</p>
<p>using a well-designed system that’s appropriately built for the job at hand.</p>
<p>I’m not convinced that notion requires its own Greek letter name, but I fully</p>
<p>support the idea in principle.</p>
<p>Quite honestly, I’d take things a step further. I would argue that well-designed</p>
<p>streaming systems actually provide a strict superset of batch functionality.</p>
<p>Modulo perhaps an efficiency delta, there should be no need for batch</p>
<p>systems as they exist today. And kudos to the Apache Flink folks for taking</p>
<p>this idea to heart and building a system that’s all-streaming-all-the-time under</p>
<p>the covers, even in “batch” mode; I love it.</p>
<p><strong>BATCH AND STREAMING EFFICIENCY DIFFERENCES</strong></p>
<p>One which I propose is not an inherent limitation of streaming systems,</p>
<p>but simply a consequence of design choices made in most streaming</p>
<p>systems thus far. The efficiency delta between batch and streaming is</p>
<p>largely the result of the increased bundling and more efficient shuffle</p>
<p>transports found in batch systems. Modern batch systems go to great</p>
<p>lengths to implement sophisticated optimizations that allow for remarkable</p>
<p>levels of throughput using surprisingly modest compute resources. There’s</p>
<p>no reason the types of clever insights that make batch systems the</p>
<p>efficiency heavyweights they are today couldn’t be incorporated into a</p>
<p>system designed for unbounded data, providing users flexible choice</p>
<p>between what we typically consider to be high-latency, higher-efficiency</p>
<p>“batch” processing and low-latency, lower-efficiency “streaming”</p>
<p>processing. This is effectively what we’ve done at Google with Cloud</p>
<p>Dataflow by providing both batch and streaming runners under the same</p>
<p>unified model. In our case, we use separate runners because we happen to</p>
<p>have two independently designed systems optimized for their specific use</p>
<p>cases. Long term, from an engineering perspective, I’d love to see us</p>
<p>merge the two into a single system that incorporates the best parts of both</p>
<p>while still maintaining the flexibility of choosing an appropriate efficiency</p>
<p>level. But that’s not what we have today. And honestly, thanks to the</p>
<p>unified Dataflow Model, it’s not even strictly necessary; so it may well</p>
<p>never happen.</p>
<p>The corollary of all this is that broad maturation of streaming systems</p>
<p>combined with robust frameworks for unbounded data processing will in time</p>
<p>allow for the relegation of the Lambda Architecture to the antiquity of big</p>
<p>data history where it belongs. I believe the time has come to make this a</p>
<p>reality. Because to do so—that is, to beat batch at its own game—you really</p>
<p>only need two things:</p>
<ul>
<li>Correctness</li>
</ul>
<p>This gets you parity with batch. At the core, correctness boils down to</p>
<p>consistent storage. Streaming systems need a method for checkpointing</p>
<p>persistent state over time (something Kreps has talked about in his “Why</p>
<p>local state is a fundamental primitive in stream processing” post), and it</p>
<p>must be well designed enough to remain consistent in light of machine</p>
<p>failures. When Spark Streaming first appeared in the public big data scene</p>
<p>a few years ago, it was a beacon of consistency in an otherwise dark</p>
<p>streaming world. Thankfully, things have improved substantially since</p>
<p>then, but it is remarkable how many streaming systems still try to get by</p>
<p>without strong consistency.</p>
<p>To reiterate—because this point is important: strong consistency is</p>
<p>required for exactly-once processing,  which is required for correctness,</p>
<p>which is a requirement for any system that’s going to have a chance at</p>
<p>meeting or exceeding the capabilities of batch systems. Unless you just</p>
<p>truly don’t care about your results, I implore you to shun any streaming</p>
<p>system that doesn’t provide strongly consistent state. Batch systems don’t</p>
<p>require you to verify ahead of time if they are capable of producing</p>
<p>correct answers; don’t waste your time on streaming systems that can’t</p>
<p>meet that same bar.</p>
<p>If you’re curious to learn more about what it takes to get strong</p>
<p>consistency in a streaming system, I recommend you check out the</p>
<p>MillWheel, Spark Streaming, and Flink snapshotting papers. All three</p>
<p>spend a significant amount of time discussing consistency. Reuven will</p>
<p>dive into consistency guarantees in Chapter 5, and if you still find</p>
<p>yourself craving more, there’s a large amount of quality information on</p>
<p>this topic in the literature and elsewhere.</p>
<ul>
<li>Tools for reasoning about time</li>
</ul>
<p>This gets you beyond batch. Good tools for reasoning about time are</p>
<p>essential for dealing with unbounded, unordered data of varying event</p>
<p>time skew. An increasing number of modern datasets exhibit these</p>
<p>characteristics, and existing batch systems (as well as many streaming</p>
<p>systems) lack the necessary tools to cope with the difficulties they impose</p>
<p>(though this is now rapidly changing, even as I write this). We will spend</p>
<p>the bulk of this book explaining and focusing on various facets of this</p>
<p>point.</p>
<p>To begin with, we get a basic understanding of the important concept of</p>
<p>time domains, after which we take a deeper look at what I mean by</p>
<p>unbounded, unordered data of varying event-time skew. We then spend</p>
<p>the rest of this chapter looking at common approaches to bounded and</p>
<p>unbounded data processing, using both batch and streaming systems.</p>
<h3><span id="event-time-versus-processing-time"><strong>Event Time Versus Processing Time</strong></span><a href="#event-time-versus-processing-time" class="header-anchor">#</a></h3><p>To speak cogently about unbounded data processing requires a clear</p>
<p>understanding of the domains of time involved. Within any data processing</p>
<p>system, there are typically two domains of time that we care about:</p>
<ul>
<li>Event time</li>
</ul>
<p>This is the time at which events actually occurred.</p>
<ul>
<li>Processing time</li>
</ul>
<p>This is the time at which events are observed in the system.</p>
<p>Not all use cases care about event times (and if yours doesn’t, hooray! your</p>
<p>life is easier), but many do. Examples include characterizing user behavior</p>
<p>over time, most billing applications, and many types of anomaly detection, to</p>
<p>name a few.</p>
<p>In an ideal world, event time and processing time would always be equal,</p>
<p>with events being processed immediately as they occur. Reality is not so kind,</p>
<p>however, and the skew between event time and processing time is not only</p>
<p>nonzero, but often a highly variable function of the characteristics of the</p>
<p>underlying input sources, execution engine, and hardware. Things that can</p>
<p>affect the level of skew include the following:</p>
<ul>
<li>Shared resource limitations, like network congestion, network</li>
</ul>
<p>partitions, or shared CPU in a nondedicated environment</p>
<ul>
<li>Software causes such as distributed system logic, contention, and so</li>
</ul>
<p>on</p>
<ul>
<li>Features of the data themselves, like key distribution, variance in</li>
</ul>
<p>throughput, or variance in disorder (i.e., a plane full of people taking</p>
<p>their phones out of airplane mode after having used them offline for</p>
<p>the entire flight)</p>
<p>As a result, if you plot the progress of event time and processing time in any</p>
<p>real-world system, you typically end up with something that looks a bit like</p>
<p>the red line in Figure 1-1.</p>
<p><em>Figure 1-1. Time-domain mapping. The x-axis represents event-time completeness in the system; that is, the time X in event time up to which all data with event times less than X have been observed. The y axis represents the progress of processing time; that is, normal clock time as observed by the data</em></p>
<p><em>processing system as it executes.</em></p>
<p>In Figure 1-1, the black dashed line with slope of 1 represents the ideal, where</p>
<p>processing time and event time are exactly equal; the red line represents</p>
<p>reality. In this example, the system lags a bit at the beginning of processing</p>
<p>time, veers closer toward the ideal in the middle, and then lags again a bit</p>
<p>toward the end. At first glance, there are two types of skew visible in this</p>
<p>diagram, each in different time domains:</p>
<ul>
<li>Processing time</li>
</ul>
<p>The vertical distance between the ideal and the red line is the lag in the</p>
<p>processing-time domain. That distance tells you how much delay is</p>
<p>observed (in processing time) between when the events for a given time</p>
<p>occurred and when they were processed. This is the perhaps the more</p>
<p>natural and intuitive of the two skews.</p>
<ul>
<li>Event time</li>
</ul>
<p>The horizontal distance between the ideal and the red line is the amount of</p>
<p>event-time skew in the pipeline at that moment. It tells you how far</p>
<p>behind the ideal (in event time) the pipeline is currently.</p>
<p>In reality, processing-time lag and event-time skew at any given point in time</p>
<p>are identical; they’re just two ways of looking at the same thing. The</p>
<p>important takeaway regarding lag&#x2F;skew is this: Because the overall mapping</p>
<p>between event time and processing time is not static (i.e., the lag&#x2F;skew can</p>
<p>vary arbitrarily over time), this means that you cannot analyze your data</p>
<p>solely within the context of when they are observed by your pipeline if you</p>
<p>care about their event times (i.e., when the events actually occurred).</p>
<p>Unfortunately, this is the way many systems designed for unbounded data</p>
<p>have historically operated. To cope with the infinite nature of unbounded</p>
<p>datasets, these systems typically provide some notion of windowing the</p>
<p>incoming data. We discuss windowing in great depth a bit later, but it</p>
<p>essentially means chopping up a dataset into finite pieces along temporal</p>
<p>boundaries. If you care about correctness and are interested in analyzing your</p>
<p>data in the context of their event times, you cannot define those temporal</p>
<p>boundaries using processing time (i.e., processing-time windowing), as many</p>
<p>systems do; with no consistent correlation between processing time and event</p>
<p>time, some of your event-time data are going to end up in the wrong</p>
<p>processing-time windows (due to the inherent lag in distributed systems, the</p>
<p>online&#x2F;offline nature of many types of input sources, etc.), throwing</p>
<p>correctness out the window, as it were. We look at this problem in more detail</p>
<p>in a number of examples in the sections that follow, as well as the remainder</p>
<p>of the book.</p>
<p>Unfortunately, the picture isn’t exactly rosy when windowing by event time,</p>
<p>either. In the context of unbounded data, disorder and variable skew induce a</p>
<p>completeness problem for event-time windows: lacking a predictable</p>
<p>mapping between processing time and event time, how can you determine</p>
<p>when you’ve observed all of the data for a given event time <em>X</em>? For many real</p>
<p>world data sources, you simply can’t. But the vast majority of data processing</p>
<p>systems in use today rely on some notion of completeness, which puts them at</p>
<p>a severe disadvantage when applied to unbounded datasets.</p>
<p>I propose that instead of attempting to groom unbounded data into finite</p>
<p>batches of information that eventually become complete, we should be</p>
<p>designing tools that allow us to live in the world of uncertainty imposed by</p>
<p>these complex datasets. New data will arrive, old data might be retracted or</p>
<p>updated, and any system we build should be able to cope with these facts on</p>
<p>its own, with notions of completeness being a convenient optimization for</p>
<p>specific and appropriate use cases rather than a semantic necessity across all</p>
<p>of them.</p>
<p>Before getting into specifics about what such an approach might look like,</p>
<p>let’s finish up one more useful piece of background: common data processing</p>
<p>patterns.</p>
<h1><span id="data-processing-patterns"><strong>Data Processing Patterns</strong></span><a href="#data-processing-patterns" class="header-anchor">#</a></h1><p>At this point, we have enough background established that we can begin</p>
<p>looking at the core types of usage patterns common across bounded and</p>
<p>unbounded data processing today. We look at both types of processing and,</p>
<p>where relevant, within the context of the two main types of engines we care</p>
<p>about (batch and streaming, where in this context, I’m essentially lumping</p>
<p>microbatch in with streaming because the differences between the two aren’t</p>
<p>terribly important at this level).</p>
<h3><span id="bounded-data"><strong>Bounded Data</strong></span><a href="#bounded-data" class="header-anchor">#</a></h3><p>Processing bounded data is conceptually quite straightforward, and likely</p>
<p>familiar to everyone. In Figure 1-2, we start out on the left with a dataset full</p>
<p>of entropy. We run it through some data processing engine (typically batch,</p>
<p>though a well-designed streaming engine would work just as well), such as</p>
<p>MapReduce, and on the right side end up with a new structured dataset with</p>
<p>greater inherent value.</p>
<p><em>Figure 1-2. Bounded data processing with a classic batch engine. A finite pool of unstructured data on</em></p>
<p><em>the left is run through a data processing engine, resulting in corresponding structured data on the right.</em></p>
<p>Though there are of course infinite variations on what you can actually</p>
<p>calculate as part of this scheme, the overall model is quite simple. Much more</p>
<p>interesting is the task of processing an unbounded dataset. Let’s now look at</p>
<p>the various ways unbounded data are typically processed, beginning with the</p>
<p>approaches used with traditional batch engines and then ending up with the</p>
<p>approaches you can take with a system designed for unbounded data, such as</p>
<p>most streaming or microbatch engines.</p>
<h3><span id="unbounded-data-batch"><strong>Unbounded Data: Batch</strong></span><a href="#unbounded-data-batch" class="header-anchor">#</a></h3><p>Batch engines, though not explicitly designed with unbounded data in mind,</p>
<p>have nevertheless been used to process unbounded datasets since batch</p>
<p>systems were first conceived. As you might expect, such approaches revolve</p>
<p>around slicing up the unbounded data into a collection of bounded datasets</p>
<p>appropriate for batch processing.</p>
<p><strong>Fixed windows</strong></p>
<p>The most common way to process an unbounded dataset using repeated runs</p>
<p>of a batch engine is by windowing the input data into fixed-size windows and</p>
<p>then processing each of those windows as a separate, bounded data source</p>
<p>(sometimes also called <em>tumbling windows</em>), as in Figure 1-3. Particularly for</p>
<p>input sources like logs, for which events can be written into directory and file</p>
<p>hierarchies whose names encode the window they correspond to, this sort of</p>
<p>thing appears quite straightforward at first blush because you’ve essentially</p>
<p>performed the time-based shuffle to get data into the appropriate event-time</p>
<p>windows ahead of time.</p>
<p>In reality, however, most systems still have a completeness problem to deal</p>
<p>with (What if some of your events are delayed en route to the logs due to a</p>
<p>network partition? What if your events are collected globally and must be</p>
<p>transferred to a common location before processing? What if your events</p>
<p>come from mobile devices?), which means some sort of mitigation might be</p>
<p>necessary (e.g., delaying processing until you’re sure all events have been</p>
<p>collected or reprocessing the entire batch for a given window whenever data</p>
<p>arrive late).</p>
<p><em>Figure 1-3. Unbounded data processing via ad hoc fixed windows with a classic batch engine. An</em></p>
<p><em>unbounded dataset is collected up front into finite, fixed-size windows of bounded data that are then</em></p>
<p><em>processed via successive runs a of classic batch engine.</em></p>
<p><strong>Sessions</strong></p>
<p>This approach breaks down even more when you try to use a batch engine to</p>
<p>process unbounded data into more sophisticated windowing strategies, like</p>
<p>sessions. Sessions are typically defined as periods of activity (e.g., for a</p>
<p>specific user) terminated by a gap of inactivity. When calculating sessions</p>
<p>using a typical batch engine, you often end up with sessions that are split</p>
<p>across batches, as indicated by the red marks in Figure 1-4. We can reduce the</p>
<p>number of splits by increasing batch sizes, but at the cost of increased latency.</p>
<p>Another option is to add additional logic to stitch up sessions from previous</p>
<p>runs, but at the cost of further complexity.</p>
<p><em>Figure 1-4. Unbounded data processing into sessions via ad hoc fixed windows with a classic batch</em></p>
<p><em>engine. An unbounded dataset is collected up front into finite, fixed-size windows of bounded data that</em></p>
<p><em>are then subdivided into dynamic session windows via successive runs a of classic batch engine.</em></p>
<p>Either way, using a classic batch engine to calculate sessions is less than</p>
<p>ideal. A nicer way would be to build up sessions in a streaming manner,</p>
<p>which we look at later on.</p>
<h3><span id="unbounded-data-streaming"><strong>Unbounded Data: Streaming</strong></span><a href="#unbounded-data-streaming" class="header-anchor">#</a></h3><p>Contrary to the ad hoc nature of most batch-based unbounded data processing</p>
<p>approaches, streaming systems are built for unbounded data. As we talked</p>
<p>about earlier, for many real-world, distributed input sources, you not only find</p>
<p>yourself dealing with unbounded data, but also data such as the following:</p>
<ul>
<li>Highly unordered with respect to event times, meaning that you need</li>
</ul>
<p>some sort of time-based shuffle in your pipeline if you want to</p>
<p>analyze the data in the context in which they occurred.</p>
<ul>
<li>Of varying event-time skew, meaning that you can’t just assume</li>
</ul>
<p>you’ll always see most of the data for a given event time <em>X</em> within</p>
<p>some constant epsilon of time <em>Y</em>.</p>
<p>There are a handful of approaches that you can take when dealing with data</p>
<p>that have these characteristics. I generally categorize these approaches into</p>
<p>four groups: time-agnostic, approximation, windowing by processing time,</p>
<p>and windowing by event time.</p>
<p>Let’s now spend a little bit of time looking at each of these approaches.</p>
<p><strong>Time-agnostic</strong></p>
<p>Time-agnostic processing is used for cases in which time is essentially</p>
<p>irrelevant; that is, all relevant logic is data driven. Because everything about</p>
<p>such use cases is dictated by the arrival of more data, there’s really nothing</p>
<p>special a streaming engine has to support other than basic data delivery. As a</p>
<p>result, essentially all streaming systems in existence support time-agnostic use</p>
<p>cases out of the box (modulo system-to-system variances in consistency</p>
<p>guarantees, of course, if you care about correctness). Batch systems are also</p>
<p>well suited for time-agnostic processing of unbounded data sources by simply</p>
<p>chopping the unbounded source into an arbitrary sequence of bounded</p>
<p>datasets and processing those datasets independently. We look at a couple of</p>
<p>concrete examples in this section, but given the straightforwardness of</p>
<p>handling time-agnostic processing (from a temporal perspective at least), we</p>
<p>won’t spend much more time on it beyond that.</p>
<p><em><strong>Filtering</strong></em></p>
<p>A very basic form of time-agnostic processing is filtering, an example of</p>
<p>which is rendered in Figure 1-5. Imagine that you’re processing web traffic</p>
<p>logs and you want to filter out all traffic that didn’t originate from a specific</p>
<p>domain. You would look at each record as it arrived, see if it belonged to the</p>
<p>domain of interest, and drop it if not. Because this sort of thing depends only</p>
<p>on a single element at any time, the fact that the data source is unbounded,</p>
<p>unordered, and of varying event-time skew is irrelevant.</p>
<p><em>Figure 1-5. Filtering unbounded data. A collection of data (flowing left to right) of varying types is</em></p>
<p><em>filtered into a homogeneous collection containing a single type.</em></p>
<p><em><strong>Inner joins</strong></em></p>
<p>Another time-agnostic example is an inner join, diagrammed in Figure 1-6.</p>
<p>When joining two unbounded data sources, if you care only about the results</p>
<p>of a join when an element from both sources arrive, there’s no temporal</p>
<p>element to the logic. Upon seeing a value from one source, you can simply</p>
<p>buffer it up in persistent state; only after the second value from the other</p>
<p>source arrives do you need to emit the joined record. (In truth, you’d likely</p>
<p>want some sort of garbage collection policy for unemitted partial joins, which</p>
<p>would likely be time based. But for a use case with little or no uncompleted</p>
<p>joins, such a thing might not be an issue.)</p>
<p><em>Figure 1-6. Performing an inner join on unbounded data. Joins are produced when matching elements</em></p>
<p><em>from both sources are observed.</em></p>
<p>Switching semantics to some sort of outer join introduces the data</p>
<p>completeness problem we’ve talked about: after you’ve seen one side of the</p>
<p>join, how do you know whether the other side is ever going to arrive or not?</p>
<p>Truth be told, you don’t, so you need to introduce some notion of a timeout,</p>
<p>which introduces an element of time. That element of time is essentially a</p>
<p>form of windowing, which we’ll look at more closely in a moment.</p>
<p><strong>Approximation algorithms</strong></p>
<p>The second major category of approaches is approximation algorithms, such</p>
<p>as approximate Top-N, streaming k-means, and so on. They take an</p>
<p>unbounded source of input and provide output data that, if you squint at them,</p>
<p>look more or less like what you were hoping to get, as in Figure 1-7. The</p>
<p>upside of approximation algorithms is that, by design, they are low overhead</p>
<p>and designed for unbounded data. The downsides are that a limited set of</p>
<p>them exist, the algorithms themselves are often complicated (which makes it</p>
<p>difficult to conjure up new ones), and their approximate nature limits their</p>
<p>utility.</p>
<p><em>Figure 1-7. Computing approximations on unbounded data. Data are run through a complex algorithm,</em></p>
<p><em>yielding output data that look more or less like the desired result on the other side.</em></p>
<p>It’s worth noting that these algorithms typically do have some element of time</p>
<p>in their design (e.g., some sort of built-in decay). And because they process</p>
<p>elements as they arrive, that time element is usually processing-time based.</p>
<p>This is particularly important for algorithms that provide some sort of</p>
<p>provable error bounds on their approximations. If those error bounds are</p>
<p>predicated on data arriving in order, they mean essentially nothing when you</p>
<p>feed the algorithm unordered data with varying event-time skew. Something</p>
<p>to keep in mind.</p>
<p>Approximation algorithms themselves are a fascinating subject, but as they</p>
<p>are essentially another example of time-agnostic processing (modulo the</p>
<p>temporal features of the algorithms themselves), they’re quite straightforward</p>
<p>to use and thus not worth further attention, given our current focus.</p>
<p><strong>Windowing</strong></p>
<p>The remaining two approaches for unbounded data processing are both</p>
<p>variations of windowing. Before diving into the differences between them, I</p>
<p>should make it clear exactly what I mean by windowing, insomuch as we</p>
<p>touched on it only briefly in the previous section. Windowing is simply the</p>
<p>notion of taking a data source (either unbounded or bounded), and chopping it</p>
<p>up along temporal boundaries into finite chunks for processing. Figure 1-8</p>
<p>shows three different windowing patterns.</p>
<p><em>Figure 1-8. Windowing strategies. Each example is shown for three different keys, highlighting the</em></p>
<p><em>difference between aligned windows (which apply across all the data) and unaligned windows (which</em></p>
<p><em>apply across a subset of the data).</em></p>
<p>Let’s take a closer look at each strategy:</p>
<ul>
<li>Fixed windows (aka tumbling windows)</li>
</ul>
<p>We discussed fixed windows earlier. Fixed windows slice time into</p>
<p>segments with a fixed-size temporal length. Typically (as shown in</p>
<p>Figure 1-9), the segments for fixed windows are applied uniformly across</p>
<p>the entire dataset, which is an example of <em>aligned</em> windows. In some</p>
<p>cases, it’s desirable to phase-shift the windows for different subsets of the</p>
<p>data (e.g., per key) to spread window completion load more evenly over</p>
<p>time, which instead is an example of <em>unaligned</em> windows because they</p>
<p>vary across the data.</p>
<ul>
<li>Sliding windows (aka hopping windows)</li>
</ul>
<p>A generalization of fixed windows, sliding windows are defined by a</p>
<p>fixed length and a fixed period. If the period is less than the length, the</p>
<p>windows overlap. If the period equals the length, you have fixed</p>
<p>windows. And if the period is greater than the length, you have a weird</p>
<p>sort of sampling window that looks only at subsets of the data over time.</p>
<p>As with fixed windows, sliding windows are typically aligned, though</p>
<p>they can be unaligned as a performance optimization in certain use cases.</p>
<p>Note that the sliding windows in Figure 1-8 are drawn as they are to give</p>
<p>a sense of sliding motion; in reality, all five windows would apply across</p>
<p>the entire dataset.</p>
<ul>
<li>Sessions</li>
</ul>
<p>An example of dynamic windows, sessions are composed of sequences of</p>
<p>events terminated by a gap of inactivity greater than some timeout.</p>
<p>Sessions are commonly used for analyzing user behavior over time, by</p>
<p>grouping together a series of temporally related events (e.g., a sequence of</p>
<p>videos viewed in one sitting). Sessions are interesting because their</p>
<p>lengths cannot be defined a priori; they are dependent upon the actual data</p>
<p>involved. They’re also the canonical example of unaligned windows</p>
<p>because sessions are practically never identical across different subsets of</p>
<p>data (e.g., different users).</p>
<p>The two domains of time we discussed earlier (processing time and event</p>
<p>time) are essentially the two we care about. Windowing makes sense in both</p>
<p>domains, so let’s look at each in detail and see how they differ. Because</p>
<p>processing-time windowing has historically been more common, we’ll start</p>
<p>there.</p>
<p><em><strong>Windowing by processing time</strong></em></p>
<p>When windowing by processing time, the system essentially buffers up</p>
<p>incoming data into windows until some amount of processing time has</p>
<p>passed. For example, in the case of five-minute fixed windows, the system</p>
<p>would buffer data for five minutes of processing time, after which it would</p>
<p>treat all of the data it had observed in those five minutes as a window and</p>
<p>send them downstream for processing.</p>
<p><em>Figure 1-9. Windowing into fixed windows by processing time. Data are collected into windows based</em></p>
<p><em>on the order they arrive in the pipeline.</em></p>
<p>There are a few nice properties of processing-time windowing:</p>
<ul>
<li>It’s simple. The implementation is extremely straightforward because</li>
</ul>
<p>you never worry about shuffling data within time. You just buffer</p>
<p>things as they arrive and send them downstream when the window</p>
<p>closes.</p>
<ul>
<li>Judging window completeness is straightforward. Because the</li>
</ul>
<p>system has perfect knowledge of whether all inputs for a window</p>
<p>have been seen, it can make perfect decisions about whether a given</p>
<p>window is complete. This means there is no need to be able to deal</p>
<p>with “late” data in any way when windowing by processing time.</p>
<ul>
<li>If you’re wanting to infer information about the source <em>as it is</em></li>
</ul>
<p><em>observed</em>, processing-time windowing is exactly what you want.</p>
<p>Many monitoring scenarios fall into this category. Imagine tracking</p>
<p>the number of requests per second sent to a global-scale web service.</p>
<p>Calculating a rate of these requests for the purpose of detecting</p>
<p>outages is a perfect use of processing-time windowing.</p>
<p>Good points aside, there is one very big downside to processing-time</p>
<p>windowing: <em>if the data in question have event times associated with them,</em></p>
<p><em>those data must arrive in event-time order if the processing-time windows are</em></p>
<p><em>to reflect the reality of when those events actually happened.</em> Unfortunately,</p>
<p>event-time ordered data are uncommon in many real-world, distributed input</p>
<p>sources.</p>
<p>As a simple example, imagine any mobile app that gathers usage statistics for</p>
<p>later processing. For cases in which a given mobile device goes offline for</p>
<p>any amount of time (brief loss of connectivity, airplane mode while flying</p>
<p>across the country, etc.), the data recorded during that period won’t be</p>
<p>uploaded until the device comes online again. This means that data might</p>
<p>arrive with an event-time skew of minutes, hours, days, weeks, or more. It’s</p>
<p>essentially impossible to draw any sort of useful inferences from such a</p>
<p>dataset when windowed by processing time.</p>
<p>As another example, many distributed input sources might <em>seem</em> to provide</p>
<p>event-time ordered (or very nearly so) data when the overall system is</p>
<p>healthy. Unfortunately, the fact that event-time skew is low for the input</p>
<p>source when healthy does not mean it will always stay that way. Consider a</p>
<p>global service that processes data collected on multiple continents. If network</p>
<p>issues across a bandwidth-constrained transcontinental line (which, sadly, are</p>
<p>surprisingly common) further decrease bandwidth and&#x2F;or increase latency,</p>
<p>suddenly a portion of your input data might begin arriving with much greater</p>
<p>skew than before. If you are windowing those data by processing time, your</p>
<p>windows are no longer representative of the data that actually occurred within</p>
<p>them; instead, they represent the windows of time as the events arrived at the</p>
<p>processing pipeline, which is some arbitrary mix of old and current data.</p>
<p>What we really want in both of those cases is to window data by their event</p>
<p>times in a way that is robust to the order of arrival of events. What we really</p>
<p>want is event-time windowing.</p>
<p><em><strong>Windowing by event time</strong></em></p>
<p>Event-time windowing is what you use when you need to observe a data</p>
<p>source in finite chunks that reflect the times at which those events actually</p>
<p>happened. It’s the gold standard of windowing. Prior to 2016, most data</p>
<p>processing systems in use lacked native support for it (though any system</p>
<p>with a decent consistency model, like Hadoop or Spark Streaming 1.x, could</p>
<p>act as a reasonable substrate for building such a windowing system). I’m</p>
<p>happy to say that the world of today looks very different, with multiple</p>
<p>systems, from Flink to Spark to Storm to Apex, natively supporting event</p>
<p>time windowing of some sort.</p>
<p>Figure 1-10 shows an example of windowing an unbounded source into one</p>
<p>hour fixed windows.</p>
<p><em>Figure 1-10. Windowing into fixed windows by event time. Data are collected into windows based on</em></p>
<p><em>the times at which they occurred. The black arrows call out example data that arrived in processing</em></p>
<p><em>time windows that differed from the event-time windows to which they belonged.</em></p>
<p>The black arrows in Figure 1-10 call out two particularly interesting pieces of</p>
<p>data. Each arrived in processing-time windows that did not match the event</p>
<p>time windows to which each bit of data belonged. As such, if these data had</p>
<p>been windowed into processing-time windows for a use case that cared about</p>
<p>event times, the calculated results would have been incorrect. As you would</p>
<p>expect, event-time correctness is one nice thing about using event-time</p>
<p>windows.</p>
<p>Another nice thing about event-time windowing over an unbounded data</p>
<p>source is that you can create dynamically sized windows, such as sessions,</p>
<p>without the arbitrary splits observed when generating sessions over fixed</p>
<p>windows (as we saw previously in the sessions example from “Unbounded</p>
<p>Data: Streaming”), as demonstrated in Figure 1-11.</p>
<p><em>Figure 1-11. Windowing into session windows by event time. Data are collected into session windows</em></p>
<p><em>capturing bursts of activity based on the times that the corresponding events occurred. The black</em></p>
<p><em>arrows again call out the temporal shuffle necessary to put the data into their correct event-time</em></p>
<p><em>locations.</em></p>
<p>Of course, powerful semantics rarely come for free, and event-time windows</p>
<p>are no exception. Event-time windows have two notable drawbacks due to the</p>
<p>fact that windows must often live longer (in processing time) than the actual</p>
<p>length of the window itself:</p>
<ul>
<li>Buffering</li>
</ul>
<p>Due to extended window lifetimes, more buffering of data is required.</p>
<p>Thankfully, persistent storage is generally the cheapest of the resource</p>
<p>types most data processing systems depend on (the others being primarily</p>
<p>CPU, network bandwidth, and RAM). As such, this problem is typically</p>
<p>much less of a concern than you might think when using any well</p>
<p>designed data processing system with strongly consistent persistent state</p>
<p>and a decent in-memory caching layer. Also, many useful aggregations do</p>
<p>not require the entire input set to be buffered (e.g., sum or average), but</p>
<p>instead can be performed incrementally, with a much smaller,</p>
<p>intermediate aggregate stored in persistent state.</p>
<ul>
<li>Completeness</li>
</ul>
<p>Given that we often have no good way of knowing when we’ve seen all of</p>
<p>the data for a given window, how do we know when the results for the</p>
<p>window are ready to materialize? In truth, we simply don’t. For many</p>
<p>types of inputs, the system can give a reasonably accurate heuristic</p>
<p>estimate of window completion via something like the watermarks found</p>
<p>in MillWheel, Cloud Dataflow, and Flink (which we talk about more in</p>
<p>Chapters 3 and 4). But for cases in which absolute correctness is</p>
<p>paramount (again, think billing), the only real option is to provide a way</p>
<p>for the pipeline builder to express when they want results for windows to</p>
<p>be materialized and how those results should be refined over time.</p>
<p>Dealing with window completeness (or lack thereof) is a fascinating topic</p>
<p>but one perhaps best explored in the context of concrete examples, which</p>
<p>we look at next.</p>
<h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>Whew! That was a lot of information. If you’ve made it this far, you are to be</p>
<p>commended! But we are only just getting started. Before forging ahead to</p>
<p>looking in detail at the Beam Model approach, let’s briefly step back and</p>
<p>recap what we’ve learned so far. In this chapter, we’ve done the following:</p>
<ul>
<li>Clarified terminology, focusing the definition of “streaming” to refer</li>
</ul>
<p>to systems built with unbounded data in mind, while using more</p>
<p>descriptive terms like approximate&#x2F;speculative results for distinct</p>
<p>concepts often categorized under the “streaming” umbrella.</p>
<p>Additionally, we highlighted two important dimensions of large</p>
<p>scale datasets: cardinality (i.e., bounded versus unbounded) and</p>
<p>encoding (i.e., table versus stream), the latter of which will consume</p>
<p>much of the second half of the book.</p>
<ul>
<li>Assessed the relative capabilities of well-designed batch and</li>
</ul>
<p>streaming systems, positing streaming is in fact a strict superset of</p>
<p>batch, and that notions like the Lambda Architecture, which are</p>
<p>predicated on streaming being inferior to batch, are destined for</p>
<p>retirement as streaming systems mature.</p>
<ul>
<li>Proposed two high-level concepts necessary for streaming systems to</li>
</ul>
<p>both catch up to and ultimately surpass batch, those being</p>
<p>correctness and tools for reasoning about time, respectively.</p>
<ul>
<li>Established the important differences between event time and</li>
</ul>
<p>processing time, characterized the difficulties those differences</p>
<p>impose when analyzing data in the context of when they occurred,</p>
<p>and proposed a shift in approach away from notions of completeness</p>
<p>and toward simply adapting to changes in data over time.</p>
<ul>
<li>Looked at the major data processing approaches in common use</li>
</ul>
<p>today for bounded and unbounded data, via both batch and streaming</p>
<p>engines, roughly categorizing the unbounded approaches into: time</p>
<p>agnostic, approximation, windowing by processing time, and</p>
<p>windowing by event time.</p>
<p>Next up, we dive into the details of the Beam Model, taking a conceptual look</p>
<p>at how we’ve broken up the notion of data processing across four related</p>
<p>axes: what, where, when, and how. We also take a detailed look at processing</p>
<p>a simple, concrete example dataset across multiple scenarios, highlighting the</p>
<p>plurality of use cases enabled by the Beam Model, with some concrete APIs</p>
<p>to ground us in reality. These examples will help drive home the notions of</p>
<p>event time and processing time introduced in this chapter while additionally</p>
<p>exploring new concepts such as watermarks.</p>
<ol>
<li>For completeness, it’s perhaps worth calling out that this definition includes</li>
</ol>
<p>both true streaming as well as microbatch implementations. For those of you</p>
<p>who aren’t familiar with microbatch systems, they are streaming systems that</p>
<p>use repeated executions of a batch processing engine to process unbounded</p>
<p>data. Spark Streaming is the canonical example in the industry.</p>
<ol>
<li>Readers familiar with my original “Streaming 101” article might recall that I</li>
</ol>
<p>rather emphatically encouraged the abandonment of the term “stream” when</p>
<p>referring to datasets. That never caught on, which I initially thought was due</p>
<p>to its catchiness and pervasive existing usage. In retrospect, however, I think I</p>
<p>was simply wrong. There actually is great value in distinguishing between the</p>
<p>two different types of dataset constitutions: tables and streams. Indeed, most</p>
<p>of the second half of this book is dedicated to understanding the relationship</p>
<p>between those two.</p>
<ol>
<li>If you’re unfamiliar with what I mean when I say <em>exactly-once</em>, it’s referring</li>
</ol>
<p>to a specific type of consistency guarantee that certain data processing</p>
<p>frameworks provide. Consistency guarantees are typically bucketed into three</p>
<p>main classes: at-most-once processing, at-least-once processing, and exactly</p>
<p>once processing. Note that the names in use here refer to the effective</p>
<p>semantics as observed within the outputs generated by the pipeline, not the</p>
<p>actual number of times a pipeline might process (or attempt to process) any</p>
<p>given record. For this reason, the term <em>effectively-once</em> is sometimes used</p>
<p>instead of exactly-once, since it’s more representative of the underlying</p>
<p>nature of things. Reuven covers these concepts in much more detail in</p>
<p>Chapter 5.</p>
<ol>
<li>Since the original publication of “Streaming 101,” numerous individuals</li>
</ol>
<p>have pointed out to me that it would have been more intuitive to place</p>
<p>processing time on the x-axis and event time on the y-axis. I do agree that</p>
<p>swapping the two axes would initially feel more natural, as event time seems</p>
<p>like the dependent variable to processing time’s independent variable.</p>
<p>However, because both variables are monotonic and intimately related,</p>
<p>they’re effectively interdependent variables. So I think from a technical</p>
<p>perspective you just have to pick an axis and stick with it. Math is confusing</p>
<p>(especially outside of North America, where it suddenly becomes plural and</p>
<p>gangs up on you).</p>
<ol>
<li>This result really shouldn’t be surprising (but was for me, hence why I’m</li>
</ol>
<p>pointing it out), because we’re effectively creating a right triangle with the</p>
<p>ideal line when measuring the two types of skew&#x2F;lag. Maths are cool.</p>
<ol>
<li>We look at aligned fixed windows in detail in Chapter 2, and unaligned</li>
</ol>
<p>fixed windows in Chapter 4.</p>
<ol>
<li>If you poke around enough in the academic literature or SQL-based</li>
</ol>
<p>streaming systems, you’ll also come across a third windowing time domain:</p>
<p><em>tuple-based windowing</em> (i.e., windows whose sizes are counted in numbers of</p>
<p>elements). However, tuple-based windowing is essentially a form of</p>
<p>processing-time windowing in which elements are assigned monotonically</p>
<p>increasing timestamps as they arrive at the system. As such, we won’t discuss</p>
<p>tuple-based windowing in detail any further.</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Wang Wei
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www6v.github.io/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/" title="《Streaming System》- Chapter 1. Streaming 101[完整]">https://www6v.github.io/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/www6vHomeHexo/tags/Streaming-System/" rel="tag"># Streaming System</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/" rel="prev" title="《Streaming System》- 第一章：流处理入门[完整]">
      <i class="fa fa-chevron-left"></i> 《Streaming System》- 第一章：流处理入门[完整]
    </a></div>
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/04/19/streamingSystemTerminology/" rel="next" title="《Streaming System》 术语">
      《Streaming System》 术语 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wang Wei</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/www6vHomeHexo/archives/">
        
          <span class="site-state-item-count">531</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/www6vHomeHexo/categories/">
          
        <span class="site-state-item-count">319</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/www6vHomeHexo/tags/">
          
        <span class="site-state-item-count">178</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/www6v" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;www6v" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:www6v@126.com" title="E-Mail → mailto:www6v@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/www6vHomeHexo/lib/anime.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.ui.min.js"></script>

<script src="/www6vHomeHexo/js/utils.js"></script>

<script src="/www6vHomeHexo/js/motion.js"></script>


<script src="/www6vHomeHexo/js/schemes/muse.js"></script>


<script src="/www6vHomeHexo/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/www6vHomeHexo/js/local-search.js"></script>













  

  

</body>
</html>
