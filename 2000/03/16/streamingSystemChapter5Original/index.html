<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/www6vHomeHexo/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/www6vHomeHexo/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/www6vHomeHexo/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/www6vHomeHexo/images/logo.svg" color="#222">

<link rel="stylesheet" href="/www6vHomeHexo/css/main.css">


<link rel="stylesheet" href="/www6vHomeHexo/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www6v.github.io","root":"/www6vHomeHexo/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="《Streaming System》-Chapter 5. Exactly-Once and Side Effects">
<meta property="og:url" content="https://www6v.github.io/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/index.html">
<meta property="og:site_name" content="www6v的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2000-03-16T11:27:27.000Z">
<meta property="article:modified_time" content="2023-03-17T03:12:59.226Z">
<meta property="article:author" content="Wang Wei">
<meta property="article:tag" content="Streaming System">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www6v.github.io/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《Streaming System》-Chapter 5. Exactly-Once and Side Effects | www6v的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/www6vHomeHexo/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">www6v的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录技术点滴</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/www6vHomeHexo/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/www6vHomeHexo/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/www6vHomeHexo/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/www6vHomeHexo/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www6v.github.io/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/www6vHomeHexo/images/avatar.gif">
      <meta itemprop="name" content="Wang Wei">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="www6v的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Streaming System》-Chapter 5. Exactly-Once and Side Effects
        </h1>

        <div class="post-meta">





          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2000-03-16 19:27:27" itemprop="dateCreated datePublished" datetime="2000-03-16T19:27:27+08:00">2000-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-03-17 11:12:59" itemprop="dateModified" datetime="2023-03-17T11:12:59+08:00">2023-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/www6vHomeHexo/categories/Streaming-System/" itemprop="url" rel="index"><span itemprop="name">Streaming System</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#why-exactly-once-matters"><strong>Why Exactly Once Matters</strong></a></li>
<li><a href="#accuracy-versus-completeness"><strong>Accuracy Versus Completeness</strong></a><ul>
<li><a href="#side-effects"><strong>Side Effects</strong></a></li>
<li><a href="#problem-definition"><strong>Problem Definition</strong></a></li>
</ul>
</li>
<li><a href="#ensuring-exactly-once-in-shuffle"><strong>Ensuring Exactly Once in Shuffle</strong></a></li>
<li><a href="#addressing-determinism"><strong>Addressing Determinism</strong></a></li>
<li><a href="#performance"><strong>Performance</strong></a><ul>
<li><a href="#graph-optimization"><strong>Graph Optimization</strong></a></li>
<li><a href="#bloom-filters"><strong>Bloom Filters</strong></a></li>
<li><a href="#garbage-collection"><strong>Garbage Collection</strong></a></li>
</ul>
</li>
<li><a href="#exactly-once-in-sources"><strong>Exactly Once in Sources</strong></a></li>
<li><a href="#exactly-once-in-sinks"><strong>Exactly Once in Sinks</strong></a></li>
</ul>
<!-- tocstop -->

</div>


<details><summary>点击 原文</summary><h1><span id="why-exactly-once-matters"><strong>Why Exactly Once Matters</strong></span><a href="#why-exactly-once-matters" class="header-anchor">#</a></h1><p>It almost goes without saying that for many users, any risk of dropped records</p>
<p>or data loss in their data processing pipelines is unacceptable. Even so,</p>
<p>historically many general-purpose streaming systems made no guarantees</p>
<p>about record processing—all processing was “best effort” only. Other systems</p>
<p>provided at-least-once guarantees, ensuring that records were always</p>
<p>processed at least once, but records might be duplicated (and thus result in</p>
<p>inaccurate aggregations); in practice, many such at-least-once systems</p>
<p>performed aggregations in memory, and thus their aggregations could still be</p>
<p>lost when machines crashed. These systems were used for low-latency,</p>
<p>speculative results but generally could guarantee nothing about the veracity of</p>
<p>these results.</p>
<p>As Chapter 1 points out, this led to a strategy that was coined the <em>Lambda</em></p>
<p><em>Architecture</em>—run a streaming system to get fast, but inaccurate results.</p>
<p>Sometime later (often after end of day), a batch system runs to the correct</p>
<p>answer. This works only if the data stream is replayable; however, this was</p>
<p>true for enough data sources that this strategy proved viable. Nonetheless,</p>
<p>many people who tried this experienced a number of issues with the Lambda</p>
<p>Architecture:</p>
<ul>
<li>Inaccuracy</li>
</ul>
<p>Users tend to underestimate the impact of failures. They often assume that</p>
<p>a small percentage of records will be lost or duplicated (often based on</p>
<p>experiments they ran), and are shocked on that one bad day when 10% (or</p>
<p>more!) of records are lost or are duplicated. In a sense, such systems</p>
<p>provide only “half” a guarantee—and without a full one, anything is</p>
<p>possible.</p>
<ul>
<li>Inconsistency</li>
</ul>
<p>The batch system used for the end-of-day calculation often has different</p>
<p>data semantics than the streaming system. Getting the two pipelines to</p>
<p>produce comparable results proved more difficult than initially thought.</p>
<ul>
<li>Complexity</li>
</ul>
<p>By definition, Lambda requires you to write and maintain two different</p>
<p>codebases. You also must run and maintain two complex distributed</p>
<p>systems, each with different failure modes. For anything but the simplest</p>
<p>of pipelines, this quickly becomes overwhelming.</p>
<ul>
<li>Unpredictability</li>
</ul>
<p>In many use cases, end users will see streaming results that differ from the</p>
<p>daily results by an uncertain amount, which can change randomly. In</p>
<p>these cases, users will stop trusting the streaming data and wait for daily</p>
<p>batch results instead, thus destroying the value of getting low-latency</p>
<p>results in the first place.</p>
<ul>
<li>Latency</li>
</ul>
<p>Some business use cases <em>require</em> low-latency correct results, which the</p>
<p>Lambda Architecture does not provide by design.</p>
<p>Fortunately, many Beam runners can do much better. In this chapter, we</p>
<p>explain how exactly-once stream processing helps users count on accurate</p>
<p>results and avoid the risk of data loss while relying on a single codebase and</p>
<p>API. Because a variety of issues that can affect a pipeline’s output are often</p>
<p>erroneously conflated with exactly-once guarantees, we first explain precisely</p>
<p>which issues are in and out of scope when we refer to “exactly once” in the</p>
<p>context of Beam and data processing.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="accuracy-versus-completeness"><strong>Accuracy Versus Completeness</strong></span><a href="#accuracy-versus-completeness" class="header-anchor">#</a></h1><p>Whenever a Beam pipeline processes a record for a pipeline, we want to</p>
<p>ensure that the record is never dropped or duplicated. However, the nature of</p>
<p>streaming pipelines is such that records sometimes show up late, after</p>
<p>aggregates for their time windows have already been processed. The Beam</p>
<p>SDK allows the user to configure how long the system should wait for late</p>
<p>data to arrive; any (and only) records arriving later than this deadline are</p>
<p>dropped. This feature contributes to <em>completeness</em>, not to accuracy: all records</p>
<p>that showed up in time for processing are accurately processed exactly once,</p>
<p>whereas these late records are explicitly dropped.</p>
<p>Although late records are usually discussed in the context of streaming</p>
<p>systems, it’s worth noting that batch pipelines have similar completeness</p>
<p>issues. For example, a common batch paradigm is to run a job at 2 AM over</p>
<p>all the previous day’s data. However, if some of yesterday’s data wasn’t</p>
<p>collected until after 2 AM, it won’t be processed by the batch job! Thus, batch</p>
<p>pipelines also provide accurate but not always complete results.</p>
<h3><span id="side-effects"><strong>Side Effects</strong></span><a href="#side-effects" class="header-anchor">#</a></h3><p>One characteristic of Beam and Dataflow is that users inject custom code that</p>
<p>is executed as part of their pipeline graph. Dataflow does <em>not</em> guarantee that</p>
<p>this code is run only once per record,  whether by the streaming or batch</p>
<p>runner. It might run a given record through a user transform multiple times, or</p>
<p>it might even run the same record simultaneously on multiple workers; this is</p>
<p>necessary to guarantee at-least-once processing in the face of worker failures.</p>
<p>Only one of these invocations can “win” and produce output further down the</p>
<p>pipeline.</p>
<p>As a result, nonidempotent side effects are not guaranteed to execute exactly</p>
<p>once; if you write code that has side effects external to the pipeline, such as</p>
<p>contacting an outside service, these effects might be executed more than once</p>
<p>for a given record. This situation is usually unavoidable because there is no</p>
<p>way to atomically commit Dataflow’s processing with the side effect on the</p>
<p>external service. Pipelines do need to eventually send results to the outside</p>
<p>world, and such calls might not be idempotent. As you will see later in the</p>
<p>chapter, often such sinks are able to add an extra stage to restructure the call</p>
<p>into an idempotent operation first.</p>
<h3><span id="problem-definition"><strong>Problem Definition</strong></span><a href="#problem-definition" class="header-anchor">#</a></h3><p>So, we’ve given a couple of examples of what we’re <em>not</em> talking about. What</p>
<p>do we mean then by exactly-once processing? To motivate this, let’s begin</p>
<p>with a simple streaming pipeline,  shown in Example 5-1.</p>
<p><em>Example 5-1. A simple streaming pipeline</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Pipeline p = Pipeline.create(options);</span><br><span class="line">// Calculate 1-minute counts of events per user.</span><br><span class="line">PCollection&lt;..&gt; perUserCounts =</span><br><span class="line">p.apply(ReadFromUnboundedSource.read())</span><br><span class="line">.apply(new KeyByUser())</span><br><span class="line">.Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1)))</span><br><span class="line">.apply(Count.perKey());</span><br><span class="line">// Process these per-user counts, and write the output somewhere.</span><br><span class="line">perUserCounts.apply(new ProcessPerUserCountsAndWriteToSink());</span><br><span class="line">// Add up all these per-user counts to get 1-minute counts of all events.</span><br><span class="line">perUserCounts.apply(Values.&lt;..&gt;create())</span><br><span class="line">.apply(Count.globally())</span><br><span class="line">.apply(new ProcessGlobalCountAndWriteToSink());</span><br><span class="line">p.run();</span><br></pre></td></tr></table></figure>

<p>This pipeline computes two different windowed aggregations. The first counts</p>
<p>how many events came from each individual user over the course of a minute,</p>
<p>and the second counts how many total events came in each minute. Both</p>
<p>aggregations are written to unspecified streaming sinks.</p>
<p>Remember that Dataflow executes pipelines on many different workers in</p>
<p>parallel. After each GroupByKey (the Count operations use GroupByKey under</p>
<p>the covers), all records with the same key are processed on the same machine</p>
<p>following a process called <em>shuffle</em>. The Dataflow workers shuffle data</p>
<p>between themselves using Remote Procedure Calls (RPCs), ensuring that</p>
<p>records for a given key all end up on the same machine.</p>
<p>Figure 5-1 shows the shuffles that Dataflow creates for the pipeline in</p>
<p>Example 5-1.  The Count.perKey shuffles all the data for each user onto a</p>
<p>given worker, whereas the Count.globally shuffles all these partial counts</p>
<p>to a single worker to calculate the global sum.</p>
<p><em>Figure 5-1. Shuffles in a pipeline</em></p>
<p>For Dataflow to accurately process data, this shuffle process must ensure that</p>
<p>every record is shuffled exactly once. As you will see in a moment, the</p>
<p>distributed nature of shuffle makes this a challenging problem.</p>
<p>This pipeline also both reads and writes data from and to the outside world, so</p>
<p>Dataflow must ensure that this interaction does not introduce any</p>
<p>inaccuracies. Dataflow has always supported this task—what Apache Spark</p>
<p>and Apache Flink call <em>end-to-end exactly once</em>—for sources and sinks</p>
<p>whenever technically feasible.</p>
<p>The focus of this chapter will be on three things:</p>
<ul>
<li>Shuffle</li>
</ul>
<p>How Dataflow guarantees that every record is shuffled exactly once.</p>
<ul>
<li>Sources</li>
</ul>
<p>How Dataflow guarantees that every source record is processed exactly</p>
<p>once.</p>
<ul>
<li>Sinks</li>
</ul>
<p>How Dataflow guarantees that every sink produces accurate output.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="ensuring-exactly-once-in-shuffle"><strong>Ensuring Exactly Once in Shuffle</strong></span><a href="#ensuring-exactly-once-in-shuffle" class="header-anchor">#</a></h1><p>As just explained, Dataflow’s streaming shuffle uses RPCs. Now, any time</p>
<p>you have two machines communicating via RPC, you should think long and</p>
<p>hard about data integrity. First of all, RPCs can fail for many reasons. The</p>
<p>network might be interrupted, the RPC might time out before completing, or</p>
<p>the receiving server might decide to fail the call. To guarantee that records are</p>
<p>not lost in shuffle, Dataflow employs <em>upstream backup</em>. This simply means</p>
<p>that the sender will retry RPCs until it receives positive acknowledgment of</p>
<p>receipt. Dataflow also ensures that it will continue retrying these RPCs even if</p>
<p>the sender crashes. This guarantees that every record is delivered <em>at least</em></p>
<p><em>once</em>.</p>
<p>Now, the problem is that these retries might themselves create duplicates.</p>
<p>Most RPC frameworks, including the one Dataflow uses, provide the sender</p>
<p>with a status indicating success or failure. In a distributed system, you need to</p>
<p>be aware that RPCs can sometimes succeed even when they have appeared to</p>
<p>fail. There are many reasons for this: race conditions with the RPC timeout,</p>
<p>positive acknowledgment from the server failing to transfer even though the</p>
<p>RPC succeeded, and so on. The only status that a sender can really trust is a</p>
<p>successful one.</p>
<p>An RPC returning a failure status generally indicates that the call might or</p>
<p>might not have succeeded. Although specific error codes can communicate</p>
<p>unambiguous failure, many common RPC failures, such as Deadline</p>
<p>Exceeded, are ambiguous. In the case of streaming shuffle, retrying an RPC</p>
<p>that really succeeded means delivering a record twice! Dataflow needs some</p>
<p>way of detecting and removing these duplicates.</p>
<p>At a high level, the algorithm for this task is quite simple (see Figure 5-2):</p>
<p>every message sent is tagged with a unique identifier. Each receiver stores a</p>
<p>catalog of all identifiers that have already been seen and processed. Every</p>
<p>time a record is received, its identifier is looked up in this catalog. If it is</p>
<p>found, the record is dropped as a duplicate. Because Dataflow is built on top</p>
<p>of a scalable key&#x2F;value store, this store is used to hold the deduplication</p>
<p>catalog.</p>
<p><em>Figure 5-2. Detecting duplicates in shuffle</em></p>
<h1><span id="addressing-determinism"><strong>Addressing Determinism</strong></span><a href="#addressing-determinism" class="header-anchor">#</a></h1><p>Making this strategy work in the real world requires a lot of care, however.</p>
<p>One immediate wrinkle is that the Beam Model allows for user code to</p>
<p>produce nondeterministic output. This means that a ParDo can execute twice</p>
<p>on the same input record (due to a retry), yet produce different output on each</p>
<p>retry. The desired behavior is that only one of those outputs will commit into</p>
<p>the pipeline; however, the nondeterminism involved makes it difficult to</p>
<p>guarantee that both outputs have the same deterministic ID. Even trickier, a</p>
<p>ParDo can output multiple records, so each of these retries might produce a</p>
<p>different number of outputs!</p>
<p>So, why don’t we simply require that all user processing be deterministic?</p>
<p>Our experience is that in practice, many pipelines require nondeterministic</p>
<p>transforms And all too often, pipeline authors do not realize that the code they</p>
<p>wrote is nondeterministic. For example, consider a transform that looks up</p>
<p>supplemental data in Cloud Bigtable in order to enrich its input data. This is a</p>
<p>nondeterministic task, as the external value might change in between retries</p>
<p>of the transform. Any code that relies on current time is likewise not</p>
<p>deterministic. We have also seen transforms that need to rely on random</p>
<p>number generators. And even if the user code is purely deterministic, any</p>
<p>event-time aggregation that allows for late data might have nondeterministic</p>
<p>inputs.</p>
<p>Dataflow addresses this issue by using checkpointing to make</p>
<p>nondeterministic processing effectively deterministic. Each output from a</p>
<p>transform is checkpointed, together with its unique ID, to stable storage</p>
<p><em>before</em> being delivered to the next stage.  Any retries in the shuffle delivery</p>
<p>simply replay the output that has been checkpointed—the user’s</p>
<p>nondeterministic code is not run again on retry. To put it another way, the</p>
<p>user’s code may be run multiple times but only one of those runs can “win.”</p>
<p>Furthermore, Dataflow uses a consistent store that allows it to prevent</p>
<p>duplicates from being written to stable storage.</p>
</details>




<details><summary>点击 原文</summary><h1><span id="performance"><strong>Performance</strong></span><a href="#performance" class="header-anchor">#</a></h1><p>To implement exactly-once shuffle delivery, a catalog of record IDs is stored</p>
<p>in each receiver key. For every record that arrives, Dataflow looks up the</p>
<p>catalog of IDs already seen to determine whether this record is a duplicate.</p>
<p>Every output from step to step is checkpointed to storage to ensure that the</p>
<p>generated record IDs are stable.</p>
<p>However, unless implemented carefully, this process would significantly</p>
<p>degrade pipeline performance for customers by creating a huge increase in</p>
<p>reads and writes. Thus, for exactly-once processing to be viable for Dataflow</p>
<p>users, that I&#x2F;O has to be reduced, in particular by preventing I&#x2F;O on every</p>
<p>record.</p>
<p>Dataflow achieves this goal via two key techniques: <em>graph optimization</em> and</p>
<p><em>Bloom filters</em>.</p>
<h3><span id="graph-optimization"><strong>Graph Optimization</strong></span><a href="#graph-optimization" class="header-anchor">#</a></h3><p>The Dataflow service runs a series of optimizations on the pipeline graph</p>
<p>before executing it. One such optimization is <em>fusion</em>, in which the service</p>
<p>fuses many logical steps into a single execution stage. Figure 5-3 shows some</p>
<p>simple examples.</p>
<p><em>Figure 5-3. Example optimizations: fusion</em></p>
<p>All fused steps are run as an in-process unit, so there’s no need to store</p>
<p>exactly-once data for each of them. In many cases, fusion reduces the entire</p>
<p>graph down to a few physical steps, greatly reducing the amount of data</p>
<p>transfer needed (and saving on state usage, as well).</p>
<p>Dataflow also optimizes associative and commutative Combine operations</p>
<p>(such as Count and Sum) by performing partial combining locally before</p>
<p>sending the data to the main grouping operation, as illustrated in Figure 5-4.</p>
<p>This approach can greatly reduce the number of messages for delivery,</p>
<p>consequently also reducing the number of reads and writes.</p>
<p><em>Figure 5-4. Example optimizations: combiner lifting</em></p>
<h3><span id="bloom-filters"><strong>Bloom Filters</strong></span><a href="#bloom-filters" class="header-anchor">#</a></h3><p>The aforementioned optimizations are general techniques that improve</p>
<p>exactly-once performance as a byproduct. For an optimization aimed strictly</p>
<p>at improving exactly-once processing, we turn to <em>Bloom filters</em>.</p>
<p>In a healthy pipeline, most arriving records will not be duplicates. We can use</p>
<p>that fact to greatly improve performance via Bloom filters, which are compact</p>
<p>data structures that allow for quick set-membership checks. Bloom filters</p>
<p>have a very interesting property: they can return false positives but never false</p>
<p>negatives. If the filter says “Yes, the element is in the set,” we know that the</p>
<p>element is <em>probably</em> in the set (with a probability that can be calculated).</p>
<p>However, if the filter says an element is <em>not</em> in the set, it definitely isn’t. This</p>
<p>function is a perfect fit for the task at hand.</p>
<p>The implementation in Dataflow works like this: each worker keeps a Bloom</p>
<p>filter of every ID it has seen. Whenever a new record ID shows up, it looks it</p>
<p>up in the filter. If the filter returns false, this record is not a duplicate and the</p>
<p>worker can skip the more expensive lookup from stable storage. It needs to do</p>
<p>that second lookup only if the Bloom filter returns true, but as long as the</p>
<p>filter’s false-positive rate is low, that step is rarely needed.</p>
<p>Bloom filters tend to fill up over time, however, and as that happens, the</p>
<p>false-positive rate increases. We also need to construct this Bloom filter anew</p>
<p>any time a worker restarts by scanning the ID catalog stored in state.</p>
<p>Helpfully, Dataflow attaches a system timestamp to each record.  Thus,</p>
<p>instead of creating a single Bloom filter, the service creates a separate one for</p>
<p>every 10-minute range. When a record arrives, Dataflow queries the</p>
<p>appropriate filter based on the system timestamp. This step prevents the</p>
<p>Bloom filters from saturating because filters are garbage-collected over time,</p>
<p>and it also bounds the amount of data that needs to be scanned at startup.</p>
<p>Figure 5-5 illustrates this process: records arrive in the system and are</p>
<p>delegated to a Bloom filter based on their arrival time. None of the records</p>
<p>hitting the first filter are duplicates, and all of their catalog lookups are</p>
<p>filtered. Record r1 is delivered a second time, so a catalog lookup is needed</p>
<p>to verify that it is indeed a duplicate; the same is true for records r4 and r6.</p>
<p>Record r8 is not a duplicate; however, due to a false positive in its Bloom</p>
<p>filter, a catalog lookup is generated (which will determine that r8 is not a</p>
<p>duplicate and should be processed).</p>
<p><em>Figure 5-5. Exactly-once Bloom filters</em></p>
<h3><span id="garbage-collection"><strong>Garbage Collection</strong></span><a href="#garbage-collection" class="header-anchor">#</a></h3><p>Every Dataflow worker persistently stores a catalog of unique record IDs it</p>
<p>has seen. As Dataflow’s state and consistency model is per-key, in reality</p>
<p>each key stores a catalog of records that have been delivered to that key. We</p>
<p>can’t store these identifiers forever, or all available storage will eventually fill</p>
<p>up. To avoid that issue, you need garbage collection of acknowledged record</p>
<p>IDs.</p>
<p>One strategy for accomplishing this goal would be for senders to tag each</p>
<p>record with a strictly increasing sequence number in order to track the earliest</p>
<p>sequence number still in flight (corresponding to an unacknowledged record</p>
<p>delivery). Any identifier in the catalog with an earlier sequence number could</p>
<p>then be garbage-collected because all earlier records have already been</p>
<p>acknowledged.</p>
<p>There is a better alternative, however. As previously mentioned, Dataflow</p>
<p>already tags each record with a system timestamp that is used for bucketing</p>
<p>exactly-once Bloom filters. Consequently, instead of using sequence numbers</p>
<p>to garbage-collect the exactly-once catalog, Dataflow calculates a garbage</p>
<p>collection watermark based on these system timestamps (this is the</p>
<p>processing-time watermark discussed in Chapter 3). A nice side benefit of this</p>
<p>approach is that because this watermark is based on the amount of physical</p>
<p>time spent waiting in a given stage (unlike the data watermark, which is based</p>
<p>on custom event times), it provides intuition on what parts of the pipeline are</p>
<p>slow. This metadata is the basis for the System Lag metric shown in the</p>
<p>Dataflow WebUI.</p>
<p>What happens if a record arrives with an old timestamp and we’ve already</p>
<p>garbage-collected identifiers for this point in time? This can happen due to an</p>
<p>effect we call <em>network remnants</em>, in which an old message becomes stuck for</p>
<p>an indefinite period of time inside the network and then suddenly shows up.</p>
<p>Well, the low watermark that triggers garbage collection won’t advance until</p>
<p>record deliveries have been acknowledged, so we know that this record has</p>
<p>already been successfully processed. Such network remnants are clearly</p>
<p>duplicates and are ignored.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="exactly-once-in-sources"><strong>Exactly Once in Sources</strong></span><a href="#exactly-once-in-sources" class="header-anchor">#</a></h1><p>Beam provides a source API for reading data into a Dataflow pipeline. Dataflow might retry reads from a source if processing fails and needs to</p>
<p>ensure that every unique record produced by a source is processed exactly</p>
<p>once.</p>
<p>For most sources Dataflow handles this process transparently; such sources</p>
<p>are <em>deterministic</em>. For example, consider a source that reads data out of files.</p>
<p>The records in a file will always be in a deterministic order and at</p>
<p>deterministic byte locations, no matter how many times the file is read. The</p>
<p>filename and byte location uniquely identify each record, so the service can</p>
<p>automatically generate unique IDs for each record. Another source that</p>
<p>provides similar determinism guarantees is Apache Kafka; each Kafka topic</p>
<p>is divided into a static set of partitions, and records in a partition always have</p>
<p>a deterministic order. Such deterministic sources will work seamlessly in</p>
<p>Dataflow with no duplicates.</p>
<p>However, not all sources are so simple. For example, one common source for</p>
<p>Dataflow pipelines is Google Cloud Pub&#x2F;Sub. Pub&#x2F;Sub is a <em>nondeterministic</em></p>
<p>source: multiple subscribers can pull from a Pub&#x2F;Sub topic, but which</p>
<p>subscribers receive a given message is unpredictable. If processing fails</p>
<p>Pub&#x2F;Sub will redeliver messages but the messages might be delivered to</p>
<p>different workers than those that processed them originally, and in a different</p>
<p>order. This nondeterministic behavior means that Dataflow needs assistance</p>
<p>for detecting duplicates because there is no way for the service to</p>
<p>deterministically assign record IDs that will be stable upon retry. (We dive</p>
<p>into a more detailed case study of Pub&#x2F;Sub later in this chapter.)</p>
<p>Because Dataflow cannot automatically assign record IDs, nondeterministic</p>
<p>sources are required to inform the system what the record IDs should be.</p>
<p>Beam’s Source API provides the UnboundedReader.getCurrentRecordId method. If a source provides unique IDs per record and notifies Dataflow that</p>
<p>it requires deduplication, records with the same ID will be filtered out.</p>
<h1><span id="exactly-once-in-sinks"><strong>Exactly Once in Sinks</strong></span><a href="#exactly-once-in-sinks" class="header-anchor">#</a></h1><p>At some point, every pipeline needs to output data to the outside world, and a</p>
<p>sink is simply a transform that does exactly that. Keep in mind that delivering</p>
<p>data externally is a side effect, and we have already mentioned that Dataflow</p>
<p>does not guarantee exactly-once application of side effects. So, how can a</p>
<p>sink guarantee that outputs are delivered exactly once?</p>
<p>The simplest answer is that a number of built-in sinks are provided as part of</p>
<p>the Beam SDK. These sinks are carefully designed to ensure that they do not</p>
<p>produce duplicates, even if executed multiple times. Whenever possible,</p>
<p>pipeline authors are encouraged to use one of these built-in sinks.</p>
<p>However, sometimes the built-ins are insufficient and you need to write your</p>
<p>own. The best approach is to ensure that your side-effect operation is</p>
<p>idempotent and therefore robust in the face of replay. However, often some</p>
<p>component of a side-effect DoFn is nondeterministic and thus might change</p>
<p>on replay. For example, in a windowed aggregation, the set of records in the</p>
<p>window can also be nondeterministic!</p>
<p>Specifically, the window might attempt to fire with elements e0, e1, e2, but</p>
<p>the worker crashes before committing the window processing (but not before</p>
<p>those elements are sent as a side effect). When the worker restarts, the</p>
<p>window will fire again, but now a late element e3 shows up. Because this</p>
<p>element shows up before the window is committed, it’s not counted as late</p>
<p>data, so the DoFn is called again with elements e0, e1, e2, e3. These are then</p>
<p>sent to the side-effect operation. Idempotency does not help here, because</p>
<p>different logical record sets were sent each time.</p>
<p>There are other ways nondeterminism can be introduced. The standard way to</p>
<p>address this risk is to rely on the fact that Dataflow currently guarantees that</p>
<p>only one version of a DoFn’s output can make it past a shuffle boundary.</p>
<p>A simple way of using this guarantee is via the built-in Reshuffle transform.</p>
<p>The pattern presented in Example 5-2 ensures that the side-effect operation</p>
<p>always receives a deterministic record to output.</p>
<p><em>Example 5-2. Reshuffle example</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c.apply(Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1))))</span><br><span class="line">.apply(GroupByKey.&lt;..&gt;.create())</span><br><span class="line">.apply(new PrepareOutputData())</span><br><span class="line">.apply(Reshuffle.&lt;..&gt;of())</span><br><span class="line">.apply(WriteToSideEffect());</span><br></pre></td></tr></table></figure>

<p>The preceding pipeline splits the sink into two steps: PrepareOutputData</p>
<p>and WriteToSideEffect.  PrepareOutputData outputs  records</p>
<p>corresponding to idempotent writes. If we simply ran one after the other, the</p>
<p>entire process might be replayed on failure, PrepareOutputData might</p>
<p>produce a different result, and both would be written as side effects. When we</p>
<p>add the Reshuffle in between the two, Dataflow guarantees this can’t</p>
<p>happen.</p>
<p>Of course, Dataflow might still run the WriteToSideEffect operation</p>
<p>multiple times. The side effects themselves still need to be idempotent, or the</p>
<p>sink will receive duplicates. For example, an operation that sets or overwrites</p>
<p>a value in a data store is idempotent, and will generate correct output even if</p>
<p>it’s run several times. An operation that appends to a list is not idempotent; if</p>
<p>the operation is run multiple times, the same value will be appended each</p>
<p>time.</p>
<p>While Reshuffle provides a simple way of achieving stable input to a DoFn,</p>
<p>a GroupByKey works just as well. However, there is currently a proposal that</p>
<p>removes the need to add a GroupByKey to achieve stable input into a DoFn.</p>
<p>Instead, the user could annotate WriteToSideEffect with a special</p>
<p>annotation, @RequiresStableInput, and the system would then ensure stable</p>
<p>input to that transform.</p>
</details>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Wang Wei
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www6v.github.io/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/" title="《Streaming System》-Chapter 5. Exactly-Once and Side Effects">https://www6v.github.io/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/www6vHomeHexo/tags/Streaming-System/" rel="tag"># Streaming System</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/" rel="prev" title="《Streaming System》-第五章：精确一次和副作用">
      <i class="fa fa-chevron-left"></i> 《Streaming System》-第五章：精确一次和副作用
    </a></div>
      <div class="post-nav-item">
    <a href="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/" rel="next" title="《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行">
      《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wang Wei</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/www6vHomeHexo/archives/">
        
          <span class="site-state-item-count">383</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/www6vHomeHexo/categories/">
          
        <span class="site-state-item-count">239</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/www6vHomeHexo/tags/">
          
        <span class="site-state-item-count">126</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/www6v" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;www6v" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:www6v@126.com" title="E-Mail → mailto:www6v@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/www6vHomeHexo/lib/anime.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.min.js"></script>
  <script src="/www6vHomeHexo/lib/velocity/velocity.ui.min.js"></script>

<script src="/www6vHomeHexo/js/utils.js"></script>

<script src="/www6vHomeHexo/js/motion.js"></script>


<script src="/www6vHomeHexo/js/schemes/muse.js"></script>


<script src="/www6vHomeHexo/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/www6vHomeHexo/js/local-search.js"></script>













  

  

</body>
</html>
