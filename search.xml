<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AIGC 汇总</title>
    <url>/www6vHomeAIGC/2022/11/16/gptSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2021/08/11/ai/" title="AI 应用场景">AI 应用场景</a> </li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiOverview/" title="人工智能 知识点">人工智能 知识点</a></li>
<li><a href="/www6vHomeAIGC/2022/06/07/aiMachineLearning/" title="Machine Learning">Machine Learning</a></li>
</ul>
<h2><span id="deeplearning">DeepLearning</span><a href="#deeplearning" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/06/11/aiDeepLearning/" title="Deep Learning">Deep Learning</a></li>
<li><a href="/www6vHomeAIGC/2023/03/28/gptPytorch/" title="(实战)Pytorch">(实战)Pytorch</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/23/gptGPU/" title="GPU 算力">GPU 算力</a> </li>
<li><a href="/www6vHomeAIGC/2023/07/01/gptGPUComputing/" title="GPU 计算">GPU 计算</a></li>
</ul>
<h2><span id="nlp">NLP</span><a href="#nlp" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2023/02/05/gptNLPTask/" title="NLP+LLM">NLP+LLM</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/18/gptDocSimilarity/" title="短文本相似度">短文本相似度</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/28/gptDialogue/" title="多轮对话">多轮对话</a></li>
</ul>
<h2><span id="model">Model</span><a href="#model" class="header-anchor">#</a></h2><ul>
<li>基础<ul>
<li><a href="/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/" title="(综述)大模型">(综述)大模型</a></li>
<li><a href="/www6vHomeAIGC/2023/02/17/gptLargeModel/" title="大模型">大模型</a> </li>
<li><a href="/www6vHomeAIGC/2022/11/30/gptTransformer/" title="(原理)Transformer">(原理)Transformer</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/16/gptTransformerCode/" title="(实战)Transformer">(实战)Transformer</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/18/gptEmbedding/" title="(原理)Embedding">(原理)Embedding</a>   </li>
<li><a href="/www6vHomeAIGC/2023/03/30/gptTemperature/" title="Temperature &amp; Top-p">Temperature &amp; Top-p</a></li>
</ul>
</li>
<li>基座模型<ul>
<li><a href="/www6vHomeAIGC/2022/12/11/gptFamily/" title="GPT 系列">GPT 系列</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/01/gptLlama/" title="LLaMA">LLaMA</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/" title="LLaMA 家族">LLaMA 家族</a>   </li>
<li><a href="/www6vHomeAIGC/2023/01/06/gptChatGLM/" title="ChatGLM">ChatGLM</a>   </li>
<li><a href="/www6vHomeAIGC/2023/01/04/gptLeaderBoard/" title="大模型 排行榜">大模型 排行榜</a></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/" title="(原理)不可能三角">(原理)不可能三角</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptEmergent/" title="(原理)涌现现象">(原理)涌现现象</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/06/gptHallucination/" title="(原理)幻觉问题">(原理)幻觉问题</a>    </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptEval/" title="测评">测评</a></li>
</ul>
<h2><span id="training">Training</span><a href="#training" class="header-anchor">#</a></h2><ul>
<li>训练<ul>
<li><a href="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/" title="(原理)Training">(原理)Training</a></li>
<li><a href="/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/" title="(实战)Training">(实战)Training</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptContinualPretraining/" title="(原理|实战)继续Pre-Training">(原理|实战)继续Pre-Training</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/21/gptChineseLlama/" title="(实战)Chinese-LLaMA PT+SFT">(实战)Chinese-LLaMA PT+SFT</a>   </li>
<li><a href="/www6vHomeAIGC/2024/02/01/gptPrecision/" title="(原理|实战)混合精度">(原理|实战)混合精度</a> </li>
<li>分布式<ul>
<li><a href="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/" title="(原理)分布式并行Training">(原理)分布式并行Training</a>    </li>
<li><a href="/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/" title="(原理)Zero Deepspeed">(原理)Zero Deepspeed</a>    </li>
<li><a href="/www6vHomeAIGC/2023/03/25/gptTrainDeepspeedPractice/" title="(实战)Deepspeed">(实战)Deepspeed</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="inference">Inference</span><a href="#inference" class="header-anchor">#</a></h2><ul>
<li>框架<ul>
<li><a href="/www6vHomeAIGC/2023/03/21/gptInferFramework/" title="(原理)推理-框架">(原理)推理-框架</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/" title="(实战)推理-lmdeploy">(实战)推理-lmdeploy</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/31/gptInfervLLM/" title="(原理)推理 vLLM">(原理)推理 vLLM</a> </li>
<li><a href="/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/" title="(实战)推理 vLLM">(实战)推理 vLLM</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/02/gptInferTensorRT/" title="(原理|实战)推理 TensorRT-LLM">(原理|实战)推理 TensorRT-LLM</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/11/gptInferRay/" title="(原理)推理 Ray">(原理)推理 Ray</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/16/gptInferRayPractice/" title="推理 Ray">推理 Ray</a></li>
</ul>
</li>
<li>优化<ul>
<li><a href="/www6vHomeAIGC/2023/01/01/gptInference/" title="(综述)推理优化">(综述)推理优化</a></li>
<li><a href="/www6vHomeAIGC/2023/06/01/gptInferKVCache/" title="(原理)推理 KV Cache">(原理)推理 KV Cache</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/13/gptFlashAttention/" title="(原理)Flash Attention">(原理)Flash Attention</a>  </li>
<li>模型压缩<ul>
<li><a href="/www6vHomeAIGC/2023/02/19/gptQuantization/" title="(原理)模型压缩-量化概述">(原理)模型压缩-量化概述</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/" title="(原理)Weight Only(LLM.int8(), GPTQ, AWQ)">(原理)Weight Only(LLM.int8(), GPTQ, AWQ)</a> </li>
<li><a href="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/" title="(实战)模型压缩-量化">(实战)模型压缩-量化</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="data">Data *</span><a href="#data" class="header-anchor">#</a></h2><ul>
<li>List<ul>
<li><a href="/www6vHomeAIGC/2023/01/08/gptDataSet/" title="(list)数据集">(list)数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/" title="(List) Pretrain 数据集">(List) Pretrain 数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/" title="(List)SFT数据集">(List)SFT数据集</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/" title="(survey)多模态  数据集">(survey)多模态  数据集</a></li>
</ul>
</li>
<li>DataProcess<ul>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/" title="(Survey)Dataset">(Survey)Dataset</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/05/gptDataProcess/" title="(Survey)数据处理">(Survey)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/" title="(实战)数据处理">(实战)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/" title="(原理|实战)Data  Annotation">(原理|实战)Data  Annotation</a></li>
</ul>
</li>
<li>Data Management<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataManagement/" title="(Survey)Data Management">(Survey)Data Management</a>  </li>
<li>Pretrain  <ul>
<li><a href="/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/" title="(质量过滤)RefinedWeb, Textbooks">(质量过滤)RefinedWeb, Textbooks</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/" title="Tokenizer">Tokenizer</a></li>
</ul>
</li>
<li>SFT <ul>
<li>Data Quality<ul>
<li>Instruction Quality<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/" title="(原理)LIMA, LESS">(原理)LIMA, LESS</a></li>
</ul>
</li>
<li>Instruction Diversity<ul>
<li><a href="/www6vHomeAIGC/2023/02/21/gptSelfInstruct/" title="(原理)SELF-INSTRUCT, Self-QA">(原理)SELF-INSTRUCT, Self-QA</a></li>
</ul>
</li>
<li>Instruction Complexity  <ul>
<li><a href="/www6vHomeAIGC/2023/03/18/gptDataWizard/" title="(原理)Wizard">(原理)Wizard</a></li>
</ul>
</li>
</ul>
</li>
<li>Task composition<ul>
<li><a href="/www6vHomeAIGC/2023/02/06/gptDatasetSFT/" title="(原理)SFT 数据组合">(原理)SFT 数据组合</a></li>
</ul>
</li>
</ul>
<ul>
<li><a href="/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/" title="(原理)SFT Scaling">(原理)SFT Scaling</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/05/gptDataSelection/" title="(原理)Data Selection">(原理)Data Selection</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="finetuning">FineTuning</span><a href="#finetuning" class="header-anchor">#</a></h2><ul>
<li>PEFT<ul>
<li><a href="/www6vHomeAIGC/2022/11/18/gptFineTuning/" title="(原理)PEFT">(原理)PEFT</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/28/gptFineTuningWhen/" title="(原理)Fine-Tuning 时机">(原理)Fine-Tuning 时机</a>  </li>
<li><a href="/www6vHomeAIGC/2022/12/20/gptFineTuningPEFT/" title="(实战)PEFT 概述">(实战)PEFT 概述</a></li>
</ul>
</li>
<li>Soft Prompt<ul>
<li><a href="/www6vHomeAIGC/2023/01/06/gptPromptTuning/" title="(原理)Prompt Tuning">(原理)Prompt Tuning</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/" title="P-Tuning">P-Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2024/01/28/gptPEFTPtuningPractice/" title="(实战)PEFT P-Tuning">(实战)PEFT P-Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/25/gptPromptTuningPractice/" title="(实战)PromptTuning">(实战)PromptTuning</a></li>
</ul>
</li>
<li>Lora<ul>
<li><a href="/www6vHomeAIGC/2023/01/05/gptPEFTLora/" title="(实战)PEFT Lora">(实战)PEFT Lora</a> </li>
<li><a href="/www6vHomeAIGC/2024/01/12/gptPEFTQLora/" title="(实战)PEFT QLoRA">(实战)PEFT QLoRA</a></li>
</ul>
</li>
<li>Instruct Tuning *<ul>
<li><a href="/www6vHomeAIGC/2023/01/06/gptInstructTuning/" title="(原理)Instruct Tuning">(原理)Instruct Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/12/gptInstructTuningSurvey/" title="(Survey)Instruct Tuning">(Survey)Instruct Tuning</a></li>
</ul>
</li>
<li>BERT<ul>
<li><a href="/www6vHomeAIGC/2024/01/26/gptFineTuningBert/" title="Fine Tuning-Bert">Fine Tuning-Bert</a></li>
</ul>
</li>
<li>平台<ul>
<li><a href="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/" title="LLama-Factory">LLama-Factory</a></li>
</ul>
</li>
</ul>
<h2><span id="multimodal">Multimodal *</span><a href="#multimodal" class="header-anchor">#</a></h2><ul>
<li>Survey<ul>
<li><a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a></li>
<li><a href="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/" title="多模态 系列">多模态 系列</a></li>
</ul>
</li>
<li>Train  *<ul>
<li><a href="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/" title="(原理)多模态预训练 概述">(原理)多模态预训练 概述</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/" title="(综述)多模态InstructTuning">(综述)多模态InstructTuning</a></li>
</ul>
</li>
<li>视觉理解<ul>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/" title="(原理)CLIP">(原理)CLIP</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/" title="(实战)CLIP">(实战)CLIP</a>   </li>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/" title="SAM">SAM</a>   </li>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalVit/" title="ViT,ViLT">ViT,ViLT</a></li>
</ul>
</li>
<li>生成<ul>
<li><a href="/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/" title="Diffusion">Diffusion</a></li>
</ul>
</li>
<li>端到端训练LLM <ul>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/" title="(图生文)BLIP-2, Flamingo">(图生文)BLIP-2, Flamingo</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/" title="(原理|实战)多模态  LLaVa">(原理|实战)多模态  LLaVa</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/" title="(原理|实战)MiniGPT4">(原理|实战)MiniGPT4</a></li>
</ul>
</li>
<li>Multimodal Agent*<ul>
<li><a href="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/" title="Agent 多模态">Agent 多模态</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/05/gptAgentWeb/" title="(原理)Web Agent">(原理)Web Agent</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/" title="App Agent">App Agent</a></li>
</ul>
</li>
</ul>
<h2><span id="rag">RAG *</span><a href="#rag" class="header-anchor">#</a></h2><ul>
<li>Overview<ul>
<li><a href="/www6vHomeAIGC/2022/11/02/gptRAG/" title="(综述)RAG">(综述)RAG</a></li>
<li><a href="/www6vHomeAIGC/2023/04/21/gptRAGModularRAG/" title="(原理)Modular RAG">(原理)Modular RAG</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/07/gptRAGPerformance/" title="(原理)Advanced RAG">(原理)Advanced RAG</a></li>
<li><a href="/www6vHomeAIGC/2023/06/07/gptRAGEval/" title="RAG 评估">RAG 评估</a> </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGKG/" title="RAG KG">RAG KG</a></li>
</ul>
</li>
<li>实战<ul>
<li><a href="/www6vHomeAIGC/2022/12/31/gptRAGPractice/" title="(实战)RAG">(实战)RAG</a></li>
<li><a href="/www6vHomeAIGC/2023/05/09/gptRAGOptimize/" title="RAG 优化">RAG 优化</a></li>
<li>framework<ul>
<li><a href="/www6vHomeAIGC/2023/05/09/gptRAGFramework/" title="RAG Framework">RAG Framework</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/31/gptRAGchatchat/" title="(实战)RAG Langchain-Chatchat">(实战)RAG Langchain-Chatchat</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGQanything/" title="RAG Qanything">RAG Qanything</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGRAGflow/" title="RAG RAGflow">RAG RAGflow</a></li>
</ul>
</li>
</ul>
</li>
<li>案例 <ul>
<li><a href="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/" title="(原理)RAG OpenAI案例">(原理)RAG OpenAI案例</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/18/gptRAGBaichuan/" title="(原理)RAG Baichuan案例">(原理)RAG Baichuan案例</a></li>
</ul>
</li>
<li>phase <ul>
<li><a href="/www6vHomeAIGC/2023/04/20/gptQueryTransformation/" title="(原理|实战)Query Transformation">(原理|实战)Query Transformation</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRouting/" title="(原理|实战)Query Routing">(原理|实战)Query Routing</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/21/gptRAGIndex/" title="(原理|实战)RAG Index">(原理|实战)RAG Index</a>   </li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRerank/" title="(原理|实战)RAG Rerank">(原理|实战)RAG Rerank</a> </li>
<li>Agentic RAG<ul>
<li><a href="/www6vHomeAIGC/2023/06/25/gptAgenticRAG/" title="Agentic RAG">Agentic RAG</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/02/gptRAGSelfReflective/" title="(原理|实战)Self-Reflective RAG">(原理|实战)Self-Reflective RAG</a></li>
</ul>
</li>
</ul>
</li>
<li>Multimodal RAG  *<ul>
<li><a href="/www6vHomeAIGC/2023/03/14/gptRAGMultimodal/" title="(原理)多模态 RAG">(原理)多模态 RAG</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/14/gptRAGMultimodalPractice/" title="(实战)多模态 RAG">(实战)多模态 RAG</a>   </li>
<li><a href="/www6vHomeAIGC/2023/04/19/gptDocumentAI/" title="文档智能">文档智能</a></li>
</ul>
</li>
</ul>
<h2><span id="agent">Agent *</span><a href="#agent" class="header-anchor">#</a></h2><ul>
<li>Overview<ul>
<li><a href="/www6vHomeAIGC/2022/11/02/gptAgent/" title="(原理)Agent">(原理)Agent</a></li>
<li><a href="/www6vHomeAIGC/2023/04/06/gptAgentCategory/" title="Agent 分类[有趣|有用]">Agent 分类[有趣|有用]</a></li>
<li><a href="/www6vHomeAIGC/2023/03/05/gptAgentList/" title="(List)Agent 开源 产品 平台">(List)Agent 开源 产品 平台</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/01/gptAgentPractice/" title="(实战)Agent">(实战)Agent</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/13/gptAgentChallenge/" title="(原理)Agent Challenge">(原理)Agent Challenge</a> </li>
<li><a href="/www6vHomeAIGC/2023/06/05/gptAgentMemory/" title="Agent  Memory">Agent  Memory</a></li>
</ul>
</li>
<li>Reflection<ul>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentReflection/" title="Reflection Agent">Reflection Agent</a></li>
</ul>
</li>
<li>Planning<ul>
<li><a href="/www6vHomeAIGC/2023/05/13/gptAgentPlanning/" title="Agent Planning">Agent Planning</a>     </li>
<li><a href="/www6vHomeAIGC/2023/03/02/gptAgentPlanAndExecute/" title="(原理|实战)Plan&amp;Execute,ReWOO">(原理|实战)Plan&amp;Execute,ReWOO</a></li>
</ul>
</li>
<li>Multi-agent collaboration<ul>
<li><a href="/www6vHomeAIGC/2023/01/21/gptMultiAgents/" title="(原理)Multi-Agents">(原理)Multi-Agents</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/07/gptMultiAgentsPractice/" title="(实战)LangGraph">(实战)LangGraph</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/05/gptAgentAutogen/" title="AutoGen">AutoGen</a></li>
</ul>
</li>
<li>Tool use  *<ul>
<li><a href="/www6vHomeAIGC/2022/11/16/gptFunctionCall/" title="(原理|实战) OpenAI Function Call">(原理|实战) OpenAI Function Call</a> </li>
<li><a href="/www6vHomeAIGC/2023/01/27/gptAgentTool/" title="(原理)Agent-Tools">(原理)Agent-Tools</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/" title="(原理)Gorilla">(原理)Gorilla</a>   </li>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentTuning/" title="Agent Tuning">Agent Tuning</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptAgentToolformer/" title="(原理)Toolformer">(原理)Toolformer</a></li>
</ul>
</li>
</ul>
<h2><span id="application">Application</span><a href="#application" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/05/09/gpt/" title="GPT-工具和应用">GPT-工具和应用</a></li>
<li><a href="/www6vHomeAIGC/2022/12/28/gptLLMOps/" title="LLMOps">LLMOps</a> </li>
<li><a href="/www6vHomeAIGC/2022/11/27/gptVectorStore/" title="向量数据库">向量数据库</a></li>
<li><a href="/www6vHomeAIGC/2023/01/03/gptNL2SQL/" title="NL2SQL">NL2SQL</a> </li>
<li>垂类模型<ul>
<li><a href="/www6vHomeAIGC/2023/01/04/gptDomain/" title="垂类大模型">垂类大模型</a> </li>
<li><a href="/www6vHomeAIGC/2022/11/24/gptDomainFinance/" title="金融大模型">金融大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptDomainMed/" title="医疗大模型">医疗大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2024/02/07/gptDomainLaw/" title="法律大模型">法律大模型</a></li>
</ul>
</li>
</ul>
<h2><span id="prompt">Prompt</span><a href="#prompt" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/11/10/gptPromptEngineering/" title="(原理)Prompt Engineering">(原理)Prompt Engineering</a></li>
<li><a href="/www6vHomeAIGC/2023/02/08/gptCOT/" title="COT">COT</a> </li>
<li><a href="/www6vHomeAIGC/2021/05/28/gptPromptCode/" title="Prompt-Code">Prompt-Code</a></li>
<li><a href="/www6vHomeAIGC/2021/05/26/gptPrompt/" title="Prompt-How to use">Prompt-How to use</a></li>
</ul>
<h2><span id="langchain">Langchain</span><a href="#langchain" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/11/02/gptLangchain/" title="Langchain">Langchain</a></li>
<li><a href="/www6vHomeAIGC/2022/12/31/gptRetrievers/" title="Retrievers">Retrievers</a> </li>
<li><a href="/www6vHomeAIGC/2023/01/11/gptLangchainAgent/" title="Langchain  Agent">Langchain  Agent</a></li>
</ul>
<h2><span id="study">Study</span><a href="#study" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/08/01/gptStudy/" title="GPT  学习资源">GPT  学习资源</a></li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiStudyResouce/" title="人工智能-学习资源">人工智能-学习资源</a></li>
</ul>
<h2><span id="research">Research</span><a href="#research" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2024/02/11/gptPaperTools/" title="科研-工具">科研-工具</a> </li>
<li><a href="/www6vHomeAIGC/2023/01/20/gptStudyPaper/" title="GPT 论文">GPT 论文</a></li>
<li><a href="/www6vHomeAIGC/2023/02/25/gptSurveyList/" title="Survey List">Survey List</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/04/gptAgentPaper/" title="Paper-Agent">Paper-Agent</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态 系列</title>
    <url>/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="模块独立">模块独立</span><a href="#模块独立" class="header-anchor">#</a></h1><h3><span id="clip">CLIP</span><a href="#clip" class="header-anchor">#</a></h3><h3><span id="vilt">ViLT</span><a href="#vilt" class="header-anchor">#</a></h3><h3><span id="albef">ALBEF</span><a href="#albef" class="header-anchor">#</a></h3><h1><span id="模块共享">模块共享</span><a href="#模块共享" class="header-anchor">#</a></h1><h3><span id="vlmo">VLMO</span><a href="#vlmo" class="header-anchor">#</a></h3><h3><span id="blip">BLIP</span><a href="#blip" class="header-anchor">#</a></h3><h3><span id="blip2">BLIP2</span><a href="#blip2" class="header-anchor">#</a></h3><h3><span id="beitv3">BEiTv3</span><a href="#beitv3" class="header-anchor">#</a></h3><h1><span id="总结-10">总结 [10]</span><a href="#总结-10" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/multimodal.webp" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/653902791">多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读</a> ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/643969218">[Transformer 101系列] 多模态的大一统之路</a>  *** </p>
<p>1xx. <a href="https://blog.csdn.net/qq_52038588/article/details/133893013">多模态论文串讲</a> ***</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409338&idx=1&sn=5445ff1e9bedc561393b6da63fdf71f9">图生文多模态大模型开源项目回顾：兼看20240307大模型进展早报</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662889725">图文多模态大模型综述</a></p>
<p>1xx. <a href="https://huyenchip.com/2023/10/10/multimodal.html">Multimodality and Large Multimodal Models (LMMs)</a><br>   <a href="https://baoyu.io/translations/lmm/multimodality-and-large-multimodal-models">多模态和多模态大模型 (LMM)[译]</a>  CLIP Flamingo</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/667942680">写在多模态征服一切之前（未来数据和模型应该是什么样的？）</a></p>
<h3><span id="single">single</span><a href="#single" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/660662864">Qwen-VL：突破视觉与语言融合的多模态模型，GPT4V的国产化替代</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/657385270">Qwen-VL: 一个通用的视觉语言模型,用于理解、定位、文本阅读等</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/675877376">InternVL：开源社区最强的多模态大模型</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)模型压缩-量化</title>
    <url>/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9E%E6%88%98-ptq1">实战-PTQ[1]</a><ul>
<li><a href="#%E9%87%8F%E5%8C%96%E4%B8%8E8bit%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">量化与8bit模型训练</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E6%88%982-%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86-%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86-8">实战2-量化推理 量化推理 [8]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="实战-ptq1">实战-PTQ[1]</span><a href="#实战-ptq1" class="header-anchor">#</a></h1><h3><span id="量化与8bit模型训练">量化与8bit模型训练</span><a href="#量化与8bit模型训练" class="header-anchor">#</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;D:/Pretrained_models/modelscope/Llama-2-7b-ms&quot;</span>, low_cpu_mem_usage=<span class="literal">True</span>, </span><br><span class="line">                                             torch_dtype=torch.half, device_map=<span class="string">&quot;auto&quot;</span>, load_in_8bit=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1><span id="实战2-量化推理-量化推理-8">实战2-量化推理 量化推理 [8]</span><a href="#实战2-量化推理-量化推理-8" class="header-anchor">#</a></h1><ul>
<li><p>Training的模型</p>
<img src="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/dirs.png" class>
</li>
<li><p>合并后的模型</p>
<img src="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/dir.png" class>
</li>
<li><p>4bit量化推理</p>
<img src="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/xtuner-chat.png" class></li>
</ul>
<blockquote>
<p>Training的时候要用tmux</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tmux new -s finetune</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tmux attach -t finetune</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ctcl +b , D</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>16bit量化推理慢,  要用4bit量化推理</p>
</blockquote>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1EN411g7Yn/"> 量化与8bit模型训练</a> V<br><a href="https://www.bilibili.com/video/BV1EN411g7Yn/">【手把手带你实战HuggingFace Transformers-低精度训练篇】量化与8bit模型训练</a><br>   <a href="https://github.com/www6v/transformers-code/blob/master/04-Kbit%20Training/26-8bits_training/llama2_lora_8bit.ipynb">llama2_lora_8bit.ipynb</a></p>
</li>
<li><p><a href="https://github.com/www6v/tutorial/tree/main/xtuner">internLM fine-tuning on xtuner</a><br><a href="https://www.bilibili.com/video/BV1yK4y1B75J/">(4)XTuner 大模型单卡低成本微调实战</a> V</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(质量过滤)RefinedWeb, Textbooks</title>
    <url>/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="动机1">动机[1]</span><a href="#动机1" class="header-anchor">#</a></h1><ul>
<li>作者执着证明网页数据好于专有数据<ul>
<li>网页数据的量级比公开数据大的多，仅用专有数据模型模型训练不到最佳效果</li>
<li>专有数据处理起来很麻烦</li>
<li>大部分专有数据其实在网页数据中也能找到</li>
</ul>
</li>
</ul>
<p>作者认为要想模型训练的大、耗费的人力少就不得不重新<strong>将网页数据精细化</strong>利用起来。</p>
<h1><span id="结论1">结论[1]</span><a href="#结论1" class="header-anchor">#</a></h1><ul>
<li>作者证明了仅用<strong>web数据</strong>如果经过恰当的<strong>清洗和过滤</strong>，可以获得超过使用了专有数据模型的效果。</li>
</ul>
<h1><span id="文本处理pipeline1">文本处理Pipeline[1]</span><a href="#文本处理pipeline1" class="header-anchor">#</a></h1><h3><span id="目标语言识别">目标语言识别</span><a href="#目标语言识别" class="header-anchor">#</a></h3><h3><span id="规则过滤">规则过滤</span><a href="#规则过滤" class="header-anchor">#</a></h3><h3><span id="通过机器学习方法过滤出高质量语料库">通过机器学习方法过滤出高质量语料库</span><a href="#通过机器学习方法过滤出高质量语料库" class="header-anchor">#</a></h3><h3><span id="去重deduplication">去重（Deduplication）</span><a href="#去重deduplication" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="refinedweb">RefinedWeb</span><a href="#refinedweb" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/641013454">数据为王：大模型预训练中的数据处理及思考—The RefinedWeb Dataset for Falcon LLM论文解读</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401484&idx=1&sn=c49b5ca5fc962ca757d3a082b74f037a">“超越LLama 65B”的Falcon40B语言模型为什么好：再看精细化的数据清洗的重要性 </a><br>   RefinedWeb Dataset for Falcon,   Falcon采用bloom架构</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402104&idx=1&sn=7d4924b2a5a840e4ff3de43299248b1d">再谈大模型的预训数据清洗与微调数据生成：RedPajama数据处理框架与entity-centric指令生成方法解读 </a><br>    llama数据的复现项目SlimPajama</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/637996787">【Falcon Paper】我们是靠洗数据洗败 LLaMA 的！</a> 未</p>
<h3><span id="textbooks-数量-gtscaling-law">Textbooks   数量-&gt;scaling law</span><a href="#textbooks-数量-gtscaling-law" class="header-anchor">#</a></h3><p>1xx. <a href="https://finisky.github.io/textbooks-are-all-you-need-summary/">数据为王: Textbooks Are All You Need </a>   以小博大  打破传统语言模型缩放定律<br>1xx. <a href="https://zhuanlan.zhihu.com/p/673021932">Textbooks Are All You Need II: phi-1.5 technical report 精读与翻译</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/672066480">小模型的惊人能力: Phi-2</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403821&idx=1&sn=7b96e0db09f05888078019cd20bc8390">再看多语种大模型预训数据如何清洗：兼论文档结构信息对大模型问答的重要性及实现思路 </a><br>二、再看训练数据集如何清洗：多语种开源训练数据集CulturaX</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-工具</title>
    <url>/www6vHomeAIGC/2024/02/11/gptPaperTools/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文管理1">论文管理[1]</span><a href="#论文管理1" class="header-anchor">#</a></h1><ul>
<li><p>Readpaper<br><a href="https://readpaper.com/new">https://readpaper.com/new</a><br>论文在线阅读，论文搜索，管理</p>
</li>
<li><p>Connected Papers 引用关系<br><a href="https://www.connectedpapers.com/">https://www.connectedpapers.com/</a></p>
</li>
<li><p>AI 顶会倒计时<br><a href="https://aideadlin.es/?sub=ML,CV,NLP">https://aideadlin.es/?sub=ML,CV,NLP</a></p>
</li>
<li><p>Aminer<br><a href="https://www.aminer.cn/">https://www.aminer.cn/</a><br>最新的进展如何<br>还有个必读论文系列</p>
</li>
</ul>
<h1><span id="写作1">写作[1]</span><a href="#写作1" class="header-anchor">#</a></h1><ul>
<li><p>DeepL翻译<br>deepl.com&#x2F;translator</p>
</li>
<li><p>overleaf<br>overleaf.com&#x2F;project</p>
</li>
</ul>
<h1><span id="论文元素-1">论文元素 [1]</span><a href="#论文元素-1" class="header-anchor">#</a></h1><ul>
<li><p>quillbot.com&#x2F;<br>改写, 检查语法, 摘要生成</p>
</li>
<li><p>Table generator<br><a href="https://www.tablesgenerator.com/latex_tables">https://www.tablesgenerator.com/latex_tables</a><br>latex表格生成</p>
</li>
<li><p>echarts<br><a href="https://echarts.apache.org/examples/en/index.html#chart-type-bar">https://echarts.apache.org/examples/en/index.html#chart-type-bar</a><br>画图</p>
</li>
<li><p>detexify Latex符号<br><a href="http://detexify.kirelabs.org/classify.html">http://detexify.kirelabs.org/classify.html</a><br>手绘符号转换成latex代码</p>
</li>
<li><p>DBLP<br>dblp.uni-trier.de&#x2F;<br>latex引用</p>
</li>
<li><p>esoda 词组用法示例<br>esoda.org&#x2F;</p>
</li>
<li><p>wikidiff 近义词理解<br><a href="https://wikidiff.com/neglect/omit">https://wikidiff.com/neglect/omit</a></p>
</li>
<li><p>在线latex公式编辑器<br><a href="https://www.latexlive.com/##">https://www.latexlive.com/##</a></p>
</li>
<li><p>mathpix 公式图片转换成latex<br><a href="https://mathpix.com/">https://mathpix.com/</a><br>公式图片识别成latex公式</p>
</li>
</ul>
<h1><span id="aigc-文档分析amp科研">AIGC 文档分析&amp;科研</span><a href="#aigc-文档分析amp科研" class="header-anchor">#</a></h1><ul>
<li><p>在线文档分析<br>Microsoft Edge Dev + new Bing  ***</p>
</li>
<li><p>文献查找 + 润色<br>Skype + new Bing  ***</p>
</li>
<li><p>本地文档分析</p>
<ul>
<li><p>VPN</p>
<ul>
<li>chatpdf  收费  ***<br><a href="https://www.chatpdf.com/">Chat with any PDF</a> 总结文献</li>
<li><a href="https://chatdoc.com/">Chat with documents</a></li>
</ul>
</li>
<li><p>非VPN</p>
<ul>
<li><a href="https://chatpaper.org/">ChatPaper</a>  Paper 中文<br><a href="https://github.com/kaixindelele/ChatPaper">ChatPaper</a> git</li>
<li><a href="https://chat2doc.cn/">阅读文档的好帮手</a>  收费</li>
<li><a href="https://lightpdf.com/chatdoc">LightPDF AI Tools</a> *** </li>
<li><a href="https://docalysis.com/files/hwylw4">docalysis</a> ***</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="newbing-chat-2">NewBing chat [2]</span><a href="#newbing-chat-2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/02/11/gptPaperTools/newBing.jpg" class>

<h3><span id="学术prompt">学术Prompt</span><a href="#学术prompt" class="header-anchor">#</a></h3><ul>
<li>在线文献全文分析 [new Bing] <ul>
<li>帮我总结一下这篇文章的<strong>要点</strong></li>
<li>帮我正对本研究论文写一篇<strong>总结报告</strong>， 600字</li>
<li>帮我总结本研究的讨论部分 采用了哪种<strong>写作框架</strong>， 是否进行了与其它研究的对比，有无表明本研究的<strong>局限性</strong>和<strong>未来研究可能性</strong>?</li>
<li>帮我总结本研究的方法部分用了哪些<strong>研究方法</strong>？</li>
<li>本研究方法部分的Western Blot是如何实施的？</li>
<li>总结下本论文Introduction部分在写作方面，有哪些<strong>词汇和句式</strong>值得在SCI论文写作中积累借鉴</li>
</ul>
</li>
</ul>
<h1><span id="tools">Tools</span><a href="#tools" class="header-anchor">#</a></h1><ul>
<li><a href="https://app.seaml.es/">Seamless for science</a> bibi1<br>abstract</li>
<li><a href="https://typeset.io/">scispace</a>  bibi1<br>润色 判重</li>
<li><a href="https://www.txyz.ai/">txyz</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/661767969">工欲善科研，必先利其器</a></li>
<li><a href="https://www.bilibili.com/video/BV18M4y1C7HY/">整合chatGPT的新必应（NewBing chat）简直就是科研神器！</a></li>
</ol>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>法律大模型</title>
    <url>/www6vHomeAIGC/2024/02/07/gptDomainLaw/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="法律大模型">法律大模型</span><a href="#法律大模型" class="header-anchor">#</a></h3><ul>
<li>ChatLaw </li>
<li>LawGPT_zh</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402872&idx=1&sn=0649e8f7490e057680cff1be16157209">再看法律领域微调模型及外挂知识库问答优化方案：从引入关键词、领域嵌入到知识库细化、意图识别及知识增强项目案例 </a></p>
<p>1xx. <a href="https://finisky.github.io/lawyer-llama-summary/">训练中文垂类大模型：Lawyer LLaMA </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)混合精度</title>
    <url>/www6vHomeAIGC/2024/02/01/gptPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%9B%AE%E7%9A%843">目的[3]</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E5%8E%9F%E5%9B%A0">使用的混合精度原因</a></li>
<li><a href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">混合精度解决方案</a><ul>
<li><a href="#fp32-%E6%9D%83%E9%87%8D%E5%A4%87%E4%BB%BD-12">FP32 权重备份 [1][2]</a></li>
<li><a href="#loss-scale12">Loss Scale[1][2]</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a><ul>
<li><a href="#llama%E5%8D%8A%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%8320">llama半精度训练[20]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81">代码</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="目的3">目的[3]</span><a href="#目的3" class="header-anchor">#</a></h1><p>为了<strong>加快训练时间</strong>、<strong>减少网络训练时候所占用的内存</strong>，并且保存训练出来的模型精度持平的条件下，业界提出越来越多的混合精度训练的方法</p>
<h1><span id="使用的混合精度原因">使用的混合精度原因</span><a href="#使用的混合精度原因" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/02/01/gptPrecision/solution.png" class>

<h1><span id="混合精度解决方案">混合精度解决方案</span><a href="#混合精度解决方案" class="header-anchor">#</a></h1><h3><span id="fp32-权重备份-12">FP32 权重备份 [1][2]</span><a href="#fp32-权重备份-12" class="header-anchor">#</a></h3><p>这种方法主要是用于<strong>解决舍入误差</strong>的问题。</p>
<img src="/www6vHomeAIGC/2024/02/01/gptPrecision/weight-backup.jpg" class> 

<h3><span id="loss-scale12">Loss Scale[1][2]</span><a href="#loss-scale12" class="header-anchor">#</a></h3><p>Loss Scale 主要是为了<strong>解决 fp16 underflow</strong>的问题。</p>
<img src="/www6vHomeAIGC/2024/02/01/gptPrecision/loss-scale.jpg" class> 


<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><h3><span id="llama半精度训练20">llama半精度训练[20]</span><a href="#llama半精度训练20" class="header-anchor">#</a></h3><ul>
<li>现象<br>loss先变大，再为0<br>loss爆炸，loss消失</li>
<li>解决方案<br>padding&#x3D;left<br>改为padding&#x3D;right</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;D:/Pretrained_models/modelscope/Llama-2-7b-ms&quot;</span>, low_cpu_mem_usage=<span class="literal">True</span>, torch_dtype=torch.half, device_map=<span class="string">&quot;auto&quot;</span>)</span><br></pre></td></tr></table></figure>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV1R94y1g78L?p=6">混合精度</a>  *** V</li>
<li><a href="https://zhuanlan.zhihu.com/p/103685761">浅谈混合精度训练</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/441591808">全网最全-混合精度训练原理</a>  ***<br>1xx. <a href="https://zhuanlan.zhihu.com/p/608634079">【深度学习】混合精度训练与显存分析</a></li>
</ol>
<h3><span id="代码">代码</span><a href="#代码" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1CB4y1R78v/">半精度训练与LLaMA2训练实战</a> 有代码<br><a href="https://github.com/www6v/transformers-code/blob/master/04-Kbit%20Training/25-16bits_training/llama2_lora_16bit.ipynb">llama2_lora_16bit.ipynb</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/165152789">PyTorch的自动混合精度（AMP）</a><br>1xx. <a href="https://tensorflow.google.cn/guide/mixed_precision?hl=zh-cn">混合精度</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Precision</category>
      </categories>
      <tags>
        <tag>Precision</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT P-Tuning</title>
    <url>/www6vHomeAIGC/2024/01/28/gptPEFTPtuningPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="最佳实践1">最佳实践[1]</span><a href="#最佳实践1" class="header-anchor">#</a></h3><ul>
<li>要看losss, 也要看<strong>业务的loss</strong></li>
<li>生成模型常用的评价方法<ul>
<li><strong>BLEU 能评估</strong>流畅度**</li>
<li>结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）</li>
</ul>
</li>
<li>垂直模型<ul>
<li><strong>stf之后失去通用能力</strong></li>
<li>要有<strong>通用能力</strong>, 需要<strong>pre-train和STF中都融入通用的语料</strong></li>
</ul>
</li>
<li><strong>每个模型的学习率lr不一样</strong><ul>
<li>chatglm的学习率<br>LR&#x3D;2e-2</li>
</ul>
</li>
</ul>
<h3><span id="学习率">学习率</span><a href="#学习率" class="header-anchor">#</a></h3><ul>
<li>改的<strong>特别大</strong><br>模型训练的时候会<strong>震荡</strong></li>
<li>改的<strong>特别小</strong><br> 模型训练的时候会<strong>收敛非常慢</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《13-基于 ChatGLM2的 Fine-tuning 实战》 AI 大模型全栈工程师培养计划  2期<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm的p-tuning<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm-2的P-tuning v2<br><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb">课件</a><br>bili有相关的总结的视频</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Fine Tuning-Bert</title>
    <url>/www6vHomeAIGC/2024/01/26/gptFineTuningBert/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="基于bert的二分类">基于bert的二分类</span><a href="#基于bert的二分类" class="header-anchor">#</a></h1><ul>
<li>代码 - 全参FT,非PEFT<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">SEED=<span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ALBERT是一种压缩过的BERT</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;albert-base-v2&quot;</span></span><br><span class="line">DATASET_NAME = <span class="string">&quot;glue&quot;</span> <span class="comment"># 一组NLP评测任务</span></span><br><span class="line">DATASET_TASK = <span class="string">&quot;mrpc&quot;</span> <span class="comment"># MRPC 是其中一个子任务 -- Microsoft Research Paraphrase Corpus</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Bert的基础上加了一个线性分类器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClassifier</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.bert_encoder = backbone</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="keyword">return</span> loss_fct(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask,labels=<span class="literal">None</span></span>):</span><br><span class="line">        output = self.bert_encoder(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        output = output.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = self.compute_loss(output, labels)</span><br><span class="line">            <span class="keyword">return</span> loss, output</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集对应的评估方法</span></span><br><span class="line">glue_metric = datasets.load_metric(DATASET_NAME, DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> glue_metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">raw_datasets = load_dataset(DATASET_NAME,DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="comment"># 验证集</span></span><br><span class="line">raw_valid_dataset = raw_datasets[<span class="string">&quot;validation&quot;</span>]</span><br><span class="line"></span><br><span class="line">columns = raw_train_dataset.column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">transformers.set_seed(SEED)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_fn</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = tokenizer(examples[<span class="string">&quot;sentence1&quot;</span>], examples[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;attention_mask&quot;</span>] = inputs[<span class="string">&quot;attention_mask&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;labels&quot;</span>] = examples[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_train_dataset = raw_train_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenized_valid_dataset = raw_valid_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据校准器（自动生成batch）</span></span><br><span class="line">collater = DataCollatorWithPadding(</span><br><span class="line">    tokenizer=tokenizer, return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型 -- 其实Transformer可以直接用AutoModelForSequenceClassification</span></span><br><span class="line"><span class="comment">#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我手工写了分类器层，为了方便大家理解什么叫在Transformer上面做分类任务</span></span><br><span class="line">backbone = AutoModel.from_pretrained(MODEL_NAME)</span><br><span class="line">model = MyClassifier(backbone)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./output&quot;</span>,        <span class="comment"># checkpoint保存路径</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;steps&quot;</span>,    <span class="comment"># 每N步做一次eval</span></span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,             <span class="comment"># 训练epoch数</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">8</span>,  <span class="comment"># 每张卡的batch大小</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,   <span class="comment"># 累加几个step做一次参数更新</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">8</span>,  <span class="comment"># evaluation batch size</span></span><br><span class="line">    logging_steps=<span class="number">20</span>,             <span class="comment"># 每20步eval一次</span></span><br><span class="line">    save_steps=<span class="number">20</span>,                <span class="comment"># 每20步保存一个checkpoint</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,             <span class="comment"># 学习率</span></span><br><span class="line">    warmup_ratio=<span class="number">0.1</span>,               <span class="comment"># 预热（可选）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练器</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, <span class="comment"># 待训练模型</span></span><br><span class="line">    args=training_args, <span class="comment"># 训练参数</span></span><br><span class="line">    data_collator=collater, <span class="comment"># 数据校准器</span></span><br><span class="line">    train_dataset=tokenized_train_dataset, <span class="comment"># 训练集</span></span><br><span class="line">    eval_dataset=tokenized_valid_dataset, <span class="comment"># 验证集</span></span><br><span class="line">    compute_metrics=compute_metrics, <span class="comment"># 评价指标</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用wandb（与huggingface.co同步的机制）</span></span><br><span class="line">os.environ[<span class="string">&quot;WANDB_DISABLED&quot;</span>] = <span class="string">&quot;true&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb">Bert fine-tuning 二分类</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>Fine-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT QLoRA</title>
    <url>/www6vHomeAIGC/2024/01/12/gptPEFTQLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86-1">技术原理 [1]</a></li>
<li><a href="#%E5%AE%9E%E6%88%981-2">实战1 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="技术原理-1">技术原理 [1]</span><a href="#技术原理-1" class="header-anchor">#</a></h1><p>使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。<br>QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。</p>
<ul>
<li><p>4bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。</p>
</li>
<li><p>双量化：对第一次量化后的那些常量再进行一次量化，减少存储空间。</p>
</li>
<li><p>分页优化器:  使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</p>
</li>
</ul>
<img src="/www6vHomeAIGC/2024/01/12/gptPEFTQLora/qlora.png" class>

<p>实验证明，无论是使用16bit、8bit还是4bit的适配器方法，都能够复制16bit全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。</p>
<h1><span id="实战1-2">实战1 [2]</span><a href="#实战1-2" class="header-anchor">#</a></h1><h1><span id="参数">参数</span><a href="#参数" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/636215898">大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/636644164">高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香</a>  先是训练llama-7b, 再是训练llama-65b<br><a href="https://github.com/www6v/llm-action/tree/main/train/qlora">qlora</a> git</p>
</li>
<li><p><a href="https://github.com/zyds/transformers-code/tree/master/04-Kbit%20Training/27-4bits_training">4bits_training</a><br><a href="https://www.bilibili.com/video/BV1DQ4y1t7e8/">【手把手带你实战HuggingFace Transformers-低精度训练篇】4bit量化与QLoRA模型训练</a> V</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671089942">[大模型微调技术] LoRA、QLoRA、QA-LoRA 原理笔记</a><br>1xx. <a href="https://cloud.tencent.com/developer/article/2375230">大模型实操 | LoRA、QLoRA微调大模型实战技巧分享，含常见QA解答！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 计算</title>
    <url>/www6vHomeAIGC/2023/07/01/gptGPUComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%AD%E7%BB%83%E5%9C%BA%E6%99%AF">训练场景</a><ul>
<li><a href="#%E5%85%AC%E5%BC%8F1">公式[1]</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F1">训练的并行计算公式[1]</a></li>
</ul>
</li>
<li><a href="#%E6%8E%A8%E7%90%86%E5%9C%BA%E6%99%AF">推理场景</a><ul>
<li><a href="#%E5%85%AC%E5%BC%8F1-1">公式[1]</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E5%85%B7">工具</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="训练场景">训练场景</span><a href="#训练场景" class="header-anchor">#</a></h1><h3><span id="公式1">公式[1]</span><a href="#公式1" class="header-anchor">#</a></h3><p>训练显存消耗（可估算部分）主要包括：<strong>模型参数（Model）+ 优化器状态（Optimizer status）+梯度值（Gradient）+激活值（Activation）</strong>。根据数值的变化，可将显存消耗分为<strong>静态&#x2F;动态值</strong>。训练过程中，<strong>模型参数、优化器状态</strong>一般不会变化，这两部分归属于<strong>静态值</strong>；<strong>激活值、梯度值</strong>会随着计算过程发生变化，将它们归类到<strong>动态值</strong>。 </p>
<ul>
<li><p>模型显存（Model Memory)</p>
</li>
<li><p>优化器状态（Optimizer status）</p>
</li>
<li><p>梯度值（Gradient）</p>
</li>
<li><p>激活值（Activation）</p>
</li>
<li><p>总结[2]<br><strong>xB的大模型，训练的显存占用约为12-16x GB</strong>。（默认全精度float32存储）</p>
</li>
</ul>
<h3><span id="训练的并行计算公式1">训练的并行计算公式[1]</span><a href="#训练的并行计算公式1" class="header-anchor">#</a></h3><ul>
<li>3D并行</li>
<li>重计算（Recomputation）</li>
<li>Zero方法</li>
</ul>
<h1><span id="推理场景">推理场景</span><a href="#推理场景" class="header-anchor">#</a></h1><h3><span id="公式1">公式[1]</span><a href="#公式1" class="header-anchor">#</a></h3><p>  1.2 * ModelMemory</p>
<h1><span id="工具">工具</span><a href="#工具" class="header-anchor">#</a></h1><ul>
<li><p>使用 <strong>HuggingFace Accelerate Web 工具</strong>计算，实际值 &#x3D; Total Size x 1.2<br><a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">model-memory-usage</a> git</p>
</li>
<li><p>使用 <strong>HuggingFace Accelerate 命令行工具</strong>计算</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">accelerate estimate-memory meta-llama/Llama-2-13b-chat-hf</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/687226668">[LLM]大模型显存计算公式与优化</a>  定量</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/680434161">大模型训练显存估算</a>  定性</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>App Agent</title>
    <url>/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="app-agent">App Agent</span><a href="#app-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/677071947">AppAgent源码分析&amp;思考</a><br><a href="https://github.com/mnotgod96/AppAgent">https://github.com/mnotgod96/AppAgent</a><br><a href="https://icoz69.github.io/">https://icoz69.github.io/</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/681424409">【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent</a><br>   <a href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion</title>
    <url>/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="图像编辑">图像编辑</span><a href="#图像编辑" class="header-anchor">#</a></h1><h3><span id="大类">大类</span><a href="#大类" class="header-anchor">#</a></h3><ul>
<li>从图片编辑的任务方面可以被分为3个大类<ul>
<li>语义编辑semantic editing </li>
<li>风格编辑stylistic editing</li>
<li>结构编辑structural editing</li>
</ul>
</li>
</ul>
<h3><span id="approaches">APPROACHES</span><a href="#approaches" class="header-anchor">#</a></h3><ul>
<li><p>TRAINING-BASED APPROACHES</p>
<ul>
<li>InstructPix2Pix</li>
</ul>
</li>
<li><p>TESTING-TIME FINETUNING APPROACHES</p>
</li>
<li><p>TRAINING AND FINETUNING FREE APPROACHES</p>
</li>
</ul>
<h1><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h1><ul>
<li>diffusers [10]</li>
</ul>
<ul>
<li>controlnet</li>
<li>dreambooth</li>
<li>instruct_pix2pix</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="图像编辑">图像编辑</span><a href="#图像编辑" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.csdn.net/huzimu_/article/details/136547375">论文阅读：Diffusion Model-Based Image Editing: A Survey</a><br><a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">Repo</a> git</li>
</ol>
<p>1xx.<br>《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》<br><a href="https://github.com/xinchengshuai/Awesome-Image-Editing">A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models</a></p>
<h3><span id="instructpix2pix">InstructPix2Pix</span><a href="#instructpix2pix" class="header-anchor">#</a></h3><p><a href="https://zhuanlan.zhihu.com/p/655135961">InstructPix2Pix：用指令给图像做修改</a><br><a href="https://zhuanlan.zhihu.com/p/655372592">Prompt-to-prompt：让生成的图像保持一致</a><br><a href="https://github.com/timothybrooks/instruct-pix2pix">Repo</a> git</p>
<h3><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://github.com/huggingface/diffusers/tree/main/examples">Repo diffusers</a> git</li>
</ol>
<h3><span id="llms-meet-multimodal-generation-and-editing-a-survey">《LLMs Meet Multimodal Generation and Editing: A Survey》</span><a href="#llms-meet-multimodal-generation-and-editing-a-survey" class="header-anchor">#</a></h3><p>Image Generation， Image Editing<br><a href="https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation">Repo</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)CLIP</title>
    <url>/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="clip-training-9">CLIP Training [9]</span><a href="#clip-training-9" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - ResNet or Vision Transformer</span><br><span class="line"># text_encoder - CBOW or Text Transformer</span><br><span class="line"># I[n, h, w, c] - minibatch of aligned images</span><br><span class="line"># T[n, l] - minibatch of aligned texts</span><br><span class="line"># W_i[d_i, d_e] - learned proj of image to embed</span><br><span class="line"># W_t[d_t, d_e] - learned proj of text to embed</span><br><span class="line"># t - learned temperature parameter</span><br><span class="line"># extract feature representations of each modality</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 1、图像/文字数据过image/text encoder，提取单模态特征</span><br><span class="line"># 每张图片对应一个基本特征I_i</span><br><span class="line"># 每张文字对应一个基本特征T_i</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_f = image_encoder(I) #[n, d_i]</span><br><span class="line">T_f = text_encoder(T) #[n, d_t]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 2. 图像/文字的基本特征过多模态Embedding，提取多模态特征</span><br><span class="line"># 同时对这两个多模态特征做Layer Norm</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1) # [n, d_i] * [d_i, d_e] = [n, d_e]</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # [n, d_t] * [d_t, d_e] = [n, d_e]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 3、计算图片-文字向量的余弦相似度</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t) # [n, n]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 4、计算Loss</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t)/2</span><br></pre></td></tr></table></figure>



<ul>
<li>CLIP分为<strong>按行计算Loss</strong>和<strong>按列计算Loss</strong></li>
<li><strong>按行计算Loss</strong>，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字。</li>
<li><strong>按列计算Loss</strong>，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片。</li>
<li><strong>最后将这两个Loss相加取平均</strong>，代表我们在模型优化过程中<strong>考虑了“图片-&gt;文字”和“文字-&gt;图片”的双向关系</strong>。</li>
</ul>
<h1><span id="demo10">Demo[10]</span><a href="#demo10" class="header-anchor">#</a></h1><p>【基于clip on  resnet,   数据集为mnist中的&lt;数字文本，数字图片&gt;对】</p>
<h1><span id="open_clip11">open_clip[11]</span><a href="#open_clip11" class="header-anchor">#</a></h1><ul>
<li>Training CLIP</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m training.main \</span><br><span class="line">    --save-frequency <span class="number">1</span> \</span><br><span class="line">    --zeroshot-frequency <span class="number">1</span> \</span><br><span class="line">    --report-to tensorboard \</span><br><span class="line">    --train-data=<span class="string">&quot;/path/to/train_data.csv&quot;</span>  \      <span class="comment"># 训练数据 </span></span><br><span class="line">    --val-data=<span class="string">&quot;/path/to/validation_data.csv&quot;</span>  \   <span class="comment"># 验证数据</span></span><br><span class="line">    --csv-img-key filepath \</span><br><span class="line">    --csv-caption-key title \</span><br><span class="line">    --imagenet-val=/path/to/imagenet/root/val/ \</span><br><span class="line">    --warmup <span class="number">10000</span> \      <span class="comment">#</span></span><br><span class="line">    --batch-size=<span class="number">128</span> \    <span class="comment">#</span></span><br><span class="line">    --lr=<span class="number">1e-3</span> \           <span class="comment">#</span></span><br><span class="line">    --wd=<span class="number">0.1</span> \       </span><br><span class="line">    --epochs=<span class="number">30</span> \         <span class="comment">#</span></span><br><span class="line">    --workers=<span class="number">8</span> \</span><br><span class="line">    --model RN50          <span class="comment"># 模型</span></span><br></pre></td></tr></table></figure>

<h1><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h1><h3><span id="方法20">方法[20]</span><a href="#方法20" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/chinese-clip.webp" class>

<p>我们的核心方法在于把训练分为<strong>两阶段</strong>（如上图所示），<strong>第一阶段</strong>和LiT是一致的，<strong>冻结图像塔</strong>，<strong>让文本塔表示接近图像塔表示</strong>。当训练继续但下游精度不能再产生显著提升，即下游零样本检索的精度，我们就把训练切换到<strong>第二阶段</strong>，即<strong>解除图像塔的参数冻结，继续用contrastive tuning预训练</strong>，同样直到下游精度没有显著提升。<strong>后者的意义在于让图像塔能拟合中文世界的图像数据的分布，学习中文世界的知识</strong>。更多实验参数欢迎查看论文的附录部分。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="9">
<li><p><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13K421v7Ar/">【多模态】复现OpenAI的CLIP模型</a> V<br><a href="https://github.com/owenliang/mnist-clip">mnist-clip Repo</a> git</p>
</li>
<li><p><a href="https://github.com/mlfoundations/open_clip">open_clip Repo</a> git<br><a href="https://colab.research.google.com/drive/1TEUe2j2oXi-sKiteGYUhsCtdvXocI24w#scrollTo=YPHN7PJgKOzb">Interacting with open_clip</a></p>
</li>
</ol>
<h3><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://zhuanlan.zhihu.com/p/580546929">中文CLIP模型卷土重来，这次加量不加价！</a> 论文</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/539374033">【已重新开源】CLIP的中文副本？说不定有惊喜呢</a></p>
<p>1xx. <a href="https://github.com/www6v/Chinese-CLIP">Chinese-CLIP Repo</a> git</p>
<p>1xx. <a href="https://modelscope.cn/studios/iic/chinese_clip_applications/summary">中文CLIP文到图搜索应用</a> demo</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/680405647">AIGC之图片生成——基于clip内容检索</a><br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu/tree/main/APP_example/clip_retrieval">clip_retrieval</a> git<br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu">demos Repo</a>  readme有解释</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. langchain 中有CLIP的实现</p>
<p>1xx. <a href="https://github.com/jina-ai/clip-as-service">GitHub - jina-ai&#x2F;clip-as-service: Scalable embedding, reasoning, ranking for images and sentences with CLIP</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RAG</title>
    <url>/www6vHomeAIGC/2023/06/25/gptAgenticRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="agentic-rag-1">Agentic RAG [1]</span><a href="#agentic-rag-1" class="header-anchor">#</a></h1><p>Agentic RAG 和简单 RAG 的最大区别在于 <strong>Agentic RAG 引入了 Agent 的动态编排机制，因此可以根据用户提问的不同意图，引入反馈和查询改写机制，并进行“多跳”式的知识推理，从而实现对复杂提问的回答</strong>。</p>
<ul>
<li><p>Self-RAG 是相对初级的 Agentic RAG，RAGFlow 中也已提供了相关实现。实践证明，Self-RAG 对于较复杂的多跳问答和多步推理可以明显提升性能。</p>
</li>
<li><p>Adaptive RAG</p>
<ul>
<li>开放域问答</li>
<li>多跳问答</li>
<li>自适应检索</li>
</ul>
</li>
<li><p>Adaptive RAG [2]<br>Adaptive-RAG的核心在于它能够通过一个<strong>分类器</strong>来评估问题的复杂性，然后根据这个评估结果选择最合适的处理策略。<br><strong>分类器是</strong>一个<strong>较小的语言模型</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s/A8kfbH70sdU5Gd20K9Y0Lw">Agentic RAG 与图任务编排</a><br>Self-RAG     Adaptive RAG</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/688547968">Adaptive-RAG：性能提升50%以上的高效RAG策略</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/wuyMN7CLAT9HGYlmjLWUtA">LlamaIndex团队技术报告：“RAG的尽头是Agent”</a></p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/">Building Agentic RAG with LlamaIndex - DeepLearning.AI</a><br>    <a href="https://github.com/www6v/deeplearningAI/tree/master/Building%20Agentic%20RAG%20with%20Llamaindex">Building Agentic RAG with LlamaIndex-Repo</a> git<br>    <a href="https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent/">Building a Custom Agent - LlamaIndex</a><br>    <a href="https://llamahub.ai/?tab=agent">Llama Hub</a></p>
<p>​</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG KG</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGKG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="vectorkg-rag1516">Vector+KG RAG[15][16]</span><a href="#vectorkg-rag1516" class="header-anchor">#</a></h3><h3><span id="rag-多跳问题">RAG 多跳问题</span><a href="#rag-多跳问题" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="rag-多跳问题">RAG 多跳问题</span><a href="#rag-多跳问题" class="header-anchor">#</a></h3><p>1xx. <a href="https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/">Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</a></p>
<p><a href="https://cloud.tencent.com.cn/developer/article/2409038">知识图谱和 LLM：多跳问答-腾讯云开发者社区-腾讯云</a></p>
<p><a href="https://blog.csdn.net/qq_41185868/article/details/138514051">LLMs之KG-RAG：KG-RAG&#x2F;GraphRAG(基于知识图谱的RAG系统)的简介(可以解决多跳问题&#x2F;同时支持结构化和非结构化数据查询)、经验技巧、案例应用之详细攻略-CSDN博客</a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_36931982/article/details/139118215">MultiHop-RAG：多跳查询的基准检索增强生成_rag多跳查询-CSDN博客</a></p>
<h3><span id="llmkg-知识图谱">LLM+KG  知识图谱</span><a href="#llmkg-知识图谱" class="header-anchor">#</a></h3><ol start="15">
<li><p><a href="https://neo4j.com/developer-blog/unstructured-knowledge-graph-neo4j-langchain/">Enhanced QA Integrating Unstructured Knowledge Graph Using Neo4j and LangChain</a>  </p>
</li>
<li><p><a href="https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/">Using a Knowledge Graph to implement a DevOps RAG application</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/buV1j4DtDiVavtGCJIsedQ">大模型辅助图谱构建的4个策略对比：兼看大模型与知识图谱结合的3个综述 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG RAGflow</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGRAGflow/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ragflow12elmo">RAGflow[1,2][ELmo]</span><a href="#ragflow12elmo" class="header-anchor">#</a></h1><p>RAGFlow 是一个端到端的 RAG 引擎，它解决数据的问题，因为如果不对用户数据加以区分和清晰，识别其中的语义，就容易导致 Garbage In Garbage Out。RAGFlow 包含了如下的完整 RAG 流程，确保数据从 Garbage In Garbage Out 变为 Quality In Quality Out。</p>
<p>RAGFlow 的最大特色，就是多样化的文档智能处理，因此它没有采用现成的 RAG 中间件，而是完全重新研发了一套智能文档理解系统，并以此为依托构建 RAG 任务编排体系。</p>
<p>这个系统的特点包含：</p>
<ol>
<li>它是一套基于 AI 模型的<strong>智能文档处理系统</strong>；</li>
<li>它是一套包含<strong>各种不同模板</strong>的智能文档处理系统；</li>
<li>智能文档处理的<strong>可视化和可解释性</strong>；</li>
<li>RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="ragflow">RAGFlow</span><a href="#ragflow" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV12T42117VT/">RAGFlow：采用OCR和深度文档理解结合的新一代 RAG 引擎</a> V</li>
<li><a href="https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs">检索增强生成引擎 RAGFlow 正式开源！</a><br>1xx. <a href="https://mp.weixin.qq.com/s/8qms4nxVsX43WSWolXgx7w">7.8K Star RAG 引擎：基于深度文档理解，最大程度降低幻觉、无限上下文快速完成 “大海捞针” 测试！</a></li>
</ol>
<p>1xx.  <a href="http://demo.ragflow.io/">RAGFlow Demo</a><br>    <a href="https://github.com/infiniflow/ragflow">ragflow Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG Qanything</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGQanything/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="qanything">QAnything</span><a href="#qanything" class="header-anchor">#</a></h1><h3><span id="arch1">Arch[1]</span><a href="#arch1" class="header-anchor">#</a></h3><p><img src="https://github.com/netease-youdao/QAnything/raw/master/docs/images/qanything_arch.png" alt="Arch"></p>
<ul>
<li><p>索引（indexing）<br>通过Embedding为每一个文本块生成一个向量表示，用于计算<strong>文本向量</strong>和<strong>问题向量</strong>之间的<strong>相似度</strong>。创建索引将原始文本块和Embedding向量以键值对的形式存储，以便将来进行快速和频繁的搜索。</p>
</li>
<li><p>检索（Retrieval）<br>使用Embedding模型将用户输入问题转换为向量，计算问题的Embedding向量和语料库中文本块Embedding向量之间的相似度，选择<strong>相似度最高的前K个文档块</strong>作为当前问题的增强上下文信息。</p>
</li>
<li><p>生成（Generation）<br>将检索得到的前K个文本块和用户问题一起送进大模型，让大模型基于给定的文本块来回答用户的问题。</p>
</li>
</ul>
<h3><span id="1st-retrievalembedding1">1st Retrieval（embedding）[1]</span><a href="#1st-retrievalembedding1" class="header-anchor">#</a></h3><ul>
<li><p>Bcembedding模型 [3]</p>
<ul>
<li>中英双语和跨语种能力</li>
<li>多领域覆盖</li>
</ul>
</li>
<li><p>Embedding 可以给出一个得分，但是这个得分描述的更多的是<strong>相似性</strong>。Embedding本质上是一个<strong>双编码器</strong>，两个文本在模型内部没有任何信息交互。只在最后计算两个向量的余弦相似度时才进行唯一一次交互。所以Embedding检索只能把<strong>最相似的</strong>文本片段给你，<strong>没有</strong>能力来判断候选文本和query之间的<strong>相关性</strong>。但是<strong>相似又不等于相关</strong>。</p>
</li>
</ul>
<p>【embedding -&gt; 相似性】</p>
<img src="/www6vHomeAIGC/2023/06/19/gptRAGQanything/embedding.png" class>

<h3><span id="2nd-retrievalrerank1">2nd Retrieval（rerank）[1]</span><a href="#2nd-retrievalrerank1" class="header-anchor">#</a></h3><ul>
<li><p>Rerank [3]</p>
</li>
<li><p>Rerank本质是一个<strong>Cross-Encoder</strong>的模型。Cross-Encoder能让两个文本片段一开始就在BERT模型各层中通过self-attention进行交互。</p>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/06/19/gptRAGQanything/reranker.png" class>

<p>【rerank -&gt; 相关性】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="qanything">QAnything</span><a href="#qanything" class="header-anchor">#</a></h3><ol>
<li><a href="https://github.com/netease-youdao/QAnything">QAnything Repo</a> git</li>
<li>xxx</li>
<li><a href="https://www.bilibili.com/video/BV1HF4m1w7rY/">有道QAnything背后的故事：关于RAG的一点经验分享</a> V<br> <a href="https://mp.weixin.qq.com/s/FUex1Q984-IhQ-FoLZTf5Q">有道QAnything背后的故事—关于RAG的一点经验分享</a>   文字版<br>[公众号有其他文章]</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489671&idx=1&sn=564a232c3c7919c70a7a1cf5efa77628">前沿重器[45] RAG开源项目Qanything源码阅读1-概述+服务</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/16/gptInferRayPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a><ul>
<li><a href="#%E7%8E%AF%E5%A2%83">环境</a></li>
<li><a href="#%E5%AE%9E%E6%88%981">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%98320">实战3[20]</a></li>
<li><a href="#%E5%AE%9E%E6%88%984">实战4</a></li>
</ul>
</li>
<li><a href="#monitor40">monitor[40]</a><ul>
<li><a href="#ray-dashboard41">Ray Dashboard[41]</a></li>
<li><a href="#ray-logging">Ray logging</a></li>
<li><a href="#built-in-ray-serve-metrics">Built-in Ray Serve metrics</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%981-1">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982-1">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%983">实战3</a></li>
<li><a href="#%E5%AE%9E%E6%88%984-1">实战4</a></li>
<li><a href="#monitor">monitor</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><h3><span id="环境">环境</span><a href="#环境" class="header-anchor">#</a></h3><p>modelscope  GPU</p>
<h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ul>
<li><p>脚本[1]</p>
</li>
<li><p>遇到的异常[2]</p>
</li>
</ul>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ul>
<li><p>脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## 变更模型名字</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## import &#x27;modelscope&#x27; package</span></span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>异常[11]</p>
</li>
</ul>
<h3><span id="实战320">实战3[20]</span><a href="#实战320" class="header-anchor">#</a></h3><ul>
<li>脚本<br>vllm   0.2.3 -&gt; 报异常<br>vllm  0.3.3 -&gt; 报另一个异常</li>
</ul>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ul>
<li><p>脚本 [30]</p>
</li>
<li><p>异常 [31]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 运行这个命令报异常</span><br><span class="line">python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="monitor40">monitor[40]</span><a href="#monitor40" class="header-anchor">#</a></h1><h3><span id="ray-dashboard41">Ray Dashboard[41]</span><a href="#ray-dashboard41" class="header-anchor">#</a></h3><h3><span id="ray-logging">Ray logging</span><a href="#ray-logging" class="header-anchor">#</a></h3><p>Loki  grafana</p>
<h3><span id="built-in-ray-serve-metrics">Built-in Ray Serve metrics</span><a href="#built-in-ray-serve-metrics" class="header-anchor">#</a></h3><p>Prometheus </p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://docs.ray.io/en/master/serve/tutorials/vllm-example.html">Serve a Large Language Model with vLLM</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/60750288/invalid-device-id-when-using-pytorch-dataparallel">Invalid device id when using pytorch dataparallel！</a>  运行时碰到的异常</p>
</li>
</ol>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://github.com/vllm-project/vllm/blob/main/examples/offline_inference_distributed.py">examples&#x2F;offline_inference_distributed.py</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device</a></p>
</li>
</ol>
<h3><span id="实战3">实战3</span><a href="#实战3" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://github.com/asprenger/ray_vllm_inference">Ray vLLM Interence</a></li>
</ol>
<p>1xx. <a href="https://github.com/ray-project/langchain-ray/tree/main">GitHub - ray-project&#x2F;langchain-ray: Examples on how to use LangChain and Ray</a> git</p>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ol start="30">
<li><p><a href="https://blog.csdn.net/engchina/article/details/135455197">在甲骨文云上用 Ray +Vllm 部署 Mixtral 8*7B 模型_mixtral 8x7b 部署-CSDN博客</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device-CSDN博客</a></p>
</li>
</ol>
<h3><span id="monitor">monitor</span><a href="#monitor" class="header-anchor">#</a></h3><ol start="40">
<li><p><a href="https://docs.ray.io/en/master/serve/monitoring.html">Monitor Your Application</a></p>
</li>
<li><p><a href="https://docs.ray.io/en/master/ray-observability/getting-started.html">Ray Dashboard </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Flash Attention</title>
    <url>/www6vHomeAIGC/2023/06/13/gptFlashAttention/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="核心要点1">核心要点[1]</span><a href="#核心要点1" class="header-anchor">#</a></h1><ul>
<li><p>为什么<strong>加快了计算</strong>？Fast</p>
<ul>
<li><strong>降低了耗时的HBM访问次数</strong>。采用Tiling技术分块<strong>从HBM加载数据到SRAM进行融合计算</strong>。</li>
</ul>
</li>
<li><p>为什么<strong>节省了内存</strong>？Memory-Efficient</p>
<ul>
<li><strong>不再对中间矩阵S，P进行存储</strong>。在反向的时候通过Recomputation重新计算来计算梯度。</li>
</ul>
</li>
<li><p>为什么是<strong>精准注意力</strong>？Exact Attention</p>
<ul>
<li>算法流程只是<strong>分块计算</strong>，无近似操作。</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1SW4y1X7kh?spm_id_from=333.880.my_history.page.click">FlashAttention: 更快训练更长上下文的GPT【论文粗读·6】</a> v</li>
</ol>
<p><a href="https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh">FlashAttention: 更快训练更长上下文的GPT</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1zs4y1J7tb?spm_id_from=333.880.my_history.page.click">论文分享：新型注意力算法FlashAttention</a> v</p>
<p>1xx.  <a href="https://www.bilibili.com/video/BV1he411d7on?spm_id_from=333.880.my_history.page.click"><em>Flash</em>Attention与标准注意力机制模型比较</a> v</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Kj411e7gJ?spm_id_from=333.880.my_history.page.click"><em>flash</em>attention原理深入分析</a> v</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>FlashAttention</category>
      </categories>
      <tags>
        <tag>FlashAttention</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理 vLLM</title>
    <url>/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><h3><span id="llama-chinese-on-vllm11">Llama-Chinese on vLLM[11]</span><a href="#llama-chinese-on-vllm11" class="header-anchor">#</a></h3><ul>
<li>vLLM异步推理</li>
<li>流式返回</li>
</ul>
<h3><span id="qwen-on-vllm12">Qwen on vLLM[12]</span><a href="#qwen-on-vllm12" class="header-anchor">#</a></h3><ul>
<li>vLLM异步推理</li>
<li>流式返回</li>
</ul>
<h3><span id="qwen-on-vllm13">Qwen on vLLM[13]</span><a href="#qwen-on-vllm13" class="header-anchor">#</a></h3><ul>
<li><p>离线推理   </p>
</li>
<li><p>适配OpenAI-API的API服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server \</span><br><span class="line">    --model Qwen/Qwen1.5-7B-Chat</span><br></pre></td></tr></table></figure>
</li>
<li><p>多卡推理<br>传递参数 <code>--tensor-parallel-size</code> 来运行多GPU服务</p>
</li>
<li><p>量化推理<br>在运行服务时添加 <code>--quantization</code> 参数</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="11">
<li><a href="https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/GPU/vllm_example/api_server.py">Llama-Chinese Repo</a> git</li>
<li><a href="https://github.com/owenliang/qwen-vllm/blob/master/vllm_server.py">qwen-vllm Repo</a> git<br><a href="https://github.com/QwenLM/Qwen/blob/main/README_CN.md#%E9%83%A8%E7%BD%B2">Qwen（通义千问） Repo</a>  git</li>
<li><a href="https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html">qwen doc</a></li>
</ol>
<h3><span id="源代码">源代码</span><a href="#源代码" class="header-anchor">#</a></h3><p>1xx.<a href="https://blog.csdn.net/just_sort/article/details/132115735">VLLM推理流程梳理</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/641999400">LLM 高速推理框架 vLLM 源代码分析 &#x2F; vLLM Source Code Analysis</a> ***<br>1xx. <a href="https://www.bilibili.com/video/BV1qE42157Rn/">vLLM源码阅读s2——是如何进行离线推理的</a> V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/11/gptInferRay/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="architecture-overview">Architecture Overview</span><a href="#architecture-overview" class="header-anchor">#</a></h1><h3><span id="application-concepts-1">Application concepts [1]</span><a href="#application-concepts-1" class="header-anchor">#</a></h3><ul>
<li>Task - A remote function invocation. </li>
<li>Object - An application value.</li>
<li>Actor - a stateful worker process (an instance of a <code>@ray.remote</code> class).</li>
<li>Driver - The program root, or the “main” program.</li>
<li>Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment.</li>
</ul>
<h3><span id="design-1">Design [1]</span><a href="#design-1" class="header-anchor">#</a></h3><ul>
<li>Components<ul>
<li>One or more worker processes</li>
<li>A raylet. <ul>
<li>scheduler</li>
<li>object store</li>
</ul>
</li>
<li>head node<ul>
<li>Global Control Service (GCS)</li>
<li>driver process(es)</li>
<li>cluster-level services</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="spark-vs-ray10">Spark vs. Ray[10]</span><a href="#spark-vs-ray10" class="header-anchor">#</a></h1><ul>
<li><p>总的来说，Ray和Spark的主要差别在于他们的<strong>抽象层次</strong>。<strong>Spark</strong>对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。<strong>Ray</strong>的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，<strong>Ray揭示和暴露了并行，而Spark抽象和隐藏了并行</strong>。</p>
</li>
<li><p>就架构而言，<strong>Spark</strong>采用<strong>BSP模型</strong>，是无副作用的，而<strong>Ray</strong>本质上是一个<strong>RPC 框架+Actor框架+对象存储</strong>。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://developer.volcengine.com/articles/7241442880106004536">基于 Ray 的大规模离线推理</a> 字节<br>   <a href="https://mp.weixin.qq.com/s/mU2RymHIHj8mJiDWBUAdWA">字节跳动基于 Ray 的大规模离线推理</a></p>
<p>1xx. <a href="https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.eg7m6lz2y48u">Ray Design Patterns</a> 查看-&gt;模式</p>
<p>1xx. <a href="https://blog.csdn.net/2401_83124266/article/details/136428395">大模型训练部署利器–开源分布式计算框架Ray原理介绍</a></p>
<h3><span id="spark-vs-ray">Spark vs. Ray</span><a href="#spark-vs-ray" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.zhihu.com/question/432813259/answer/2335473370">加州大学伯克利分校为何能连续孵化出 Mesos,Spark,Alluxio,Ray 等重量级开源项目?</a> 孙挺Sunt</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/junerli/article/details/138476201">分布式领域计算模型及Spark&amp;Ray实现对比</a></p>
<h3><span id="internal">Internal</span><a href="#internal" class="header-anchor">#</a></h3><ol>
<li><a href="https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.iyrm5j2gcdoq">Ray v2 Architecture</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/111340572">Ray 分布式计算框架介绍</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/344736949">Ray 1.0 架构解读</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 评估</title>
    <url>/www6vHomeAIGC/2023/06/07/gptRAGEval/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





<p>1xx. <a href="https://www.bilibili.com/video/BV1Jz421Q7Lw/">如何利用RAGAs评估RAG系统的好坏</a></p>
<p><a href="https://github.com/blackinkkkxi/RAG_langchain/blob/main/learn/evaluation/RAGAS-langchian.ipynb">使用LangChain和RAGAS对RAG系统进行自动有效评估</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1aZ421W7DB/">一次搞懂RAG评估，三个角度LangChain，LlamaIndex，RAGAS</a><br>   <a href="https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0053">RAG评估资料大全 </a><br>   RAG评估指标：两种视角理解评估指标<br>   <a href="https://docs.smith.langchain.com/old/cookbook/testing-examples/rag_eval">RAG Evaluation</a><br>   <a href="https://docs.smith.langchain.com/old/cookbook/testing-examples/ragas">RAG evaluation with RAGAS</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>AutoGen</title>
    <url>/www6vHomeAIGC/2023/06/05/gptAgentAutogen/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="autogen">AutoGen</span><a href="#autogen" class="header-anchor">#</a></h1><h3><span id="demo-0123">Demo [0,1,2,3]</span><a href="#demo-0123" class="header-anchor">#</a></h3><h3><span id="autogen-studio10">AutoGen Studio[10]</span><a href="#autogen-studio10" class="header-anchor">#</a></h3><h3><span id="pattern20">Pattern[20]</span><a href="#pattern20" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="demos">Demos</span><a href="#demos" class="header-anchor">#</a></h3><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/671782824">AutoGen实战应用(一)：代码生成、执行和调试</a><br> 基于官方例子</p>
</li>
<li><p><a href="https://microsoft.github.io/autogen/docs/Examples">Examples</a><br><a href="https://github.com/www6v/AIGC/tree/master/agent/autogen">Repo</a> git</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/664937747">Autogen 新手指南：基础概念和应用</a><br> Autogen的高级应用 官方例子</p>
</li>
<li><p><a href="https://developer.aliyun.com/article/1394332">AutoGen多代理对话项目示例和工作流程分析</a><br> 自定义方法fetch_prices,  多agent</p>
</li>
</ol>
<p>1xx. <a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat/">Multi-agent Conversation Framework</a>   </p>
<p>1xx. <a href="https://microsoft.github.io/autogen/blog">Blog</a></p>
<h3><span id="autogen-studio">AutoGen Studio</span><a href="#autogen-studio" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/680797754">LLM之Agent（十）| 本地安装Microsoft AutoGen Studio 2.0教程</a></li>
</ol>
<h3><span id="pattern">Pattern</span><a href="#pattern" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://github.com/www6v/deeplearningAI/tree/master/AI%20Agentic%20Design%20Patterns%20with%20AutoGen">AI Agentic Design Patterns with AutoGen</a></li>
</ol>
<p>1xx. <a href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/">Agentic Design Patterns Part 1</a></p>
<h3><span id="源码">源码</span><a href="#源码" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/670586507">Autogen的基本框架,人工智能的管理系统——Autogen系列02</a><br>    源码解析<br>1xx. <a href="https://zhuanlan.zhihu.com/p/699819907">AUTOGEN | 上手与源码分析</a>    </p>
<h3><span id="paper">paper</span><a href="#paper" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/660027092">AutoGen：通过多agent对话支持下一代 LLM 应用程序</a> paper 中文</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent  Memory</title>
    <url>/www6vHomeAIGC/2023/06/05/gptAgentMemory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="memory-sources1">Memory Sources[1]</span><a href="#memory-sources1" class="header-anchor">#</a></h1><ul>
<li><p>Inside-trial Information: 智能体在进行本次任务时与环境交互的历史信息。</p>
</li>
<li><p>Cross-trial Information: 智能体在此前完成该类任务的历史经验信息。</p>
</li>
<li><p>External Knowledge: 智能体在当前交互环境之外所获得的信息。</p>
</li>
</ul>
<p>与充当<strong>短期记忆</strong>的<strong>inside-trial information</strong>相反，<strong>cross-trial information</strong>可以被视为<strong>长期记忆</strong>；</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/696745373">论文分享：A Survey on the Memory Mechanism of Large Language Model based Agents</a><br>1xx. <a href="https://blog.csdn.net/DLparkour/article/details/138506437">论文阅读：A Survey on the Memory Mechanism of Large Language Model based Agents</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/696105075">【Agent技术洞察】01-增强大语言模型 Agents 的工作记忆能力</a></p>
<h3><span id="项目">项目</span><a href="#项目" class="header-anchor">#</a></h3><p><a href="https://github.com/kingjulio8238/memary">memary Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)推理 TensorRT-LLM</title>
    <url>/www6vHomeAIGC/2023/06/02/gptInferTensorRT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#tensorrt-llm">TensorRT-LLM</a><ul>
<li><a href="#key-features-2">key features [2]</a></li>
</ul>
</li>
<li><a href="#tensorrt-llm-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2">TensorRT-LLM 推理部署</a></li>
<li><a href="#triton-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2">Triton 推理部署</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="tensorrt-llm">TensorRT-LLM</span><a href="#tensorrt-llm" class="header-anchor">#</a></h1><h3><span id="key-features-2">key features [2]</span><a href="#key-features-2" class="header-anchor">#</a></h3><ul>
<li>Flash Attention</li>
<li>MHA&#x2F;MQA&#x2F;GQA</li>
<li><strong>Quantization</strong><ul>
<li>Weight-Only</li>
<li>SmoothQuant</li>
<li><strong>GPTQ</strong></li>
<li><strong>AWQ</strong></li>
<li>FP8</li>
</ul>
</li>
<li>Paged <strong>KV Cache</strong> for the Attention</li>
<li>Multi-GPU Multi-Node</li>
<li><strong>TP(Tensor Parallelism)&#x2F;PP(Pipeline Parallelism)</strong></li>
<li>In-flight <strong>Batching</strong></li>
</ul>
<h1><span id="tensorrt-llm-推理部署">TensorRT-LLM 推理部署</span><a href="#tensorrt-llm-推理部署" class="header-anchor">#</a></h1><p>[基于docker的部署]</p>
<h1><span id="triton-推理部署">Triton 推理部署</span><a href="#triton-推理部署" class="header-anchor">#</a></h1><p>[基于k8s的部署]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol start="2">
<li><a href="https://github.com/NVIDIA/TensorRT-LLM/">TensorRT-LLM</a> git</li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/666849728">TensorRT-LLM保姆级教程（一）-快速入门</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/667572720">TensorRT-LLM保姆级教程（二）-离线环境搭建、模型量化及推理</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/669576221">TensorRT-LLM（持续更新）</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/629336492">模型推理服务化框架Triton保姆式教程（一）：快速入门</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/634143650">模型推理服务化框架Triton保姆式教程（二）：架构解析</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/634444666">模型推理服务化框架Triton保姆式教程（三）：开发实践</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理 KV Cache</title>
    <url>/www6vHomeAIGC/2023/06/01/gptInferKVCache/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache是否使用对比1">KV Cache是否使用对比[1]</span><a href="#kv-cache是否使用对比1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/06/01/gptInferKVCache/1.webp" class>
<img src="/www6vHomeAIGC/2023/06/01/gptInferKVCache/2.webp" class>
<img src="/www6vHomeAIGC/2023/06/01/gptInferKVCache/3.webp" class>
<img src="/www6vHomeAIGC/2023/06/01/gptInferKVCache/4.webp" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h3><ol>
<li><a href="https://juejin.cn/post/7362789570217885759">大模型推理优化技术-KV Cache </a> 有代码 ***<br>1xx. <a href="https://zhuanlan.zhihu.com/p/659770503">NLP（二十）：漫谈 KV Cache 优化方法，深度理解 StreamingLLM</a> ***<br>1xx. <a href="https://zhuanlan.zhihu.com/p/662498827">大模型推理加速：看图学KV Cache</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理 vLLM</title>
    <url>/www6vHomeAIGC/2023/05/31/gptInfervLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#key-features-2">Key features [2]</a></li>
<li><a href="#architecture3">Architecture[3]</a></li>
<li><a href="#pagedattention4">PagedAttention[4]</a></li>
<li><a href="#continuous-batching101">continuous batching[10.1]</a><ul>
<li><a href="#%E6%9C%B4%E7%B4%A0%E6%89%B9%E5%A4%84%E7%90%86-%E9%9D%99%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86">朴素批处理 &#x2F; 静态批处理</a></li>
<li><a href="#%E8%BF%9E%E7%BB%AD%E6%89%B9%E5%A4%84%E7%90%86">连续批处理</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#continuous-batching">continuous batching</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="key-features-2">Key features [2]</span><a href="#key-features-2" class="header-anchor">#</a></h1><ul>
<li><strong>page attention</strong>[1]<br> memory sharing</li>
<li>Continuous <strong>batching</strong> of incoming requests</li>
<li>Quantization: <ul>
<li><strong>GPTQ</strong></li>
<li><strong>AWQ</strong></li>
<li>SqueezeLLM</li>
<li><strong>FP8 KV Cache</strong></li>
</ul>
</li>
</ul>
<h1><span id="architecture3">Architecture[3]</span><a href="#architecture3" class="header-anchor">#</a></h1><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/663e103a10d15c2ecfd84ac9_K3J79pEM-PAXQiK8DfT2YlotBhcZeLy-UFqhja4dFMDp4478X1tfCGnEgWDkNAMIOMXQvCVo_5EWsVspC07wSLaD4T3n_oqCf3i8mdFFcV1uDCbcmD0-thbwVdbcpTA41teD8ErJxi3jTIrXZsAjcm4.png" alt="Architecture"></p>
<h1><span id="pagedattention4">PagedAttention[4]</span><a href="#pagedattention4" class="header-anchor">#</a></h1><p><img src="https://blog.vllm.ai/assets/figures/annimation0.gif" alt="PagedAttention: KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space. "></p>
<p><img src="https://blog.vllm.ai/assets/figures/annimation1.gif" alt="Example generation process for a request with PagedAttention. "></p>
<h1><span id="continuous-batching101">continuous batching[10.1]</span><a href="#continuous-batching101" class="header-anchor">#</a></h1><h3><span id="朴素批处理-x2f-静态批处理">朴素批处理 &#x2F; 静态批处理</span><a href="#朴素批处理-x2f-静态批处理" class="header-anchor">#</a></h3><p><img src="https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png" alt="朴素批处理 / 静态批处理"></p>
<h3><span id="连续批处理">连续批处理</span><a href="#连续批处理" class="header-anchor">#</a></h3><p><img src="https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png" alt="连续批处理"></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV1cP41187wY/">VLLM ——高效GPU训练框架</a>  V</li>
<li><a href="https://github.com/vllm-project/vllm">vllm</a> git</li>
<li><a href="https://www.hopsworks.ai/dictionary/vllm">vLLM</a></li>
<li><a href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</a>   看动图</li>
</ol>
<h3><span id="continuous-batching">continuous batching</span><a href="#continuous-batching" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a><br>   10.1. <a href="https://juejin.cn/post/7277917295271723048">如何通过连续批处理(continuous batching)将LLM推理吞吐量提升23倍，同时减少延迟</a> ***<br>   <a href="https://zhuanlan.zhihu.com/p/652165071">continuous batching在LLM推理中的意义</a> ***<br>   <a href="https://mp.weixin.qq.com/s/bs3puOXFZYg5K-zfyDfpOw">Continuous Batching：一种提升 LLM 部署吞吐量的利器</a> *<br>   <a href="https://zhuanlan.zhihu.com/p/655700809">Continuous Batching：解锁LLM潜力！让LLM推断速度飙升23倍，降低延迟！</a> QA<br>   <a href="https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py">vllm_example  Repo</a> git<br>   <a href="https://github.com/anyscale/llm-continuous-batching-benchmarks">llm-continuous-batching-benchmarks Repo</a> git</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/qq_27590277/article/details/135710435">大模型推理核心技术：Continuous Batching详解</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/688551989">从continuous batching到vLLM中的batching</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)RAG Langchain-Chatchat</title>
    <url>/www6vHomeAIGC/2023/05/31/gptRAGchatchat/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="langchain-chatchat-架构">Langchain-Chatchat 架构</span><a href="#langchain-chatchat-架构" class="header-anchor">#</a></h1>

<ul>
<li>组件<ul>
<li>本地知识库</li>
<li>Embedding 模型</li>
<li>向量数据库</li>
<li>Prompt Template</li>
</ul>
</li>
</ul>
<h1><span id="langchain-chatchat">Langchain-Chatchat</span><a href="#langchain-chatchat" class="header-anchor">#</a></h1><ul>
<li>部署 <ul>
<li>windows 10 [5]<br>部署本地， 没显存，卡</li>
<li>Linux [2]<br>部署   32C125G ，没显存， 推理很慢 </li>
<li>Docker</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/chatchat-space/Langchain-Chatchat">Langchain-Chatchat </a> master<br>Langchain 与 ChatGLM 等语言模型的本地知识库问答<br><a href="https://github.com/chatchat-space/Langchain-Chatchat/tree/v0.2.4">Langchain-Chatchat</a>  v0.2.4<br><a href="https://gitee.com/deepeye/langchain-ChatGLM">langchain-ChatGLM</a>  gitee </p>
</li>
<li><p><a href="https://github.com/www6v/Langchain-Chatchat-Colab">Colab for Langchain-Chatchat</a>   linux 可以部署  v0.2.6</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/649055955">langChain-ChatGLM 尝试，踩坑记录</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651189680">Langchain-Chatchat + 阿里通义千问Qwen 保姆级教程 | 次世代知识管理解决方案</a>    Langchain-Chatchat + 通义千问</p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_43094965/article/details/133044128">win10 安装 Langchain-Chatchat 避坑指南（2023年9月18日v0.2.4版本，包含全部下载内容！）</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>多轮对话</title>
    <url>/www6vHomeAIGC/2023/05/28/gptDialogue/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D">传统的多轮对话</a><ul>
<li><a href="#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D12">多轮对话[1][2]</a></li>
<li><a href="#%E9%9A%BE%E7%82%B93">难点[3]</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B-2">基础能力 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Ellm%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D4">基于LLM的多轮对话[4]</a><ul>
<li><a href="#types">Types</a></li>
<li><a href="#evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="传统的多轮对话">传统的多轮对话</span><a href="#传统的多轮对话" class="header-anchor">#</a></h1><h3><span id="多轮对话12">多轮对话[1][2]</span><a href="#多轮对话12" class="header-anchor">#</a></h3><ul>
<li>NLU<ul>
<li>意图(intent)分类 [3]   </li>
<li>槽位抽取</li>
</ul>
</li>
<li>DM<br>DST + DP(Policy)</li>
<li>NLG</li>
</ul>
<blockquote>
<p>多轮-上下文</p>
</blockquote>
<h3><span id="难点3">难点[3]</span><a href="#难点3" class="header-anchor">#</a></h3><ul>
<li>上下文信息丢失</li>
<li>指代词识别</li>
</ul>
<h3><span id="基础能力-2">基础能力 [2]</span><a href="#基础能力-2" class="header-anchor">#</a></h3><ul>
<li>意图识别</li>
<li>情绪识别</li>
</ul>
<h1><span id="基于llm的多轮对话4">基于LLM的多轮对话[4]</span><a href="#基于llm的多轮对话4" class="header-anchor">#</a></h1><h3><span id="types">Types</span><a href="#types" class="header-anchor">#</a></h3><ul>
<li><p>Task-oriented dialogue system<br> NLU -&gt; DST -&gt; DPL-&gt; NLG</p>
</li>
<li><p>open-domain dialogue system</p>
</li>
</ul>
<h3><span id="evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</span><a href="#evolution-of-lm-based-dialogue-system" class="header-anchor">#</a></h3><ul>
<li><p>Fusion within Task-oriented dialogue system(task)</p>
<ul>
<li>task<ul>
<li>NLU  DST  DPL【可有可无】</li>
<li>NLG【保留】</li>
</ul>
</li>
<li><strong>end2end Task-oriented DS的出现</strong></li>
</ul>
</li>
<li><p>fusion between TOD and ODD(data)</p>
<ul>
<li>TOD -&gt; ODD<br>Q-TOD</li>
<li>ODD -&gt; TOD<br>UniDS</li>
</ul>
</li>
<li><p>fusion between dialogue modal and language model(model)<br><strong>LLM as DM</strong><br>【LLM本身有对话的能力，需要被激发出来】【instruction tuning】<br>【chat模型是做价值观对齐】</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1dt4y1S7kn/">自然语言处理：多轮对话在工业中的应用-贪心学院</a> *** V</li>
<li><a href="https://www.bilibili.com/video/BV1vZ4y147Qv/">1-人-人对话数据驱动的多轮对话技术探索与实践-孙超博</a> V 美团</li>
<li><a href="https://www.bilibili.com/video/BV1Yt4y1S75w/">人工智能如何在多轮对话中进行意图理解——祝凯华</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1Mb4y137yB/">基于大模型对话系统的前世今生</a>  V<br>《An Survey of the Evolution of Language Model-Based Dialogue Systems》</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489281&idx=1&sn=0273bf49530a93df16ecf5cb5fbc8f65">前沿重器[37] | 大模型对任务型对话的作用研究</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>对话</category>
      </categories>
      <tags>
        <tag>对话</tag>
      </tags>
  </entry>
  <entry>
    <title>LLama-Factory</title>
    <url>/www6vHomeAIGC/2023/05/24/gptLLamaFactory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86llama31">推理llama3[1]</a><ul>
<li><a href="#llama-factory-%E5%AE%89%E8%A3%85">LLaMA-Factory 安装</a></li>
<li><a href="#%E6%8E%A8%E7%90%86llama3">推理llama3</a></li>
</ul>
</li>
<li><a href="#agent-tuning2">Agent Tuning[2]</a><ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">环境准备</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B">训练流程</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理llama31">推理llama3[1]</span><a href="#推理llama31" class="header-anchor">#</a></h1><h3><span id="llama-factory-安装">LLaMA-Factory 安装</span><a href="#llama-factory-安装" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装llamafactory-cli命令</span></span><br><span class="line">git clone https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">conda create -n llama_factory python=3.10</span><br><span class="line">conda activate llama_factory</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line">pip install -e .[metrics]</span><br></pre></td></tr></table></figure>

<h3><span id="推理llama3">推理llama3</span><a href="#推理llama3" class="header-anchor">#</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型下载</span></span><br><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line">model_dir = snapshot_download(<span class="string">&#x27;LLM-Research/Meta-Llama-3-8B-Instruct&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vim examples/inference/llama3.yaml</span></span><br><span class="line">model_name_or_path: /home/wei/models/model/LLM-Research/Meta-Llama-3-8B-Instruct</span><br><span class="line">template: llama3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里云必须加这句，不然页面会报异常</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> GRADIO_ROOT_PATH=/<span class="variable">$&#123;JUPYTER_NAME&#125;</span>/proxy/7860/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">llamafactory-cli webchat examples/inference/llama3.yaml</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Command</p>
<img src="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/command.JPG" class>
</li>
<li><p>WebUI</p>
<img src="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/webui.JPG" class></li>
</ul>
<h1><span id="agent-tuning2">Agent Tuning[2]</span><a href="#agent-tuning2" class="header-anchor">#</a></h1><h3><span id="环境准备">环境准备</span><a href="#环境准备" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span> code</span></span><br><span class="line">git clone -b v0.7.1  https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">git switch -c v0.7.1</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">package 安装</span></span><br><span class="line">conda create -n llama_factory python=3.10</span><br><span class="line">conda activate llama_factory</span><br><span class="line">pip install llmtuner==0.5.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">环境变量</span></span><br><span class="line">export CUDA_VISIBLE_DEVICES=0 # 使用第一块 GPU</span><br><span class="line">export USE_MODELSCOPE_HUB=1 # 使用魔搭社区下载渠道</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里云必须加这句，不然页面会报异常</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> GRADIO_ROOT_PATH=/<span class="variable">$&#123;JUPYTER_NAME&#125;</span>/proxy/7860/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">python -m llmtuner.webui.interface</span><br></pre></td></tr></table></figure>

<h3><span id="训练流程">训练流程</span><a href="#训练流程" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flash-attn 安装</span></span><br><span class="line">pip install flash-attn --no-build-isolation</span><br><span class="line"></span><br><span class="line">pip install modelscope -U</span><br></pre></td></tr></table></figure>



<ul>
<li><p>训练脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">训练轮数 1.0</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \</span><br><span class="line">    --stage sft \</span><br><span class="line">    --do_train True \</span><br><span class="line">    --model_name_or_path 01ai/Yi-6B \</span><br><span class="line">    --finetuning_type lora \</span><br><span class="line">    --template default \</span><br><span class="line">    --flash_attn True \</span><br><span class="line">    --dataset_dir data \</span><br><span class="line">    --dataset glaive_toolcall,alpaca_gpt4_en,alpaca_gpt4_zh,oaast_sft_zh \</span><br><span class="line">    --cutoff_len 1024 \</span><br><span class="line">    --learning_rate 5e-05 \</span><br><span class="line">    --num_train_epochs 1.0 \</span><br><span class="line">    --max_samples 8000 \</span><br><span class="line">    --per_device_train_batch_size 4 \</span><br><span class="line">    --gradient_accumulation_steps 4 \</span><br><span class="line">    --lr_scheduler_type cosine \</span><br><span class="line">    --max_grad_norm 1.0 \</span><br><span class="line">    --logging_steps 5 \</span><br><span class="line">    --save_steps 100 \</span><br><span class="line">    --warmup_steps 0 \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_dropout 0.1 \</span><br><span class="line">    --lora_target all \</span><br><span class="line">    --output_dir saves/Yi-6B/lora/yi-agent-6b \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --plot_loss True </span><br></pre></td></tr></table></figure>
</li>
<li><p>训练配置</p>
<img src="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/agentTuningUI.png" class>

</li>
<li><p>训练结果</p>
<img src="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/agentTuningUI-result.png" class>
</li>
<li><p>效果展示<br>工具调用 - 查询天气<br>【1个epoch好像有点问题】</p>
<img src="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/agentTuningUI-chat.png" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/695287607">LLaMA-Factory QuickStart</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/678989191">单卡 3 小时训练专属大模型 Agent：基于 LLaMA Factory 实战</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/699777943">LLaMA-Factory微调多模态大语言模型教程</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLamaFactory</category>
      </categories>
      <tags>
        <tag>LLamaFactory</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 算力</title>
    <url>/www6vHomeAIGC/2023/05/23/gptGPU/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="gpu算力">GPU算力</span><a href="#gpu算力" class="header-anchor">#</a></h1><h3><span id="免费1">免费[1]</span><a href="#免费1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/free.JPG" class>

<ul>
<li>modelscope 100小时 GPU</li>
</ul>
<h3><span id="专业收费2">专业收费[2]</span><a href="#专业收费2" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/cost.JPG" class>


<h1><span id="显卡">显卡</span><a href="#显卡" class="header-anchor">#</a></h1><ul>
<li><p>显卡天梯榜<br> <a href="https://topic.expreview.com/GPU">显卡天梯榜</a></p>
</li>
<li><p>显卡<br>显卡 &#x3D; GPU +  显存</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1fC4y1N7qV/">5种在线GPU算力资源白嫖指南</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1q5411z7HM/">5种专业在线GPU算力资源白嫖指南</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV1Pv4y1f7VV/">【PyTorch深度学习】01 GPU购买与白嫖指南</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)RAG Index</title>
    <url>/www6vHomeAIGC/2023/05/21/gptRAGIndex/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%9D%9720">分块[20]</a><ul>
<li><a href="#%E5%88%86%E5%9D%97%E6%96%B9%E6%B3%95">分块方法</a></li>
<li><a href="#%E5%88%86%E5%9D%97%E4%BC%98%E5%8C%96">分块优化</a></li>
<li><a href="#%E5%88%86%E5%9D%97%E5%8F%82%E6%95%B0">分块参数</a></li>
</ul>
</li>
<li><a href="#%E7%B4%A2%E5%BC%95-langchain13">索引-Langchain[13]</a><ul>
<li><a href="#smaller-chunks-1112">Smaller chunks [11][12]</a></li>
<li><a href="#hypothetical-questions-1112">Hypothetical questions [11][12]</a></li>
<li><a href="#summary-1112">Summary [11][12]</a></li>
</ul>
</li>
<li><a href="#%E7%B4%A2%E5%BC%95-1">索引 [1]</a><ul>
<li><a href="#multi-representation-indexing">Multi-representation Indexing</a></li>
<li><a href="#%E5%B1%82%E7%BA%A7%E6%80%A7%E7%B4%A2%E5%BC%95-raptor2">层级性索引-RAPTOR[2]</a></li>
<li><a href="#%E5%81%9Atoken%E5%88%B0text%E7%BA%A7-colbert">做token到text级-ColBERT</a></li>
<li><a href="#sprag-3">spRAG [3]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%88%86%E5%9D%97">分块</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="分块20">分块[20]</span><a href="#分块20" class="header-anchor">#</a></h1><h3><span id="分块方法">分块方法</span><a href="#分块方法" class="header-anchor">#</a></h3><ul>
<li>固定大小分块</li>
<li>内容感知分块<ul>
<li>句子分块<ul>
<li>直接分割<br>按句点（“.”）和换行符分割句子</li>
<li>NLTK<br><code>from langchain.text_splitter import NLTKTextSplitter</code></li>
<li>spaCy<br><code>from langchain.text_splitter import SpacyTextSplitter</code></li>
</ul>
</li>
<li>递归分块<br>使用一组分隔符以分层和迭代的方式将输入文本划分为更小的块<br><code>from langchain.text_splitter import RecursiveCharacterTextSplitter</code></li>
<li>专门分块<ul>
<li>Markdown</li>
<li>LaTex</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="分块优化">分块优化</span><a href="#分块优化" class="header-anchor">#</a></h3><ul>
<li>预处理数据</li>
<li>选择块大小范围</li>
<li>评估每个块大小的性能</li>
</ul>
<h3><span id="分块参数">分块参数</span><a href="#分块参数" class="header-anchor">#</a></h3><p>chuck_size, ,chunk overlap<br>top_k</p>
<blockquote>
<p>最佳实践<br>  按<strong>逻辑分块</strong>可以明显提升<strong>检索器的准确率</strong></p>
</blockquote>
<h1><span id="索引-langchain13">索引-Langchain[13]</span><a href="#索引-langchain13" class="header-anchor">#</a></h1><h3><span id="smaller-chunks-1112">Smaller chunks [11][12]</span><a href="#smaller-chunks-1112" class="header-anchor">#</a></h3><p>Indexing by <strong>small data chunks</strong><br>按子部分索引数据块：将文本块拆分为较小的部分，如句子，进行多次索引。这有助于<br>处理复杂文本块，减少噪音输出，确保更准确匹配用户查询。</p>
<h3><span id="hypothetical-questions-1112">Hypothetical questions [11][12]</span><a href="#hypothetical-questions-1112" class="header-anchor">#</a></h3><p>Indexing by <strong>the questions the document answers</strong><br>按文本块回答的问题索引数据块：让LLM生成与拆分的文本块相关的假设性问题，并用<br>于索引。这种方法保持用户查询与数据核心内容一致，降低模糊性。</p>
<h3><span id="summary-1112">Summary [11][12]</span><a href="#summary-1112" class="header-anchor">#</a></h3><p>Indexing by <strong>the summary of the document</strong></p>
<p>按文本块摘要索引数据块：类似于第二种方法，使用块摘要而不是回答的假设问题来创<br>建索引。特别适用于文本块中包含多余信息或与用户查询无关的情况。</p>
<h1><span id="索引-1">索引 [1]</span><a href="#索引-1" class="header-anchor">#</a></h1><h3><span id="multi-representation-indexing">Multi-representation Indexing</span><a href="#multi-representation-indexing" class="header-anchor">#</a></h3><p>使用<strong>LLM生成</strong>针对检索进行优化的<strong>文档摘要</strong>（“命题”）。<strong>嵌入这些摘要以进行相似性搜索</strong>，但将完整文档返回给LLM进行生成。</p>
<p>【多模态的例子】</p>
<blockquote>
<p>相关的: 父级documents的索引</p>
</blockquote>
<h3><span id="层级性索引-raptor2">层级性索引-RAPTOR[2]</span><a href="#层级性索引-raptor2" class="header-anchor">#</a></h3><p>【raptor 效果也很好，就是需要总结，看总结的咋样了】<br>【raptor 原文是对 chunk 聚类，然后每个聚类做总结】</p>
<p>【ragflow 使用了RAPTOR】</p>
<h3><span id="做token到text级-colbert">做token到text级-ColBERT</span><a href="#做token到text级-colbert" class="header-anchor">#</a></h3><h3><span id="sprag-3">spRAG [3]</span><a href="#sprag-3" class="header-anchor">#</a></h3><p>【这个 spRAG 的 rse 比子母 chunk 要灵活一点,   试了一下 】<br>AutoContext在嵌入各个文本块之前，先自动将文档级别的上下文信息注入到每个块中。RSE对检索到的相关文本块进行聚类，将内容相似或语义相关的块归为一组。然后，它会根据查询的需求，智能地选择和组合这些块，形成长度适当、信息相关的文本段。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a><br><a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git   </p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SeOG15Z2RJmiJ5sAP91B4w">基于RAPTOR实现高质量长上下文的RAG </a>   未</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/1a-h9CBZtRARG7_sOK9g8Q">20240520大模型&amp;KG&amp;RAG进展回顾：spRAG优化策略、电信领域大模型及手语生成大模型</a> 未<br>RAPTOR  spRAG</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407870&idx=1&sn=8073f0fc8edc0897e3627f26478063c3">20240205大模型进展早报：兼看引入chunk层级结构的大模型RAG的思路：RAPTOR </a> RAPTOR</p>
<h3><span id></span><a href="#" class="header-anchor">#</a></h3><ol start="11">
<li><p><a href="https://www.bilibili.com/video/BV1dH4y1C7Ck/">3种高级索引方法，有效提升RAG性能</a> V<br>  <a href="https://thetechbuffet.substack.com/p/rag-indexing-methods">The Tech Buffet #12: Improve RAG Pipelines With These 3 Indexing Methods</a><br>  <a href="https://newsletter.theaiedge.io/p/how-to-optimize-your-rag-pipelines">How To Optimize Your RAG Pipelines</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Vu4y1H72s/">【RAG实战】 Multi-Vector-Retrieval实现三种高级索引方法</a> V<br><a href="https://github.com/www6v/AIGC/blob/master/retriever%2Bindex/MultiVectorRetriever">MultiVectorRetriever</a>  git<br> <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector">MultiVector Retriever</a></p>
</li>
<li><a href="/www6vHomeAIGC/2022/12/31/gptRetrievers/" title="Retrievers">Retrievers</a> self</li>
</ol>
<h2><span id="分块">分块</span><a href="#分块" class="header-anchor">#</a></h2><ol start="20">
<li><a href="https://hustai.gitee.io/zh/posts/rag/Chunking-Strategies.html">大语言模型应用中的文本分块策略</a><br>  <a href="https://yangfei.me/tutorials/chunking-strategies">LLM 应用中的分块策略 </a></li>
</ol>
<p>1xx. <a href="https://baoyu.io/translations/rag/5-levels-of-text-splitting">文本分割的五个层次 [译]</a></p>
<p>1xx. <a href="https://chunkviz.up.railway.app/">ChunkViz v0.1</a>  可视化</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/Ytu6B0Me7BDwueud9djFHg">大模型文档理解前沿动向-细粒度多页文档理解：兼看文本切分组件semchunk </a><br>    问题2:关于一个快速文档语义切分组件semchunk</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)RAG Rerank</title>
    <url>/www6vHomeAIGC/2023/05/14/gptRAGRerank/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#reranker-22">Reranker [22]</a></li>
<li><a href="#%E4%BA%A7%E5%93%81">产品</a><ul>
<li><a href="#bge-ranker-20">BGE Ranker [20]</a></li>
<li><a href="#bce24">BCE[24]</a></li>
<li><a href="#%E4%BC%98%E7%A7%80%E7%9A%84%E7%BB%84%E5%90%88-21">优秀的组合 [21]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="reranker-22">Reranker [22]</span><a href="#reranker-22" class="header-anchor">#</a></h1><p>A reranking model — also known as a <strong>cross-encoder</strong> — is a type of model that,<strong>given a query and document pair, will output a similarity score.</strong> </p>
<p><img src="https://www.pinecone.io/_next/image/?url=https://cdn.sanity.io/images/vr8gru94/production/9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&w=3840&q=65" alt="Rerankers"></p>
<h1><span id="产品">产品</span><a href="#产品" class="header-anchor">#</a></h1><h3><span id="bge-ranker-20">BGE Ranker [20]</span><a href="#bge-ranker-20" class="header-anchor">#</a></h3><p><strong>交叉编码器</strong>将对查询和答案实时计算相关性分数，这比**向量模型(即双编码器)**更准确，但比向量模型更耗时。 因此，它可以用来对嵌入模型返回的前k个文档重新排序。 我们在多语言数据上训练了交叉编码器，数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。 </p>
<h3><span id="bce24">BCE[24]</span><a href="#bce24" class="header-anchor">#</a></h3><p>中文效果比BGE好[老刘说nlp]</p>
<h3><span id="优秀的组合-21">优秀的组合 [21]</span><a href="#优秀的组合-21" class="header-anchor">#</a></h3><p>OpenAI + CohereRerank<br>Voyage + big-reranker-large</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="20">
<li><p><a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md">BGE Reranker</a><br>  <a href="https://www.bilibili.com/video/BV1sQ4y137Ft/">transformers二次开发——bge-reranker模型微调流程</a> V<br><a href="https://mp.weixin.qq.com/s/XnkQFCdbvjox1Y06IbIlYw">RAG 再添新利器！智源开源最强检索排序模型 BGE Re-Ranker v2.0 </a></p>
</li>
<li><p><a href="https://luxiangdong.com/2023/11/06/rerank-ev/#">提升RAG——选择最佳Embedding和重新排名模型 </a><br>  <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">Boosting RAG: Picking the Best Embedding &amp; Reranker models</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/series/rag/rerankers/">Rerankers and Two-Stage Retrieval</a>     ***          文中的第二阶段就是指Reranker</p>
</li>
<li><p><a href="https://github.com/netease-youdao/BCEmbedding">youdao RerankerModal</a> BCE</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a><br>   <a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Query Routing</title>
    <url>/www6vHomeAIGC/2023/05/14/gptRAGRouting/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="类型1">类型[1]</span><a href="#类型1" class="header-anchor">#</a></h1><ul>
<li><strong>LLM Routers</strong><ul>
<li>LLM Completion Routers</li>
<li>LLM Function Calling Routers</li>
</ul>
</li>
<li><strong>Semantic Routers</strong> [2]</li>
<li>Zero Shot Classification Routers</li>
<li>Language Classification Routers</li>
</ul>
<p><img src="https://miro.medium.com/v2/format:webp/1*fJnUoOwsykBTU1MyLgHQFg.png" alt="Routers"></p>
<h1><span id="logical-and-semantic-routing3">Logical and Semantic routing[3]</span><a href="#logical-and-semantic-routing3" class="header-anchor">#</a></h1><h3><span id="logical-routing">Logical routing</span><a href="#logical-routing" class="header-anchor">#</a></h3><h3><span id="semantic-routing">Semantic routing</span><a href="#semantic-routing" class="header-anchor">#</a></h3><p>【基于embedding的相似度匹配】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220">Routing in RAG-Driven Applications</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1H64y1E75Y/">Sematic router 让LLM更加快速做出决策</a> V<br><a href="https://github.com/aurelio-labs/semantic-router/">semantic-router Repo</a> git</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a><br><a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git<br><a href="https://www.bilibili.com/video/BV1eJ4m1p7kj/">RAG(检索增强） 从入门到精通 路由（routing)</a> V</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent Challenge</title>
    <url>/www6vHomeAIGC/2023/05/13/gptAgentChallenge/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





<h1><span id="问题和局限性-4">问题和局限性 [4]</span><a href="#问题和局限性-4" class="header-anchor">#</a></h1><ul>
<li><p>记忆召回问题<br>只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好</p>
</li>
<li><p>错误累积问题</p>
</li>
<li><p>探索效率问题<br>中途引入人工的判断干预和反馈输入</p>
</li>
<li><p>任务终止与结果验证<br>模型 agent 的工作如何终止也是一个挑战</p>
</li>
</ul>
<h1><span id="挑战-8">挑战 [8]</span><a href="#挑战-8" class="header-anchor">#</a></h1><h3><span id="如何让-agent-选择合适的工具">如何让 agent 选择合适的工具</span><a href="#如何让-agent-选择合适的工具" class="header-anchor">#</a></h3><ul>
<li>Toolformer - fine tune</li>
<li>Gorilla - retrieval，fine tune</li>
</ul>
<h3><span id="不必要的工具使用">不必要的工具使用</span><a href="#不必要的工具使用" class="header-anchor">#</a></h3><p>“Human Input”也写成一种工具，让模型来主动发起对人类的提问<br><a href="https://python.langchain.com/docs/integrations/tools/human_tools">Human as a tool</a></p>
<h3><span id="agent-返回的格式不稳定">Agent 返回的格式不稳定</span><a href="#agent-返回的格式不稳定" class="header-anchor">#</a></h3><p>这里常见的做法是让 LLM <strong>按照 json 这类常见的 schema 来返回</strong>，一般稳定性会高一些（相比“Action:”这种）。<br>此外自动修复重试也很实用，可以利用 LangChain 里的 <strong>output parsers</strong> 来帮助完成。</p>
<h3><span id="记住之前的操作避免重复">记住之前的操作，避免重复</span><a href="#记住之前的操作避免重复" class="header-anchor">#</a></h3><p>AutoGPT - retrieval 结合近期操作记录</p>
<h3><span id="处理超长的-observation">处理超长的 observation</span><a href="#处理超长的-observation" class="header-anchor">#</a></h3><p>需要用一些工具从中<strong>提取有用信息</strong>，或者<strong>放到外部存储中再借助 retrieval 来使用</strong>。</p>
<h3><span id="专注于目标">专注于目标</span><a href="#专注于目标" class="header-anchor">#</a></h3><p>简单的做法是<strong>在 prompt 结尾处再把目标加上</strong>，引起 agent 的注意。<br>另外像 BabyAGI，HuggingGPT 这种把 <strong>planning 和 execution 分开</strong>的做法也是很有用。<strong>拆分的比较细</strong>的任务往往步骤比较短，也不容易丢失目标。</p>
<h3><span id="结果评估">结果评估</span><a href="#结果评估" class="header-anchor">#</a></h3><ul>
<li><strong>评估最终结果</strong>是否正确</li>
<li><strong>过程的细化评估</strong><ul>
<li>选择的中间步骤是否正确。</li>
<li>生成 action 的 input 是否正确。</li>
<li>生成的步骤序列是否合理高效。</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="4">
<li><a href="https://zhuanlan.zhihu.com/p/622947810">AutoGPT与LLM Agent解析</a> *** </li>
<li><a href="https://zhuanlan.zhihu.com/p/633033220">LLM 全栈开发指南补遗</a>  Agents  ***<br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/">Harrison Chase: Agents</a>  ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/679032270">LLM Agent 现状和一些思考 （202401）</a><br>   当前 Agent 的缺陷和挑战</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/kCXZN7Wli-RCvZXRb6mF7g">Agent开发者坦白：窘境中前行</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent Planning</title>
    <url>/www6vHomeAIGC/2023/05/13/gptAgentPlanning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="types1">Types[1]</span><a href="#types1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/05/13/gptAgentPlanning/plans.webp" class>

<ul>
<li><p><strong>任务分解</strong> </p>
</li>
<li><p>多计划选择</p>
</li>
<li><p>外部规划器辅助规划</p>
</li>
<li><p><strong>反思和提炼</strong>[20] </p>
</li>
<li><p>记忆增强规划</p>
</li>
</ul>
<h1><span id="任务分解">任务分解</span><a href="#任务分解" class="header-anchor">#</a></h1><ul>
<li><p>ReACT 范式 [2]<br>把<strong>融合了Reasoning和Acting</strong>的一种范式，推理过程是浅显易懂，仅仅<strong>包含thought-action-observation步骤</strong>，很容易判断推理的过程的正确性，使用ReAct做决策甚至超过了强化学习.  </p>
<ul>
<li>chain-of-thought推理-问题<br> 事实幻想（fact hallucination）和错误传递（error propagation）</li>
</ul>
</li>
<li><p>Plan-and-execute agents [2]<br>本质上是先计划再执行，即先把用户的问题分解成一个个的子任务，然后再执行各个子任务，最后合并输出得到结果</p>
</li>
</ul>
<h1><span id="patterns">Patterns</span><a href="#patterns" class="header-anchor">#</a></h1><ul>
<li>Self-ask [2]<br>Self-ask是一种follow-up的使用范式，仅仅包含follow-up, immediate answer步骤，至于follow-up多少个step，完全由它自己决定，估计这就是Self-ask的名字的由来。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《Understanding the planning of LLM agents: A survey》<br><a href="https://mp.weixin.qq.com/s/1POXDVJDv3ob1HqpKjb3Mg">大语言模型智能体规划能力综述: 分类、任务分解、选择、反思、记忆增强 </a> 翻译<br>  <a href="https://zhuanlan.zhihu.com/p/693264551">Agent四大范式 | 综述：全面理解Agent工作原理</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/642357544">2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用</a> ***  论文+代码</p>
</li>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentReflection/" title="Reflection Agent">Reflection Agent</a> self</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/NhpJMmIcnF57qEuUkxD4kQ">AI Agent规划能力全面拆解</a></p>
<p>1xx. <a href="https://baoyu.io/translations/ai-paper/2311.11797-igniting-language-intelligence-the-hitchhikers-guide-from-chain-of-thought-reasoning-to-language-agents">引领语言智能：从思维链推理到语言智能体的探索指南 [译]</a> paper</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488040&idx=1&sn=f404a5fc2b0380eac00564046abc77d5">2023年大语言模型智能体规划技术(LLM Agent Planning)研究进展汇总</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 优化</title>
    <url>/www6vHomeAIGC/2023/05/09/gptRAGOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%B4%E7%B4%A0rag-embedding">朴素RAG Embedding</a><ul>
<li><a href="#embedding-%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%A1%88%E5%8F%8A%E5%B1%80%E9%99%90%E6%80%A71">Embedding 召回方案及局限性[1]</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
</ul>
</li>
<li><a href="#%E8%A1%8C%E4%B8%9A%E9%97%AE%E7%AD%943">行业问答[3]</a><ul>
<li><a href="#%E6%8C%91%E6%88%98">挑战</a></li>
<li><a href="#%E4%BC%98%E5%8C%96">优化</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#xxx-1">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="朴素rag-embedding">朴素RAG Embedding</span><a href="#朴素rag-embedding" class="header-anchor">#</a></h1><h3><span id="embedding-召回方案及局限性1">Embedding 召回方案及局限性[1]</span><a href="#embedding-召回方案及局限性1" class="header-anchor">#</a></h3><ul>
<li>召回<strong>精度低</strong></li>
<li><strong>粒度过粗</strong></li>
<li>不支持条件查询&#x2F;统计</li>
<li>不能替代信息提取</li>
</ul>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><ul>
<li><p>问题理解——准确识别<strong>用户意图</strong>(传统NLP)  [2]</p>
</li>
<li><p>基于<strong>关键词Embedding</strong>的入库和搜索 [2]</p>
<ul>
<li><strong>关键词提取</strong><ul>
<li>实现信息抽取（Information Extraction，IE）<ul>
<li>实体关系三元组抽取(RE, Relation Extraction )</li>
<li>命名实体识别(NER, Name-Entity Recognition)</li>
<li>事件抽取(EE, Event Extraction)</li>
</ul>
</li>
</ul>
</li>
<li>基于 LLM 提取 [不推荐]<ul>
<li>结果不准确、开销也大</li>
</ul>
</li>
<li><strong>传统 NLP 方法提取</strong>[推荐]<ul>
<li>名词短语提取与整合</li>
<li>依存分析</li>
<li>成分句法分析</li>
</ul>
</li>
<li>总结<br>从<strong>完整语句的 Embedding</strong>，切换为<strong>关键词 Embedding</strong>：</li>
<li>优势<ul>
<li>相比传统 Embedding，大幅提升<strong>召回精准度</strong>。</li>
<li>使用传统 NLP 在专项问题处理上，相比 LLM 提供更好的精度和性能。</li>
</ul>
</li>
</ul>
</li>
<li><p>知识库存储选型</p>
<ul>
<li>Vector Store<ul>
<li>分片:  区分<strong>层级结构</strong></li>
</ul>
</li>
<li>Relational Database</li>
<li>Graph Database   <ul>
<li><strong>图数据检索</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="行业问答3">行业问答[3]</span><a href="#行业问答3" class="header-anchor">#</a></h1><h3><span id="挑战">挑战</span><a href="#挑战" class="header-anchor">#</a></h3><ul>
<li>版面复杂多样</li>
<li>文本分块<br><strong>存在知识点被分割、不完整的情况</strong>。</li>
<li>多因素影响内容召回效果<ul>
<li>例如：文档内容相似度高(专业文档细分领域、版本迭代等)；</li>
<li>通用的<strong>向量相似度算法</strong>效果不好(问题与问题匹配 VS问题与答案匹配)；</li>
<li>召回率受文档库增大而降低</li>
</ul>
</li>
</ul>
<h3><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h3><ul>
<li><p>向量化上的优化</p>
<ul>
<li>训练目标优化为提升<strong>Query与段落的相关性</strong>，使得<strong>问题和相关段落的语义向量表示更接近</strong>，训练模型有<strong>sbert</strong>，<strong>cosent</strong>等</li>
</ul>
</li>
<li><p>关键信息上的优化</p>
<ul>
<li>在文档内容的信息压缩上，进行文本<strong>关键词和摘要的提取</strong><ul>
<li>从完整语句的Embedding，切换为<strong>关键词Embedding</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/664921095">RAG探索之路的血泪史及曙光</a>  腾讯<br> Embedding, Retrieval</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/641132245">LLM+Embedding构建问答系统的局限性及优化方案</a><br>基于关键词Embedding的入库和搜索的流程图,  结合传统nlp任务<br>1xx. <a href="https://zhuanlan.zhihu.com/p/627655485">基于大语言模型构建知识问答系统</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404338&idx=1&sn=3c8f8c44ac7a1d925216b40833525b25">再看业界大模型行业问答的困难及若干业界实践：兼看智能客服常用路线及多场景prompt </a><br>问题 优化</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407281&idx=2&sn=f39b46cad1787123b485d76dff33bc93">大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407056&idx=1&sn=0a0ce93a9199a2eae36493a515e42181">温故而知新:大模型RAG问答研发的7个失分点及MOE专家组合模型的若干浅析 </a><br>   《Seven Failure Points When Engineering a Retrieval Augmented Generation System》</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403693&idx=1&sn=e47f34cd58f103d37998dbbfd01c41ee">大模型行业落地实践的一些总结和观点：大模型行业问答落地中的现实挑战以及潜在的缓解策略</a><br>   《DataFunCon2023深圳站-20231125-刘焕勇-大模型行业问答的现实挑战及潜在的缓解策略》 pdf</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404651&idx=2&sn=335db95e104a5b09e33ac2245bae4fd2">再看RAG在真实金融文档问答场景的实践方案：SMP2023 金融大模型挑战赛的两种代表实现思路</a></p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. <a href="https://baoyu.io/translations/ai-paper/2005.11401-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">知识密集型自然语言处理任务的检索增强生成技术研究 [译]</a><br>1xx. <a href="https://baoyu.io/translations/rag/mastering-rag-how-to-architect-an-enterprise-rag-system">构建企业级 RAG 系统的高级指南 [译]</a><br>1xx. <a href="https://baoyu.io/translations/ai-paper/2401.05856v1-seven-failure-points-when-engineering-a-retrieval-augmented-generation-system">在构建检索增强型生成系统时的七大挑战 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG Framework</title>
    <url>/www6vHomeAIGC/2023/05/09/gptRAGFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%86%E6%9E%B6-0">框架 [0]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="框架-0">框架 [0]</span><a href="#框架-0" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/infiniflow/ragflow/tree/main"><strong>ragflow</strong></a> </p>
</li>
<li><p><a href="https://github.com/netease-youdao/QAnything/tree/master"><strong>QAnything</strong></a> </p>
</li>
<li><p><a href="https://github.com/chatchat-space/Langchain-Chatchat/releases/tag/v0.2.8"><strong>langchainchat</strong></a></p>
</li>
<li><p><a href="https://github.com/labring/FastGPT"><strong>FastGPT</strong></a>  </p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/"><strong>LangChain</strong></a> </p>
</li>
<li><p><a href="https://github.com/run-llama/llama_index/"><strong>LlamaIndex</strong></a></p>
</li>
<li><p><a href="https://github.com/langchain4j/langchain4j">langchain4j</a> </p>
</li>
<li><p><a href="https://github.com/Azure/GPT-RAG">GPT-RAG</a> </p>
</li>
<li><p><a href="https://github.com/Unstructured-IO/unstructured"><strong>Unstructured</strong></a></p>
</li>
<li><p><a href="https://github.com/StanGirard/quivr">Quivr</a> </p>
</li>
<li><p><a href="https://github.com/langgenius/dify"><strong>Dify</strong></a> </p>
</li>
<li><p><a href="https://github.com/weaviate/Verba">Verba</a> </p>
</li>
<li><p><a href="https://github.com/danswer-ai/danswer">danswer</a></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407281&idx=2&sn=f39b46cad1787123b485d76dff33bc93">大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 </a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/ZoI4Dscm9f9m5-q4Dq4bag">大模型RAG问答开源框架的两个风向:兼看大模型安全的学术评测</a><br>   RAGFlow - 引入文档理解及溯源机制<br>   QAnything - 优化embeddding+召回侧方向的</p>
<p>1xx. <a href="https://llamahub.ai/">LlamaHub</a><br>      Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/XRfSqYwvuGB6sDJzRm0QVA">FlashRAG：可能是最全的、最快搭建RAG的开源框架 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)LangGraph</title>
    <url>/www6vHomeAIGC/2023/05/07/gptMultiAgentsPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#langgraph-1">LangGraph [1]</a><ul>
<li><a href="#agent-supervisor">Agent Supervisor</a></li>
<li><a href="#multi-agent-collaboration">Multi Agent Collaboration</a></li>
<li><a href="#hierarchical-agent-teams">Hierarchical Agent Teams</a></li>
</ul>
</li>
<li><a href="#framework">Framework</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#langgraph">LangGraph</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="langgraph-1">LangGraph [1]</span><a href="#langgraph-1" class="header-anchor">#</a></h1><h3><span id="agent-supervisor">Agent Supervisor</span><a href="#agent-supervisor" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb">Agent Supervisor Repo</a> git</p>
<h3><span id="multi-agent-collaboration">Multi Agent Collaboration</span><a href="#multi-agent-collaboration" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb">Basic Multi-agent Collaboration</a> git</p>
<h3><span id="hierarchical-agent-teams">Hierarchical Agent Teams</span><a href="#hierarchical-agent-teams" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb">Hierarchical Agent Teams</a> git</p>
<h1><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h1><ul>
<li>LangGraph</li>
<li>AutoGen</li>
<li>MetaGPT</li>
<li>CrewAI - OpenAI</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="langgraph">LangGraph</span><a href="#langgraph" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.langchain.dev/langgraph-multi-agent-workflows/">LangGraph: Multi-Agent Workflows</a></li>
<li><a href="https://www.bilibili.com/video/BV1F541117kW/">LangGraph：Multi-Agent 实战</a> V</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agents</category>
      </categories>
      <tags>
        <tag>Agents</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Data Selection</title>
    <url>/www6vHomeAIGC/2023/05/05/gptDataSelection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#ifd1">IFD[1]</a></li>
<li><a href="#mods2">MoDS[2]</a></li>
<li><a href="#deita-3">DEITA [3]</a><ul>
<li><a href="#%E5%A4%8D%E6%9D%82%E6%80%A7%E8%AF%84%E5%88%86">复杂性评分</a></li>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%AF%84%E5%88%86">质量评分</a></li>
<li><a href="#%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%AD%9B%E9%80%89">多样性筛选</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="ifd1">IFD[1]</span><a href="#ifd1" class="header-anchor">#</a></h1><ul>
<li>三个步骤<ul>
<li>Learning from Brief Experience<br>利用少量进行进行<strong>模型初学</strong> </li>
<li>Evaluating Based on Experience<br>利用初学模型计算原始数据中所有<strong>IFD指标</strong><ul>
<li>算法<ul>
<li>条件回答分数（ Conditioned Answer Score，CAS）</li>
<li>直接答案分数（Direct Answer Score，DAS）</li>
<li>指令跟随难度（Instruction-Following Difficulty，IFD）分数</li>
</ul>
</li>
</ul>
</li>
<li>Retraining from Self-Guided Experience<br>利用<strong>樱桃数据</strong>进行模型<strong>重训练</strong></li>
</ul>
</li>
</ul>
<h1><span id="mods2">MoDS[2]</span><a href="#mods2" class="header-anchor">#</a></h1><ul>
<li><p>质量筛选<br>采用OpenAssistant的<strong>reward-model</strong>-debertav3-large-v2模型（一个基于<strong>DeBERTa架构</strong>设计的奖励模型）对数据进行<strong>质量打分</strong>。</p>
</li>
<li><p>多样性筛选<br>为了避免所选质量数据高度相似，通过<strong>K-Center-Greedy算法</strong>进行数据筛选，在最大化多样性的情况下，使指令数据集最小。<br>在该步骤中，采用<strong>BERT模型</strong>为指令数据生成句向量来计算不同数据之间的距离。</p>
</li>
<li><p>必要性筛选</p>
</li>
</ul>
<h1><span id="deita-3">DEITA [3]</span><a href="#deita-3" class="header-anchor">#</a></h1><h3><span id="复杂性评分">复杂性评分</span><a href="#复杂性评分" class="header-anchor">#</a></h3><ul>
<li>复杂性评估的方法  <ul>
<li>Random Selection：随机选择样本。</li>
<li>Instruction Length：按照指令的长度计算复杂性。</li>
<li><strong>Perplexity</strong>：通过预训练模型计算回复的困惑度作为复杂性指标，困惑值越大意味着数据样本越难。</li>
<li><strong>Direct Scoring</strong>：利用ChaGPT给指令的复杂性打分。</li>
<li>Instruction Node：利用ChatGPT将指令转换成语义树，通过树的节点数作为复杂性指标。</li>
<li><strong>Instag Complexity</strong>：利用ChatGPT对部分数据进行打标签，再训练一个Llama模型，再利用训练后的Llama模型对全量数据预测，标签越多说明数据约复杂。</li>
<li><strong>IFD</strong>：指令跟随难度作为复杂性指标。</li>
</ul>
</li>
</ul>
<p>DEITA评估复杂性的方法，主要先对一个小规模种子数据集（2k）进行数据复杂性<strong>扩展</strong>，再利<strong>用ChatGPT对扩展数据进行打分</strong>，并<strong>训练一个Llama1-7B的模型</strong>，最后利用训练后的模型对数据的打分作为复杂性评估指标。</p>
<h3><span id="质量评分">质量评分</span><a href="#质量评分" class="header-anchor">#</a></h3><ul>
<li>质量评估的方法有<ul>
<li>Random Selection：随机选择样本。</li>
<li>Response Length：采用输出长度作为质量评估指标。</li>
<li>Direct Scoring：利用ChatGPT直接评估对特定指令输出结果的准确性。</li>
</ul>
</li>
</ul>
<p>DEITA评估质量的方法，<strong>与评估复杂性方法一致</strong>。先对一个小规模种子数据集（2k，与复杂性数据一致）进行数据质量扩展，再利用ChatGPT对扩展数据进行打分并训练一个Llama1-7B的模型，最后利用训练后的模型对数据的打分作为质量评估指标。</p>
<p><strong>数据质量扩展</strong>，通过特殊的提示词利用ChatGPT对数据的回复部分进行改写，主要是增强回复的有用性、相关性、丰富深度、创造力和提供额外的细节描述。</p>
<h3><span id="多样性筛选">多样性筛选</span><a href="#多样性筛选" class="header-anchor">#</a></h3><p>多样性筛选方法，首先将数据池中的数据按照复杂性和质量的综合得分（复杂性分数*质量分数）进行降序<strong>排序</strong>；<br>然后按顺序逐个取出样本数据x ，<strong>计算x 与筛选池中相邻最近的样本之间距离值</strong>，其中，数据利用Llama1-13B模型进行向量表征，距离计算采用<strong>余弦相似度</strong>。<br>如果<strong>距离值小于 r时</strong>，认为该样本与筛选池中数据相似程度不高，可以<strong>纳入筛选池</strong>；否则<strong>不纳入筛选池</strong>。当筛选池中样本数达到规定样本个数，完成多样性筛选。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/658128530">如何从数据集中自动识别高质量的指令数据-IFD指标的使用</a><br>《From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning》<br>ChatLaw就这么训的</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/671183709">大模型微调技巧 | 高质量指令数据筛选方法-MoDS</a><br>《MoDS: Model-oriented Data Selection for Instruction Tuning》<br> 质量筛选， 多样性筛选，必要性筛选   </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/675928711">DEITA-大模型指令微调的数据高效筛选方法</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/687339776">DEITA：融合复杂度、质量、多样性的高效数据筛选</a><br>   复杂度、质量、多样性</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报 </a><br>《A Survey on Data Selection for Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Data Selection</category>
      </categories>
      <tags>
        <tag>Data Selection</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)LIMA, LESS</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lima-1kimi">LIMA [1][kimi]</a></li>
<li><a href="#less-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-10">LESS 核心思想 [10]</a></li>
<li><a href="#less10kimi">LESS[10][kimi]</a><ul>
<li><a href="#%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95">实验方法：</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论：</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#lima">LIMA</a></li>
<li><a href="#less">LESS</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="lima-1kimi">LIMA [1][kimi]</span><a href="#lima-1kimi" class="header-anchor">#</a></h1><p>LIMA（Less Is More for Alignment）的实验通过一系列设计精良的步骤来探究数据质量、多样性以及数量对模型性能的影响，从而得出了<strong>提高数据质量和增加提示多样性比单纯增加数据量更能提升模型性能的结论</strong>。以下是<strong>实验方法</strong>的关键步骤：</p>
<ol>
<li><p><strong>精心策划的微调数据</strong>：LIMA模型在<strong>1000个</strong>精心策划的提示和回复上进行了<strong>微调</strong>，这些数据被设计为模拟真实用户与AI助手的交互。</p>
</li>
<li><p><strong>消融实验</strong>：通过消融实验，研究者们观察了在增加数据量的同时不增加提示多样性时，模型性能的提升是否有限；而在优化数据质量时，性能是否有显著提升。</p>
</li>
<li><p><strong>数据构造</strong>：研究者从Stack Exchange、wikiHow和Pushshift Reddit数据集收集数据，并进行了<strong>质量和多样性</strong>的控制。这些数据集被用来构造训练样本，以确保输入的多样性和输出的一致性。</p>
</li>
<li><p><strong>质量与多样性的对比</strong>：研究者比较了经过质量过滤的Stack Exchange数据和同质化的wikiHow数据对模型性能的影响。结果显示，更<strong>多样化的Stack Exchange数据在性能上优于同质化的wikiHow数据</strong>。 【多样化】</p>
</li>
<li><p><strong>数量的对比</strong>：研究者对从Stack Exchange抽取的指数级增加的训练集进行了测试，发现<strong>训练集的翻倍并没有改善响应质量</strong>，从而说明单纯增加数据量并不一定能提升性能。【数量】</p>
</li>
<li><p><strong>质量控制的实验</strong>：研究者还比较了未经过任何质量或风格过滤的Stack Exchange数据集与经过过滤的数据集上训练的模型性能，发现<strong>过滤后</strong>的数据集上训练的模型性能<strong>更优</strong>。【质量】</p>
</li>
<li><p><strong>人类评估</strong>：为了评估LIMA模型的性能，研究者进行了人类偏好研究，将LIMA的输出与其他几个基线模型的输出进行比较，并让人群工作者选择他们更喜欢的输出。</p>
</li>
</ol>
<p>通过这些实验步骤，LIMA的研究得出了<strong>数据质量和提示多样性对于提升模型性能的重要性远超过单纯增加数据量的结论</strong>。这些发现支持了“浅层对齐假说”，即模型在预训练阶段已经学习到了几乎所有知识和能力，而微调过程主要是学习与人类交互的风格和格式。</p>
<ul>
<li><p>总结 [1]</p>
<p>消融实验显示，<strong>当扩大数据量而不同时扩大提示多样性时，收益会大大减少，而在优化数据质量时，收益会大大增加</strong><br>【<strong>数量</strong> &lt;–&gt; <strong>多样性</strong>  <strong>质量</strong>】</p>
</li>
</ul>
<h1><span id="less-核心思想-10">LESS 核心思想 [10]</span><a href="#less-核心思想-10" class="header-anchor">#</a></h1><p>通过仅给出<strong>少数体现特定能力的示例</strong>，从大量指令数据集中<strong>有效地选择5%有影响力的数据</strong>用于目标指令微调，结果优于全量数据集进行微调，并且所选子集在不同模型参数规模和不同模型系列中仍然普遍有效。</p>
<h1><span id="less10kimi">LESS[10][kimi]</span><a href="#less10kimi" class="header-anchor">#</a></h1><p>LESS（Selecting Influential Data for Targeted Instruction Tuning）的实验方法和相应的结论如下：</p>
<h3><span id="实验方法">实验方法：</span><a href="#实验方法" class="header-anchor">#</a></h3><ol>
<li><p><strong>热身训练（Warmup Training）</strong>：使用LoRA（Low-Rank Adaptation）技术对预训练模型进行热身训练，以适应特定的数据分布。</p>
</li>
<li><p><strong>梯度数据存储（Gradient Data Store）</strong>：构建了一个具有投影低维梯度特征的梯度数据存储，该存储可以重复用于不同的目标任务。</p>
</li>
<li><p><strong>数据选择算法</strong>：利用数据存储和算法选择与体现特定能力的少数示例最相似的训练数据点。?</p>
</li>
<li><p><strong>模型训练</strong>：使用选择的数据子集来训练目标模型。</p>
</li>
<li><p><strong>评估</strong>：在不同的下游任务上评估LESS选择的数据子集的性能，包括MMLU、TYDIQA和BBH数据集。</p>
</li>
</ol>
<h3><span id="结论">结论：</span><a href="#结论" class="header-anchor">#</a></h3><ol>
<li><p><strong>LESS的有效性</strong>：LESS<strong>在不同的模型中都是有效的</strong>，能够在多个评估数据集上提高性能。</p>
</li>
<li><p><strong>数据子集的性能</strong>：<strong>使用LESS选择的5%的数据通常优于使用完整数据集进行训练的结果</strong>。这表明完整数据集可能包含与特定目标任务无关或有害的数据点。</p>
</li>
<li><p><strong>数据的可转移性</strong>：使用较小模型选择的数据可以提高较大模型和不同模型系列的性能，证明了LESS选择的数据具有高度的可转移性。</p>
</li>
<li><p><strong>与其他方法的比较</strong>：LESS是唯一一致有效的方法，相较于其他基线方法（如随机选择、BM25、DSIR、RDS）表现出更好的性能。</p>
</li>
<li><p><strong>计算成本</strong>：LESS的计算成本较高，但由于其有效性，这一成本是合理的。</p>
</li>
<li><p><strong>定性分析</strong>：LESS选择的数据能够体现预期下游应用所需的推理技能，而不是仅仅基于表面形式线索。</p>
</li>
<li><p><strong>局限性</strong>：LESS需要热身训练阶段，这增加了计算负载。此外，使用补全Token的平均梯度可能导致性能问题。还有，最小化验证损失并不总能提高任务性能，且数据选择中的线性度假设是LESS的一个限制。</p>
</li>
</ol>
<p>总体而言，LESS通过选择与目标任务高度相关的数据点，能够在指令微调中实现高效的性能提升，尽管存在一些局限性和计算成本。</p>
<p>【总结:  少量有<strong>质量</strong>的数据  优于  全量数据 】 【数据选择算法】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="lima">LIMA</span><a href="#lima" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/c50HrOfKOqgqGPVRHf6EpA">大模型微调究竟需要多少数据：从三个现有代表工作看几组结论及一点思考 </a><br>  <strong>指令格式的多样性</strong><br>  《LIMA: Less Is More for Alignment》<br>  《MAYBE ONLY 0.5% DATA IS NEEDED》</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/jinniulema/article/details/133915276">【论文笔记】LIMA: Less Is More for Alignment</a></p>
<h3><span id="less">LESS</span><a href="#less" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/686007325">LESS：仅选择5%有影响力的数据优于全量数据集进行目标指令微调</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/686687923">LESS 实践：用少量的数据进行目标指令微调</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Data Management</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataManagement/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%863">数据管理[3]</a><ul>
<li><a href="#pretraining-of-llm">Pretraining of LLM</a></li>
<li><a href="#sft">SFT</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据管理3">数据管理[3]</span><a href="#数据管理3" class="header-anchor">#</a></h1><h3><span id="pretraining-of-llm">Pretraining of LLM</span><a href="#pretraining-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
<ul>
<li><strong>Scaling Laws</strong></li>
<li>Data Repetition</li>
</ul>
</li>
<li><p>Data Quality</p>
<ul>
<li>Deduplication<ul>
<li>N-gram和Hash技术<br><strong>MinHash算法</strong></li>
<li>神经网络方法</li>
<li>语义去重<br>SemDeDup</li>
</ul>
</li>
<li>Quality Filtering   <ul>
<li><strong>分类器</strong></li>
<li><strong>启发式规则</strong></li>
<li>阈值过滤<br>例如基于困惑度（Perplexity）</li>
</ul>
</li>
<li>Diversity &amp; Age<ul>
<li><strong>数据多样性（Diversity）</strong></li>
<li>数据时效性（Age）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="sft">SFT</span><a href="#sft" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
</li>
<li><p>Data Quality</p>
<ul>
<li>Instruction Quality<br>Instruction Mining,  LIMA</li>
<li><strong>Instruction Diversity</strong><br><strong>Self-Instruct</strong>,  <strong>#InsTag</strong>， Alpaca</li>
<li><strong>Instruction Complexity</strong><br><strong>WizardLM</strong>,  <strong>#InsTag</strong>, <strong>Evol-Instruct</strong></li>
<li>Prompt Design</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li>《Data Management For Large Language Models: A Survey》huawei<br> <a href="https://blog.csdn.net/weixin_60760661/article/details/136058893">大模型的数据管理——论文精读</a><br> <a href="https://github.com/www6v/data_management_LLM">Data Management for LLM</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DataManagement</category>
      </categories>
      <tags>
        <tag>DataManagement</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT Scaling</title>
    <url>/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《When Scaling Meets LLM Fine-tuning: The Effect of Data, Model and Fine-tuning Method》</li>
</ul>
<h1><span id="摘要1">摘要[1]</span><a href="#摘要1" class="header-anchor">#</a></h1><p>这篇论文研究了大型语言模型（LLMs）的微调（finetuning）问题，尤其是在不同规模因素下的微调性能。作者探讨了包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小在内的多个因素，并考虑了两种微调方法：全模型微调（FMT）和参数高效微调（PET，包括prompt tuning和LoRA）。研究发现LLM微调遵循基于<strong>功率的乘法联合规模法则</strong>，<strong>LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加，而PET参数规模的增加通常效果不佳</strong>。此外，<strong>微调方法的选择高度依赖于具体任务和微调数据</strong>。</p>
<p>【 功率的乘法联合规模法则: 微调数据数量 &lt;–&gt; xxx】<br>【模型大小(标题里的Model ) &gt; 预训练数据(标题里的Data),   PET参数(标题里的Fine-tuning Method) 无效】<br>【微调方法的选择高度依赖于具体任务和微调数据】</p>
<h1><span id="实验方法1">实验方法[1]</span><a href="#实验方法1" class="header-anchor">#</a></h1><p>实验基于两组预训练的双语LLMs（英语&amp;德语，英语&amp;中文），模型大小从1B到16B。作者在WMT机器翻译（英语-德语、英语-中文）和多语言摘要（英语、德语、法语和西班牙语）任务上进行了大规模研究，最多使用20M微调示例。实验设置包括：</p>
<ul>
<li><strong>下游任务</strong>：选择机器翻译和多语言摘要作为微调的下游任务。</li>
<li><strong>LLMs和预训练</strong>：采用解码器仅Transformer模型，使用修改后的UL2目标进行训练。</li>
<li><strong>微调设置</strong>：研究了三种微调方法（FMT、Prompt和LoRA），并探索了四种不同的规模因素。</li>
<li><strong>评估</strong>：使用基于token级别的困惑度（PPL）选择最佳检查点进行评估，并使用BLEURT和RougeL评估生成质量。</li>
</ul>
<h1><span id="结论1">结论[1]</span><a href="#结论1" class="header-anchor">#</a></h1><ul>
<li>提出了一个乘法联合规模法则来描述微调数据大小和其他规模因素之间的规模关系。</li>
<li>LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加。</li>
<li>PET参数规模的增加对于LoRA和Prompt的效果有限，且有时甚至会导致反向规模效应。</li>
<li>微调方法的选择对于下游任务来说并不简单，需要根据任务特性和微调数据的可用性来决定。</li>
<li>微调可能会提高模型对相关任务的零样本泛化能力，尤其是当基础LLM较大时，Prompt和LoRA通常比FMT表现得更好。</li>
</ul>
<p>作者指出，尽管研究提供了有价值的见解，但也存在一些局限性，如联合规模法则主要基于封闭生成任务的实证结果，缺乏理论基础。未来的工作将扩展到多模态LLMs，探索微调数据质量的影响，并考虑开放和创造性的生成任务以及微调的多任务设置。</p>
<h1><span id="重要结论2">重要结论[2]</span><a href="#重要结论2" class="header-anchor">#</a></h1><p>作者们探讨了大型语言模型（LLMs）在微调（finetuning）过程中不同规模因素对性能的影响。以下是论文的一些重要结论及其对“SCALING”概念的解释：</p>
<ol>
<li><p><strong>乘法联合缩放法则</strong>：作者提出了一个基于<strong>乘法的联合缩放法则（multiplicative joint scaling law）</strong>，用于描述微调数据大小与其他缩放因素（如LLM模型大小、预训练数据大小、PET参数大小）之间的关系。这个法则表明，<strong>微调性能与这些因素的乘法组合有关</strong>，而不是简单的加法关系。</p>
</li>
<li><p><strong>模型大小对微调的影响</strong>：研究发现，<strong>增加LLM模型的大小对微调性能的提升比增加预训练数据的大小更为显著</strong>。这表明在有限资源下，<strong>优先考虑扩大模型规模而不是数据规模</strong>，可能会带来更好的微调效果。</p>
</li>
<li><p><strong>参数高效微调（PET）的局限性</strong>：尽管PET方法（如prompt tuning和LoRA）旨在通过优化少量参数来提高性能，但研究发现<strong>增加PET参数的大小对于微调性能的提升效果有限，有时甚至会出现反向缩放现象</strong>。</p>
</li>
<li><p><strong>任务和数据依赖性</strong>：微调的缩放特性高度依赖于具体任务和数据。这意味着<strong>没有一种通用的最优微调方法</strong>，选择哪种微调方法需要根据下游任务的特性和可用的微调数据量来决定。</p>
</li>
<li><p><strong>微调对零样本泛化能力的影响</strong>：尽管微调通常是为了提高特定任务的性能，但研究发现，基于LLM的微调仍然可以促进对相关任务的零样本泛化能力。特别是PET方法在保留模型的泛化能力方面表现更好。</p>
</li>
<li><p><strong>微调数据量的临界点</strong>：论文中还讨论了不同微调方法之间的临界点，即在特定的微调数据量下，一种方法可能比另一种方法表现得更好。这个临界点会随着任务和模型大小的不同而变化。</p>
</li>
</ol>
<p>这些结论对理解LLM微调过程中的“SCALING”具有重要意义。它们揭示了不同规模因素如何相互作用以及它们对微调性能的共同影响，为在实际应用中选择和优化微调策略提供了理论依据。通过这些发现，研究者和实践者可以更好地理解在特定条件下如何有效地缩放和配置他们的模型以获得最佳性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href>请帮我读篇论文，详细的写出摘要，实验方法，结论</a> kimi</li>
<li><a href>请帮我读篇论文，论文有哪些重要的结论？ 这些结论是如何解释题目中的SCALING的？</a> kimi</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报 </a><br>《When Scaling Meets LLM Finetuning: The Effect of Data， Model and Finetuning Method»</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Data  Annotation</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h3><p>1xx.  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408650&idx=2&sn=ef8424969be749489188ebd810800f08">如何利用大模型进行数据标注与知识蒸馏：兼看ActiveRAG上下文去噪的大模型RAG问答范式</a><br>   大模型用于数据标注<br>   《Large Language Models for Data Annotation: A Survey》<br>1xx. <a href="https://mp.weixin.qq.com/s/U3kWk_jPaeBzloOhUJ5DXQ">LLM数据标注技术调研：定义、框架、提示、反馈、评价、挑战、机遇 </a> 翻译<br>   《Large Language Models for Data Annotation: A Survey》</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399919&idx=1&sn=66fc1dfdba57744a80c6869b8cf941af">ChatGPT用于数据标注是否可行：基于推特分类、生成内容排序任务的代表性实验报告介绍 </a></p>
<h3><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h3><p>1xx.  AutoLabel - 自助数据标注 </p>
<p>1xx. <a href="https://opendatalab.github.io/labelU/">LabelU 介绍</a><br>   <a href="https://github.com/opendatalab/labelU">LabelU Repo</a> git 上海人工智能实验室</p>
<p>1xx. <a href="https://developer.aliyun.com/article/1311807">InsTag：大语言模型监督微调数据标签标注工具</a>  有相关的paper<br>   <a href="https://www.modelscope.cn/studios/lukeminglkm/instagger_demo/summary">InsTag指令打标工具</a> demo</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(List) Pretrain 数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="pretrain数据集">Pretrain数据集</span><a href="#pretrain数据集" class="header-anchor">#</a></h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/641187337">LLM大模型数据集之谜</a> Pretrain数据集</li>
<li><a href="https://github.com/brightmart/nlp_chinese_corpus">大规模中文自然语言处理语料</a></li>
<li><a href="https://github.com/Glanvery/LLM-Travel/blob/main/LLM_Pretrain_Datasets.md">开源的可用于LLM Pretrain数据集</a> Pretrain数据集</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399359&idx=1&sn=502c65376e14b20a7dc1ceb35c62141d">大规模语言模型训练必备数据集-The Pile：涵盖22类、800GB的多样性文本数据集概述 </a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>3、RLFH强化与预训练数据集 </li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403405&idx=1&sn=cb53c35efda2b771b4c1f289ae97c1d3">大模型研发必备：两大开源可用且清洗过的中文文本语料库及大模型FLOPS、参数量快速估计工具推荐 </a>           书生·万卷1.0 ,    wudao数据集</li>
<li><a href="https://mp.weixin.qq.com/s/JDkKlD9IKvagCYucPey6UQ">大规模中文开源文本训练数据集的几点启发：兼看两个知识图谱与大模型的练手竞赛 </a><br> 万卷数据集   wudao数据集  MVBNC数据集  OpenNewsArchive</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(List)SFT数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="sft数据集12">SFT数据集[1][2]</span><a href="#sft数据集12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>1、通用指令微调数据</p>
</li>
<li><p><a href="https://github.com/chaoswork/sft_datasets">开源SFT数据集整理</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Modular RAG</title>
    <url>/www6vHomeAIGC/2023/04/21/gptRAGModularRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#modular-rag1">Modular RAG[1]</a><ul>
<li><a href="#indexing">indexing</a></li>
<li><a href="#pre-retrival%E9%98%B6%E6%AE%B5">pre-retrival阶段</a></li>
<li><a href="#retrieval">Retrieval</a></li>
<li><a href="#post-retrieval-%E5%8C%85%E6%8B%AC%E4%B8%80%E4%BA%9B%E5%90%8E%E5%A4%84%E7%90%86%E7%9A%84%E6%A8%A1%E5%9D%97">post-retrieval 包括一些后处理的模块</a></li>
<li><a href="#generation%E9%98%B6%E6%AE%B5">Generation阶段</a></li>
<li><a href="#orchestraction%E9%98%B6%E6%AE%B5">orchestraction阶段</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#modular-rag">Modular RAG</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="modular-rag1">Modular RAG[1]</span><a href="#modular-rag1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/04/21/gptRAGModularRAG/moduleRAG.webp" class>

<h3><span id="indexing">indexing</span><a href="#indexing" class="header-anchor">#</a></h3><ul>
<li><strong>chunk优化</strong><ul>
<li>small-to-big<br>用小块做索引，但召回大块</li>
<li>sliding window<br>滑动窗口，提高语义连贯性</li>
<li>summary摘要（解决跨文档）以及结构化的组织<br>例如使用知识图谱进行文档内容的组织，根据文档结构进行层级组织</li>
</ul>
</li>
</ul>
<h3><span id="pre-retrival阶段">pre-retrival阶段</span><a href="#pre-retrival阶段" class="header-anchor">#</a></h3><ul>
<li><strong>query-routing</strong> [2]<ul>
<li>Metadata Router&#x2F; Filter  问题的分发</li>
<li>Semantic Router  意图分类</li>
</ul>
</li>
<li><strong>query-expansion</strong>  <ul>
<li>Multi-Query 一变多</li>
<li>Sub-Query 拆分子query</li>
<li>CoVe[3]</li>
</ul>
</li>
<li><strong>query transformer</strong> <ul>
<li>query rewrite改写</li>
<li>HyDE</li>
<li>Step-back Prompting</li>
</ul>
</li>
<li><strong>query construction</strong>  <ul>
<li>text-cypher  </li>
<li>text2sql </li>
<li>将结构化知识利用起来</li>
</ul>
</li>
</ul>
<h3><span id="retrieval">Retrieval</span><a href="#retrieval" class="header-anchor">#</a></h3><ul>
<li><strong>Retriver Selection</strong> 检索方式的选择<ul>
<li>Sparse Retriever<br>稀疏检索（es字符串匹配）</li>
<li>Dense Retriever<br>稠密检索（向量化检索）</li>
</ul>
</li>
<li>Retriever Fine-tuning  检索的微调  [4] #<ul>
<li>SFT<br>【embedding tuning】</li>
<li>adapter</li>
</ul>
</li>
</ul>
<h3><span id="post-retrieval-包括一些后处理的模块">post-retrieval 包括一些后处理的模块</span><a href="#post-retrieval-包括一些后处理的模块" class="header-anchor">#</a></h3><ul>
<li><strong>rerank重排</strong><ul>
<li>Rule-base Rerank  基于规则的</li>
<li>Model-base Rerank 基于模型的<br>基于大模型llm本身的</li>
</ul>
</li>
<li>compresion&#x2F;selection 上下文压缩 <ul>
<li>llmlingua </li>
<li>recomp</li>
<li>selective context</li>
<li>核心在于利用不同的手段，将上下文中不重要的信息进行剔除</li>
</ul>
</li>
</ul>
<h3><span id="generation阶段">Generation阶段</span><a href="#generation阶段" class="header-anchor">#</a></h3><ul>
<li>Generator Selection<ul>
<li>Cloud API-base Generator</li>
<li>On-Premises</li>
</ul>
</li>
<li>Generator Fine-tuning  [4] #<ul>
<li>SFT </li>
<li>Distillation</li>
<li>Dual FT<br>Fine-tuning both Generator and Retriever to align their preferences<br>RA-DIT</li>
</ul>
</li>
<li>则包括对底层基础模型的一些事情，比如基于cloud-api，还是进行SFT微调。<br>【Generator 指的就是LLM】</li>
</ul>
<h3><span id="orchestraction阶段">orchestraction阶段</span><a href="#orchestraction阶段" class="header-anchor">#</a></h3><ul>
<li>Scheduling</li>
<li>Fusion<ul>
<li>Possibility Ensemble</li>
<li>RRF (Reciprocal Rank Fusion )</li>
</ul>
</li>
<li>则包括对各个模块之间的执行和通信进行管理</li>
</ul>
<blockquote>
<p> #符号   Modular RAG 相对 advanced RAG 的特殊阶段</p>
</blockquote>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="modular-rag">Modular RAG</span><a href="#modular-rag" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://mp.weixin.qq.com/s/j07PkTCoxBzAhkyON1puPg">值得一看的大模型RAG问答总括性梳理：模块化(Modular)RAG范式的定义、构成及机遇 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407638&idx=1&sn=5c167b4a11bc483f5790ef1e0340d670">大模型RAG问答行业最佳案例及微调、推理双阶段实现模式：基于模块化(Modular)RAG自定义RAG Flow </a></p>
</li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRouting/" title="(原理|实战)Query Routing">(原理|实战)Query Routing</a>  self
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/669977863">如何使用LLMs：Chain of Verification (CoVe)</a><br> <a href="https://sourajit16-02-93.medium.com/chain-of-verification-cove-understanding-implementation-e7338c7f4cb5">Chain of Verification (CoVe) — Understanding &amp; Implementation</a></p>
</li>
<li><a href="/www6vHomeAIGC/2022/12/07/gptRAGPerformance/" title="(原理)Advanced RAG">(原理)Advanced RAG</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Query Transformation</title>
    <url>/www6vHomeAIGC/2023/04/20/gptQueryTransformation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#transformation-%E5%A4%9A%E6%A0%B7%E6%80%A7">Transformation-多样性</a><ul>
<li><a href="#multi-query%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%AD%96%E7%95%A53">Multi Query多查询策略[3]</a></li>
<li><a href="#rag-fusion%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A53">RAG-Fusion多查询结果融合策略[3]</a></li>
<li><a href="#decomposition%E9%97%AE%E9%A2%98%E5%88%86%E8%A7%A3%E7%AD%96%E7%95%A53">Decomposition问题分解策略[3]</a></li>
<li><a href="#query-rewrite-12">query rewrite [1][2]</a></li>
</ul>
</li>
<li><a href="#transformation-%E6%8A%BD%E8%B1%A1%E5%8C%96">Transformation-抽象化</a><ul>
<li><a href="#step-back%E9%97%AE%E7%AD%94%E5%9B%9E%E9%80%80%E7%AD%96%E7%95%A5-3">Step Back问答回退策略 [3]</a></li>
<li><a href="#step-back-prompting-12">Step-back Prompting [1][2]</a></li>
</ul>
</li>
<li><a href="#transformation-%E5%85%B7%E4%BD%93%E5%8C%96">Transformation-具体化</a><ul>
<li><a href="#hyde%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A53">HyDE混合策略[3]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="transformation-多样性">Transformation-多样性</span><a href="#transformation-多样性" class="header-anchor">#</a></h1><h3><span id="multi-query多查询策略3">Multi Query多查询策略[3]</span><a href="#multi-query多查询策略3" class="header-anchor">#</a></h3><p>该方法<strong>从多个角度重写用户问题</strong>，为每个重写的问题检索文档，返回所有查询的唯一文档。</p>
<h3><span id="rag-fusion多查询结果融合策略3">RAG-Fusion多查询结果融合策略[3]</span><a href="#rag-fusion多查询结果融合策略3" class="header-anchor">#</a></h3><p>将多个召回查询的结果进行<strong>合并</strong></p>
<h3><span id="decomposition问题分解策略3">Decomposition问题分解策略[3]</span><a href="#decomposition问题分解策略3" class="header-anchor">#</a></h3><ul>
<li><p>Answer recursively迭代式回答<br>在问题分解的基础上，逐步迭代出答案，<strong>将上一步问题的答案，与下一步骤的答案进行拼接</strong>，送入大模型进行问答</p>
</li>
<li><p>Answer individually<br>也可以<strong>让每个subquery分别进行处理</strong>，然后得到答案，然后再拼接成一个QA pairspprompt最终形成答案。</p>
</li>
</ul>
<h3><span id="query-rewrite-12">query rewrite [1][2]</span><a href="#query-rewrite-12" class="header-anchor">#</a></h3><ul>
<li><a href="https://arxiv.org/pdf/2305.14283.pdf">论文</a><strong>使用LLM重写用户查询</strong>，而不是直接使用原始用户查询进行检索。<br>因为对于LLM 而言，<strong>原始查询不可能总是最佳检索结果</strong>，可以让LLM重写查询。</li>
<li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb">Repo</a> git<br>【问题的多样化】</li>
</ul>
<h1><span id="transformation-抽象化">Transformation-抽象化</span><a href="#transformation-抽象化" class="header-anchor">#</a></h1><h3><span id="step-back问答回退策略-3">Step Back问答回退策略 [3]</span><a href="#step-back问答回退策略-3" class="header-anchor">#</a></h3><p>Step Back问答回退，首先提示LLM提出一个<strong>关于高级概念或原则的通用后退问题</strong>，并检索有关它们的相关事实，使用此基础来帮助回答用户问题。</p>
<h3><span id="step-back-prompting-12">Step-back Prompting [1][2]</span><a href="#step-back-prompting-12" class="header-anchor">#</a></h3><ul>
<li><a href="https://arxiv.org/pdf/2310.06117.pdf">论文</a>使用退一步提示，<strong>使用LLM生成”后退”(Step back prompting)问题</strong>。<br>使用检索时，”后退”问题和原始问题都会被用来进行检索，然后这两个结果都会被用来作为语言模型回复的基础。</li>
<li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb">Repo</a> git<br>【问题的抽象化】</li>
</ul>
<h1><span id="transformation-具体化">Transformation-具体化</span><a href="#transformation-具体化" class="header-anchor">#</a></h1><h3><span id="hyde混合策略3">HyDE混合策略[3]</span><a href="#hyde混合策略3" class="header-anchor">#</a></h3><p>LLM将<strong>问题</strong>转换为回答问题的<strong>假设文档</strong>。<strong>使用嵌入的假设文档检索真实文档</strong>，前提是doc-doc相似性搜索可以产生更多相关匹配。</p>
<ul>
<li>HyDE<br>At a high level, HyDE is an embedding technique that takes queries, <strong>generates a hypothetical answer</strong>, and then embeds that generated document and uses that as the final example.</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406156&idx=1&sn=d91a4df105c4fc4c9523f7141bc1c24d">知识图谱用于细粒度大模型幻觉评估：兼论Langchain-RAG问答中的问题改写范式 </a><br> RAG:  rewrite , Step back, fusion </p>
</li>
<li><p><a href="https://blog.langchain.dev/query-transformations/">Query Transformations</a>  </p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a> ***   原理paper，代码示例<br>Multi Query多查询策略， Decomposition问题，RAG-Fusion， Step Back， HyDE混合<br><a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git<br><a href="https://www.bilibili.com/video/BV1Vx421U7a4/">RAG(检索增强） 从入门到精通 虚拟文档嵌入（Hyde)</a> V</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/393914267">业界总结｜搜索中的Query理解</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/149429784">智能扩充机器人的“标准问”库之Query生成</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489295&idx=1&sn=fcb269e47dc27fcaf31201aa1c75dafb">前沿重器[38] | 微软新文query2doc：用大模型做query检索拓展</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>文档智能</title>
    <url>/www6vHomeAIGC/2023/04/19/gptDocumentAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="文档理解-10">文档理解 [10]</span><a href="#文档理解-10" class="header-anchor">#</a></h1><h3><span id="基于大模型的ocr-free微调方案">基于大模型的OCR-FREE微调方案</span><a href="#基于大模型的ocr-free微调方案" class="header-anchor">#</a></h3><ul>
<li>LLaVAR [12]</li>
<li>TextMonkey [11]</li>
</ul>
<h3><span id="文档版式分析数据集">文档版式分析数据集</span><a href="#文档版式分析数据集" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="10">
<li><a href="https://mp.weixin.qq.com/s/FsjoUUFssMv2UkbxM-IJ3A">值得一看的文档理解前沿方案及版式分析开源数据：三种模式、九大数据集 </a></li>
<li><a href="https://github.com/Yuliang-Liu/Monkey">Monkey</a><br><a href="http://vlrlab-monkey.xyz:7684/">Monkey Demo</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/670175648">LLaVAR：增强的视觉指令微调</a><br><a href="https://llavar.github.io/">LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding</a></li>
</ol>
<p>1xx. <a href="https://huggingface.co/blog/zh/document-ai">加速 Document AI (文档智能) 发展</a><br>    <a href="https://baijiahao.baidu.com/s?id=1755096032832674219&wfr=spider&for=pc">加速 Document AI (文档智能) 发展</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/d2Nns1qashMbcXPMG-4McQ">阿里面向企业数字化的文档智能技术与应用</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DocumentAI</category>
      </categories>
      <tags>
        <tag>DocumentAI</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)RAG Baichuan案例</title>
    <url>/www6vHomeAIGC/2023/04/18/gptRAGBaichuan/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="baichuan-rag1">Baichuan RAG[1]</span><a href="#baichuan-rag1" class="header-anchor">#</a></h1><ul>
<li>借鉴了Meta的CoVe技术</li>
<li>自研的TSF（Think-Step Further)技术<br>猜测其本质应该是对Step-back prompting方法的改良</li>
<li>自研了Baichuan-Text-Embedding向量模型 </li>
<li>混合检索<br>向量检索与稀疏检索并行的</li>
<li>self-Critique</li>
</ul>
<h1><span id="总结2">总结[2]</span><a href="#总结2" class="header-anchor">#</a></h1><ol>
<li><strong>多轮问答等场景的召回和传统搜索引擎的召回分布还不太一样。</strong>百川借助子问题检索效果更高的特点，对原始复杂问题进行拆解、拓展来解决复杂问题检索质量偏差的问题。</li>
<li><strong>对于没见过的语料直接用向量检索的结果可能不太理想。</strong>百川在大量语料上利用无监督方法训练embedding模型来优化效果。而行业大模型更倾向于私有的数据，要提升私有数据的训练效果还得继续在私有化数据上训练效果会更佳。</li>
<li><strong>Query拓展 + 多路召回 + Rerank + self-Critique可能是现阶段比较好的一种RAG方式，但是其也会带来更多成本。</strong>总体思路有点像ReAct[3]系列的进阶版本，其在搜索侧和答案修正侧都做了更多的一些工作来优化实际效果。其缺点是需要多次调用大模型，会带来额外的成本，真实线上是否采用这种策略还有待验证。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407638&idx=1&sn=5c167b4a11bc483f5790ef1e0340d670">大模型RAG问答行业最佳案例及微调、推理双阶段实现模式：基于模块化(Modular)RAG自定义RAG Flow</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/675770700">百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/658469464">LLM&#x2F;百川Baichuan2-53B搜索增强-开放API</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650901201&idx=1&sn=3a9bd61403fb4b024ec5d8c128990495">大模型+搜索构建完整技术栈，百川智能用搜索增强给企业定制化下了一剂「猛药」</a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_27590277/article/details/135421245">百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Embedding</title>
    <url>/www6vHomeAIGC/2023/04/18/gptEmbedding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#example-5">example [5]</a></li>
<li><a href="#embedding-%E4%BB%B7%E5%80%BC-6">Embedding 价值 [6]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8-6">应用 [6]</a></li>
<li><a href="#%E5%A4%A9%E6%A2%AF%E6%A6%9C">天梯榜</a></li>
<li><a href="#example7">example[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

 

<h1><span id="example-5">example [5]</span><a href="#example-5" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong>:   t-SNE  </li>
<li>K-Means 聚类</li>
<li>文本搜索  相似度搜索</li>
</ul>
<h1><span id="embedding-价值-6">Embedding 价值 [6]</span><a href="#embedding-价值-6" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong><br>将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</li>
<li>捕捉语义信息<br>Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。</li>
<li>泛化能力<br>由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示</li>
</ul>
<h1><span id="应用-6">应用 [6]</span><a href="#应用-6" class="header-anchor">#</a></h1><ul>
<li>语义表示和语义相似度</li>
<li>词语关系和类比推理</li>
<li>上下文理解</li>
<li>文本分类和情感分析</li>
<li>机器翻译和生成模型</li>
</ul>
<h1><span id="天梯榜">天梯榜</span><a href="#天梯榜" class="header-anchor">#</a></h1><p>  <a href="https://huggingface.co/spaces/mteb/leaderboard">mteb&#x2F;leaderboard</a></p>
<h1><span id="example7">example[7]</span><a href="#example7" class="header-anchor">#</a></h1><ul>
<li>m3e模型</li>
<li>bge模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="5">
<li><p><a href="https://github.com/www6v/openai-quickstart/blob/main/openai_api/embedding.ipynb">embedding</a> git</p>
</li>
<li><p>《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding</p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/135311471">一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge</a></p>
</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Hk4y1X7aG/">如何选取RAG中的embedding模型</a><br>   <a href="https://huggingface.co/spaces/mteb/leaderboard">huggingface embedding模型排行榜</a><br>   <a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence Bert</a><br>   <a href="https://github.com/blackinkkkxi/RAG_langchain">Demo Repo</a>  git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/qIh07eU8_lYL2gBVzTFzKA">引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案</a><br>   <a href="https://github.com/xlang-ai/instructor-embedding">Repo</a> git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406715&idx=1&sn=a680597afdb7d5439a11302c7911795f">也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 </a>        《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/676589001">如何提高LLMs的文本表征(Text Embedding)能力?</a><br>    《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1ex4y1S7u5/?p=2">文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速）</a>   V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Embedding</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Gorilla</title>
    <url>/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E6%96%B9%E6%B3%95%E8%AE%BA1">方法论[1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%B6%E9%9B%86">数据集收集</a></li>
<li><a href="#gorilla">Gorilla</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://ar5iv.labs.arxiv.org/html/2305.15334">Gorilla: Large Language Model Connected with Massive APIs</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/ShishirPatil/gorilla">gorilla</a> git</p>
</li>
</ul>
<h1><span id="方法论1">方法论[1]</span><a href="#方法论1" class="header-anchor">#</a></h1><h3><span id="数据集收集">数据集收集</span><a href="#数据集收集" class="header-anchor">#</a></h3><p><strong>API文档</strong></p>
<ul>
<li>HuggingFace平台托管和提供了约203,681个模型。然而，其中许多模型的文档质量较差，缺乏依赖项，模型卡中没有信息等问题。</li>
<li>为了筛选出质量较好的模型，从每个领域选择了前20个模型。考虑了多模态数据领域的7个领域，CV领域的8个领域，NLP领域的12个领域，音频领域的5个领域，表格数据领域的2个领域和强化学习领域的2个领域。</li>
<li>经过筛选，从HuggingFace获得了总共925个模型。从TensorFlow Hub获得了801个模型，并从Torch Hub获得了95个模型。</li>
<li>这些模型的信息被转换为<strong>JSON对象</strong>，其中包含了领域（domain）、框架（framework）、功能（functionality）、API名称（api_name）、API调用（api_call）、API参数（api_arguments）、环境要求（environment_requirements）、示例代码（example_code）、性能（performance）和描述（description）等字段。</li>
<li>选择这些字段是为了将其泛化到机器学习领域之外的其他领域，包括RESTful API调用。<br><strong>总而言之</strong>，通过筛选和整理，从HuggingFace、TensorFlow Hub和Torch Hub等平台获取了<strong>总共1,645个模型</strong>的信息，并将其以<strong>JSON对象</strong>的形式进行了记录和描述。这些信息包括模型的领域、框架、功能、API调用示例、性能等，以便在机器学习和其他领域中使用和参考。</li>
</ul>
<p><strong>指令生成 （Instruction Generation ）</strong></p>
<ul>
<li>在<strong>self-instruct</strong>范例[42]的指导下，使用GPT-4生成了合成指令数据。</li>
<li>提供了三个上下文示例和一个参考API文档，要求模型生成调用API的真实世界用例。</li>
<li>明确指示模型在创建指令时不要使用任何API名称或提示。</li>
<li>为每个三个模型中心构建了六个示例（<strong>指令-API对</strong>），共计18个点，这些数据是手动生成或修改的。</li>
<li>对于<strong>1,645个</strong>API数据点中的每一个，从相应的六个指令示例中随机选择3个，生成<strong>总共10个指令-API对</strong>。</li>
<li>强调只需要使用GPT-4生成指令，可以与开源替代方案（如LLaMA、Alpaca等）进行交换。<br><strong>总而言之</strong>，通过使用GPT-4生成指令，并结合上下文示例和参考API文档，在每个模型中心构建了六个示例，共计18个点。这些示例被用于<strong>生成1,645个API数据点中的每一个的指令-API对，生成总共10个对应关系</strong>。与开源替代方案相比，GPT-4的指令生成功能被应用在这个过程中。</li>
</ul>
<h3><span id="gorilla">Gorilla</span><a href="#gorilla" class="header-anchor">#</a></h3><p><strong>带有约束的API调用（API Call with Constraints）</strong></p>
<ul>
<li>API调用通常具有固有的<strong>约束</strong>，这些约束要求LLM不仅理解API调用的功能，还要<strong>根据不同的约束参数对调用进行分类</strong>。</li>
<li>机器学习API调用中常见的约束集是参数大小和准确性的下限。这些约束要求LLM能够根据提示理解和回答问题，例如根据提示选择参数少于10M的图像分类模型，并且至少保持70%的ImageNet准确率。</li>
<li><strong>对LLM来说，理解和推理出请求中嵌入的各种约束是一个巨大的挑战</strong>。LLM需要细致地理解用户的功能描述，并能够正确地处理伴随这些调用的复杂约束。</li>
<li>这个挑战凸显了在实际API调用中对LLM的复杂要求。仅仅理解API调用的基本功能是不够的，<strong>模型还必须能够应对伴随这些调用的约束，如参数大小和准确性要求</strong>。<br>总而言之，在机器学习API调用中，LLM面临着理解和处理约束的挑战。除了理解API调用的基本功能外，LLM还需要能够识别和满足伴随调用的约束要求，如参数大小和准确性的下限。这需要模型具备更细致的理解和推理能力，以满足实际API调用的复杂需求。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/640697382">Gorilla：与大规模API相连的大型语言模型</a> ***</li>
</ol>
<p>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b">Gorilla：链接海量API的大型语言模型</a> V<br>1xx. <a href="https://zhuanlan.zhihu.com/p/632583909">大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？</a> ***</p>
<p>1xx. <a href="https://gorilla.cs.berkeley.edu/">Gorilla: Large Language Model Connected with Massive APIs</a><br>1xx. <a href="https://gorilla.cs.berkeley.edu/blog.html">Gorilla blog</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent Tuning</title>
    <url>/www6vHomeAIGC/2023/04/07/gptAgentTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="tuning">Tuning</span><a href="#tuning" class="header-anchor">#</a></h1><h3><span id="agenttuning">AgentTuning</span><a href="#agenttuning" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404626&idx=1&sn=da5ac106548dd30f14a57a5ce4d90f08">基于llama7B的文本嵌入模型ANGLE：兼看Agent微调数据的生成方案</a>  AgentTuning<br>1xx. <a href="https://zhuanlan.zhihu.com/p/671295938">LLM之Agent（五）| AgentTuning：清华大学与智谱AI提出AgentTuning提高大语言模型Agent能力</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663362992?utm_id=0">AgentTuning解读</a></p>
<h3><span id="agenttuning-实战">AgentTuning 实战</span><a href="#agenttuning-实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/690012170">2024年大模型Agent tuning关键技术Fireact, Agent-FLAN, AgentOhana, Agent LUMOS, STE, ETO,MoE, DebateGPT等</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/UCO_m38QcWdCoT_DIFc96A">Agent-FLAN 技术报告——社区翻译版 </a></p>
<p>1xx. <a href="https://cloud.tencent.com/developer/article/2421687">LLM 大模型学习必知必会系列(九)：Agent微调最佳实践，用消费级显卡训练属于自己的Agent！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Reflection Agent</title>
    <url>/www6vHomeAIGC/2023/04/07/gptAgentReflection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="react">ReAct</span><a href="#react" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://react-lm.github.io/">ReAct: Synergizing Reasoning and Acting in Language Models</a> paper</li>
</ol>
<h3><span id="reflexion">Reflexion</span><a href="#reflexion" class="header-anchor">#</a></h3><ol start="21">
<li><a href="https://zhuanlan.zhihu.com/p/639254455">【论文阅读】Reflexion: 大模型如何从错误经验中学习？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/671508578">Reflexion: 带言语强化学习的语言智体</a><br>  1xx. <a href="https://blog.langchain.dev/reflection-agents/">Reflection Agents</a><br>  <a href="https://www.bilibili.com/video/BV1KJ4m1a7rZ/">LangGraph：Reflection Agents 实战</a> V</li>
</ol>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p><a href="https://zhuanlan.zhihu.com/p/691370751">Agent四大范式 | CRITIC：吴恩达力推Agent设计范式</a></p>
<h3><span id="practice">Practice</span><a href="#practice" class="header-anchor">#</a></h3><p><a href="https://github.com/andrewyng/translation-agent">Translation Agent: Agentic translation using reflection workflow</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent 分类[有趣|有用]</title>
    <url>/www6vHomeAIGC/2023/04/06/gptAgentCategory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%89%E8%B6%A3%E7%9A%84ai%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%9A%84ai">有趣的AI：更像人的AI</a><ul>
<li><a href="#%E5%A5%BD%E7%9C%8B%E7%9A%84%E7%9A%AE%E5%9B%8A-%E5%A4%9A%E6%A8%A1%E6%80%81">好看的皮囊 多模态</a></li>
<li><a href="#%E6%9C%89%E8%B6%A3%E7%9A%84%E7%81%B5%E9%AD%82">有趣的灵魂</a></li>
</ul>
</li>
<li><a href="#%E6%9C%89%E7%94%A8%E7%9A%84ai%E6%9B%B4%E5%83%8F%E5%B7%A5%E5%85%B7%E7%9A%84ai">有用的AI：更像工具的AI</a><ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B">大模型基础能力</a></li>
<li><a href="#1p-3p-%E4%BA%A7%E5%93%81%E6%B3%95%E5%88%99">1P-3P 产品法则</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E5%92%8C%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7">解决复杂任务和使用工具</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="有趣的ai更像人的ai">有趣的AI：更像人的AI</span><a href="#有趣的ai更像人的ai" class="header-anchor">#</a></h1><h3><span id="好看的皮囊-多模态">好看的皮囊  多模态</span><a href="#好看的皮囊-多模态" class="header-anchor">#</a></h3><ul>
<li><p>多模态<strong>理解能力</strong></p>
<ul>
<li>多模态数据端到端预训练的模型<br>Gemini </li>
<li>工程化<br>  projection layer</li>
<li>直接用文本去粘接 encoder、decoder 和文本大模型</li>
<li>eg【自己动手做出Gemini演示视频的效果】</li>
</ul>
</li>
<li><p>多模态<strong>生成能力</strong></p>
<ul>
<li>视频生成<ul>
<li>Live2D，3D 模型</li>
<li>DeepFake<br>录制一个真人视频， 把视频中的人脸换成指定的人脸照片</li>
<li>Image Animation<br>给定一张照片，随后根据这张照片生成一系列的对应视频 </li>
<li><strong>Video Diffusion</strong><br>对物理世界的建模<br>成本最高</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="有趣的灵魂">有趣的灵魂</span><a href="#有趣的灵魂" class="header-anchor">#</a></h3><ul>
<li><p>个性</p>
<ul>
<li>基于prompt<br>完整地刻画出一个人物的历史、个性、记忆和性格<br>长文本</li>
<li>基于微调的 agent<ul>
<li>更关键的还是数据<ul>
<li><strong>对话性语料</strong> &amp; <strong>事实性语料</strong></li>
<li>第一步，我们先用对话性语料去微调他的个性和说话风格</li>
<li>第二步，再去把事实性语料进行数据清洗后，基于各种角度提问，生成这个人物第一人称口吻的回答，这叫做<strong>数据增强</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>慢思考</strong>与记忆</p>
<ul>
<li>组件<br><strong>记忆、情感</strong>、任务规划、工具</li>
<li>长期记忆<ul>
<li>事实性的记忆<ul>
<li>总结<br>文本总结  MemGPT</li>
<li><strong>RAG 和信息压缩</strong></li>
<li>长上下文  <strong>长上下文</strong><br>结合持久化 KV Cache<br>成本还是太高<br>【eg.  文本总结 + RAG】</li>
</ul>
</li>
<li>程序性的记忆<ul>
<li>few-shot</li>
<li>微调<br>短期来看仍然是效果最好的路线</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="有用的ai更像工具的ai">有用的AI：更像工具的AI</span><a href="#有用的ai更像工具的ai" class="header-anchor">#</a></h1><h3><span id="大模型基础能力">大模型基础能力</span><a href="#大模型基础能力" class="header-anchor">#</a></h3><ul>
<li><strong>复杂任务的规划和分解</strong></li>
<li>遵循复杂指令</li>
<li><strong>自主使用工具</strong></li>
<li>减少幻觉</li>
</ul>
<h3><span id="1p-3p-产品法则">1P-3P 产品法则</span><a href="#1p-3p-产品法则" class="header-anchor">#</a></h3><ul>
<li><p>分类</p>
<ul>
<li>个人助理类</li>
<li>商业智能类</li>
</ul>
</li>
<li><p>OpenAI 的 1P-3P 产品法则</p>
<ul>
<li>只要一两个人（1P）开发的产品就自己（first Party）做<ul>
<li>1P 产品例子<ul>
<li>导游</li>
<li>企业 ERP 助手</li>
<li>大模型采集数据</li>
<li><strong>手机语音助手</strong><br>RPA（机器人流程自动化）<ul>
<li>腾讯的<strong>AppAgent</strong><br>视觉方案</li>
</ul>
</li>
<li>会议和生活记录器</li>
</ul>
</li>
</ul>
</li>
<li>需要三个人（3P）以上开发的产品就让第三方（third Party）做</li>
</ul>
</li>
</ul>
<h3><span id="解决复杂任务和使用工具">解决复杂任务和使用工具</span><a href="#解决复杂任务和使用工具" class="header-anchor">#</a></h3><ul>
<li>慢思考<ul>
<li><strong>思维链</strong><br> 思维链是非常自然的一种慢思考的模式</li>
<li>复杂任务的规划和分解   <ul>
<li>用<strong>多步的网络搜索</strong>去回答难题</li>
</ul>
</li>
<li>AI 需要能够按照流程<strong>调用工具</strong><ul>
<li>工具使用属于过程记忆，使用场景和条件不是语言可以明确描述的<br>使用 <strong>fine-tuning 方法</strong>告诉模型一些工具使用的样例，甚至在预训练时就加入</li>
<li>工具使用可以用代码形式表达，因此属于<strong>代码生成能力</strong><br>使用<strong>RAG方法</strong>获取到工具使用的代码</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/689816790">AI Agent 应该更有趣还是更有用？</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(survey)多模态  数据集</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#survey0">Survey[0]</a></li>
<li><a href="#pre-training%E6%95%B0%E6%8D%AE%E9%9B%86">Pre-training数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86">SFT数据集</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
<li><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86">预训练数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86-1">SFT数据集</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="survey0">Survey[0]</span><a href="#survey0" class="header-anchor">#</a></h1><ul>
<li>Pre-training</li>
<li>Adaptation</li>
</ul>
<h1><span id="pre-training数据集">Pre-training数据集</span><a href="#pre-training数据集" class="header-anchor">#</a></h1><ul>
<li><p>LAION[1]<br><a href="https://laion.ai/projects/">LAION</a></p>
</li>
<li><p>wukong[1]<br><a href="https://zhuanlan.zhihu.com/p/473794131">[论文]中文多模态数据集WuKong &amp; FILIP &amp; LiT-tuning</a><br><a href="https://zhuanlan.zhihu.com/p/551622338">Wukong：一亿规模的中文跨模态预训练基准</a></p>
</li>
<li><p>MMDialog<br><a href="https://zhuanlan.zhihu.com/p/584894471">百万量级的多模态对话数据集来了，153万张图片4000多主题</a> </p>
</li>
<li><p>OBELISC[2]</p>
</li>
<li><p>ShareGPT4V[3]<br>opensource</p>
</li>
</ul>
<h1><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h1><ul>
<li>LAMM</li>
<li>MultiIntruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol start="0">
<li><a href="https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw">多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 </a><br>《A Survey of Multimodal Large Language Model from A Data-centric Perspective》</li>
</ol>
<h3><span id="预训练数据集">预训练数据集</span><a href="#预训练数据集" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/686757824">多模态数据集收集</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/670149958">[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents</a><br>从网页文档里得到的数据集</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/669485001">超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能</a><br><a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">ShareGPT4V</a> git</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/527182857">多模态预训练数据集</a></p>
<p>1xx. <a href="https://opendatalab.org.cn/">OpenDataLab</a></p>
<h3><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h3><p>1xx. <a href="https://datac.blog.csdn.net/article/details/135434897">【LMM 015】LAMM：多模态指令微调数据集，框架和基准</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678489834">[NeurIPS2023] LAMM：多模态指令微调数据集、框架、评测基准</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV12p4y1M7RV/">Talk | ACL’23 杰出论文，MultiIntruct：通过多模态指令集微调提升VLM的零样本学习</a><br>1xx. <a href="https://blog.csdn.net/qq_45978862/article/details/132008907">【ACL2023】MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Dataset</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《Datasets for Large Language Models: A Comprehensive Survey》</p>
</li>
<li><p>开源地址<br> <a href="https://github.com/lmmlzn/Awesome-LLMs-Datasets">Awesome-LLMs-Datasets</a> git</p>
</li>
</ul>
<h1><span id="微调数据">微调数据</span><a href="#微调数据" class="header-anchor">#</a></h1><h3><span id="微调数据的构造方式">微调数据的构造方式</span><a href="#微调数据的构造方式" class="header-anchor">#</a></h3><ul>
<li>人工生成的数据集(HG)</li>
<li>模型构建的数据集(MC)<ul>
<li>Alpaca</li>
<li>BELLE</li>
<li>Self-Instruct</li>
<li>ShareGPT</li>
<li>Wizard</li>
</ul>
</li>
<li>现有数据集的收集和改进(CI) </li>
<li>使用多种方法创建的数据集</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409066&idx=1&sn=54e68bbbd45b4cc5bef8fd446fa187f8">大模型训练数据集(从预训到强化)全面综述：兼看20240229大模型早报 </a>***</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>Temperature &amp; Top-p</title>
    <url>/www6vHomeAIGC/2023/03/30/gptTemperature/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="temperature">Temperature</span><a href="#temperature" class="header-anchor">#</a></h1><h1><span id="top-p">Top-p</span><a href="#top-p" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://www.bilibili.com/video/BV1rm411R7d9/">LLM解码参数Temperature Top K &amp; Top P有啥作用？#小工蚁</a> V<br>   <a href="https://docs.cohere.com/docs/predictable-outputs">Predictable Outputs</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/666315413">创造性vs确定性：大语言模型(LLM)中的温度(Temperature)和Top_P怎么调？</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Temperature</category>
      </categories>
      <tags>
        <tag>Temperature</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Pytorch</title>
    <url>/www6vHomeAIGC/2023/03/28/gptPytorch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="教程12">教程[1,2]</span><a href="#教程12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="基础">基础</span><a href="#基础" class="header-anchor">#</a></h3><ol>
<li><a href="https://github.com/www6v/AIGC/tree/master/framework/pytorch">Pytorch</a>  卢老师</li>
<li><a href="https://mp.weixin.qq.com/s/qrSLPcc9Pmyc2cWOYgQ1Jw">《PyTorch实用教程》（第二版）开源了！</a>  PDF电子版<br><a href="https://github.com/www6v/PyTorch-Tutorial-2nd">PyTorch 实用教程（第二版）</a> git<br><a href="https://tingsongyu.github.io/PyTorch-Tutorial-2nd/">PyTorch 实用教程（第二版）</a>  gitbook</li>
</ol>
<h3><span id="分布式">分布式</span><a href="#分布式" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/343951042">PyTorch 源码解读之 DP &amp; DDP：模型并行和分布式训练解析</a> 源码<br>1xx. <a href="https://www.bilibili.com/video/BV1xZ4y1S7dG/">33、完整讲解PyTorch多GPU分布式训练代码编写</a>   V<br>    DP不建议使用,  DDP建议使用<br>    <a href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch Distributed Overview</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Weight Only(LLM.int8(), GPTQ, AWQ)</title>
    <url>/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="llmint8-2">LLM.int8() [2]</span><a href="#llmint8-2" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><p>LLM.int8()是一种采用<strong>混合精度分解</strong>的量化方法。该方案先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（vector-wise）。对<strong>离群特征</strong>的几个维度保留16bit，对其做高精度的矩阵乘法。</p>
<h3><span id="算法">算法</span><a href="#算法" class="header-anchor">#</a></h3><p>LLM.int8() 通过三个步骤完成矩阵乘法计算:</p>
<ul>
<li>从输入的隐含状态中，按列提取<strong>异常值 (离群特征，即大于某个阈值的值)。</strong></li>
<li>对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8 矩阵运算；</li>
<li>反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的 FP16 结果。</li>
</ul>
<h1><span id="gptq2">GPTQ[2]</span><a href="#gptq2" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><p>GPTQ 采用<strong>int4&#x2F;fp16 (W4A16) 的混合量化</strong>方案，其中<strong>模型权重</strong>被量化为 <strong>int4</strong> 数值类型，而<strong>激活值</strong>则保留在 <strong>float16</strong>，是一种<strong>仅权重量化</strong>方法。在推理阶段，模型权重被动态地反量化回 float16 并在该数值类型下进行实际的运算；同 OBQ  一样，GPTQ还是从<strong>单层量化</strong>的角度考虑，<strong>希望找到一个量化过的权重，使的新的权重和老的权重之间输出的结果差别最小</strong>。</p>
<p>GPTQ 将权重分组（如：128列为一组）为多个子矩阵（block）。对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。因此，GPTQ 量化需要准备校准数据集。</p>
<h3><span id="实现">实现</span><a href="#实现" class="header-anchor">#</a></h3><p>AutoGPTQ 代码库集成到了 Transformers 中，让用户使用 GPTQ 算法在 <strong>8 bit、4 bit、3 bit</strong>，甚至是 <strong>2 bit</strong> 精度下量化和运行模型成为可能。当使用 <strong>int4 量化</strong>时，精度的下降可以忽略不计，同时在小批量推理上保持着与 fp16 基线相当的速度。需要注意的是，GPTQ 方法与 bitsandbytes 提出的训练后量化方法有所不同，GPTQ 需要<strong>在量化阶段</strong>提供一个<strong>校准数据集</strong>。</p>
<h1><span id="awq-3">AWQ [3]</span><a href="#awq-3" class="header-anchor">#</a></h1><h3><span id="技术原理">技术原理</span><a href="#技术原理" class="header-anchor">#</a></h3><p>AWQ是一种对大模型<strong>仅权重量化</strong>方法。通过<strong>保护更“重要”的权重不进行量化</strong>，从而在不进行训练的情况下提高准确率。</p>
<h3><span id="实现">实现</span><a href="#实现" class="header-anchor">#</a></h3><p>目前，除了官方提供了对于AWQ的支持（llm-awq）之外，社区有相当多的工具（如：<strong>AutoAWQ</strong>、<strong>vLLM</strong>、 HuggingFace TGI、LMDeploy、 <strong>TensorRT-LLM</strong>、FastChat 等）提供了对AWQ的支持。</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><ul>
<li><p>LLM.int8()<br> 属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。<br>【keyword: 混合精度分解    离群值】</p>
</li>
<li><p>GPT-Q<br> 把量化问题视作<strong>优化问题</strong>，逐层寻找最优的量化权重。<br>【keyword: 混合量化  优化问题】</p>
</li>
</ul>
<h3><span id="awq-vs-gptq-1">AWQ vs GPTQ [1]</span><a href="#awq-vs-gptq-1" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th><strong>特征&#x2F;算法</strong></th>
<th><strong>AWQ</strong></th>
<th><strong>GPTQ</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>设计目的</strong></td>
<td>量化大型语言模型，<strong>特别强调保护显著权重</strong>，以减少量化误差。</td>
<td><strong>专为GPT模型设计</strong>，高效地完成权重量化，以减少计算和存储成本。</td>
</tr>
<tr>
<td><strong>量化方法</strong></td>
<td>基于<strong>激活分布</strong>而不是权重来选择保护的权重。</td>
<td><strong>一次性权重量化</strong>，基于近似二阶信息。</td>
</tr>
<tr>
<td><strong>精度和效率</strong></td>
<td>在不同模型和位精度上<strong>都表现优异</strong>，能够提高视觉语言模型的性能。</td>
<td>在<strong>极低位数量化（如2位）</strong>下仍保持合理准确度，能在<strong>短时间</strong>内量化大规模模型。</td>
</tr>
<tr>
<td><strong>硬件适应性</strong></td>
<td>支持高效推理框架，适用于桌面和移动GPU。</td>
<td>使得在单个GPU上执行大规模模型成为可能，提高了推理速度。</td>
</tr>
<tr>
<td><strong>应用范围</strong></td>
<td>适用于<strong>多种模型和任务</strong>，包括<strong>多模态</strong>语言模型。</td>
<td><strong>专门针对GPT模型</strong>，适用于高计算需求的模型。</td>
</tr>
<tr>
<td><strong>推理性能提升</strong></td>
<td>提供显著的速度提升，尤其在边缘设备上表现突出。</td>
<td>在高端和成本效益高的GPU上均实现显著的推理速度提升。</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《8-实战Transformers模型量化》 Ai大模型微调</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/680212402">大模型量化技术原理-LLM.int8()、GPTQ</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/681578090">大模型量化技术原理-AWQ、AutoAWQ</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399136&idx=1&sn=bd0a7237940c2ac800e06ae6d247349e">NLP大模型压缩关键技术解读：用于大型Transformer的8-bit矩阵乘法原理及其简单实现</a><br>   <a href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes </a>  LLM.int8()</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Deepspeed</title>
    <url>/www6vHomeAIGC/2023/03/25/gptTrainDeepspeedPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





<h1><span id="deepspeed-配置0">DeepSpeed 配置[0]</span><a href="#deepspeed-配置0" class="header-anchor">#</a></h1><h3><span id="zero-2-配置-12">ZeRO-2 配置 [1][2]</span><a href="#zero-2-配置-12" class="header-anchor">#</a></h3><h3><span id="zero-3-配置34">ZeRO-3 配置[3][4]</span><a href="#zero-3-配置34" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li>《大模型分布式训练框架 Microsoft DeepSpeed》 Ai大模型微调</li>
<li><a href="https://github.com/www6v/LLM-quickstart/blob/main/deepspeed/config/ds_config_zero2.json">ds_config_zero2.json</a></li>
<li><a href="https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero2.json">ds_config_zero2.json</a></li>
<li><a href="https://github.com/www6v/LLM-quickstart/blob/main/deepspeed/config/ds_config_zero3.json">ds_config_zero3.json</a></li>
<li><a href="https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero3.json">ds_config_zero3.json</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399768&idx=1&sn=3be1a2e9d8753c06b65f474c289b710f">NLP大规模语言模型微调实践：DeepSpeed+Transformers实现简单快捷上手百亿参数模型微调 </a> FLAN-T5  + DeepSpeed</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Deepspeed</category>
      </categories>
      <tags>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>P-Tuning</title>
    <url>/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="p-tuning2">P-Tuning[2]</span><a href="#p-tuning2" class="header-anchor">#</a></h1><ul>
<li>P-Tuning 的创新之处在于将提示（Prompt）转化为<strong>可学习的嵌入层（Embedding Layer）</strong></li>
</ul>
<h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/ptuning.png" class>

<ul>
<li><p>一个关于“The capital of Britain is [MASK]” 示例：</p>
<ul>
<li>蓝色是上下文 “Britain” </li>
<li>红色是目标单词 “[MASK]”， </li>
<li>橙色区域是提示词。</li>
</ul>
</li>
<li><p>传统方式 与 P-Tuning 对比： </p>
<ul>
<li>在（a）中，提示生成器只接收离散奖励； </li>
<li>在（b）中，连续的<strong>提示嵌入（Prompt Embedding）</strong> 和<strong>提示编码器（Prompt Encoder）</strong>以可微的方式进行 优化。</li>
</ul>
</li>
</ul>
<h1><span id="p-tuning-v22">P-Tuning v2[2]</span><a href="#p-tuning-v22" class="header-anchor">#</a></h1><h3><span id="背景">背景</span><a href="#背景" class="header-anchor">#</a></h3><p>之前的方法在以下两方面有所<strong>限制</strong>：<br>• 模型规模差异：在大型预训练模型中，Prompt Tuning 和<br>P-Tuning 能取得与全面微调相似的效果，但在参数较少<br>的模型上则表现不佳。<br>• 任务类型差异：无论是 Prompt Tuning 还是 P-Tuning，<br>在序列标注任务上的表现都较差。</p>
<h3><span id="目的">目的</span><a href="#目的" class="header-anchor">#</a></h3><p>P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模的预训练模型上，针对各种下游任务都能达到类似全面微调（Fine-tuning）的效果。</p>
<h3><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/ptuning-v2.png" class>
<p>在每一层都加入了Prompts tokens 作为输入,  而不是仅仅加在输入层</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/compare.png" class>
<ul>
<li>P-tuning  和 Prompt Tuning 仅仅更新<strong>第一个Transformer层</strong></li>
<li>Prefix tuning 和 P-Tuning v2 针对<strong>每一个Transformer 层</strong>进行更新</li>
<li>Prefix tuning 和 P-Tuning 需要<strong>重新参数化(PromptEncoder)</strong>, 而Prompt Tuning 和 P-Tuning v2则<strong>不需要</strong></li>
<li>简单将<strong>P-Tuning</strong>认为是针对 <strong>Prompt Tuning</strong>的<strong>改进</strong>,    <strong>P-Tuning v2</strong> 认为是针对 <strong>Prefix tuning</strong> 的<strong>改进</strong>.</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://aicarrier.feishu.cn/file/H1YvbRyacopEs6xzgZ8c9DDcnIh">大模型参数高效微调技术原理及实践</a> pdf<br><a href="https://www.bilibili.com/video/BV1qw411c7Hd/">如何高效微调大模型？技术原理与最佳实践揭秘！</a> V *** </p>
</li>
<li><p>《3-大模型微调技术揭秘-PEFT》 Ai大模型微调</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Zero Deepspeed</title>
    <url>/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="zero-dp-1-2">ZeRO-DP [1] [2]</span><a href="#zero-dp-1-2" class="header-anchor">#</a></h1><ul>
<li>ZeRO-DP<br>是一种<strong>分布式数据并行</strong>训练方法，通过<strong>减少冗余数据</strong>来降低每个设备的显存占用，从而允许训练更大的模型</li>
</ul>
<h3><span id="zero-dp-三个优化阶段">ZeRO-DP 三个优化阶段</span><a href="#zero-dp-三个优化阶段" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/zero.png" class>

<p>以 Adam优化器，64张GPU为例计算</p>
<ul>
<li><p>naive DP的通信量<br>通信量是<strong>2X</strong><br>显存占用<strong>16X</strong></p>
</li>
<li><p>ZeRO Stage 1 （Pos）<br>总的通信量为<strong>2X，跟naive DP一致</strong><br>显存方面 约为<strong>naive DP的 1&#x2F;4</strong></p>
</li>
<li><p>ZeRO Stage 2 （Pos+g）<br>通信量也是<strong>2X, 跟naive DP一致</strong><br>显存方面  约为<strong>naive DP的1&#x2F;8</strong></p>
</li>
<li><p>ZeRO Stage 3 （Pos+g+p）<br>总的通信量为，为<strong>naive DP的1.5倍</strong>，增加50%通信量<br>显存方面  约为<strong>naive DP的1&#x2F;32</strong></p>
</li>
</ul>
<h1><span id="zero-offload3">ZeRO-Offload[3]</span><a href="#zero-offload3" class="header-anchor">#</a></h1><ul>
<li>ZeRO-Offload[2]<br>ZeRO技术的一个扩展，它将部分数据和计算从GPU（或其他主要训练设备）卸载到CPU，从而减轻了GPU的显存负担，并使得在有限GPU资源下训练更大的模型成为可能</li>
</ul>
<p><strong>现在要做的就是沿着边把数据流图切分为两部分，分别对应GPU和CPU，</strong>计算节点（矩形节点）落在哪个设备，哪个设备就执行计算，数据节点（圆形）落在哪个设备，哪个设备就负责存储，将被切分的边权重加起来，就是CPU和GPU的通信数据量。<br>ZeRO-Offload的切分思路是：图中有四个计算类节点：<strong>FWD、BWD、Param update和float2half</strong>，前两个计算复杂度大致是 O(MB) ， B 是batch size，后两个计算复杂度是 O(M) 。为了不降低计算效率，<strong>将前两个节点放在GPU，后两个节点不但计算量小还需要和Adam状态打交道，所以放在CPU上</strong>，Adam状态自然也放在内存中，为了简化数据图，将前两个节点融合成一个节点<strong>FWD-BWD Super Node</strong>，将后两个节点融合成一个节点<strong>Update Super Node</strong>。如下图右边所示，沿着gradient 16和parameter 16两条边切分。</p>
<img src="/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/zeroOffload.png" class>


<h1><span id="deepspeed-2">DeepSpeed [2]</span><a href="#deepspeed-2" class="header-anchor">#</a></h1><h3><span id="key-feature">Key feature</span><a href="#key-feature" class="header-anchor">#</a></h3><ul>
<li>ZeRO（Zero Redundancy Optimizer）</li>
<li>模型并行（Model Parallelism）</li>
<li>流水线并行（Pipeline Parallelism）</li>
<li>稀疏注意力（Sparse Attention）</li>
<li>显存和带宽优化</li>
</ul>
<h1><span id="pytorch-fsdp-10">PyTorch FSDP [10]</span><a href="#pytorch-fsdp-10" class="header-anchor">#</a></h1><p>FairScale 说 <strong>FSDP 相当于 ZeRO3 的优化</strong></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/664604792">[Transformer 101系列] LLM分布式训练面面观</a> ***</p>
</li>
<li><p>《大模型分布式训练框架Microsoft DeepSpeed》 Ai大模型微调</p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_43336281/article/details/126475071">震惊！我竟然在1080Ti上加载了一个35亿参数的模型（ZeRO, Zero Redundancy Optimizer）</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/644133265">FSDP 深度解析：2023 年了，大模型训练还要不要用 PyTorch 的 FSDP ？</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/610587671">【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP</a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_18555105/article/details/130513812">Zero系列三部曲：Zero、Zero-Offload、Zero-Infinity</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/617133971">图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1mc411y7jW/">Deepspeed大模型分布式框架精讲</a>  V 原理+实操    ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Zero</category>
      </categories>
      <tags>
        <tag>Zero</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理-框架</title>
    <url>/www6vHomeAIGC/2023/03/21/gptInferFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E6%A1%86%E6%9E%B61">推理 框架[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-框架1">推理 框架[1]</span><a href="#推理-框架1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/21/gptInferFramework/inference.jpg" class>

<ul>
<li><p>inference execute engine(server)<br>vLLM，TensorRT， deepspeed</p>
</li>
<li><p>inference execute engine(pc&#x2F;edge 移动端)<br> llama.cpp<br> mlc-llm<br> ollama</p>
</li>
<li><p>inference Server<br>Triton Server,  Ray</p>
</li>
<li><p>Chat Server [2]<br>FastChat, XInference,  modelscope  SWIFT</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzA5MTIxNTY4MQ==&scene=1&album_id=2959126655292211206">探秘LLM应用开发</a>   8-19</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2422454">LLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference&#x2F;FastChat等框架]</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142079&idx=1&sn=07d9033203c0064408fe0af33d1f9414">一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142012&idx=1&sn=dafb0b676cdf6d41fd9bd54f9b6a82d3">一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/659792625">大模型推理框架概述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)数据处理</title>
    <url>/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a><ul>
<li><a href="#%E5%8D%83%E5%B8%86llama-2%E4%B8%AD%E6%96%87%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B2%BE%E7%AE%80">数据精简</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">数据配比</a></li>
</ul>
</li>
<li><a href="#%E5%BA%A6%E5%B0%8F%E6%BB%A1%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B31">度小满轩辕金融大模型[31]</a><ul>
<li><a href="#%E9%80%9A%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%B5%81%E6%B0%B4%E7%BA%BF">通用的数据清洗流水线</a></li>
<li><a href="#%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%9C%80%E4%BD%B3%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">增量预训练 最佳数据配比</a></li>
<li><a href="#%E6%9E%84%E9%80%A0%E9%80%9A%E7%94%A8%E5%92%8C%E9%87%91%E8%9E%8D%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE">构造通用和金融指令数据</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B-1">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h1><h2><span id="千帆llama-2中文增强技术介绍-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</span><a href="#千帆llama-2中文增强技术介绍-sft30" class="header-anchor">#</a></h2><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><ul>
<li>Self-instruct</li>
<li>wizard [20]</li>
</ul>
<h3><span id="数据精简">数据精简</span><a href="#数据精简" class="header-anchor">#</a></h3><ul>
<li>低质量过滤</li>
<li>相似数据过滤</li>
</ul>
<h3><span id="数据配比">数据配比</span><a href="#数据配比" class="header-anchor">#</a></h3><ul>
<li>领域数据</li>
<li>多语言数据</li>
</ul>
<h2><span id="度小满轩辕金融大模型31">度小满轩辕金融大模型[31]</span><a href="#度小满轩辕金融大模型31" class="header-anchor">#</a></h2><h3><span id="通用的数据清洗流水线">通用的数据清洗流水线</span><a href="#通用的数据清洗流水线" class="header-anchor">#</a></h3><ul>
<li>文本抽取<ul>
<li>多来源数据收集</li>
<li>正文提取</li>
</ul>
</li>
<li>数据清洗<ul>
<li>规则过滤</li>
<li>模型过滤</li>
</ul>
</li>
<li>去重与校验<ul>
<li>MinHashLSH</li>
<li>质量校验</li>
</ul>
</li>
</ul>
<h3><span id="增量预训练-最佳数据配比">增量预训练 最佳数据配比</span><a href="#增量预训练-最佳数据配比" class="header-anchor">#</a></h3><ul>
<li><p><strong>英文数据  vs 中文数据</strong><br><strong>1  :  3</strong></p>
</li>
<li><p>中文数据中的  <strong>通用数据 vs 金融数据</strong><br>从 9:1 变成  <strong>4:1</strong></p>
<ul>
<li>通用领域指令数据<br> 8大类 50小类</li>
<li>金融领域指令数据<br> 4大类 20小类</li>
</ul>
</li>
</ul>
<h3><span id="构造通用和金融指令数据">构造通用和金融指令数据</span><a href="#构造通用和金融指令数据" class="header-anchor">#</a></h3>
<ul>
<li>Self-Instruct</li>
<li>Evol-Instruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="30">
<li>《千帆增强版 Llama 2》 百度 有ppt</li>
<li>《金融行业实战：度小满轩辕金融大模型应用探索与开发实践》 百度  有ppt</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399885&idx=1&sn=4f49c5148715c38aa4eaee3080435f17">也看大模型训练语料如何清洗：Common Crawl概述、代表性清洗方案及代码实现          </a> 代码<br>   <a href="https://zhuanlan.zhihu.com/p/610659484?utm_id=0">GPT-3 训练语料 Common Crawl 处理流程</a></p>
<p>1xx. <a href="https://github.com/alibaba/data-juicer/blob/main/README_ZH.md">Data-Juicer: 为大语言模型提供更高质量、更丰富、更易“消化”的数据</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Wizard</title>
    <url>/www6vHomeAIGC/2023/03/18/gptDataWizard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#wizard-%E6%96%B9%E6%B3%95">Wizard 方法</a><ul>
<li><a href="#%E8%87%AA%E5%8A%A8%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96-1">自动指令数据进化 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F-%E5%A4%9A%E6%A0%B7%E6%80%A7-%E5%A4%8D%E6%9D%82%E5%BA%A6">质量-&gt; 多样性, 复杂度</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="wizard-方法">Wizard 方法</span><a href="#wizard-方法" class="header-anchor">#</a></h1><h3><span id="自动指令数据进化-1">自动指令数据进化 [1]</span><a href="#自动指令数据进化-1" class="header-anchor">#</a></h3><p>1）指令进化</p>
<ul>
<li>In-Depth Evolving 提示 [深度]<ul>
<li>五种类型的提示来增强指令<br>增加约束 + 深化 + 具体化 + 增加推理步骤 + 复杂化输入</li>
<li>核心部分<br><strong>In-Depth Evolving的提示的核心部分是 “你的目标是将一个给定的提示改写成更复杂的版本，使那些著名的人工智能系统（如ChatGPT和GPT4）更难处理。但改写后的提示必须是合理的，能被人理解，并能被人回应”</strong></li>
</ul>
</li>
<li>In-Breadth Evolving提示 [广度]<ul>
<li>目的<br>旨在提高<strong>主题覆盖率</strong>、<strong>技能覆盖率</strong>和整体数据集的<strong>多样性</strong></li>
</ul>
</li>
</ul>
<p>2）响应生成</p>
<p>3）消除进化<br>   即过滤未能进化的指令</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="质量-gt-多样性-复杂度">质量-&gt; 多样性, 复杂度</span><a href="#质量-gt-多样性-复杂度" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401462&idx=1&sn=764f0302918174cea29ae22ac5760033">如何构造复杂多样的微调指令数据：WizardLM复杂指令构造思想与实验分析工作总结 </a><br> <a href="https://github.com/nlpxucan/WizardLM">WizardLM</a> git</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)多模态</title>
    <url>/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#overview-0">overview [0]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3-1">视觉理解 [1]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90-1">视觉生成 [1]</a></li>
<li><a href="#%E7%BB%9F%E4%B8%80%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B2">统一的视觉模型[2]</a></li>
<li><a href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%AD%E7%BB%83llm2">端到端的方式训练LLM[2]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent3">多模态 Agent[3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》  microsoft</li>
</ul>
<h1><span id="overview-0">overview [0]</span><a href="#overview-0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/overview.jpeg" class>

<h1><span id="视觉理解-1">视觉理解 [1]</span><a href="#视觉理解-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding.png" class>

<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding-method.png" class>


<h1><span id="视觉生成-1">视觉生成 [1]</span><a href="#视觉生成-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/generation.png" class>

<h1><span id="统一的视觉模型2">统一的视觉模型[2]</span><a href="#统一的视觉模型2" class="header-anchor">#</a></h1><h1><span id="端到端的方式训练llm2">端到端的方式训练LLM[2]</span><a href="#端到端的方式训练llm2" class="header-anchor">#</a></h1><h1><span id="多模态-agent3">多模态 Agent[3]</span><a href="#多模态-agent3" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><p>《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》  </p>
<ol start="0">
<li><p><a href="https://blog.csdn.net/qq_41185868/article/details/133594461">AGI之MFM：《Multimodal Foundation Models: From Specialists to General-Purpose Assistants多模态基础模型：从专家到通用助</a> 翻译</p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594554">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之视觉理解、视觉生成</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594624">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之统一的视觉模型、加持LLMs的大型多模态模型</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133606408">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之与LLM协同工作的多模态智能体、结论和研究趋势</a></p>
<p><a href="https://blog.csdn.net/qq_41200212/article/details/134663233">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(图生文)BLIP-2, Flamingo</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="blip-2">BLIP-2</span><a href="#blip-2" class="header-anchor">#</a></h1><h3><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h3><p>用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，<strong>只训练Qformer</strong>，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练</p>
<h3><span id="两阶段的训练策略-1">两阶段的训练策略 [1]</span><a href="#两阶段的训练策略-1" class="header-anchor">#</a></h3><p>BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。</p>
<ul>
<li>第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(<strong>ITC</strong>)，Image-grounded Text Generation(<strong>ITG</strong>)，Image-Text Matching(<strong>ITM</strong>)让Qformer学会如何从<strong>视觉编码器中抽取文本相关的特征</strong>。</li>
<li>第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。</li>
</ul>
<h3><span id="架构3">架构[3]</span><a href="#架构3" class="header-anchor">#</a></h3><ul>
<li><strong>两个阶段训练</strong><ul>
<li>阶段一<br>获得高质量的 <strong>图文对齐向量表征</strong><br>通过<strong>ITC ITM  ITG 三个损失函数</strong>获得了很好的图片文本 <strong>对齐向量表征能力</strong>，仅训练<strong>Qformer</strong>中很少的参数<br>【ITM:  image-text 是否是匹配的 |    image 和text 都能相互看到】<br>【ITG: image生成text |    image 能全看到, text只能逐个的看】<br>【ITC: image和text的对比学习, 对比学习分类分错了的  送入ITM 负样本 |  image和 text  之间是不能看到的】</li>
<li>阶段二<br>通过向量表征进行<strong>文字生成</strong></li>
</ul>
</li>
</ul>
<h3><span id="code-2">code [2]</span><a href="#code-2" class="header-anchor">#</a></h3><h1><span id="flamingo1">Flamingo[1]</span><a href="#flamingo1" class="header-anchor">#</a></h1><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><p>它在Frozen模型的基础上做进一步的改进，不同点主要有两个：一是使用了更大的LLMs，二是<strong>冻结视觉编码器</strong>，引入<strong>perceiver resampler</strong>和<strong>XAttn-Dense</strong>两个适配单元作为可训练的模块。</p>
<ul>
<li>perceiver resampler：<br>  类似DETR，通过设计多个Perceiver Resampler来生成<strong>64个固定长度的tokens</strong>，主要作用在于可以<strong>从图像中提取固定长度的特征向量</strong>，能够解决图像甚至多帧视频的<strong>feature map不一致的问题</strong>。【图像和文本对齐】</li>
<li>XAttn-Dense：在每一层LLM上都会增加<strong>corss- attention</strong>以入到<strong>LLM中与视觉向量进行交互</strong>，<strong>融合多模态信息</strong>。【融合】</li>
</ul>
<h1><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h1><h3><span id="mplug-docowl15-20">mPLUG-DocOwl1.5  [20]</span><a href="#mplug-docowl15-20" class="header-anchor">#</a></h3><p>DocOwl1.5由mPLUG-Owl2初始化，使用<strong>ViT&#x2F;L-14作为视觉编码器</strong>，并使用带有模态自适应模块的7B大模型作为<strong>解码器</strong>。<br>每个子图像由ViT&#x2F;L-14编码为1,024个特征，然后由<strong>H-Reducer缩减为256个特征</strong>。</p>
<h3><span id="textmonkey-20">TextMonkey [20]</span><a href="#textmonkey-20" class="header-anchor">#</a></h3><p>为了减少图像特征的冗余，继承了<strong>Qwen-VL</strong>中的图像<strong>重采样器</strong>，在每个窗口中都会使用。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="blip2">blip2</span><a href="#blip2" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130757157?spm=1001.2014.3001.5502">基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）</a></p>
</li>
<li><p><a href="https://github.com/www6v/LAVIS/tree/main/projects/blip2">blip2</a> git<br><a href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">blip2_instructed_generation</a> git 运行过</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Ek4y1G74J">强推！科大讯飞和中科院终于把多模态大模型讲明白了，CLIP、blip、blip2三种模型原理一口气学完</a> V ***</p>
</li>
</ol>
<p>1xx.  <a href="https://www.bilibili.com/video/BV18u4y137ZV/">AI论文精读之多模态大模型BLIP-2</a> V</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践</a> *</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/606364639">BLIP2：下一代多模态模型的雏形</a></p>
<h3><span id="flamingo">Flamingo</span><a href="#flamingo" class="header-anchor">#</a></h3><p>1xx. <a href="https://www.bilibili.com/video/BV1pu411G7ce">[论文速览]Flamingo: a Visual Language Model for Few-Shot Learning[2204.14198]</a> V<br>1xx. <a href="https://github.com/Luodian/Otter">Otter  on OpenFlamingo</a> git<br>1xx. <a href="https://github.com/mlfoundations/open_flamingo">open_flamingo</a> git</p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://mp.weixin.qq.com/s/1MSOZfbKcPW1BTT4f9XvQg">也看跨模态大模型遇见文档理解：mPLUG-DocOwl1.5及TextMonkey方案中的数据工程 </a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态InstructTuning</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</a><ul>
<li><a href="#single-turn">Single-turn</a></li>
<li><a href="#multi-turn">Multi-turn</a></li>
</ul>
</li>
<li><a href="#vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</a><ul>
<li><a href="#annotation-adaption">Annotation Adaption</a></li>
<li><a href="#self-instruct">Self-Instruct</a></li>
</ul>
</li>
<li><a href="#high-quality-vlit-data2">High-Quality VLIT Data[2]</a><ul>
<li><a href="#correctness">Correctness</a></li>
<li><a href="#diversity">Diversity</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#method-12">Method [1][2]</a><ul>
<li><a href="#annotation-adaption-si">Annotation Adaption-&gt; SI</a></li>
<li><a href="#self-instruct-aa">Self-Instruct -&gt; AA</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</span><a href="#datasets-for-visual-instruction-tuning1" class="header-anchor">#</a></h1><h3><span id="single-turn">Single-turn</span><a href="#single-turn" class="header-anchor">#</a></h3><ul>
<li><p>MiniGPT-4<br><strong>MiniGPT-4</strong> [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 <strong>randomly selects 5000 images from the Conceptual Caption dataset</strong> [38], [39] and prompts its <strong>pre-trained VLM model</strong> to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.</p>
</li>
<li><p>MultiInstruct<br>MultiInstruct [43] build a comprehensive instruction dataset that covers 62 diverse multimodal tasks from 10 broad categories, such VQA, Image-text matching, grounded generation, and so on. These tasks include 34 existing tasks derived from 21 public dataset and 28 new tasks extended from them. Each task is equipped with 5 instruction templates to prompt the model to perform the specific task.</p>
</li>
</ul>
<h3><span id="multi-turn">Multi-turn</span><a href="#multi-turn" class="header-anchor">#</a></h3><ul>
<li>LLaVA<br><strong>LLaVA-Instruct-158k</strong> [9] contains 158 image-text instruction data, including <strong>58k conversation data</strong> asking about the visual content of the image,<strong>23k description data</strong>, and <strong>77k complex reasoning data</strong> where the question may involve multi-step reasoning process.</li>
</ul>
<h1><span id="vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</span><a href="#vlit-data-construction-strategy2" class="header-anchor">#</a></h1><h3><span id="annotation-adaption">Annotation Adaption</span><a href="#annotation-adaption" class="header-anchor">#</a></h3><ul>
<li>MiniGPT-4</li>
</ul>
<h3><span id="self-instruct">Self-Instruct</span><a href="#self-instruct" class="header-anchor">#</a></h3><ul>
<li>LLaVA</li>
</ul>
<h1><span id="high-quality-vlit-data2">High-Quality VLIT Data[2]</span><a href="#high-quality-vlit-data2" class="header-anchor">#</a></h1><h3><span id="correctness">Correctness</span><a href="#correctness" class="header-anchor">#</a></h3><h3><span id="diversity">Diversity</span><a href="#diversity" class="header-anchor">#</a></h3><h3><span id="complexity">Complexity</span><a href="#complexity" class="header-anchor">#</a></h3><h1><span id="method-12">Method [1][2]</span><a href="#method-12" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Method</th>
<th>Training Paradigm[2]</th>
<th>Vision Encoder</th>
<th>Language Encoder</th>
<th>Inst[2]</th>
<th>Tuning Data</th>
</tr>
</thead>
<tbody><tr>
<td>MiniGPT-4</td>
<td>FA → VLIT</td>
<td>EvaCLIP ViT</td>
<td>Vicuna</td>
<td>AA</td>
<td>CC3M, CC12M, SBU, LAION 400M, MiniGPT-3.5K</td>
</tr>
<tr>
<td>MiniGPT-v2</td>
<td></td>
<td>EVA</td>
<td>LLaMA2-chat</td>
<td>AA+SI</td>
<td>LAION, CC3M, SBU, GRIT-20M, COCO caption, Text Captions, RefCOCO, RefCOCO+, RefCOCOg, GQA, VQA-v2, OCR-VQA, OKVQA, AOK-VQA, Flickr30k Dataset, Unnatural Instruction Dataset</td>
</tr>
<tr>
<td>LLaVa</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td>SI</td>
<td>CC3M Concept-balanced 595K, LLaVA-Instruct-158K</td>
</tr>
<tr>
<td>LLaVA-1.5</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td></td>
<td>LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG</td>
</tr>
<tr>
<td>MultiInstruct</td>
<td>VLIT</td>
<td>OFA</td>
<td>OFA</td>
<td>AA</td>
<td>VQAv2, Visual7w, GQA, OK-VQA, Visual Genome, MSCOCO, RefCOCO, COCO-Text, TDIUC, IQA, VAW, MOCHEG, WikiHow</td>
</tr>
<tr>
<td>Otter</td>
<td></td>
<td>CLIP ViT</td>
<td>MPT</td>
<td>SI</td>
<td>MIMIC-IT</td>
</tr>
<tr>
<td>LAMM</td>
<td>VLIT</td>
<td>CLIP ViT-L&#x2F;14</td>
<td>Vicuna</td>
<td>SI</td>
<td>Language-Assisted Multi-Modal Instruction-Tuning Dataset</td>
</tr>
<tr>
<td>Qwen-VL</td>
<td>FA → VLIT(Multi-Task Tuning)</td>
<td>ViT</td>
<td>Qwen-7B</td>
<td></td>
<td>LAION-en&amp;zh, DataComp, Coyo, CC12M&amp;3M, SBU, COCO, In-house Data, GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D, GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg, SynthDoG-en&amp;zh, Common Crawl pdf&amp;HTML</td>
</tr>
<tr>
<td>CogVLM</td>
<td>FA → VLIT</td>
<td>EVA2-CLIP-E</td>
<td>Vicuna-7Bv-1.5</td>
<td></td>
<td>VQAv2, TextVQA</td>
</tr>
<tr>
<td>StableLLaVA</td>
<td>FA → VLIT</td>
<td>CLIP-ViT-L&#x2F;14</td>
<td>LLaMA</td>
<td>AA</td>
<td>Synthesized Image-Dialogue Dataset</td>
</tr>
</tbody></table>
<h3><span id="annotation-adaption-gt-si">Annotation Adaption-&gt; SI</span><a href="#annotation-adaption-gt-si" class="header-anchor">#</a></h3><h3><span id="self-instruct-gt-aa">Self-Instruct -&gt; AA</span><a href="#self-instruct-gt-aa" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey》 ***  第4 5章  南洋大学 </p>
</li>
<li><p>《Vision-Language Instruction Tuning: A Review and Analysis》 ***  第2 3 4 5章   腾讯</p>
</li>
<li><p>《Instruction Tuning for Large Language Models: A Survey》 第5章</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)MiniGPT4</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#introduction1">INTRODUCTION[1]</a></li>
<li><a href="#method1">METHOD[1]</a><ul>
<li><a href="#first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</a></li>
<li><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</a></li>
<li><a href="#second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="introduction1">INTRODUCTION[1]</span><a href="#introduction1" class="header-anchor">#</a></h1><p>MiniGPT-4 增加了一个<strong>投影层</strong>，将<strong>编码的视觉特征与 Vicuna 语言模型对齐</strong>，并<strong>冻结了所有其他视觉和语言组件</strong></p>
<h1><span id="method1">METHOD[1]</span><a href="#method1" class="header-anchor">#</a></h1><ul>
<li><p>图 1</p>
</li>
<li><p>MiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，</p>
<ul>
<li>使用 <strong>Vicuna作为语言解码器</strong>，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。</li>
<li>视觉感知方：采用与 <strong>BLIP-2</strong> 相同的<strong>视觉编码器</strong>，<strong>ViT Backbone</strong>及其预先训练好的 <strong>Q-Former</strong>。<br>语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。</li>
</ul>
</li>
</ul>
<h3><span id="first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</span><a href="#first-pretraining-stage" class="header-anchor">#</a></h3><ul>
<li><p>第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。</p>
</li>
<li><p>Traditional alignment method [2]</p>
<ul>
<li>Input: Image</li>
<li>Output: Caption</li>
<li>Training Objective: Maximize the likelihood of GT captions</li>
<li>Training Dataset 组合数据集 [postprocessed by BLIP] <ul>
<li>Conceptual Caption</li>
<li>SBU </li>
<li>LAION</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</span><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain" class="header-anchor">#</a></h3><ul>
<li>Create a dataset with detailed, human-perfered descriptions[2][1]<ul>
<li>model  generates descriptions<br>在初始阶段，我们使用从第一个预训练阶段得到的模型来<strong>生成输入图像的描述</strong>。      </li>
<li>polishing and filtering by chatgpt<br>上述自动生成的图片说明包含<strong>噪音或不连贯的描述</strong>，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了<strong>ChatGPT</strong>，通过以下提示对描述进行<strong>修补</strong></li>
<li>further polishing and filtering by rules &amp; human<br>完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。</li>
</ul>
</li>
</ul>
<h3><span id="second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></span><a href="#second-stage-finetuning" class="header-anchor">#</a></h3><ul>
<li>第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。</li>
</ul>
<p>【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】  [2]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><a href="https://datac.blog.csdn.net/article/details/135399033">【LMM 009】MiniGPT-4：使用 Vicuna 增强视觉语言理解能力的多模态大模型</a> *** </li>
<li><a href="https://www.bilibili.com/video/BV1n24y1F7kv/">MiniGPT-4、表格推理、代码生成、生成式推理-来自斯坦福、北大、阿卜杜拉、达摩院的四位论文一作思辨大模型</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV12Q4y1b7nY/">miniGPT4：多模态图文理解训练</a> V<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 </a><br>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d282e4b007b201a34052">使用大型语言模型为MiniGPT-4构建视觉语言理解能力</a> V</li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/627671257">大杀器，多模态大模型MiniGPT-4入坑指南</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)多模态  LLaVa</title>
    <url>/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="gpt-assisted-visual-instruction-data-generation-1">GPT-assisted Visual Instruction Data Generation [1]</span><a href="#gpt-assisted-visual-instruction-data-generation-1" class="header-anchor">#</a></h3><ul>
<li><p>detail<br>为了缓解这一问题，我们利用<strong>纯语言 GPT-4 或 ChatGPT 作为强大的教师（两者都只接受文本作为输入），来创建涉及视觉内容的指令遵循数据</strong>。具体来说，为了将图像编码为视觉特征以提示纯文本 GPT，我们使用了<strong>两类符号</strong>表示：</p>
<p>  i）<strong>图像描述（Captions）</strong>通常从不同角度描述视觉场景；<br>  ii）<strong>边框（Bounding Boxes）</strong>通常定位场景中的物体，每个框编码物体概念及其空间位置。表 14 顶部图块就是一个例子。</p>
</li>
<li><p>158K   语言图像指令遵循样本</p>
<ul>
<li>58K  对话样本</li>
<li>23K  详细描述样本 </li>
<li>77K  复杂推理样本</li>
</ul>
</li>
</ul>
<h1><span id="llava-2elmo">LLaVa [2][ELmo]</span><a href="#llava-2elmo" class="header-anchor">#</a></h1><h3><span id="摘要">摘要</span><a href="#摘要" class="header-anchor">#</a></h3><ul>
<li><p>微软 LLaVa 模型是一个在视觉 - 语言多模态领域的先进实现，它利用了预训练的大型语言模型（LLaMA）和视觉模型（CLIP）。</p>
</li>
<li><p>模型的<strong>训练数据集</strong>包括预训练集 CC3M 和精调集 Instruct-150K。</p>
</li>
<li><p>LLaVa 的<strong>模型结构</strong>采用了简单的<strong>线性层</strong>将视觉特征转换为语言嵌入标记，并与文本特征拼接输入给语言模型。</p>
</li>
<li><p>训练过程分为<strong>两个阶段</strong><br>特征对齐的预训练和端到端的微调，后者包括针对视觉聊天和 Science QA 两种使用场景的微调。</p>
</li>
<li><p>LLaVa 在测试数据集上的表现显示，它在 GPT-4 生成的数据集上达到了 85.1% 的相对分数，并在 ScienceQA 数据集上实现了最先进的效果。</p>
</li>
<li><p>作者通过对比 miniGPT-4，强调了 LLaVa 在数据和实验量化、Chat Demo 质量以及数据中心项目方面的特点。此外，作者还提供了模型权重、代码和相关数据集的链接，以便社区可以利用这些资源来复现和扩展多模态 GPT-4 的能力。</p>
</li>
</ul>
<h3><span id="观点">观点</span><a href="#观点" class="header-anchor">#</a></h3><ol>
<li>LLaVa 模型在多模态领域的实现被认为是优秀的，因为它提供了严格的量化结果，包括与 GPT-4 的接近程度和在 Science QA 上的最佳准确率。</li>
<li>与 miniGPT-4 相比，LLaVa 在 Chat Demo 的质量上表现出色，具有较强的<strong>视觉推理和 OCR 能力</strong>，并且在数据质量和模型设计的 ablation study 上有更全面的展示。</li>
<li>LLaVa 项目强调的是<strong>数据中心</strong>的方法，而不是模型中心的方法，即在模型性能差异逐渐缩小的情况下，<strong>数据质量对结果的影响至关重要</strong>。作者鼓励社区利用提供的多模态指令跟随数据来复现和进一步扩展多模态 GPT-4 的能力。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://datac.blog.csdn.net/article/details/135329498">【LMM 001】LLaVA：大型语言和视觉助手</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/625723805">miniGPT-4的同期工作: 微软LLaVa模型论文笔记</a></p>
</li>
</ol>
<p>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d4fde4b0d1e42e7fc7e6">基于视觉指令调整的多模态聊天机器人 LLaVA｜AI新青年讲座·大型语言模型专场</a> V</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/683137074">[LLaVA系列]📒CLIP&#x2F;LLaVA&#x2F;LLaVA1.5&#x2F;VILA笔记</a></p>
<h3><span id="plus">plus</span><a href="#plus" class="header-anchor">#</a></h3><p>1xx. <a href="https://datac.blog.csdn.net/article/details/135329602">【LMM 002】LLaVA-1.5：大型语言和视觉助手</a></p>
<p>1xx. <a href="https://datac.blog.csdn.net/article/details/135329898">【LMM 006】LLaVA-Plus：可以学习如何使用工具的多模态Agent</a></p>
<h3><span id="微调llava-实战">微调llava 实战</span><a href="#微调llava-实战" class="header-anchor">#</a></h3><p><a href="https://github.com/InternLM/Tutorial/blob/camp2/xtuner/readme.md">XTuner 微调 LLM：1.8B、多模态、Agent (更新撰写中)</a><br><a href="https://github.com/InternLM/Tutorial/blob/camp2/xtuner/llava/xtuner_llava.md">XTuner多模态训练与测试</a></p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/train_lora/llava1_5_lora_sft.yaml">LLaMA-Factory  llava1_5_lora_sft</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)多模态 RAG</title>
    <url>/www6vHomeAIGC/2023/03/14/gptRAGMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag-12">多模态RAG [1][2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="多模态rag-12">多模态RAG [1][2]</span><a href="#多模态rag-12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> *** </p>
</li>
<li><p>《Retrieving Multimodal Information for Augmented Generation: A Survey》<br><a href="https://zhuanlan.zhihu.com/p/665078079">万字综述：2023年多模态检索增强生成技术(mRAG)最新进展与趋势-图片、代码、图谱、视频、声音、文本</a><br><a href="https://zhuanlan.zhihu.com/p/678812531">多模态RAG综述</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409004&idx=2&sn=7f36d3ff5e170442486a5d413373c563">朴素多模态RAG如何实现？兼看RAG上下文过滤方案FILCO及202402大模型早报 </a>    多模态RAG 两种实现方式</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/9xU7OOBqee4sM_TZuzQ-Ew">视觉的跨界 Wiki-LLaVA | lmage + Question 的奇妙反应，生成多模态大型语言模型（MLLMs）！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)多模态 RAG</title>
    <url>/www6vHomeAIGC/2023/03/14/gptRAGMultimodalPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag-%E5%A4%9A%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%99%A8-1011">多模态RAG-多向量检索器 [10][11]</a><ul>
<li><a href="#semi-structured-tables-text-rag-20">semi-structured (tables + text) RAG [20]</a></li>
<li><a href="#multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG  [13]</a></li>
<li><a href="#private-multi-modal-text-tables-images-rag-22">private multi-modal (text + tables + images)  RAG [22]</a></li>
<li><a href="#%E7%BB%84%E4%BB%B6">组件</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
<li><a href="#notebook">notebook</a></li>
<li><a href="#template">template</a></li>
<li><a href="#llamaindex">llamaindex</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="多模态rag-多向量检索器-1011">多模态RAG-多向量检索器 [10][11]</span><a href="#多模态rag-多向量检索器-1011" class="header-anchor">#</a></h1><h3><span id="semi-structured-tables-text-rag-20">semi-structured (tables + text) RAG [20]</span><a href="#semi-structured-tables-text-rag-20" class="header-anchor">#</a></h3><p> 分析pdf中表格 </p>
<h3><span id="multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG  [13]</span><a href="#multi-modal-text-tables-images-rag-13" class="header-anchor">#</a></h3><p>分析PDF中图片</p>
<ul>
<li><p><strong>Option 1</strong>  [基于CLIP] [23][30][32][33]</p>
<ul>
<li>Use multimodal embeddings <strong>(such as <a href="https://openai.com/research/clip">CLIP</a>)</strong> to embed images and text</li>
<li>Retrieve both using similarity search</li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis<br> {选项1：对文本和表格生成summary，然后应用多模态embedding模型把文本&#x2F;表格summary、原始图片转化成embedding存入多向量检索器。对话时，根据query召回原始文本&#x2F;表格&#x2F;图像。然后将其喂给多模态LLM生成应答结果。}[10]</li>
</ul>
</li>
<li><p><strong>Option 2</strong>   [21] </p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce <strong>text summaries from images</strong></li>
<li>Embed and retrieve text </li>
<li>Pass text chunks to an LLM for answer synthesis<br>【将图片转成摘要，和其他文本信息整合在文本粒度进行检索】[12]<br> {选项2：首先应用多模态大模型（GPT4-V、LLaVA、FUYU-8b）生成图片summary。然后对文本&#x2F;表格&#x2F;图片summary进行向量化存入多向量检索器中。当生成应答的多模态大模型不具备时，可根据query召回原始文本&#x2F;表格+图片summary。}[10]</li>
</ul>
</li>
<li><p>Option 3 [24] [31][34]</p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce text summaries from images</li>
<li>Embed and retrieve image summaries with a reference to the raw image </li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis<br> 【实际模型输入使用的是图片】<br>   【图片概要依然是用于检索（GPT-4V，LLaVA，FUYU-8b）】[12]<br>  {选项3：前置阶段同选项2相同。对话时，根据query召回原始文本&#x2F;表格&#x2F;图片。构造完整Prompt，访问多模态大模型生成应答结果。}[10]</li>
</ul>
</li>
</ul>
<h3><span id="private-multi-modal-text-tables-images-rag-22">private multi-modal (text + tables + images)  RAG [22]</span><a href="#private-multi-modal-text-tables-images-rag-22" class="header-anchor">#</a></h3><h3><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h3><ul>
<li>pdf解析<br>unstructured</li>
<li>store<br>MultiVectorRetriever - 元数据+数据</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://www.zhihu.com/question/628651389/answer/3321989558">检索增强生成（RAG）有什么好的优化方案？</a> </p>
</li>
<li><p><a href="https://blog.langchain.dev/semi-structured-multi-modal-rag/">Multi-Vector Retriever for RAG on tables, text, and images</a> ***<br><a href="https://blog.csdn.net/lichunericli/article/details/135724777">基于多向量检索器的多模态RAG实现：用于表格、文本和图像</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/665814914">langchain的multi model RAG-以多模态pdf文件为例子</a></p>
</li>
<li><p><a href="https://blog.langchain.dev/multi-modal-rag-template/">Multi-modal RAG on slide decks</a></p>
</li>
</ol>
<p>1xx. <a href="https://docs.google.com/presentation/d/19x0dvHGhbJOOUWqvPKrECPi1yI3makcoc-8tFLj9Sos/edit?ref=blog.langchain.dev&pli=1#slide=id.g2642e7050fc_0_370">Using Multi-Modal LLMs</a>  page21</p>
<h3><span id="notebook">notebook</span><a href="#notebook" class="header-anchor">#</a></h3><ol start="20">
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi_Structured_RAG</a>  notebook<br><a href="https://github.com/www6v/AIGC/blob/master/Advanced-RAG/01_semi_structured_data.ipynb">Advanced-RAG semi_structured_data</a>   notebook  {半结构化-解析pdf中的表格，  运行没问题，能问表格中的数据}</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb">Semi_structured_and_multi_modal_RAG</a> notebook </p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/blob/master/langchain-cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a>  notebook {多模态- 解析pdf中的图片  运行有问题}<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a> notebook</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb">Chroma multi-modal RAG</a> notebook</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb">Multi-modal RAG</a> notebook</p>
</li>
</ol>
<h3><span id="template">template</span><a href="#template" class="header-anchor">#</a></h3><ol start="30">
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-multi-modal-local">rag-multi-modal-local</a><br>OpenCLIP(image embedding)  + bakllava(answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-multi-modal-mv-local">rag-multi-modal-mv-local</a><br>bakllava(image summaries embedding) +  bakllava (answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-chroma-multi-modal">rag-chroma-multi-modal</a><br>OpenCLIP(image embedding) + GPT-4V (answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-gemini-multi-modal">rag-gemini-multi-modal</a><br>OpenCLIP(image embedding) + Gemini(answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-chroma-multi-modal-multi-vector">rag-chroma-multi-modal-multi-vector</a><br>GPT-4V(image summaries embedding) + GPT-4V (answer synthesis)</li>
</ol>
<h3><span id="llamaindex">llamaindex</span><a href="#llamaindex" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s/93CdvD8FLZjaA7E724bf7g">朴素多模态RAG如何实现？兼看RAG上下文过滤方案FILCO及202402大模型早报 </a><br>1xx. <a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval/">Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index&#x2F;Retriever</a><br>1xx. <a href="https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206">Multimodal RAG pipeline with LlamaIndex and Neo4j</a><br>1xx. <a href="https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb">neo4j_llama_multimodal.ipynb</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Instruct Tuning</title>
    <url>/www6vHomeAIGC/2023/03/12/gptInstructTuningSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://arxiv.org/abs/2308.10792">大语言模型指令微调综述</a><br>    <a href="https://zhuanlan.zhihu.com/p/654054370">一篇关于LLM指令微调的综述</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/657138921">[论文]大语言模型指令调优综述</a><br>1xx. <a href="https://blog.csdn.net/qq_41185868/article/details/132613338">Paper：《Instruction Tuning for Large Language Models: A Survey—大型语言模型的指令调优的综述》翻译与解读</a><br>1xx. <a href="https://github.com/xiaoya-li/Instruction-Tuning-Survey">Instruction Tuning for Large Language Models: A Survey</a> git</p>
<p>【前面大部分是Instruct-Tuning， 中间一部分是Multi-modality Instruction Tuning】</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Instruct-Tuning</category>
      </categories>
      <tags>
        <tag>Instruct-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(List)Agent 开源 产品 平台</title>
    <url>/www6vHomeAIGC/2023/03/05/gptAgentList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#agentgeneral">Agent（General）</a></li>
<li><a href="#agenttoolassistant">Agent(tool&#x2F;assistant)</a></li>
<li><a href="#agentsimulation">Agent(simulation)</a></li>
<li><a href="#agent">Agent</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#%E5%88%86%E7%B1%BB-101112">分类 [10][11][12]</a></li>
<li><a href="#hugginggpt">HuggingGPT</a></li>
<li><a href="#babyagi-aigc">BabyAGI [AIGC]</a></li>
<li><a href="#autogpt10">AutoGPT[10]</a></li>
</ul>
</li>
<li><a href="#platform20">Platform[20]</a><ul>
<li><a href="#%E5%AD%97%E8%8A%82-coze2122">字节 Coze[21,22]</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6-appbuilder">百度 AppBuilder</a></li>
<li><a href="#dify">Dify</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#platform">Platform</a></li>
<li><a href="#%E4%BA%A7%E5%93%81-%E8%B0%83%E7%A0%94">产品 &amp;调研</a></li>
<li><a href="#modelscope-agent-agentfabric">modelscope-agent AgentFabric</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="agentgeneral">Agent（General）</span><a href="#agentgeneral" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent（General）</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><strong>AutoGen</strong> <a href="https://www.bilibili.com/video/BV1DH4y1Z7Ep">video</a> paper ***</td>
<td>Multi Agent</td>
<td>customizable, <strong>conversable</strong>,  <strong>seamlessly allow human participation</strong> [微软]</td>
<td><a href="https://github.com/microsoft/autogen">code</a><img src="https://img.shields.io/github/stars/microsoft/autogen.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>MetaGPT</strong> paper***</td>
<td>Multi Agent-role base</td>
<td>覆盖软件公司全生命流程</td>
<td><a href="https://github.com/geekan/MetaGPT">code</a><img src="https://img.shields.io/github/stars/geekan/MetaGPT.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>CrewAI</strong><a href="https://www.bilibili.com/video/BV12C4y1Y7xm">video</a></td>
<td>Multi Agent</td>
<td>流程定义更灵活</td>
<td><a href="https://github.com/joaomdmoura/CrewAI">code</a><img src="https://img.shields.io/github/stars/joaomdmoura/CrewAI.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>BabyAGI</strong></td>
<td><strong>plan and execute</strong></td>
<td></td>
<td><a href="https://github.com/yoheinakajima/babyagi">code</a><img src="https://img.shields.io/github/stars/yoheinakajima/babyagi.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>AutoGPT</strong></td>
<td>General</td>
<td></td>
<td><a href="https://github.com/Torantulino/Auto-GPT">code</a><img src="https://img.shields.io/github/stars/Torantulino/Auto-GPT.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>LangGraph</strong> <a href="https://www.bilibili.com/video/BV1VN4y1n7bt/">video</a> ***</td>
<td>flow engineering</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>XAgent</strong> <a href="https://www.bilibili.com/video/BV1D34y1M74F">video</a></td>
<td><strong>双循环，人可参与</strong></td>
<td>autogpt，babyagi - 没法收敛，有时候会不可控<br>metagpt，chatdev sop优化- 有一定的局限性，通用性不够好[面壁智能]</td>
<td><a href="https://github.com/OpenBMB/XAgent">code</a><img src="https://img.shields.io/github/stars/OpenBMB/XAgent.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>Agents <a href="https://www.bilibili.com/video/BV1C8411k7UL">video</a> paper</td>
<td>single agent|multi agent</td>
<td>基于SOP</td>
<td><a href="https://github.com/aiwaves-cn/agents">code</a></td>
</tr>
<tr>
<td>phidata</td>
<td></td>
<td>Memory, knowledge and tools for LLMs</td>
<td><a href="https://github.com/phidatahq/phidata">code</a></td>
</tr>
<tr>
<td>MiniAGI <a href="https://www.bilibili.com/video/BV1Hh4y1k7Jz">video</a></td>
<td></td>
<td>simple general-purpose autonomous agent based on the OpenAI API</td>
<td><a href="https://github.com/muellerberndt/mini-agi">code</a></td>
</tr>
<tr>
<td>AL Legion</td>
<td></td>
<td>An LLM-powered autonomous agent platform</td>
<td><a href="https://github.com/eumemic/ai-legion">code</a></td>
</tr>
</tbody></table>
<h1><span id="agenttoolx2fassistant">Agent(tool&#x2F;assistant)</span><a href="#agenttoolx2fassistant" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent(tool&#x2F;assistant)</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ResearchGPT</strong> ***</td>
<td>plan and execute</td>
<td>融合论文拆解+网络爬虫</td>
<td><a href="https://github.com/assafelovic/gpt-researcher">code</a><img src="https://img.shields.io/github/stars/assafelovic/gpt-researcher.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>WorkGPT</td>
<td>tools</td>
<td>A GPT agent framework for invoking APIs</td>
<td><a href="https://github.com/team-openpm/workgpt">code</a></td>
</tr>
<tr>
<td><strong>AgentTuning</strong><a href="https://www.bilibili.com/video/BV1E84y197Cj/">video</a>  paper</td>
<td>指令微调Agent</td>
<td>[清华]</td>
<td><a href="https://github.com/THUDM/AgentTuning">code</a></td>
</tr>
<tr>
<td><strong>ModelScope-Agent</strong> <a href="https://www.bilibili.com/video/BV1u34y137if">video</a></td>
<td>tools, MSAgent-Bench 训练agent</td>
<td>[阿里魔塔]</td>
<td><a href="https://github.com/modelscope/modelscope-agent">code</a></td>
</tr>
<tr>
<td>open-interpreter</td>
<td>os agent</td>
<td>A natural language interface for computers</td>
<td><a href="https://github.com/KillianLucas/open-interpreter">code</a></td>
</tr>
<tr>
<td>Qwen-Agent <a href="https://www.bilibili.com/video/BV1c34y1P7Yg">video</a></td>
<td></td>
<td>Agent framework and applications built upon Qwen, featuring Function Calling, Code Interpreter, RAG, and Chrome extension</td>
<td><a href="https://github.com/QwenLM/Qwen-Agent">code</a></td>
</tr>
<tr>
<td>AutoGPT-Plugins</td>
<td></td>
<td>Auo-GPT插件</td>
<td><a href="https://github.com/Significant-Gravitas/Auto-GPT-Plugins">code</a></td>
</tr>
<tr>
<td>GPTEngineer</td>
<td>code  assistant</td>
<td><strong>自动</strong>工具构建和代码生成</td>
<td><a href="https://github.com/AntonOsika/gpt-engineer">code</a><img src="https://img.shields.io/github/stars/AntonOsika/gpt-engineer.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>Aider</td>
<td>code  assistant</td>
<td><strong>交互式</strong></td>
<td><a href="https://github.com/paul-gauthier/aider">code</a></td>
</tr>
</tbody></table>
<h1><span id="agentsimulation">Agent(simulation)</span><a href="#agentsimulation" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent(simulation)</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>Hyperwriteai <a href="https://www.bilibili.com/video/BV1BZ421B7ar/">video</a><a href="https://www.hyperwriteai.com/personal-assistant">website</a>***</td>
<td>web agent, os agent</td>
<td></td>
<td><a href="https://github.com/OthersideAI/self-operating-computer">code</a></td>
</tr>
<tr>
<td><strong>MultiON</strong> <a href="https://www.bilibili.com/video/BV1mt421W7sw/">video</a>***</td>
<td>web agent</td>
<td></td>
<td></td>
</tr>
<tr>
<td>webarena paper</td>
<td>web agent</td>
<td>WebArena: A Realistic Web Environment for Building Autonomous Agents  网络拟真环境，可用于自主智能体的测试，支持在线购物，论坛，代码仓库etc</td>
<td><a href="https://github.com/web-arena-x/webarena">code</a></td>
</tr>
<tr>
<td>MiniWoB++：</td>
<td>web agent</td>
<td>a web interaction benchmark for reinforcement learning</td>
<td><a href="https://github.com/Farama-Foundation/miniwob-plusplus">code</a></td>
</tr>
<tr>
<td><strong>OpenAgents</strong> <a href="https://www.bilibili.com/video/BV1wM41197N7/">video</a> paper***</td>
<td>web agent</td>
<td>[港大]</td>
<td><a href="https://github.com/xlang-ai/OpenAgents">code</a></td>
</tr>
<tr>
<td>Mobile-Agent <a href="https://www.bilibili.com/video/BV1Xv42117hh/">video</a></td>
<td>app agent</td>
<td>[阿里]</td>
<td></td>
</tr>
<tr>
<td><strong>AppAgent</strong> <a href="https://www.bilibili.com/video/BV1De411S7ka">video</a> [paper]</td>
<td>app agent</td>
<td>[腾讯]</td>
<td></td>
</tr>
<tr>
<td>CAMEL</td>
<td></td>
<td>CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society</td>
<td><a href="https://github.com/camel-ai/camel">code</a></td>
</tr>
<tr>
<td><strong>Generative Agents</strong></td>
<td></td>
<td>斯坦福AI小镇</td>
<td><a href="https://github.com/joonspk-research/generative_agents">code</a><img src="https://img.shields.io/github/stars/joonspk-research/generative_agents.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>AgentVerse</td>
<td></td>
<td>多模型交互环境</td>
<td><a href="https://github.com/OpenBMB/AgentVerse">code</a><img src="https://img.shields.io/github/stars/OpenBMB/AgentVerse.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>GPTeam</td>
<td>Multi Agent</td>
<td>多智能体交互</td>
<td><a href="https://github.com/101dotxyz/GPTeam">code</a></td>
</tr>
<tr>
<td><strong>ChatDev</strong><a href="https://www.bilibili.com/video/BV1334y1T7K5/">video</a></td>
<td>Multi Agent</td>
<td>虚拟软件公司[面壁智能]</td>
<td><a href="https://github.com/OpenBMB/ChatDev">code</a><img src="https://img.shields.io/github/stars/OpenBMB/ChatDev.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>GPT PILOT</td>
<td></td>
<td><strong>交互式</strong></td>
<td></td>
</tr>
<tr>
<td>DevOpsGPT</td>
<td></td>
<td>自动</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>TaskingAI <a href="https://www.bilibili.com/video/BV1gp4y1m75S/">video</a></td>
<td>LLMOps</td>
<td></td>
<td><a href="https://github.com/TaskingAI/TaskingAI">code</a></td>
</tr>
<tr>
<td>DIFY <a href="https://www.bilibili.com/video/BV14V411Q7wP/">video</a></td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>AutoGen Studio</strong> <a href="https://www.bilibili.com/video/BV1fi4y1i7g7/">video</a> ***</td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>L3AGI <a href="https://www.bilibili.com/video/BV1s94y1K7fP">video</a></td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>agenta</td>
<td>LLMOps</td>
<td>The all-in-one LLM developer platform: prompt management, evaluation, human feedback, and deployment all in one place.</td>
<td><a href="https://github.com/Agenta-AI/agenta">code</a></td>
</tr>
<tr>
<td>SuperAGI</td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>N8N <a href="https://www.bilibili.com/video/BV1vT4y1h7UM/">video</a></td>
<td>work flow</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TaskWeaver <a href="https://www.bilibili.com/video/BV16C4y1c7rd">video</a></td>
<td></td>
<td>以代码为中心[微软]</td>
<td></td>
</tr>
<tr>
<td>ProAgent <a href="https://www.bilibili.com/video/BV1eu4y1b7DN">video</a></td>
<td>work flow</td>
<td>[清华]</td>
<td></td>
</tr>
<tr>
<td>Prompt flow <a href="https://www.bilibili.com/video/BV1aG411m7A4/">video</a></td>
<td></td>
<td>[微软]</td>
<td></td>
</tr>
<tr>
<td>AgentGPT <a href="https://www.bilibili.com/video/BV1V94y1s7uT">video</a></td>
<td>agent store, agent template</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bisheng *</td>
<td></td>
<td>dify+flowise的结合体</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h1><h3><span id="分类-101112">分类 [10][11][12]</span><a href="#分类-101112" class="header-anchor">#</a></h3><ul>
<li><p>Action agents  </p>
<ul>
<li>Function Call</li>
<li>ReACT</li>
</ul>
</li>
<li><p>Simulation agents<br>  生成式智能体， CAMEL，  Generative Agents</p>
</li>
<li><p>Automomous Agent<br>  <strong>AutoGPT</strong>， <strong>BabyAGI</strong>,  <strong>AutoGen</strong><br>  <strong>MetaGPT</strong>     ChatDev</p>
</li>
<li><p>跨模态Agents<br>  HuggingGPT</p>
</li>
</ul>
<h3><span id="hugginggpt">HuggingGPT</span><a href="#hugginggpt" class="header-anchor">#</a></h3><h3><span id="babyagi-aigc">BabyAGI  [AIGC]</span><a href="#babyagi-aigc" class="header-anchor">#</a></h3><p>Plan-and-execute agents<br>The <strong>planning</strong> is almost always done <strong>by an LLM</strong>.<br>The <strong>execution</strong> is usually done by a <strong>separate agent (equipped with tools)</strong>.</p>
<h3><span id="autogpt10">AutoGPT[10]</span><a href="#autogpt10" class="header-anchor">#</a></h3><p>AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下</p>
<ol>
<li>AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals </li>
<li>Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如<code>browser_website(url = &quot;www.baidu.com&quot;)</code> )</li>
<li>这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，</li>
<li>回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）</li>
</ol>
<h1><span id="platform20">Platform[20]</span><a href="#platform20" class="header-anchor">#</a></h1><h3><span id="字节-coze2122">字节 Coze[21,22]</span><a href="#字节-coze2122" class="header-anchor">#</a></h3><p>优势:  有RAG，结构化数据<br>劣势:  只能发布到飞书，微信</p>
<h3><span id="百度-appbuilder">百度 AppBuilder</span><a href="#百度-appbuilder" class="header-anchor">#</a></h3><h3><span id="dify">Dify</span><a href="#dify" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/awesome-ai-agents">awesome-ai-agents</a> git list</li>
<li><a href="https://github.com/www6v/DecryptPrompt">DecryptPrompt</a>  ***  git list</li>
<li><a href="https://space.bilibili.com/471000665/video?tid=0&pn=1&keyword=&order=pubdate">AIGCLINK</a> V</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/664281311">「Agent」通俗易懂地聊聊AI Agent（附66个开源+44个闭源Agent项目）</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/IubrsvB0ypn8KC-_SMPvLQ">主流Agent框架及金融Agent-FinRobot：兼看面向实体增强的细粒度实体描述知识库项目 </a><br>   Agent-FinRobot  基于autogen 实现</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/642357544">2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用</a> ***  论文+代码</li>
<li>公开课</li>
<li>公开课</li>
</ol>
<h3><span id="platform">Platform</span><a href="#platform" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1Bm411B79m/">AgentBuilder 中小企业如何选择：coze、dify、appbuilder、毕晟</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1DA4m1w72p/">COZE：中小企业均可0门槛创建业务agent</a> V</li>
<li><a href="https://mp.weixin.qq.com/s/WDkYZF9-JRhzM467Uf8lUA">利用Coze 实现吴恩达的4种 AI Agent 设计模式</a></li>
</ol>
<h3><span id="产品-amp调研">产品 &amp;调研</span><a href="#产品-amp调研" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s/RrymA1NwHzz9Q34wITZo6w">2024 年最完整的 AI Agents 清单来了，涉及 13 个领域，上百个 Agents！ </a>  开源 闭源 ***<br>1xx. <a href="https://mp.weixin.qq.com/s/v_BvcMqd-FpwRbOD09MR0A">全球AI Agent大盘点，大语言模型创业一定要参考的60个AI智能体</a><br>1xx. <a href="https://mp.weixin.qq.com/s/KbDOBkmYsTiwkjg2-YoRrQ">大模型时代的APP–2024年 AI Agent行业报告</a> ***</p>
<h3><span id="modelscope-agent-agentfabric">modelscope-agent  AgentFabric</span><a href="#modelscope-agent-agentfabric" class="header-anchor">#</a></h3><p>1xx. <a href="https://cloud.tencent.com/developer/article/2421616">LLM 大模型学习必知必会系列(十)：基于AgentFabric实现交互式智能体应用,Agent实战-腾讯云开发者社区-腾讯云</a></p>
<p>1xx. <a href="https://github.com/modelscope/modelscope-agent/tree/master/apps/agentfabric">modelscope-agent&#x2F;apps&#x2F;agentfabric at master · modelscope&#x2F;modelscope-agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/669397935">Modelscope Agent实操（一）：0代码创建、发布并分享一个专属Agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/672235050">从agentfabric开始体验魔搭Agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/679918404">社区供稿 | GLM-4适配ModelScope-Agent最佳实践</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Web Agent</title>
    <url>/www6vHomeAIGC/2023/03/05/gptAgentWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="web-scenarios-1">web scenarios [1]</span><a href="#web-scenarios-1" class="header-anchor">#</a></h1><p>在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。</p>
<p>通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。</p>
<p>为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。</p>
<p>Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs. </p>
<p>Agents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping [392] and search engine retrieval [90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management [391], agents often face challenges in performance. </p>
<p>In order to enable successful interactions between agents and more realistic web pages, some researchers [393; 394] have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web [389] combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [388] in real-world scenarios and extract valuable information. Furthermore, WebGum [390] empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive understanding of web pages.</p>
<h1><span id="papers-2">papers [2]</span><a href="#papers-2" class="header-anchor">#</a></h1><p><strong>In web scenarios</strong></p>
<ul>
<li>[2023&#x2F;10] <strong>OpenAgents: An Open Platform for Language Agents in the Wild.</strong> <em>XLang Lab (The University of Hong Kong) arXiv.</em> [<a href="https://arxiv.org/abs/2310.10634">paper</a>] [<a href="https://docs.xlang.ai/">project page</a>] [<a href="https://github.com/xlang-ai/OpenAgents">code</a>] [<a href="https://chat.xlang.ai/">demo</a>]  ***</li>
<li>[2023&#x2F;07] <strong>WebArena: A Realistic Web Environment for Building Autonomous Agents.</strong> <em>Shuyan Zhou (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.13854">paper</a>] [<a href="https://webarena.dev/">code</a>] *</li>
<li>[2023&#x2F;07] <strong>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.</strong> <em>Izzeddin Gur (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.12856">paper</a>]</li>
<li>[2023&#x2F;06] <strong>SYNAPSE: Leveraging Few-Shot Exemplars for<br>Human-Level Computer Control.</strong> <em>Longtao Zheng (Nanyang Technological University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.07863">paper</a>] [<a href="https://github.com/ltzheng/synapse">code</a>] *</li>
<li>[2023&#x2F;06] <strong>Mind2Web: Towards a Generalist Agent for the Web.</strong> <em>Xiang Deng (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.06070">paper</a>] [<a href="https://osu-nlp-group.github.io/Mind2Web/">code</a>] ***</li>
<li>[2023&#x2F;05] <strong>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</strong> <em>Hiroki Furuta (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11854">paper</a>]</li>
<li>[2023&#x2F;03] <strong>Language Models can Solve Computer Tasks.</strong> <em>Geunwoo Kim (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17491">paper</a>] [<a href="https://github.com/posgnu/rci-agent">code</a>]</li>
<li>[2022&#x2F;07] <strong>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.01206">paper</a>] [<a href="https://webshop-pnlp.github.io/">code</a>] *</li>
<li>[2021&#x2F;12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano (OpenAI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332">paper</a>]</li>
<li>[2023&#x2F;05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> <em>Wangchunshu Zhou (AIWaves) et al. arXiv.</em> [<a href="https://arxiv.org/pdf/2309.07870.pdf">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]  ***</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《The Rise and Potential of Large Language Model Based Agents: A Survey》、</li>
<li><a href="https://github.com/woooodyy/llm-agent-paper-list">The Rise and Potential of Large Language Model Based Agents: A Survey</a> git<br>1xx. <a href="https://sites.google.com/view/mm-webnav/">Multimodal Web Navigation with Instruction-Finetuned Foundation Models</a><br>1xx. <a href="https://hub.baai.ac.cn/view/28104">Google DeepMind｜具备规划长程上下文理解和程序合成能力的真实世界WebAgent</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/662146234">LLMs-Agent 论文: WebAgent, 2023, Izzeddin Gur et al., Google DeepMind.</a><br>1xx. <a href="https://github.com/www6v/OpenAgents">OpenAgents</a></li>
</ol>
<h3><span id="web-agent">web  agent</span><a href="#web-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper-Agent</title>
    <url>/www6vHomeAIGC/2023/03/04/gptAgentPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>1xx. <a href="https://github.com/zjunlp/LLMAgentPapers">LLM Agents Papers</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662506575">28 篇最新论文解读 LLMs-based Agents</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/661741663">ICLR’24 Agent论文合集：RL-based、LLM-based 前沿研究汇总</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/665282216">ICLR’24 大语言模型智能体最新研究进展丨智能体应用篇</a><br>1xx. <a href="https://mp.weixin.qq.com/s/GGRWQJ-eBvHerB9H9JPCjg">ICLR’24 大语言模型智能体最新研究进展丨智能体能力篇 </a><br>   <a href="https://mp.weixin.qq.com/s/eYnZY1GFWMKdU_Z57iSEJg">ICLR’24 大语言模型智能体最新研究进展 </a><br>1xx. <a href="https://github.com/www6v/LLM-Agents-Papers"> LLM-Agents-Papers</a></p>
<p>1xx. <a href="https://www.bilibili.com/read/cv27126779/">AGI新突破！52篇论文尽览大模型Agent最新研究进展！</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)多模态预训练 概述</title>
    <url>/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83">多模态预训练</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84transformer">架构Transformer</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">模型 - 自监督学习</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1">下游任务</a><ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83">多模态下游任务-模型微调</a></li>
</ul>
</li>
<li><a href="#%E6%9B%B4%E5%A4%A7%E6%9B%B4%E5%BC%BA%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">更大更强的多模态预训练模型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/overview.png" class>


<h1><span id="多模态预训练">多模态预训练</span><a href="#多模态预训练" class="header-anchor">#</a></h1><h2><span id="数据集">数据集</span><a href="#数据集" class="header-anchor">#</a></h2><ul>
<li>大规模无标注</li>
<li>内容杂  噪音多</li>
</ul>
<h2><span id="架构transformer">架构Transformer</span><a href="#架构transformer" class="header-anchor">#</a></h2><ul>
<li><p>基于transformer encoder-理解任务<br>单流 - vl-bert  UNITER<br>双流 - ViLBERT， CLIP（双流结构，对比学习）</p>
</li>
<li><p>基于transformer decoder-生成任务<br>DALL-E  （VQVAE+GPT,  Text-to-Image Generation）<br>现在都用 → SD 扩散模型</p>
</li>
<li><p>基于encoder+decoder-理解+生成<br>文本的decoder</p>
</li>
</ul>
<ol>
<li>encoder + decoder 串行,  交叉注意力</li>
<li>encoder + decoder 并行</li>
</ol>
<h2><span id="模型-自监督学习">模型 - 自监督学习</span><a href="#模型-自监督学习" class="header-anchor">#</a></h2><ul>
<li><p>模态内掩码学习<br>文本 语音 视觉自身token级别mask</p>
</li>
<li><p>模态间掩码学习<br>不同模态信息的相互预测<br>mask视觉， 输出对应文本</p>
</li>
<li><p>模态间匹配学习<br>匹配与否的分类问题 - 正负样本(二分类)<br>对比学习 - 模态间的图文匹配对</p>
</li>
</ul>
<h1><span id="下游任务">下游任务</span><a href="#下游任务" class="header-anchor">#</a></h1><h2><span id="多模态下游任务-模型微调">多模态下游任务-模型微调</span><a href="#多模态下游任务-模型微调" class="header-anchor">#</a></h2><ul>
<li><p>模型微调</p>
<ul>
<li>p+ finetune（全参数）</li>
<li>p+ prompt-tuning</li>
<li>p+ adaptor-tuning</li>
<li>p+ lora</li>
</ul>
</li>
<li><p>多模态下游任务</p>
<ul>
<li>理解： text&#x2F;audio&#x2F;visual 内容生成</li>
<li>生成： 跨模态 检索&#x2F;问答&#x2F;推理</li>
</ul>
</li>
</ul>
<h1><span id="更大更强的多模态预训练模型">更大更强的多模态预训练模型</span><a href="#更大更强的多模态预训练模型" class="header-anchor">#</a></h1><ul>
<li><strong>强大的语言模型</strong></li>
<li>更大的视觉模型</li>
<li>更大规模的预训练数据</li>
<li>更多模态形式的数据</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV13P411q7tH/">中科院刘静：多模态预训练的进展回顾与展望</a>  V</li>
</ol>
<p>1xx. <a href="https://github.com/Coobiw/MiniGPT4Qwen">MiniGPT4Qwen</a> git<br><a href="https://zhuanlan.zhihu.com/p/664612306">多模态大模型实战-MiniGPT4Qwen系列1：3090+2小时+通义千问&#x3D;个人版双语多模态大模型</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Self-Reflective RAG</title>
    <url>/www6vHomeAIGC/2023/03/02/gptRAGSelfReflective/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#cognitive-architecture-2">Cognitive Architecture [2]</a></li>
<li><a href="#crag">CRAG</a></li>
<li><a href="#self-rag-3">Self-RAG [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#crag-1">CRAG</a></li>
<li><a href="#self-rag">SELF-RAG</a></li>
<li><a href="#self-rag">Self-RAG</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="cognitive-architecture-2">Cognitive Architecture [2]</span><a href="#cognitive-architecture-2" class="header-anchor">#</a></h1><ul>
<li>Cognitive architectures for RAG [1]</li>
</ul>
<h1><span id="crag">CRAG</span><a href="#crag" class="header-anchor">#</a></h1><h1><span id="self-rag-3">Self-RAG [3]</span><a href="#self-rag-3" class="header-anchor">#</a></h1><p>Self-RAG 则是更加主动和智能的实现方式，主要步骤概括如下：</p>
<ol>
<li>判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回</li>
<li>平行处理每个片段：生产prompt+一个片段的生成结果</li>
<li>使用**反思字段(Reflection tokens)**，检查输出是否相关，选择最符合需要的片段；</li>
<li>再重复检索</li>
<li>生成结果会引用相关片段，以及输出结果是否符合该片段，便于查证事实。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/agentic-rag-with-langgraph/">Self-Reflective RAG with LangGraph</a></p>
</li>
<li><p><a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/">OpenAI’s Bet on a Cognitive Architecture</a></p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/2301_78285120/article/details/136103211">写的太通透了！大模型自省式 RAG 与 LangGraph 的实践！</a></p>
<h3><span id="crag">CRAG</span><a href="#crag" class="header-anchor">#</a></h3><p>1xx. <a href="https://arxiv.org/pdf/2401.15884.pdf">Corrective Retrieval Augmented Generation</a> Figure 2<br>1xx. <a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb">Corrective RAG (CRAG)</a> git</p>
<p>1xx. 【社区第十三讲】 老刘说NLP线上交流</p>
<h3><span id="self-rag">SELF-RAG</span><a href="#self-rag" class="header-anchor">#</a></h3><ol start="3">
<li><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> ***</li>
</ol>
<p>1xx. <a href="https://arxiv.org/pdf/2310.11511.pdf">SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND<br>CRITIQUE THROUGH SELF-REFLECTION</a> Figure 1<br>1xx. <a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb">Self-RAG</a> git</p>
<h3><span id="self-rag">Self-RAG</span><a href="#self-rag" class="header-anchor">#</a></h3><p>1xx. <a href="https://github.com/www6v/self-rag">original implementation of SELF-RAG</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Plan&amp;Execute,ReWOO</title>
    <url>/www6vHomeAIGC/2023/03/02/gptAgentPlanAndExecute/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="plan-and-execute-0">Plan-and-execute [0]</span><a href="#plan-and-execute-0" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Figure 2 - 基于prompt [1]</li>
</ul>
</li>
<li><p>代码</p>
<ul>
<li>plan [2]<ul>
<li>Planning Step</li>
<li>Re-Plan Step</li>
</ul>
</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li>冗余的提示和重复的执行 -&gt; ReWOO</li>
</ul>
</li>
</ul>
<h1><span id="rewoo-0">ReWOO [0]</span><a href="#rewoo-0" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Abstract [10]<br>增强语言模型（ALM）将大型语言模型（LLM）的推理能力与允许知识检索和执行操作的工具相结合。现有的ALM系统以交错的方式触发LLM的思考过程，同时从这些工具中获取观察结果。<strong>具体而言，LLM推理过程中会调用外部工具，然后在获取工具响应后停止，基于之前的响应令牌来决定下一步的操作。这种范式虽然直观且易于实现，但常常由于冗余的提示和重复的执行而导致计算复杂度极高</strong>。本研究首次解决了这些挑战，提出了一种模块化的范式ReWOO（无观察推理），<strong>将推理过程与外部观察结果分离，从而显著减少了令牌的消耗</strong>。</li>
<li>Figure 1 [10]<br>Planner里有格式化的#E</li>
<li>Figure 2  [10]</li>
</ul>
</li>
<li><p>代码 [11]</p>
<ul>
<li>Executor-tool_execution() -&gt; 状态机</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li>是否可以并行？-&gt; LLMCompiler</li>
</ul>
</li>
</ul>
<h1><span id="llmcompiler">LLMCompiler</span><a href="#llmcompiler" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Abstract [20]<br>LLM的多函数调用能力催生了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多函数调用方法通常需要<strong>针对每个函数进行顺序推理和执行，这可能导致较高的延迟、成本以及不准确的行为</strong>。为了解决这个问题，我们引入了LLMCompiler，它可以<strong>并行执行函数，以高效地编排多函数调用</strong>。LLMCompiler<strong>借鉴了经典编译器的原理</strong>，在LLM中使用<strong>三个组件</strong>来简化并行函数调用：（i）LLM规划器，制定执行策略和依赖关系；（ii）任务获取单元，分派和更新函数调用任务；（iii）执行器，以并行方式执行这些任务。通过LLMCompiler，用户可以指定工具以及可选的上下文示例，LLMCompiler会自动计算函数调用的优化编排。重要的是，LLMCompiler可以与LLaMA-2等开源模型以及OpenAI的GPT模型一起使用。</li>
<li>Figure 2  [20]</li>
</ul>
</li>
<li><p>代码 [21]</p>
<ul>
<li>Planner</li>
<li>Task Fetching Unit </li>
<li>Joiner</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="plan-and-execute">Plan-and-execute</span><a href="#plan-and-execute" class="header-anchor">#</a></h3><ol start="0">
<li><a href="https://www.bilibili.com/video/BV1vJ4m1s7Zn/">LangGraph：Plan-Execute Agents 实战</a> V<br><a href="https://blog.langchain.dev/planning-agents/">Plan-and-Execute Agents</a> ***</li>
<li><a href="https://arxiv.org/pdf/2305.04091.pdf">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought<br>Reasoning by Large Language Models</a>  Figure 2</li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb">plan-and-execute</a>    git</li>
</ol>
<h3><span id="rewoo">ReWOO</span><a href="#rewoo" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://arxiv.org/pdf/2305.18323.pdf">ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</a></li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb">Reasoning without Observation</a> git<br><a href="https://www.bilibili.com/video/BV1Au4m1N7ix/">ReWoo Agent框架代码实现</a> V<br>1xx.  <a href="https://zhuanlan.zhihu.com/p/671491031">ReWOO: 高效增强语言模型中解偶观测和推理</a></li>
</ol>
<h3><span id="llmcompiler">LLMCompiler</span><a href="#llmcompiler" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://arxiv.org/pdf/2312.04511v1.pdf">An LLM Compiler for Parallel Function Calling</a></li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb">LLMCompiler</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT,ViLT</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalVit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="vilt">ViLT</span><a href="#vilt" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/369733979">ViLT：最简单的多模态Transformer</a><br>1xx. <a href="https://github.com/dandelin/vilt">ViLT</a> git<br>1xx. <a href="https://blog.csdn.net/qq_42030496/article/details/134641704">ViLT 论文精读【论文精读】</a><br>   <a href="https://www.bilibili.com/video/BV14r4y1j74y/">ViLT 论文精读【论文精读】</a> V<br>1xx. <a href="https://blog.csdn.net/m0_56722835/article/details/125071550">多模态ViLT模型下游任务微调原理及代码</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ViT</category>
      </categories>
      <tags>
        <tag>ViT</tag>
      </tags>
  </entry>
  <entry>
    <title>SAM</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://blog.csdn.net/weixin_44386956/article/details/130262260">【模型解读】【代码复现】Segment Anything Model(SAM)</a><br><a href="https://zhuanlan.zhihu.com/p/620355474">【论文解读】MetaAi SAM(Segment Anything) 分割一切</a><br><a href="https://zhuanlan.zhihu.com/p/630529550">Segment Anything(sam)项目整理汇总</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)CLIP</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#clip-%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%81%9A%E4%BA%86%E5%93%AA%E4%BA%9B%E4%BA%8B%E6%83%85elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</a></li>
<li><a href="#clip-zero-shot%E6%8E%A8%E7%90%861">CLIP Zero-shot推理[1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E5%B1%80%E9%99%90">局限</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="clip-在训练过程中做了哪些事情elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</span><a href="#clip-在训练过程中做了哪些事情elmo1" class="header-anchor">#</a></h1><p>在训练过程中，CLIP（Contrastive Language-Image Pre-training）模型主要进行了以下几个步骤：</p>
<ol>
<li><strong>数据预处理</strong> : CLIP 使用了一个大规模的数据集，包含 4 亿个 “图像 - 文本” 对，这些数据需要进行清洗和预处理，以便于模型学习。</li>
<li><strong>特征提取</strong> : 通过 Text Encoder 和 Image Encoder 分别对文本和图像进行特征提取。Text Encoder 通常是基于 Transformer 的模型，而 Image Encoder 可以是基于 CNN（卷积神经网络）或者 VIT（Vision Transformer）的模型。</li>
<li><strong>对比学习</strong> : CLIP 采用对比学习的策略，通过对比正确的图像 - 文本对与错误的图像 - 文本对，使得模型能够学习到正确对的特征表示与其他对的区分。具体来说，CLIP 通过 InfoNCE 损失函数来最大化正确对的相似度，同时最小化错误对的相似度。</li>
<li><strong>特征空间对齐</strong> : 通过对比学习，CLIP 将图像和文本的特征映射到一个共享的多模态特征空间中，使得图像特征和文本特征可以直接进行相似度比较。</li>
<li><strong>参数优化</strong> : 通过反向传播和梯度下降等方法，不断调整模型参数，以最小化损失函数，从而优化模型性能。</li>
<li><strong>Zero-shot 推理能力的培养</strong> : 在训练过程中，CLIP 学习了如何通过文本提示（prompts）来进行 zero-shot 的图像分类，即在没有直接观测到的类别标签下，通过文本描述来识别图像内容。</li>
<li><strong>模型评估与调整</strong> : 通过在验证集上的评估，调整模型结构和超参数，以提高模型的泛化能力和性能。</li>
</ol>
<p>通过这些训练步骤，CLIP 能够学习到一个强大的多模态特征表示，使其能够在多种视觉任务上进行 zero-shot 或 few-shot 的推理。</p>
<h1><span id="clip-zero-shot推理1">CLIP Zero-shot推理[1]</span><a href="#clip-zero-shot推理1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>首先，我们创建一个<strong>标签全集</strong>，如图中（2）所示，并得到每一个<strong>标签的特征向量</strong></li>
<li>然后，我们取一张图片，如图中（3）所示，过Image Encoder后得到该<strong>图片的特征向量</strong></li>
<li>最后，计算图片向量和文字向量间的<strong>相似度</strong>，取相似度最高的那条label即可。</li>
</ul>
<h3><span id="局限">局限</span><a href="#局限" class="header-anchor">#</a></h3><p>  当你喂给CLIP一张图时，不管这张图片它是否有见过，CLIP都不会生成一个全新的标签，而是去全集标签中找一个最相似的给你。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a> 代码</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/493489688">神器CLIP：连接文本和图像，打造可迁移的视觉模型</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/486857682">【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/646790176">CLIP 模型解读</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/521151393">详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50&#x2F;101</a></p>
<p>1xx. <a href="https://blog.csdn.net/lsb2002/article/details/132275132">openai多模态大模型：clip详解及实战</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/555314976">CLIP：多模态领域革命者</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Tokenizer</title>
    <url>/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="tokenizer-分词">tokenizer 分词</span><a href="#tokenizer-分词" class="header-anchor">#</a></h3><ul>
<li>单词分词法</li>
<li>单字分词法</li>
<li>子词分词法<br>BPE [GPT系列], WordPiece</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/630696264">大模型词表扩充必备工具SentencePiece</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/458452872">NLP（二）：浅谈分词</a><br>1xx. <a href="https://www.bilibili.com/video/BV1vN411p7t2/">https://www.bilibili.com/video/BV1vN411p7t2/</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400849&idx=1&sn=58006756cccde4d06d273df59e2c8dd8">开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tokenizer</category>
      </categories>
      <tags>
        <tag>Tokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey List</title>
    <url>/www6vHomeAIGC/2023/02/25/gptSurveyList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="awesome-llm-survey">Awesome-LLM-Survey</span><a href="#awesome-llm-survey" class="header-anchor">#</a></h1><ul>
<li><a href="#awesome-llm-survey">Awesome-LLM-Survey</a><ul>
<li><p><a href="#general-survey">General Survey</a> *** </p>
</li>
<li><p><a href="#training-of-llm">Training of LLM</a></p>
<ul>
<li><a href="#instruction-tuning">Instruction Tuning</a> *** </li>
<li><a href="#human-alignment-for-llm">Human Alignment for LLM</a> *</li>
</ul>
</li>
<li><p><a href="#prompt-of-llm">Prompt of LLM</a></p>
<ul>
<li><a href="#chain-of-thought-for-llm">Chain of Thought for LLM</a> *** </li>
<li><a href="#prompt-engineering-for-llm">Prompt Engineering for LLM</a> * </li>
<li><a href="#retrieval-augmented-llm">Retrieval-Augmented LLM</a> ***</li>
</ul>
</li>
<li><p><a href="#challenge-of-llm">Challenge of LLM</a></p>
<ul>
<li><p><a href="#hallucination-in-llm">Hallucination in LLM</a> ***</p>
</li>
<li><p><a href="#compression-for-llm">Compression for LLM</a> ***</p>
</li>
<li><p><a href="#evaluation-of-llm">Evaluation of LLM</a></p>
</li>
<li><p><a href="#reasoning-with-llm">Reasoning with LLM</a></p>
</li>
<li><p><a href="#long-context-for-llm">Long-Context for LLM</a></p>
</li>
<li><p><a href="#factuality-in-llm">Factuality in LLM</a></p>
</li>
<li><p><a href="#knowledge-for-llm">Knowledge for LLM</a></p>
</li>
<li><p><a href="#self-correction-for-llm">Self-Correction for LLM</a></p>
</li>
<li><p><a href="#tool-using-of-llm">Tool Using of LLM</a> ***</p>
</li>
<li><p><a href="#agent-of-llm">Agent of LLM</a> ***</p>
</li>
<li><p><a href="#efficiency-of-llm">Efficiency of LLM</a> *** </p>
</li>
<li><p><a href="#data-of-llm">Data of LLM</a> ***</p>
</li>
<li><p><a href="#continual-learning-of-llm">Continual Learning of LLM</a></p>
</li>
</ul>
</li>
<li><p><a href="#mulitmodal-of-llm">Mulitmodal of LLM</a></p>
<ul>
<li><a href="#visual-llm">Visual LLM</a></li>
</ul>
</li>
<li><p><a href="#llm-for-domain-application">LLM for Domain Application</a></p>
<ul>
<li><a href="#llm-for-health">LLM for Health</a></li>
<li><a href="#llm-for-finance">LLM for Finance</a> ***</li>
<li><a href="#llm-for-education">LLM for Education</a></li>
<li><a href="#llm-for-law">LLM for Law</a></li>
<li><a href="#llm-for-mental-health">LLM for Mental Health</a></li>
</ul>
</li>
<li><p><a href="#llm-for-downstream-tasks">LLM for Downstream Tasks</a></p>
<ul>
<li><p><a href="#llm-for-recommendation">LLM for Recommendation</a></p>
</li>
<li><p><a href="#llm-for-information-retrieval">LLM for Information Retrieval</a></p>
</li>
<li><p><a href="#llm-for-software-engineering">LLM for Software Engineering</a></p>
</li>
<li><p><a href="#llm-for-time-series">LLM for Time Series</a></p>
</li>
<li><p><a href="#detection-of-llms-generated-content">Detection of LLMs-Generated Content</a></p>
</li>
<li><p><a href="#llm-for-information-extraction">LLM for Information Extraction</a></p>
</li>
</ul>
</li>
<li><p><a href="#star-history">Star History</a></p>
</li>
</ul>
</li>
</ul>
<hr>
<h2><span id="general-survey">General Survey</span><a href="#general-survey" class="header-anchor">#</a></h2><ul>
<li><p>Challenges and Applications of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.10169">[paper]</a> *** </p>
</li>
<li><p>A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT, 2023.02 <a href="https://arxiv.org/abs/2302.09419">[paper]</a> ***</p>
</li>
<li><p>A Survey of Large Language Models, 2023.11 <a href="https://arxiv.org/abs/2303.18223">[paper]</a><a href="https://github.com/RUCAIBox/LLMSurvey">[project]</a>  ***</p>
</li>
<li><p>A Comprehensive Overview of Large Language Models *</p>
</li>
<li><p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</p>
</li>
<li><p>Pre-Trained Models: Past, Present and Future</p>
</li>
<li><p>A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4, 2023.10 <a href="https://arxiv.org/pdf/2310.12321.pdf">[paper]</a></p>
</li>
<li><p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, 2023.04  <a href="https://arxiv.org/abs/2304.13712">[paper]</a><a href="https://github.com/Mooler0410/LLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects, 2023.12 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.23589741.v4">[paper]</a> <a href="https://github.com/anas-zafar/LLM-Survey">[project]</a></p>
</li>
<li><p>The future of gpt: A taxonomy of existing chatgpt research, current challenges, and possible future directions, 2023.04 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4413921">[paper]</a></p>
</li>
<li><p>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges, 2023.10 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.24171183.v1">[paper]</a></p>
</li>
<li><p>Understanding LLMs: A Comprehensive Overview from Training to Inference, 2024.01 <a href="https://arxiv.org/pdf/2401.02038.pdf">[paper]</a></p>
</li>
</ul>
<hr>
<h2><span id="training-of-llm">Training of LLM</span><a href="#training-of-llm" class="header-anchor">#</a></h2><h3><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h3><ul>
<li>Are Prompts All the Story? No. A Comprehensive and Broader View of Instruction Learning, 2023.03 <a href="https://arxiv.org/pdf/2303.10475.pdf">[paper]</a> <a href="https://github.com/RenzeLou/awesome-instruction-learning">[project]</a></li>
<li>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a></li>
<li>Instruction Tuning for Large Language Models: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.10792">[paper]</a>  ***</li>
</ul>
<h3><span id="human-alignment-for-llm">Human Alignment for LLM</span><a href="#human-alignment-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>AI Alignment: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19852">[paper]</a><a href="https://www.alignmentsurvey.com/">[project]</a></p>
</li>
<li><p>Large Language Model Alignment: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.15025">[paper]</a></p>
</li>
<li><p>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Model, 2023.08 <a href="https://arxiv.org/abs/2308.12014">[paper]</a><a href="https://github.com/ValueCompass/Alignment-Goal-Survey">[project]</a></p>
</li>
<li><p>Aligning Large Language Models with Human: A Survey, 2023.07 <a href="https://arxiv.org/abs/2307.12966">[paper]</a><a href="https://github.com/GaryYufei/AlignLLMHumanSurvey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="prompt-of-llm">Prompt of LLM</span><a href="#prompt-of-llm" class="header-anchor">#</a></h2><h3><span id="chain-of-thought-for-llm">Chain of Thought for LLM</span><a href="#chain-of-thought-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
<li><p>A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future, 2023.09 <a href="https://arxiv.org/abs/2309.06256">[paper]</a><a href="https://github.com/zchuz/CoT-Reasoning-Survey">[project]</a></p>
</li>
<li><p>Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents, 2023.11 <a href="https://arxiv.org/pdf/2311.11797.pdf">[paper]</a> <a href="https://github.com/Zoeyyao27/CoT-Igniting-Agent">[project]</a></p>
</li>
</ul>
<h3><span id="prompt-engineering-for-llm">Prompt Engineering for LLM</span><a href="#prompt-engineering-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Prompting Frameworks for Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12785.pdf">[paper]</a><a href="https://github.com/lxx0628/Prompting-Framework-Survey">[project]</a></p>
</li>
<li><p>Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review, 2023.10 <a href="https://arxiv.org/pdf/2310.14735.pdf">[paper]</a></p>
</li>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="retrieval-augmented-llm">Retrieval-Augmented LLM</span><a href="#retrieval-augmented-llm" class="header-anchor">#</a></h3><ul>
<li><p>Retrieving Multimodal Information for Augmented Generation: A Survey  *** </p>
</li>
<li><p>Retrieval-Augmented Generation for AI-Generated Content: A Survey *** </p>
</li>
<li><p>A Survey on Retrieval-Augmented Text Generation, 2022.02 <a href="https://arxiv.org/abs/2202.01110">[paper]</a></p>
</li>
<li><p>Retrieval-Augmented Generation for Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.10997.pdf">[paper]</a> <a href="https://github.com/Tongji-KGLLM/RAG-Survey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="challenge-of-llm">Challenge of LLM</span><a href="#challenge-of-llm" class="header-anchor">#</a></h2><h3><span id="hallucination-in-llm">Hallucination in LLM</span><a href="#hallucination-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.07914">[paper]</a></p>
</li>
<li><p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, 2023.11 <a href="https://arxiv.org/pdf/2311.05232">[paper]</a><a href="https://github.com/LuckyyySTA/Awesome-LLM-hallucination">[project]</a> ***</p>
</li>
<li><p>A Survey of Hallucination in “Large” Foundation Models, 2023.09  <a href="https://arxiv.org/paper/2309.05922">[paper]</a><a href="https://github.com/vr25/hallucination-foundation-model-survey">[project]</a></p>
</li>
<li><p>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models, 2023.09 <a href="https://arxiv.org/abs/2309.01219">[paper]</a><a href="https://arxiv.org/abs/2309.01219">[project]</a></p>
</li>
<li><p>Cognitive Mirage: A Review of Hallucinations in Large Language Models, 2023.09 <a href="https://arxiv.org/paper/2309.06794.paper">[paper]</a><a href="https://github.com/hongbinye/Cognitive-Mirage-Hallucinations-in-LLMs">[project]</a></p>
</li>
<li><p>Augmenting LLMs with Knowledge: A survey on hallucination prevention, 2023.09 <a href="https://arxiv.org/pdf/2309.16459.pdf">[paper]</a></p>
</li>
<li><p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models, 2024.01 <a href="https://arxiv.org/pdf/2401.01313.pdf">[paper]</a></p>
</li>
<li><p>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment, 2023.08 <a href="https://arxiv.org/abs/2308.05374">[paper]</a></p>
</li>
</ul>
<h3><span id="compression-for-llm">Compression for LLM</span><a href="#compression-for-llm" class="header-anchor">#</a></h3><ul>
<li>A Survey on Model Compression for Large Language Models, 2023.08 <a href="https://arxiv.org/abs/2308.07633">[paper]</a>  ***</li>
<li>A Comprehensive Survey of Compression Algorithms for Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.15347.pdf">paper</a>]</li>
</ul>
<h3><span id="evaluation-of-llm">Evaluation of LLM</span><a href="#evaluation-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Evaluating Large Language Models: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19736.pdf">[paper]</a><a href="https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers">[project]</a> ***</p>
</li>
<li><p>A Survey on Evaluation of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.03109">[paper]</a><a href="https://llm-eval.github.io/">[project]</a> ***</p>
</li>
</ul>
<h3><span id="reasoning-with-llm">Reasoning with LLM</span><a href="#reasoning-with-llm" class="header-anchor">#</a></h3><ul>
<li><p>Reasoning with Language Model Prompting: A Survey, 2022.12 <a href="https://arxiv.org/abs/2212.09597">[paper]</a><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">[project]</a></p>
</li>
<li><p>A Survey of Reasoning with Foundation Models, 2023.12 [[papaer]] (<a href="https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)">https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)</a> ***</p>
</li>
</ul>
<h3><span id="long-context-for-llm">Long-Context for LLM</span><a href="#long-context-for-llm" class="header-anchor">#</a></h3><ul>
<li>Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12351">[paper]</a></li>
<li>Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding, 2023.12 <a href="https://arxiv.org/abs/2312.17044">[paper]</a></li>
</ul>
<h3><span id="factuality-in-llm">Factuality in LLM</span><a href="#factuality-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>A Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity, 2023.10 <a href="https://arxiv.org/abs/2310.07521">[paper]</a><a href="https://github.com/wangcunxiang/LLM-Factuality-Survey">[project]</a></p>
</li>
<li><p>Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models, 2023.10 <a href="https://arxiv.org/pdf/2310.16570.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="knowledge-for-llm">Knowledge for LLM</span><a href="#knowledge-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges, 2023.11 <a href="https://arxiv.org/pdf/2311.15766">[paper]</a></p>
</li>
<li><p>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications, 2023.11 <a href="https://arxiv.org/pdf/2311.05876.pdf">[paper]</a></p>
</li>
<li><p>Knowledge Editing for Large Language Models: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.16218.pdf">[paper]</a></p>
</li>
<li><p>Editing Large Language Models: Problems, Methods, and Opportunities, 2023.05 <a href="https://arxiv.org/abs/2305.13172">[paper]</a><a href="https://github.com/zjunlp/EasyEdit">[project]</a></p>
</li>
<li><p>Building trust in conversational ai: A comprehensive review and solution architecture for explainable, privacy-aware systems using llms and knowledge graph, 2023.08 <a href="https://arxiv.org/pdf/2308.13534.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="self-correction-for-llm">Self-Correction for LLM</span><a href="#self-correction-for-llm" class="header-anchor">#</a></h3><ul>
<li>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies, 2023.08 <a href="https://arxiv.org/abs/2308.03188">[paper]</a><a href="https://github.com/teacherpeterpan/self-correction-llm-papers">[project]</a></li>
</ul>
<h3><span id="tool-using-of-llm">Tool Using of LLM</span><a href="#tool-using-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Foundation Models for Decision Making: Problems, Methods, and Opportunities, 2023.03 <a href="https://arxiv.org/abs/2303.04129">[paper]</a></p>
</li>
<li><p>Augmented Language Models: a Survey, 2023.02 <a href="https://arxiv.org/abs/2302.07842">[paper]</a> ***</p>
</li>
<li><p>Tool Learning with Foundation Models</p>
</li>
</ul>
<h3><span id="agent-of-llm">Agent of LLM</span><a href="#agent-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Understanding the planning of LLM agents: A survey, 2024 </p>
</li>
<li><p>A Survey on Large Language Model based Autonomous Agents, 2023.08 <a href="https://arxiv.org/abs/2308.11432">[paper]</a><a href="https://github.com/Paitesanshi/LLM-Agent-Survey">[project]</a> ***</p>
</li>
<li><p>The Rise and Potential of Large Language Model Based Agents: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.07864">[paper]</a><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">[project]</a> ***</p>
</li>
<li><p>Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives, 2023.12 <a href="https://arxiv.org/pdf/2312.11970.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="efficiency-of-llm">Efficiency of LLM</span><a href="#efficiency-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning ***</p>
</li>
<li><p>The Power of Scale for Parameter-Efficient Prompt Tuning</p>
</li>
<li><p>Efficient Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03863">[paper]</a><a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">[project]</a> ***</p>
</li>
<li><p>The Efficiency Spectrum of Large Language Models: An Algorithmic Survey, 2023.12 <a href="https://arxiv.org/pdf/2310.10844.pdf">[paper]</a><a href="https://github.com/tding1/Efficient-LLM-Survey">[project]</a></p>
</li>
<li><p>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment, 2023.12 <a href="https://arxiv.org/pdf/2312.12148.pdf">[paper]</a></p>
</li>
<li><p>A Survey on Hardware Accelerators for Large Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.09890.pdf">paper</a>]</p>
</li>
</ul>
<h3><span id="data-of-llm">Data of LLM</span><a href="#data-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Management For Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.01700">[paper]</a><a href="https://github.com/ZigeW/data_management_LLM">[project]</a></p>
</li>
<li><p>Data-centric Artificial Intelligence: A Survey</p>
</li>
</ul>
<h3><span id="continual-learning-of-llm">Continual Learning of LLM</span><a href="#continual-learning-of-llm" class="header-anchor">#</a></h3><ul>
<li>Continual Learning with Pre-Trained Models: A Survey, 2024.01 <a href="https://arxiv.org/pdf/2401.16386">[paper]</a> <a href="https://github.com/sun-hailong/LAMDA-PILOT">[project]</a></li>
</ul>
<hr>
<h2><span id="mulitmodal-of-llm">Mulitmodal of LLM</span><a href="#mulitmodal-of-llm" class="header-anchor">#</a></h2><h3><span id="visual-llm">Visual LLM</span><a href="#visual-llm" class="header-anchor">#</a></h3><ul>
<li><p>An Empirical Study of Training End-to-End Vision-and-Language Transformers, 2022.03 *** microsoft +</p>
</li>
<li><p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants, 2023.09 *** microsoft +</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 ***  大学 +</p>
</li>
<li><p>MM-LLMs: Recent Advances in MultiModal Large Language Models, 2024.02 *** 腾讯  +</p>
</li>
<li><p>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a> *** + </p>
</li>
<li><p>How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model, 2023.11 <a href="https://arxiv.org/pdf/2311.07594.pdf">[paper]</a> *</p>
</li>
<li><p>A Survey on Multimodal Large Language Models, 2023.06  <a href="https://arxiv.org/abs/2306.13549">[paper]</a> <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">[project]</a> ***</p>
</li>
<li><p>Multimodal Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.13165.pdf">[paper]</a> **</p>
</li>
<li><p>Large Language Models Meet Computer Vision: A Brief Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.16673.pdf">[paper]</a> *</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 <a href="https://arxiv.org/pdf/2307.13721.pdf">[paper]</a><a href="https://github.com/awaisrauf/Awesome-CV-Foundational-Models">[project]</a> ***  + </p>
</li>
<li><p>Video Understanding with Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17432.pdf">[paper]</a> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">[project]</a></p>
</li>
</ul>
<hr>
<h2><span id="llm-for-domain-application">LLM for Domain Application</span><a href="#llm-for-domain-application" class="header-anchor">#</a></h2><h3><span id="domain">domain</span><a href="#domain" class="header-anchor">#</a></h3><ul>
<li>Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey</li>
</ul>
<h3><span id="llm-for-health">LLM for Health</span><a href="#llm-for-health" class="header-anchor">#</a></h3><ul>
<li><p>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge, 2023.11 <a href="https://arxiv.org/pdf/2311.05112">[paper]</a><a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large Language Models Illuminate a Progressive Pathway to Artificial  Healthcare Assistant: A Review, 2023.10 <a href="https://arxiv.org/pdf/2311.01918">[paper]</a><a href="https://github.com/mingze-yuan/Awesome-LLM-Healthcare">[project]</a></p>
</li>
<li><p>Large AI Models in Health Informatics: Applications, Challenges, and the Future, 2023.03 <a href="https://arxiv.org/abs/2303.11568">[paper]</a><a href="https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models">[project]</a></p>
</li>
<li><p>A SWOT (Strengths, Weaknesses, Opportunities, and Threats) Analysis of ChatGPT in the Medical Literature: Concise Review, 2023.11 <a href="https://www.jmir.org/2023/1/e49368/PDF">[paper]</a></p>
</li>
<li><p>ChatGPT in Healthcare: A Taxonomy and Systematic Review, 2023.03 <a href="https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1">[paper]</a></p>
</li>
</ul>
<h3><span id="llm-for-finance">LLM for Finance</span><a href="#llm-for-finance" class="header-anchor">#</a></h3><ul>
<li><p>Large Language Models in Finance: A Survey, 2023.09 <a href="https://arxiv.org/abs/2311.10723">[paper]</a></p>
</li>
<li><p>A Survey of Large Language Models in Finance (FinLLMs) ***</p>
</li>
</ul>
<h3><span id="llm-for-education">LLM for Education</span><a href="#llm-for-education" class="header-anchor">#</a></h3><ul>
<li>ChatGPT and Beyond: The Generative AI Revolution in Education, 2023.11 <a href="https://arxiv.org/pdf/2311.15198">[paper]</a></li>
</ul>
<h3><span id="llm-for-law">LLM for Law</span><a href="#llm-for-law" class="header-anchor">#</a></h3><ul>
<li>Large Language Models in Law: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03718">[paper]</a></li>
</ul>
<h3><span id="llm-for-mental-health">LLM for Mental Health</span><a href="#llm-for-mental-health" class="header-anchor">#</a></h3><ul>
<li>A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement, 2023.10 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10601652/">[paper]</a></li>
<li>Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects, 2023.12 <a href="https://arxiv.org/pdf/2312.04578.pdf">[paper]</a></li>
<li>Large Language Models in Mental Health Care: a Scoping Review, 2024.01 <a href="https://arxiv.org/pdf/2401.02984.pdf">[paper]</a></li>
</ul>
<hr>
<h2><span id="llm-for-downstream-tasks">LLM for Downstream Tasks</span><a href="#llm-for-downstream-tasks" class="header-anchor">#</a></h2><h3><span id="llm-for-recommendation">LLM for Recommendation</span><a href="#llm-for-recommendation" class="header-anchor">#</a></h3><ul>
<li>User Modeling in the Era of Large Language Models: Current Research and Future Directions, 2023.12 <a href="https://doi.org/10.48550/arXiv.2312.11518">[paper]</a><a href="https://github.com/TamSiuhin/LLM-UM-Reading">[project]</a></li>
<li>A Survey on Large Language Models for Personalized and Explainable  Recommendations, 2023.11 <a href="https://arxiv.org/pdf/2311.12338">[paper]</a></li>
<li>Large Language Models for Generative Recommendation: A Survey and Visionary Discussions, 2023.09 <a href="https://arxiv.org/abs/2309.01157">[paper]</a></li>
<li>A Survey on Large Language Models for Recommendation, 2023.08 <a href="https://arxiv.org/abs/2305.19860">[paper]</a><a href="https://github.com/WLiK/LLM4Rec-Awesome-Papers">[project]</a></li>
<li>How Can Recommender Systems Benefit from Large Language Models: A Survey, 2023.06 <a href="https://arxiv.org/abs/2306.05817">[paper]</a><a href="https://github.com/CHIANGEL/Awesome-LLM-for-RecSys">[project]</a></li>
</ul>
<h3><span id="llm-for-information-retrieval">LLM for Information Retrieval</span><a href="#llm-for-information-retrieval" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Information Retrieval: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.07107">[paper]</a><a href="https://github.com/RUC-NLPIR/LLM4IR-Survey">[project]</a></li>
</ul>
<h3><span id="llm-for-software-engineering">LLM for Software Engineering</span><a href="#llm-for-software-engineering" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Software Engineering: Survey and Open Problems, 2023.10 <a href="https://arxiv.org/abs/2310.03533">[paper]</a></li>
<li>Large Language Models for Software Engineering: A Systematic Literature Review, 2023.08 <a href="https://arxiv.org/abs/2308.10620">[paper]</a></li>
</ul>
<h3><span id="llm-for-time-series">LLM for Time Series</span><a href="#llm-for-time-series" class="header-anchor">#</a></h3><ul>
<li>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook, 2023.10 <a href="https://arxiv.org/abs/2310.10196">[paper]</a><a href="https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM">[project]</a></li>
</ul>
<h3><span id="detection-of-llms-generated-content">Detection of LLMs-Generated Content</span><a href="#detection-of-llms-generated-content" class="header-anchor">#</a></h3><ul>
<li>A Survey on Detection of LLMs-Generated Content, 2023.10 <a href="https://arxiv.org/abs/2310.15654">[paper]</a><a href="https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection">[project]</a></li>
<li>A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions, 2023.10 <a href="https://arxiv.org/pdf/2310.14724.pdf">[paper]</a><br><a href="https://github.com/NLP2CT/LLM-generated-Text-Detection">[project]</a></li>
<li>Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text, 2023.09 <a href="https://arxiv.org/pdf/2309.07689.pdf">[paper]</a></li>
<li></li>
</ul>
<h3><span id="llm-for-information-extraction">LLM for Information Extraction</span><a href="#llm-for-information-extraction" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Generative Information Extraction: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17617.pdf">[paper]</a> <a href="https://github.com/quqxui/Awesome-LLM4IE-Papers">[project]</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/Awesome-LLM-Survey">Awesome-LLM-Survey</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA 家族</title>
    <url>/www6vHomeAIGC/2023/02/24/gptLlamaFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llama-家族1">LLaMA 家族[1]</span><a href="#llama-家族1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>项目</th>
<th>描述</th>
<th>数据集</th>
</tr>
</thead>
<tbody><tr>
<td>LLaMa</td>
<td>基座模型</td>
<td>公开可用的数据集(1T token)</td>
</tr>
<tr>
<td>Stanford Alpaca</td>
<td>结合英文语料通过Self Instruct方式微调LLaMA 7B</td>
<td>Self Instruct from davinci-003 API(52K)</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune)</td>
<td>用户共享对话(70K sample)</td>
</tr>
<tr>
<td>BELLE</td>
<td>结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA</td>
<td></td>
</tr>
<tr>
<td>Chinese-LLaMA&#x2F;Chinese-Alpaca</td>
<td>通过中文数据预训练&#x2F;指令微调LLaMA</td>
<td></td>
</tr>
<tr>
<td>姜子牙系列模型Ziya-LLaMA-13B-v1</td>
<td>基于LLaMA-13B的中英文模型</td>
<td></td>
</tr>
<tr>
<td>ChatLLaMA(英文版)</td>
<td>LLaMA的RLHF版</td>
<td></td>
</tr>
<tr>
<td>ColossalChat</td>
<td>通过self-instruct技术指令微调LLaMA且加上RLHF</td>
<td></td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/llama2-famaly.jpg" class>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="家族">家族</span><a href="#家族" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/129709105">LLaMA的解读与其微调：Alpaca-LoRA&#x2F;Vicuna&#x2F;BELLE&#x2F;中文LLaMA&#x2F;姜子牙&#x2F;LLaMA 2</a> ***<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485019&idx=1&sn=e3417472c0c1f98aede498fbe905e1a0&">我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 </a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/618695885">NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究</a><br>1xx. <a href="https://github.com/www6v/Llama2-Chinese">https://github.com/www6v/Llama2-Chinese</a><br>1xx.  <a href="https://zhuanlan.zhihu.com/p/618321077">从0到1复现斯坦福羊驼（Stanford Alpaca 7B）</a><br> GPUs: 8 卡 A800 80GB GPUs<br>1xx. &lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;    </p>
<p>1xx. <a href="https://github.com/www6v/Linly">中文 LLaMA</a><br>   <a href="https://www.bilibili.com/video/BV1Np4y1j783/">掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402185&idx=2&sn=55901b89381e27aedee56c69041f6af8">近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 </a>    llama-2-7b-32k -  LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Chinese-LLaMA PT+SFT</title>
    <url>/www6vHomeAIGC/2023/02/21/gptChineseLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-%E6%A8%A1%E5%9E%8B-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87">代码、模型、数据集准备</a><ul>
<li><a href="#%E4%BB%A3%E7%A0%81%E5%87%86%E5%A4%87-5">代码准备 [5]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D-%E5%8F%8A-tokenizer-%E5%87%86%E5%A4%87-4">模型权重 及 Tokenizer 准备 [4]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87-3">数据集准备 [3]</a></li>
</ul>
</li>
<li><a href="#%E8%AF%8D%E8%A1%A8%E6%89%A9%E5%85%85">词表扩充</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82">模型训练细节</a><ul>
<li><a href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E9%A2%84%E8%AE%AD%E7%BB%83">第二阶段预训练</a></li>
<li><a href="#%E5%B0%86-lora-%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将 LoRA 权重与基础模型合并</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E7%B2%BE%E8%B0%83">指令精调</a></li>
<li><a href="#%E5%B0%86%E5%A4%9A%E4%B8%AAlora%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将多个LoRA权重与基础模型合并</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86">模型推理</a></li>
<li><a href="#%E7%BB%93%E8%AF%AD">结语</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="环境搭建">环境搭建</span><a href="#环境搭建" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install transformers==4.28.1 sentencepiece==0.1.97 google protobuf deepspeed -i https://pypi.tuna.tsinghua.ed</span></span><br><span class="line">u.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/huggingface/peft.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git checkout 13e53fc</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install torch==1.13.1</span></span><br></pre></td></tr></table></figure>
<h1><span id="代码-模型-数据集准备">代码、模型、数据集准备</span><a href="#代码-模型-数据集准备" class="header-anchor">#</a></h1><h3><span id="代码准备-5">代码准备 [5]</span><a href="#代码准备-5" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3e2f2529</span></span><br><span class="line">git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意: 一定要用 commitid &#x3D;3e2f2529的代码， 用最新代码会有很多异常</p>
</blockquote>
<h3><span id="模型权重-及-tokenizer-准备-4">模型权重 及 Tokenizer 准备 [4]</span><a href="#模型权重-及-tokenizer-准备-4" class="header-anchor">#</a></h3><h3><span id="数据集准备-3">数据集准备 [3]</span><a href="#数据集准备-3" class="header-anchor">#</a></h3><h1><span id="词表扩充">词表扩充</span><a href="#词表扩充" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python3 merge_tokenizers.py \</span></span><br><span class="line"><span class="language-bash">  --llama_tokenizer_dir /root/internLM/model/skyline2006/llama-7b \</span></span><br><span class="line"><span class="language-bash">  --chinese_sp_model_file /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/chinese_sp.model</span></span><br></pre></td></tr></table></figure>

<h1><span id="模型训练细节">模型训练细节</span><a href="#模型训练细节" class="header-anchor">#</a></h1><h3><span id="第二阶段预训练">第二阶段预训练</span><a href="#第二阶段预训练" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改运行脚本run_pt.sh</span><br><span class="line"></span><br><span class="line">lr=2e-4</span><br><span class="line">lora_rank=8</span><br><span class="line">lora_alpha=32</span><br><span class="line">lora_trainable=&quot;q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj&quot;</span><br><span class="line">modules_to_save=&quot;embed_tokens,lm_head&quot;</span><br><span class="line">lora_dropout=0.05</span><br><span class="line"></span><br><span class="line">pretrained_model=/root/internLM/model/skyline2006/llama-7b #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  #</span><br><span class="line">dataset_dir=/root/internLM/shu-master/books  #</span><br><span class="line">data_cache=/root/cache/books #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/output_dir #</span><br><span class="line">RANDOM=100 #</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>具体执行过程如下所示：<br>sh run_pt.sh </p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/3.png" class>

<p>模型输出文件：</p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/result.png" class>


<h3><span id="将-lora-权重与基础模型合并">将 LoRA 权重与基础模型合并</span><a href="#将-lora-权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python merge_llama_with_chinese_lora.py \</span><br><span class="line">    --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">    --tokenizer_path /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  \</span><br><span class="line">    --lora_model /root/internLM/llamazh/output_dir/lora/ \</span><br><span class="line">    --output_type huggingface \</span><br><span class="line">    --output_dir /root/internLM/llamazh/pt_merged/book-merge-hf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并LLaMA和LoRA后的权重</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ll -h /root/internLM/llamazh/pt_merged/book-merge-hf</span></span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 10:48 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 10:47 ../</span><br><span class="line">-rw-r--r-- 1 root root  598 Feb 23 10:47 config.json</span><br><span class="line">-rw-r--r-- 1 root root  132 Feb 23 10:47 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9.3G Feb 23 10:48 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3.6G Feb 23 10:48 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root  27K Feb 23 10:48 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root  411 Feb 23 10:47 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root 741K Feb 23 10:47 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root  727 Feb 23 10:47 tokenizer_config.json</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">原始llama权重</span></span><br><span class="line">ll -h /root/internLM/model/skyline2006/llama-7b</span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 21 20:04 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 21 19:38 ../</span><br><span class="line">-rw-r--r-- 1 root root   43 Feb 21 19:38 .mdl</span><br><span class="line">-rw------- 1 root root 3.6K Feb 21 20:04 .msc</span><br><span class="line">-rw-r--r-- 1 root root   36 Feb 21 20:04 .mv</span><br><span class="line">-rw------- 1 root root  11K Feb 21 19:39 LICENSE</span><br><span class="line">-rw------- 1 root root 9.0K Feb 21 20:04 README.md</span><br><span class="line">-rw------- 1 root root  22M Feb 21 19:38 alpaca_data.json</span><br><span class="line">-rw------- 1 root root  427 Feb 21 19:38 config.json</span><br><span class="line">-rw------- 1 root root  302 Feb 21 19:39 configuration.json</span><br><span class="line">-rw------- 1 root root 1.2K Feb 21 19:39 default_offload_opt_param.json</span><br><span class="line">-rw------- 1 root root  124 Feb 21 19:39 generation_config.json</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:40 pytorch_model-00001-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:42 pytorch_model-00002-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:44 pytorch_model-00003-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:47 pytorch_model-00004-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:50 pytorch_model-00005-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:51 pytorch_model-00006-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:53 pytorch_model-00007-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00008-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00009-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00010-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00011-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00012-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00013-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00014-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:58 pytorch_model-00015-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00016-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00017-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00018-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00019-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00020-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00021-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00022-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00023-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00024-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00025-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00026-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00027-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00028-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00029-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00030-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00031-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00032-of-00033.bin</span><br><span class="line">-rw------- 1 root root 501M Feb 21 20:04 pytorch_model-00033-of-00033.bin</span><br><span class="line">-rw------- 1 root root  25K Feb 21 20:04 pytorch_model.bin.index.json</span><br><span class="line">-rw------- 1 root root    2 Feb 21 20:04 special_tokens_map.json</span><br><span class="line">-rw------- 1 root root 489K Feb 21 20:04 tokenizer.model</span><br><span class="line">-rw------- 1 root root  141 Feb 21 20:04 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h3><span id="指令精调">指令精调</span><a href="#指令精调" class="header-anchor">#</a></h3><p>修改模型精调脚本run_sft.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pretrained_model=/root/internLM/llamazh/pt_merged/book-merge-hf  #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf #</span><br><span class="line">dataset_dir=/root/internLM/Chinese-LLaMA-Alpaca-main/data #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/sft_output  #</span><br><span class="line">#peft_model=path/to/peft/model/dir</span><br><span class="line">validation_file=/root/internLM/llm-action-main/train/chinese-llama-alpaca/alpaca_eval.json  #</span><br><span class="line">RANDOM=1000</span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; sh run_sft.sh </span><br></pre></td></tr></table></figure>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result2.png" class>


<p>模型输出文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -al -h /root/internLM/llamazh/sft_output/lora</span></span><br><span class="line">total 819M</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 15:29 .</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 15:29 ..</span><br><span class="line">-rw-r--r-- 1 root root  501 Feb 23 15:29 adapter_config.json</span><br><span class="line">-rw-r--r-- 1 root root 819M Feb 23 15:29 adapter_model.bin</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="将多个lora权重与基础模型合并">将多个LoRA权重与基础模型合并</span><a href="#将多个lora权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python merge_llama_with_chinese_lora.py \</span><br><span class="line">     --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">     --tokenizer_path /root/internLM/llamazh/output_dir,/root/internLM/llamazh/sft_output \</span><br><span class="line">     --lora_model /root/internLM/llamazh/output_dir/lora/,/root/internLM/llamazh/sft_output/lora \</span><br><span class="line">     --output_type huggingface \</span><br><span class="line">     --output_dir /root/internLM/llamazh/all_output</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ll  /root/internLM/llamazh/all_output</span><br><span class="line">total 13449132</span><br><span class="line">drwxr-xr-x 2 root root       4096 Feb 23 16:38 ./</span><br><span class="line">drwxr-xr-x 7 root root       4096 Feb 23 16:38 ../</span><br><span class="line">-rw-r--r-- 1 root root         21 Feb 23 16:38 added_tokens.json</span><br><span class="line">-rw-r--r-- 1 root root        598 Feb 23 16:38 config.json</span><br><span class="line">-rw-r--r-- 1 root root        132 Feb 23 16:38 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9943340890 Feb 23 16:38 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3827767515 Feb 23 16:38 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root      26788 Feb 23 16:38 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root        435 Feb 23 16:38 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root     757958 Feb 23 16:38 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root        747 Feb 23 16:38 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h1><span id="模型推理">模型推理</span><a href="#模型推理" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py \</span><br><span class="line">     --base_model /root/internLM/llamazh/all_output \</span><br><span class="line">     --with_prompt \</span><br><span class="line">     --interactive</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py      --base_model /root/internLM/llamazh/all_output      --with_prompt      --<span class="keyword">in</span></span><br><span class="line">teractive</span><br><span class="line">Loading checkpoint shards: <span class="number">100</span>%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| <span class="number">2</span>/<span class="number">2</span> [<span class="number">00</span>:<span class="number">26</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">13.09</span>s/it]</span><br><span class="line">Vocab of the base model: <span class="number">49954</span></span><br><span class="line">Vocab of the tokenizer: <span class="number">49954</span></span><br><span class="line">Start inference <span class="keyword">with</span> instruction mode.</span><br><span class="line">=====================================================================================</span><br><span class="line">+ 该模式下仅支持单轮问答，无多轮对话能力。</span><br><span class="line">+ 如要进行多轮对话，请使用llama.cpp或llamachat工具。</span><br><span class="line">-------------------------------------------------------------------------------------</span><br><span class="line">+ This mode only supports single-turn QA.</span><br><span class="line">+ If you want to experience multi-turn dialogue, please use llama.cpp <span class="keyword">or</span> llamachat.</span><br><span class="line">=====================================================================================</span><br><span class="line">Input:who are you？</span><br><span class="line">Response:  I am <span class="number">10</span> years old, my name <span class="keyword">is</span> Lilly.</span><br></pre></td></tr></table></figure>

<h1><span id="结语">结语</span><a href="#结语" class="header-anchor">#</a></h1><p>整个训练流程:<br>词表扩充+预训练(继续预训练)  -&gt;  输出lora模型<br>指令精调sft   -&gt;  输出lora模型<br>合并2个lora模型，在进行推理</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/631360711">中文LLaMA&amp;Alpaca大语言模型词表扩充+预训练+指令精调</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/">Chinese-LLaMA-Alpaca</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki">中文文档</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC">预训练脚本</a></li>
<li><a href="https://github.com/shjwudp/shu">继续预训练 DataSet</a></li>
<li><a href="https://www.modelscope.cn/models/skyline2006/llama-7b/summary">llama-7b</a> 基础模型</li>
<li><a href="https://github.com/www6v/AIGC/tree/master/chinese-llama-alpaca">chinese-llama-alpaca</a>  git 代码以这个为主<br><a href="https://github.com/www6v/llm-action/tree/main/train/chinese-llama-alpaca">chinese-llama-alpaca</a> 参考这个代码，有很多遗漏的文件，都补齐了，已提交到AIGC</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SELF-INSTRUCT, Self-QA</title>
    <url>/www6vHomeAIGC/2023/02/21/gptSelfInstruct/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#self-instruct">SELF-INSTRUCT</a><ul>
<li><a href="#%E8%87%AA%E5%8A%A8%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%901">自动指令数据生成[1]</a></li>
</ul>
</li>
<li><a href="#self-qa3">Self-QA[3]</a><ul>
<li><a href="#%E6%80%9D%E6%83%B3">思想</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E7%94%9F%E6%88%90%E9%98%B6%E6%AE%B5">指令生成阶段</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%E9%98%B6%E6%AE%B5">指令答案生成阶段</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="self-instruct">SELF-INSTRUCT</span><a href="#self-instruct" class="header-anchor">#</a></h1><h3><span id="自动指令数据生成1">自动指令数据生成[1]</span><a href="#自动指令数据生成1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptSelfInstruct/selfInstruct.jpg" class>
<p>1）指令生成<br>2）识别指令是否代表分类任务<br>3）用输入优先或输出优先的方法生成实例<br>4）过滤低质量数据</p>
<ul>
<li>实现步骤[3]<ul>
<li>人工设计175个表示不同任务的指令，并且给每条数据都编写了（指令, 输入, 输出）&#x2F;（指令, 输出），将这175条数据作为种子池。</li>
<li>使用模型生成新的指令；</li>
<li>对该模型生成的指令判断是否分类任务；</li>
<li>使用模型生成实例；</li>
<li>对上述模型生成的数据进行过滤和后处理；</li>
<li>将经过过滤和后处理的数据添加到种子池中；</li>
<li>一直重复上述2到6步直到种子池有足够多的数据；</li>
</ul>
</li>
</ul>
<h1><span id="self-qa3">Self-QA[3]</span><a href="#self-qa3" class="header-anchor">#</a></h1><h3><span id="思想">思想</span><a href="#思想" class="header-anchor">#</a></h3><p>知识引导的指令生成Knowledge-Guided Instruction Generation</p>
<h3><span id="指令生成阶段">指令生成阶段</span><a href="#指令生成阶段" class="header-anchor">#</a></h3><ul>
<li>采用语言模型本身来根据无监督的文本生成指令。这种方法使生成的指令具有领域针对性，并与所提供的无监督文本的内容相关。<ul>
<li>非结构化的知识，如网页和书籍数据，直接使用。</li>
<li><strong>结构化数据</strong>，如表格和知识图谱，在被利用之前需要<strong>转换为非结构化文本数据</strong>。如通过使用模板填充槽或将每个数据条目与相应的属性名称连接起来来实现。</li>
</ul>
</li>
</ul>
<h3><span id="指令答案生成阶段">指令答案生成阶段</span><a href="#指令答案生成阶段" class="header-anchor">#</a></h3><ul>
<li>将<strong>生成的指令问题</strong>让大模型进行预测，<strong>生成答案</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399792&idx=1&sn=c70e1d13b68399b0c19cfbf658f35d77">面向大模型微调的instruction指令自动化生成技术：SELF-INSTRUCT指令自动化生成框架工作介绍</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1nQ4y1A7Po">Stanford Alpaca</a> V<br><a href="https://github.com/www6v/stanford_alpaca/blob/main/generate_instruction.py">stanford_alpaca generate_instruction</a> git</p>
</li>
<li><p>《第二章 大模型训练与微调研发背后的数据艺术》 LLM大语言模型算法特训  那位科技 ***<br> <strong>SELF-INSTRUCT</strong>， Baize， <strong>Evol-instruct</strong>， <strong>Self-QA</strong>， Ultra-chat</p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/qq_16949707/article/details/131266543">ACL2023 | 大模型如何快速构建指令遵循数据集？self-instruct：用175条种子数据追上InstructGPT001效果</a></p>
<p>1xx. <a href="https://github.com/yizhongw/self-instruct/">self-instruct</a> git</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/650596719">大模型SFT微调指令数据的生成</a><br>   SELF-INSTRUCT， Wizard， Backtranslation</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/618334308">让ChatGPT生成训练ChatGPT的训练数据</a><br>   BELLE</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>STF</category>
      </categories>
      <tags>
        <tag>STF</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent 多模态</title>
    <url>/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="多模态-agent1">多模态 Agent[1]</span><a href="#多模态-agent1" class="header-anchor">#</a></h1><ul>
<li><p>核心组件</p>
<ul>
<li><strong>感知</strong>组件关注处理多模态信息</li>
<li><strong>规划器</strong>负责推理和制定计划</li>
<li><strong>行动</strong>组件执行计划</li>
<li><strong>记忆</strong>组件则涉及长期和短期记忆</li>
</ul>
</li>
<li><p>四种类型</p>
<ul>
<li>无长期记忆的闭源 LLMs 作为规划器</li>
<li>无长期记忆的微调 LLMs 作为规划器</li>
<li>具有间接长期记忆的规划器 </li>
<li>具有本地长期记忆的规划器</li>
</ul>
</li>
<li><p>多智能体协作</p>
<ul>
<li>讨论了 LMAs 如何通过协作框架共同实现共同目标。</li>
</ul>
</li>
</ul>
<h1><span id="多模态-agent10">多模态 Agent[10]</span><a href="#多模态-agent10" class="header-anchor">#</a></h1><h3><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/tasks.JPG" class>

<ul>
<li><p>MM-ReAct </p>
</li>
<li><p>HuggingGPT[21, 22] </p>
</li>
<li><p>Chameleon</p>
</li>
<li><p>Visual ChatGPT [20]</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488499&idx=1&sn=ac8c5092ddc8fd724965d12aff3f9392">2024年大型多模态智能体(Large Multimodal Agents)综述：组件, 分类，协作，评估，应用，展望</a> ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/678238642">个人LLM智体的综述</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678203245">智体AI在多模态交互领域的综述（上）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678222381">智体AI在多模态交互领域的综述（下）</a></p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.bilibili.com/video/BV1mM411X7Zn/">多模态 Agents：用大模型语言模型串联多模态专家</a> V</li>
</ol>
<h3><span id="多模态agent">多模态Agent</span><a href="#多模态agent" class="header-anchor">#</a></h3><p>1xx. <a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> self<br>1xx. <a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a> self</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="20">
<li><p>《Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models》<br><a href="https://github.com/chenfei-wu/TaskMatrix">Visual ChatGPT</a> git</p>
</li>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130856470">LLMs的自动化工具系统（HuggingGPT、AutoGPT、WebGPT、WebCPM）</a>  </p>
</li>
<li><p><a href="https://github.com/microsoft/JARVIS">HuggingGPT</a> git<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb">hugginggpt in langchain</a> git<br><a href="https://github.com/camille-vanhoffelen/langchain-huggingGPT">langchain-huggingGPT</a> git</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)模型压缩-量化概述</title>
    <url>/www6vHomeAIGC/2023/02/19/gptQuantization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%87%8F%E5%8C%96">量化</a><ul>
<li><a href="#%E9%87%8F%E5%8C%96%E7%9A%84%E5%AE%9A%E4%B9%89-3">量化的定义 [3]</a></li>
<li><a href="#%E9%87%8F%E5%8C%96%E7%9A%84%E4%B8%A4%E4%B8%AA%E9%98%B6%E6%AE%B5-3">量化的两个阶段 [3]</a></li>
<li><a href="#%E9%87%8F%E5%8C%96%E5%88%86%E7%B1%BB">量化分类</a></li>
<li><a href="#quantization">Quantization</a></li>
</ul>
</li>
<li><a href="#ptq-%E5%88%86%E7%B1%BB">PTQ 分类</a><ul>
<li><a href="#weight-quantization-106">Weight Quantization  [10][6]</a></li>
<li><a href="#weight-and-activation-quantization-10">Weight and Activation Quantization [10]</a></li>
<li><a href="#%E6%AF%94%E8%BE%831">比较[1]</a></li>
</ul>
</li>
<li><a href="#%E4%BD%8E%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95chat">低精度训练方法[chat]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="量化">量化</span><a href="#量化" class="header-anchor">#</a></h1><h3><span id="量化的定义-3">量化的定义  [3]</span><a href="#量化的定义-3" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/19/gptQuantization/quantization.jpeg" class>

<h3><span id="量化的两个阶段-3">量化的两个阶段  [3]</span><a href="#量化的两个阶段-3" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/19/gptQuantization/twoProcedures.jpeg" class>


<h3><span id="量化分类">量化分类</span><a href="#量化分类" class="header-anchor">#</a></h3><ul>
<li><p>量化分类 [3][5]</p>
<ul>
<li>Quantization-Aware Training (QAT)<br><strong>Need more data and time, More accurate</strong></li>
<li>Quantization-Aware Fine-tuning(QAF)   [9]</li>
<li>Post-Training Quantization (PTQ)<br><strong>Need fewer data and time, Less accurate</strong></li>
</ul>
</li>
<li><p>PyTorch 支持的三种量化类型 [4]</p>
<ul>
<li>dynamic quantization (weights quantized with activations read&#x2F;stored in floating point and quantized for compute)</li>
<li>static quantization (weights quantized, activations quantized, calibration required post training)    <strong>-&gt;PTQ</strong></li>
<li>static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)   <strong>-&gt;QAT</strong></li>
</ul>
</li>
</ul>
<h3><span id="quantization">Quantization</span><a href="#quantization" class="header-anchor">#</a></h3><ul>
<li>QAT -&gt; Expensive</li>
<li>PTQ -&gt; More feasible than QAT</li>
</ul>
<h1><span id="ptq-分类">PTQ 分类</span><a href="#ptq-分类" class="header-anchor">#</a></h1><h3><span id="weight-quantization-106">Weight Quantization  [10][6]</span><a href="#weight-quantization-106" class="header-anchor">#</a></h3><ul>
<li>LLM.int8() </li>
<li>GPTQ </li>
<li>AWQ</li>
</ul>
<h3><span id="weight-and-activation-quantization-10">Weight and Activation Quantization [10]</span><a href="#weight-and-activation-quantization-10" class="header-anchor">#</a></h3><ul>
<li>SmoothQuant[8]</li>
</ul>
<h3><span id="比较1">比较[1]</span><a href="#比较1" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th></th>
<th>Weight only quant</th>
<th>smoothquant(PTQ )</th>
<th>fp8(PTQ )</th>
</tr>
</thead>
<tbody><tr>
<td>Latency Reduction</td>
<td>★★</td>
<td>★★★(best)</td>
<td>★★★(best)</td>
</tr>
<tr>
<td>Modal Acc</td>
<td>★★</td>
<td>★</td>
<td>★★★(best)</td>
</tr>
<tr>
<td>Memory Saving</td>
<td>★★</td>
<td>★★★(best)</td>
<td>★★★(best)</td>
</tr>
<tr>
<td>Ease of use</td>
<td>★★★(best)</td>
<td>★★</td>
<td>★</td>
</tr>
</tbody></table>
<h1><span id="低精度训练方法chat">低精度训练方法[chat]</span><a href="#低精度训练方法chat" class="header-anchor">#</a></h1><ul>
<li>半精度浮点数（FP16）训练</li>
<li>混合精度训练（Mixed Precision Training）</li>
<li>量化训练（Quantization Training）</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1h44y1c72B">大语言模型推理：低精度最佳实践</a> V</li>
<li><a href="https://zhuanlan.zhihu.com/p/649460612">大模型训练｜概念篇</a></li>
<li>&lt;&lt; An Introduction to Quantization of Large Language Model &gt;&gt; </li>
<li>4.1<a href="https://pytorch.org/docs/stable/quantization.html">pytorch Quantization</a><br>4.2 <a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">Introduction to Quantization on PyTorch</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/662881352">大模型量化概述</a>  ***</li>
<li><a href="/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/" title="(原理)Weight Only(LLM.int8(), GPTQ, AWQ)">(原理)Weight Only(LLM.int8(), GPTQ, AWQ)</a> self</li>
<li>xxx</li>
<li><a href="https://juejin.cn/post/7330079146515611687">大模型量化技术原理-SmoothQuant </a></li>
<li><a href="/www6vHomeAIGC/2024/01/12/gptPEFTQLora/" title="(实战)PEFT QLoRA">(实战)PEFT QLoRA</a>   self</li>
<li>《A Survey on Model Compression for Large Language Models》</li>
</ol>
<p>1xx. <a href="https://www.zhihu.com/question/510246227">神经网络低比特量化中训练和推理是如何实现的？</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>短文本相似度</title>
    <url>/www6vHomeAIGC/2023/02/18/gptDocSimilarity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/111414376">短文本相似度算法研究</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/113133510">Sentence-Bert论文笔记</a><br>1xx. <a href="https://www.bilibili.com/video/BV13h4y1a7z6/">SentenceBert模型：文本语义去重</a> V</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/113017752">传统方法TF-IDF解决短文本相似度问题</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/113224707">传统方法BM25解决短文本相似度问题</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>短文本相似度</category>
      </categories>
      <tags>
        <tag>短文本相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型</title>
    <url>/www6vHomeAIGC/2023/02/17/gptLargeModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="http://arthurchiao.art/blog/llm-practical-guide-zh/">[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）</a>   实战  未<br>1xx. <a href="https://zhuanlan.zhihu.com/p/597586623">通向AGI之路：大型语言模型（LLM）技术精要</a> *** </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671710012">高效大语言模型：综述</a>  *** 大模型各个维度的优化<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403847&idx=1&sn=9af731e9f8418a2d869f5464530c8bd6">必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 </a> 12个综述</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Transformer</title>
    <url>/www6vHomeAIGC/2023/02/16/gptTransformerCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://github.com/www6v/AIGC/blob/master/transformer/transformer.ipynb">transformer.ipynb</a> git<br>   <a href="https://www.bilibili.com/video/BV1nc411y7m4/">Transformer代码实现</a></p>
<p>1xx. <a href="https://paperswithcode.com/method/transformer">Transformer</a><br>   <a href="https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201">transformer.py</a> git</p>
<p>1xx. <a href="http://arthurchiao.art/blog/transformers-from-scratch-zh/">[译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）</a> V, github<br>    Transformers from scratch</p>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/130090649">从零实现Transformer的简易版与强大版：从300多行到3000多行</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/398039366">Transformer源码详解（Pytorch版本）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>COT</title>
    <url>/www6vHomeAIGC/2023/02/08/gptCOT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="cot4">CoT[4]</span><a href="#cot4" class="header-anchor">#</a></h1><ul>
<li><p>CoT(Chain of Thought)</p>
<ul>
<li>CoT-SC(Self Consistency)</li>
</ul>
</li>
<li><p>ToT(Tree of Thoughts)<br>分为了Thought Decomposition，Thought Generator，State Evaluator，Search algorithms</p>
</li>
<li><p>GoT(Graph of Thoughts)</p>
</li>
<li><p>AoT(Algorithm of Thoughts)</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="4">
<li><a href="https://zhuanlan.zhihu.com/p/654034193">2023年能够解决复杂问题的思维链技术：Cot，ToT，GoT，AoT</a></li>
</ol>
<p>1xx. <a href="https://github.com/zchuz/CoT-Reasoning-Survey">CoT-Reasoning-Survey </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404176&idx=1&sn=2eafdf5426bfe1347869b9af268d4238">大模型COT思维链推理的几个关键问题：从评测基准、结构变体到增强方案的系统综述 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/X2lcVLFFlFgQCzacret4Vg">大模型思维链推理的综述：进展、前沿和未来</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>COT</category>
      </categories>
      <tags>
        <tag>COT</tag>
      </tags>
  </entry>
  <entry>
    <title>医疗大模型</title>
    <url>/www6vHomeAIGC/2023/02/07/gptDomainMed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="医疗大模型">医疗大模型</span><a href="#医疗大模型" class="header-anchor">#</a></h3><ul>
<li>LLaMA<ul>
<li>ChatDoctor  </li>
<li>华驼&#x2F;本草  哈工大</li>
<li>PMC-LLaMA 上海交大</li>
</ul>
</li>
<li>ChatGLM-6B<ul>
<li>ChatGLM-Med  哈工大</li>
<li>DoctorGLM</li>
<li>明医 (MING)  MedicalGPT-zh  上海交通大学</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402886&idx=1&sn=0552d60744645a84d13bb0cef57f321c">再看23个医疗领域微调大模型集合：兼看CareLlama医疗模型的一些实践经验与开放医疗数据 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402589&idx=1&sn=3ba9d50fad433adeb8dd6c623b06c42d">大模型遇上心理健康咨询：MeChat、QiaoBan、SoulChat、MindChat四大心理健康领域微调模型总结 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402638&idx=1&sn=b9329498806e2b93b2d6817a17941bff">大模型常见错误、反馈的来源及自我修正方法：兼论两个有趣的同名中医微调垂域模型 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>测评</title>
    <url>/www6vHomeAIGC/2023/02/07/gptEval/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402223&idx=1&sn=f2ec30cd04600129bb90bc9c81413d95">一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 </a><br>1xx. <a href="https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese">https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402549&idx=1&sn=07a8af1db44df6125939c5c9e90f6267">如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403046&idx=1&sn=0a9b612e9790c0bf49d5cede8fda365c">大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 </a> 二、CEVAL榜单评测中能够得到一些启示<br><a href="https://github.com/hkust-nlp/ceval/blob/main/resources/tutorial.md">1. C-Eval 数据集评测简明教程</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403295&idx=1&sn=126c949d0a00eb85b4a3a6b0106f55a6&poc_token=HApExGWjou7N5NVcTKmJpWt9LZ8ul6wynjV5VHnQ">大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 </a>   CEVAl</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>eval</category>
      </categories>
      <tags>
        <tag>eval</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT 数据组合</title>
    <url>/www6vHomeAIGC/2023/02/06/gptDatasetSFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E9%97%AE%E9%A2%981">问题[1]</a></li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C1">实验结果[1]</a></li>
<li><a href="#%E9%97%AE%E9%A2%982-%E5%9C%A8sft%E4%B8%AD%E7%BB%93%E5%90%88%E4%B8%89%E7%A7%8D%E8%83%BD%E5%8A%9B%E6%97%B6%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%80%A7%E8%83%BD%E5%86%B2%E7%AA%81kimipaper">问题2 在SFT中结合三种能力时是否存在性能冲突？[kimi][paper]</a><ul>
<li><a href="#%E7%BB%93%E8%AE%BA">结论：</a></li>
</ul>
</li>
<li><a href="#%E9%97%AE%E9%A2%983-%E5%AF%BC%E8%87%B4%E6%80%A7%E8%83%BD%E5%86%B2%E7%AA%81%E7%9A%84%E5%85%B3%E9%94%AE%E5%9B%A0%E7%B4%A0%E6%98%AF%E4%BB%80%E4%B9%88kimipaper">问题3 导致性能冲突的关键因素是什么？[kimi][paper]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《HOW ABILITIES IN LARGE LANGUAGE MODELS ARE AFFECTED BY SUPERVISED FINE-TUNING DATA COM- POSITION》<br>keyword: SFT 数据组合</li>
</ul>
<h1><span id="问题1">问题[1]</span><a href="#问题1" class="header-anchor">#</a></h1><p>1、推理、编码和通用能力如何随SFT数据量而变化？<br>2、在SFT中结合三种能力时是否存在性能冲突？<br>3、导致性能冲突的关键因素是什么？<br>4、不同的SFT策略对组合数据有什么影响？</p>
<h1><span id="实验结果1">实验结果[1]</span><a href="#实验结果1" class="header-anchor">#</a></h1><p>1、不同的能力表现出不同的扩展模式，在数据量相同的情况下，<strong>较大的模型通常表现出更优越的性能</strong>。<br>2、随着数据量的持续增加，<strong>数学推理和代码生成能力也在不断提高</strong>，<strong>一般能力</strong>则是在样本数达到<strong>一千左右</strong>时才得到提升，且提升速度较慢。<br>3、在<strong>数据量较低</strong>的情况下，数据组合会带来各种能力的<strong>提高</strong>，而在<strong>数据量较高</strong>的情况下，能力则会发生<strong>冲突</strong>。<br>4、组成<strong>数据量</strong>会影响<strong>性能</strong>，而<strong>组成比例</strong>的影响则<strong>微乎其微</strong>。</p>
<p>【模型大小】</p>
<p>【数据数量】</p>
<p>【数据数量 &lt;–&gt;  多样性】？</p>
<p>【组成比例】</p>
<h1><span id="问题2-在sft中结合三种能力时是否存在性能冲突kimipaper">问题2 在SFT中结合三种能力时是否存在性能冲突？[kimi][paper]</span><a href="#问题2-在sft中结合三种能力时是否存在性能冲突kimipaper" class="header-anchor">#</a></h1><p>问题2 探讨的是在监督式微调（Supervised Fine-Tuning, SFT）中结合推理、编码和通用能力时是否存在性能冲突。</p>
<h3><span id="结论">结论：</span><a href="#结论" class="header-anchor">#</a></h3><ol>
<li><p><strong>性能冲突的存在</strong>：在高资源设置下，即当SFT数据集混合使用时，不同能力领域（如数学推理、编码和通用对齐能力）之间会发生性能冲突。然而，在低资源设置下，混合数据源能够提升性能。</p>
</li>
<li><p><strong>性能冲突与资源量的关系</strong>：随着数据量的增加，特定任务的性能可能会因为其他任务的存在而下降。这表明在数据量较大时，不同任务之间可能会相互干扰，导致性能冲突。</p>
</li>
<li><p><strong>模型大小对性能的影响</strong>：随着模型大小的增加，在低资源设置下，数学和通用能力的性能提升更加明显。</p>
</li>
</ol>
<h1><span id="问题3-导致性能冲突的关键因素是什么kimipaper">问题3 导致性能冲突的关键因素是什么？[kimi][paper]</span><a href="#问题3-导致性能冲突的关键因素是什么kimipaper" class="header-anchor">#</a></h1><p>在SFT（监督式微调）中结合推理、编码和通用能力时，导致性能冲突的关键因素包括：</p>
<ol>
<li><p><strong>数据组成和比例</strong>：当不同能力领域的数据混合在一起进行SFT时，如果<strong>数据量充足</strong>，来自其他领域的数据可能会被视为<strong>噪声</strong>，从而影响特定领域的性能。</p>
</li>
<li><p><strong>模型大小</strong>：<strong>较大的模型</strong>在相同数据量下通常表现<strong>更好</strong>，并且在低资源设置下对于数学和通用能力的性能增益更大。</p>
</li>
<li><p><strong>训练策略</strong>：多任务学习虽然能够保留专业能力，但对通用能力的伤害最大；而顺序训练和混合顺序训练虽然保留了通用能力，但会丢失太多的专业能力。</p>
</li>
<li><p><strong>数据量与能力的关系</strong>：数学推理和编码能力随着数据量的增加而持续提高，而通用能力在大约一千个样本后趋于平稳。</p>
</li>
<li><p><strong>任务特性差异</strong>：推理和编码任务需要复杂的逻辑来分解任务指令和处理非语言和符号特征，而对齐人类意图则需要多样性和理解模糊的人类指令。</p>
</li>
</ol>
<p>相应的结论包括：</p>
<ul>
<li>在<strong>低资源设置下</strong>，混合数据源可以<strong>提高性能</strong>，但在<strong>高资源设置</strong>下，可能会导致<strong>性能下降</strong>。</li>
<li><strong>数据量</strong>直接<strong>影响力能表现</strong>，而<strong>数据比例</strong>的影响<strong>不显著</strong>。</li>
<li>提出的<strong>双阶段混合微调（DMT）策略</strong>有效地减轻了多任务学习中的性能冲突和顺序训练中的灾难性遗忘，实现了通用与专业能力之间的平衡。</li>
</ul>
<p>这些结论强调了在SFT阶段理解和解决数据组成问题对于全面提高LLMs（大型语言模型）的能力至关重要。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404728&idx=2&sn=1cb2203648271720d421c963ebcc03b3">SFT微调的数据组合及训练策略如何影响大模型性能：4个经典问题及实验结论分享 </a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401381&idx=1&sn=c24d896aab990ffdf30107a7c6c1ea4f">再看大模型微调与应用：3大行业18个开源垂直微调模型、微调数据、工具资源及有趣的AIGC应用集合</a> 二 三</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400009&idx=1&sn=f72c0a9cb7c19184995156c3ef169b74">也谈大模型研发中的微调数据规模评估与质量问题：数据规模大小的影响评估、数据主要问题及清洗项目</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400342&idx=1&sn=d344ced5035fc804f490b00469746fc8">也谈微调数据质量、多样性规模对大模型性能的影响与评估方案：Belle项目开源实验工作报告介绍 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)幻觉问题</title>
    <url>/www6vHomeAIGC/2023/02/06/gptHallucination/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="幻觉-vs-事实性1">幻觉 vs 事实性[1]</span><a href="#幻觉-vs-事实性1" class="header-anchor">#</a></h1><p><strong>幻觉</strong>主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于”生成与某些来源相关的无意义或不真实的内容”。这与<strong>事实性问题</strong>不同，后者强调模型学习、获取和利用事实性知识的能力。</p>
<p>举例说明两者的<strong>区别</strong>：</p>
<p>如果一个LLM在被要求创作”一个关于兔子和狼交朋友的童话故事”时，创作出了一个关于”兔子和狗交朋友”的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。<br>如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是<strong>幻觉</strong>，而<strong>不是事实性问题</strong>。<br>例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是<strong>幻觉</strong>。</p>
<p>相反，如果LLM避免给出直接答案，而是说”我不知道”，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是<strong>事实性问题</strong>，而<strong>不是幻觉</strong>。</p>
<p>此外，值得注意的是，<strong>幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的</strong>。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404394&idx=1&sn=d7cfcf2cd9aa6756d3cbff938f5f4cf2">再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403998&idx=1&sn=400cc902434bc04df508a55e192d2455">再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405983&idx=2&sn=95dc9c7a12bed99b63c775d4b90519d8">也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 </a></p>
<p>1xx. <a href="https://arxiv.org/abs/2309.01219">大模型幻觉综述</a><br>   <a href="https://arxiv.org/abs/2309.05922">大模型幻觉综述</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405791&idx=2&sn=d7dada69e6d5ab5fba1333d234b947ef">网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 </a> 大模型幻觉综述</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403602&idx=1&sn=f2365b05630094f8d0de7ff784abe233">大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介</a> 大模型微调遗忘</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403341&idx=1&sn=86cdaaf2c3a73439d2591a2f3dd0b9e0">值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642648601">大模型的幻觉问题调研: LLM Hallucination Survey</a><br>   <a href="https://mp.weixin.qq.com/s?__biz=MzU5NDg2MjgxMg==&mid=2247485189&idx=1&sn=95d6eb333dde007f262a2955b90bc7ec">人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） </a><br>   <a href="https://mp.weixin.qq.com/s/eGMwNz0F1dQsNDnsLNYr8Q">大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二）</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/N7NOsLHr8HYCMp5XGCBDjg">LLM之幻觉（一）：大语言模型幻觉解决方案综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Hallucination</category>
      </categories>
      <tags>
        <tag>Hallucination</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)数据处理</title>
    <url>/www6vHomeAIGC/2023/02/05/gptDataProcess/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-pipeline">数据处理 pipeline</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%80%9A%E7%94%A8-1">数据处理[通用] [1]</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%BF%87%E6%BB%A4">质量过滤</a></li>
<li><a href="#%E5%86%97%E4%BD%99%E5%8E%BB%E9%99%A4">冗余去除</a></li>
<li><a href="#%E8%AF%8D%E5%85%83%E5%88%87%E5%88%86">词元切分</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%862">数据处理[2]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%87%E8%AE%B0">数据标记</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">数据准备</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-1">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F">数据质量</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据处理-pipeline">数据处理 pipeline</span><a href="#数据处理-pipeline" class="header-anchor">#</a></h1><h2><span id="数据处理通用-1">数据处理[通用] [1]</span><a href="#数据处理通用-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/data_process.png" class>

<h3><span id="质量过滤">质量过滤</span><a href="#质量过滤" class="header-anchor">#</a></h3><ul>
<li>基于<strong>分类器</strong>的方法</li>
<li>基于<strong>启发 式</strong>的方法</li>
</ul>
<h3><span id="冗余去除">冗余去除</span><a href="#冗余去除" class="header-anchor">#</a></h3><p>可以在<strong>句子级</strong>、<strong>文档级</strong>和<strong>数据集级</strong>等不同粒度上去重<br>在实践中应该 共同使用这三个级别的去重</p>
<h3><span id="词元切分">词元切分</span><a href="#词元切分" class="header-anchor">#</a></h3><ul>
<li>BPE</li>
<li>WordPiece</li>
<li>Unigram 词元分析</li>
</ul>
<h2><span id="数据处理2">数据处理[2]</span><a href="#数据处理2" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/pipeline.webp" class>
<h3><span id="数据标记">数据标记</span><a href="#数据标记" class="header-anchor">#</a></h3><ul>
<li>包标签</li>
<li>半监督标签</li>
<li>主动学习</li>
<li>数据编程</li>
<li>远程监督</li>
</ul>
<h3><span id="数据准备">数据准备</span><a href="#数据准备" class="header-anchor">#</a></h3><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《大规模语言模型》 </li>
<li>《Data-centric Artificial Intelligence: A Survey》 大学<br><a href="https://zhuanlan.zhihu.com/p/620890799">Data-centric Artificial Intelligence: A Survey</a><br> <a href="https://cloud.tencent.com/developer/article/2359824">机器学习数据工程的概述</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/639207933">大模型时代下数据的重要性</a> 综述</p>
<p>1xx. <a href="https://hub.baai.ac.cn/view/28740">大模型研发核心：数据工程、自动化评估及与知识图谱的结合</a><br>   <a href="https://mp.weixin.qq.com/s/SvDnQD886E3DBtw8k9asgg">大模型研发核心：数据工程、自动化评估及与知识图谱的结合 </a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_16949707/article/details/133875958">符尧：别卷大模型训练了，来卷数据吧！【干货十足】</a> 看最后的5个结论 </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488088&idx=1&sn=f401a5a12e7b3727a15abbcff1a0ec51">合成数据(Synthetic data)微调大语言模型实战指南：背景、方案、案例、代码、评估 </a></p>
<ol start="50">
<li><a href="/www6vHomeAIGC/2023/01/06/gptInstructTuning/" title="(原理)Instruct Tuning">(原理)Instruct Tuning</a> self</li>
</ol>
<h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/420295576">哈工大｜15种NLP数据增强方法总结与对比</a></p>
<h3><span id="数据质量">数据质量</span><a href="#数据质量" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403976&idx=1&sn=694db5e2b3085b1610e8d19daa93a474">再看大模型预训数据质量如何评估：困惑度、错误L2范数和记忆化三种度量方法的效果对比分析研究</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP+LLM</title>
    <url>/www6vHomeAIGC/2023/02/05/gptNLPTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#nlp%E4%BB%BB%E5%8A%A12">NLP任务[2]</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%863">自然语言处理[3]</a><ul>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a></li>
<li><a href="#%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96">信息抽取</a></li>
<li><a href="#%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94">智能问答</a></li>
</ul>
</li>
<li><a href="#nlpllm">NLP+LLM</a><ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E4%BA%8Enlp%E7%9A%84%E8%8C%83%E5%BC%8F-1">大模型用于NLP的范式 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="nlp任务2">NLP任务[2]</span><a href="#nlp任务2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/02/05/gptNLPTask/NLP_tasks.jpg" class>


<ul>
<li><strong>文本摘要</strong> text summarization</li>
<li>信息提取 information extraction</li>
<li><strong>问答</strong> question answering</li>
<li><strong>文本分类</strong> text classification</li>
<li>对话 conversation</li>
<li>代码生成 code generation</li>
<li><strong>推理</strong> reasoning</li>
</ul>
<h1><span id="自然语言处理3">自然语言处理[3]</span><a href="#自然语言处理3" class="header-anchor">#</a></h1><h3><span id="基本概念">基本概念</span><a href="#基本概念" class="header-anchor">#</a></h3><ul>
<li><p>类型</p>
<ul>
<li>自然语言理解（Natural Language  Understanding，NLU）</li>
<li>自然语言生成（Natural Language Generation，NLG）</li>
</ul>
</li>
<li><p>主要难点</p>
<ul>
<li>自然语言处理难的<strong>根本原因</strong>：自然语言在各个层面都广泛存在的各种各样的<strong>歧义性</strong></li>
</ul>
</li>
</ul>
<h3><span id="信息抽取">信息抽取</span><a href="#信息抽取" class="header-anchor">#</a></h3><ul>
<li><p>概述<br>从自然语言构成的<strong>非结构化文本</strong>中抽取指定类型的实体、关系、事件等信息，进而形成<strong>结构化数据</strong>。</p>
</li>
<li><p>命名实体识别（Named Entity Recognition， NER）</p>
</li>
<li><p>关系抽取（Relation Extraction，RE）</p>
</li>
<li><p>事件抽取（Event Extraction）</p>
</li>
</ul>
<h3><span id="智能问答">智能问答</span><a href="#智能问答" class="header-anchor">#</a></h3><ul>
<li><p>类型</p>
<ul>
<li>事实类（Factoid）</li>
<li>是非类（Yes&#x2F;No）</li>
<li>定义类（Definition）</li>
<li>列表类（List）</li>
<li>比较类（Comparison）<br>【多跳】</li>
<li>意见类（Opinion）</li>
<li>指导类（How-to）</li>
</ul>
</li>
<li><p>主要类型</p>
<ul>
<li>阅读理解（Machine Reading Comprehension，MRC）</li>
<li>表格问答（Table based Question Answering, TBQA）</li>
<li>社区问答（Community Question Answering，CQA）</li>
<li>知识图谱问答（Knowledge based Question Answering，KBQA）</li>
<li>开放领域问答（Open-domain Question Answering，ODQA）</li>
</ul>
</li>
</ul>
<h1><span id="nlpllm">NLP+LLM</span><a href="#nlpllm" class="header-anchor">#</a></h1><h3><span id="大模型用于nlp的范式-1">大模型用于NLP的范式 [1]</span><a href="#大模型用于nlp的范式-1" class="header-anchor">#</a></h3><ul>
<li>用于NLP的LLM分类法<ul>
<li>包括参数冻结范式（a）<br>zero-shot  few-shot </li>
<li>参数调整范式（b）<br>【PEFT 全参数】</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1.《Large Language Models Meet NLP: A Survey》<br>  <a href="https://mp.weixin.qq.com/s/dRlkv24vTCeUiVvRGmDdQg">有趣的两个RAG大模型问答新优化思路：兼看大模型与NLP的结合范式 </a><br>大模型时代的NLP的一些结合方式</p>
<ol start="2">
<li><p><a href="https://www.nlplanet.org/">NLPlanet | Natural Language Processing Community</a></p>
</li>
<li><p><a href="https://intro-nlp.github.io/">自然语言处理导论</a><br>第一章 绪论<br>第七章 信息抽取<br>第十章 智能问答</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399456&idx=1&sn=af2ee30aee9e7f6ed441b8335de033b1">关于ChatGPT解锁NLP任务的一次总结汇报：从应用变化、14类NLP任务使用案例看选型思考</a></p>
<p>1xx. <a href="https://www.deeplearning.ai/resources/natural-language-processing/">A Complete Guide to Natural Language Processing</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Toolformer</title>
    <url>/www6vHomeAIGC/2023/02/03/gptAgentToolformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a>  </p>
</li>
<li><p>开源地址<br><a href="https://github.com/lucidrains/toolformer-pytorch">Implementation of Toolformer</a>  git</p>
</li>
</ul>
<h1><span id="toolformer1">Toolformer[1]</span><a href="#toolformer1" class="header-anchor">#</a></h1><ul>
<li>🔑关键词和摘要<ul>
<li>Keywords: Large-scale PLMs,  Tool Learning</li>
<li>xxx<ul>
<li>驱动语言模型去使用简单的模型来调用外部的工具</li>
<li>Toolformer通过语言模型的方法去决定去调用哪些API，传入哪些参数</li>
<li>Tooformer是在自监督层面执行的，只需要对每个API的语言描述</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>方法   <ul>
<li>Toolformer调用示例：xxx</li>
<li>关键要素：<ul>
<li>模型对工具的使用应该是自监督的，这样可以省去很大的标注开销</li>
<li>模型应该自行地去决定在何时间，用何方法来调用工具</li>
</ul>
</li>
<li><strong>方法概要：</strong><ul>
<li>受到in-context learning的启发，给定少量的人写的关于API的描述，让模型去自行生成潜在API调用的语言建模数据</li>
<li>构建一个自监督的Loss函数，让模型来决定哪些API的调用有助于它的语言建模的预测</li>
</ul>
</li>
<li><strong>方法细节：</strong><ul>
<li>xxx<ul>
<li>给定一个纯文本数据集，构建出一个带有API调用的数据集，然后在此数据集上做微调</li>
<li>第一步：使用in-context learning来生成大量的潜在可能的API调用</li>
<li>第二步：执行这些API，返回得到结果</li>
<li>第三步：检查返回的结果是否有助于语言模型的预测，过滤掉其他的API</li>
</ul>
</li>
<li>API调用采样<ul>
<li>给每一个API来撰写提示来鼓励模型使用这些API，例如QA的提示是 xxx</li>
<li>对于文本的每一个位置，如果这个位置是<api>（即API调用的开始）的概率大于一个阈值，则将此位置保留到一个集合I中</api></li>
<li>对于集合I中的每一个位置，通过模型生成最多m个API调用，并且以结尾（如果生成的调用没有以结尾，直接舍去）</li>
</ul>
</li>
<li>API执行<ul>
<li>去执行所有的API调用，返回文本序列</li>
</ul>
</li>
<li>API过滤<ul>
<li>构建自监督的语言模型的loss函数</li>
<li>第一个的含义：进行API的调用，并且使用API结果的Loss</li>
<li>第二个的含义：空字符串的Loss和调用API但不返回结果Loss的最小值</li>
<li>这时我们希望模型使用API并且返回结果对语言建模有帮助，且帮助很明显-&gt;前者的loss显著比后者小</li>
</ul>
</li>
<li>微调和推理<ul>
<li>在经过如上操作后，就可以得到带有API调用的数据集，然后将模型在上面进行微调</li>
<li>当模型在解码阶段输出”-&gt;”符号时，意味着需要调用API了，调用得到返回结果然后拼接上去</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实验<ul>
<li>模型：GPT-J （67亿参数）</li>
<li>原始数据：CCNet</li>
<li>知识探测任务LAMA<ul>
<li>Toolformer可以大幅超过之前的方法，甚至是GPT-3等大模型</li>
</ul>
</li>
<li>数学数据集</li>
<li>问答</li>
<li>这里即使是Toolformer也无法超越GPT-3，可见预训练规模可以囊括更多知识</li>
<li>模型规模的影响</li>
<li>模型的参数量到一定规模后才拥有使用工具的能力</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>将语言模型使用外部工具的进行很自然的结合</li>
<li><strong>不需要标注大量数据，使用自监督的方法进行学习</strong></li>
</ul>
</li>
<li>缺点<ul>
<li><strong>工具无法交互，也无法链式使用（每个API调用都是独立的）</strong></li>
<li>定义的工具尚且有限，扩展工具则需要用模型标注新的数据</li>
<li>随着基础模型zero-shot能力的增强，这种需要构建数据并且fine-tune的做法可能会比较麻烦</li>
</ul>
</li>
</ul>
</li>
<li>OpenBMB BMTools: <a href="https://github.com/OpenBMB/BMTools">https://github.com/OpenBMB/BMTools</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18s4y1u7nJ/">清华博士带你搞懂大模型自学工具使用（Toolformer)【论文速读】</a> V 有思维导图<br>1xx. <a href="https://finisky.github.io/toolformer-summary/">使LLM善假于物: Toolformer </a><br>1xx. <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis">Prompt Engineering </a><br>1xx. <a href="https://nakaizura.blog.csdn.net/article/details/130817902">Toolformer and Tool Learning（LLMs如何使用工具）</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)涌现现象</title>
    <url>/www6vHomeAIGC/2023/02/03/gptEmergent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="emergent-abilities">Emergent Abilities</span><a href="#emergent-abilities" class="header-anchor">#</a></h1><ul>
<li>🔗 文章：Emergent Abilities of Large Language Models  (2022.10)  (arxiv.org)</li>
<li>🔑关键词和摘要<ul>
<li>Keywords: LLMs, Emergent Ability, Scaling</li>
<li>abstract<ul>
<li>不可预测</li>
<li>不能从小模型的的性能外推</li>
<li>是否能通过继续扩大模型规模来获得更多涌现能力</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>定义<ul>
<li>通常的涌现现象</li>
<li>大模型的涌现现象<ul>
<li>小模型接近随机</li>
<li><strong>大模型突然出现</strong></li>
<li>相变</li>
</ul>
</li>
<li>实验框架<ul>
<li>performance vs 1. FLOPs, model parameters</li>
<li><input checked disabled type="checkbox"> Training datasets</li>
<li>叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。</li>
</ul>
</li>
<li>实验1<ul>
<li>Few-shot Prompting</li>
<li>测试数据说明:<ul>
<li>A: 三位数加法，两位数乘法</li>
<li>B: [dɪfərənt], 复原 “different,” </li>
<li>C: 从 e l h l o 复原 hello</li>
<li>D: 波斯语问答</li>
<li>E: 针对GPT-3 对抗标的问答</li>
<li>…</li>
</ul>
</li>
<li>结果<ul>
<li>这些 task，以 few-shot 形式展示过以后，都有 emergent</li>
<li>不同模型 emergent scale 不一样</li>
<li>有的 task，只有 540B 的 PaLM  emerge了</li>
</ul>
</li>
</ul>
</li>
<li>实验2<ul>
<li>增强语言模型能力的 emerge 现象</li>
<li>已知的一些大模型技巧在何种规模下发挥作用？<ul>
<li>大模型技巧<ul>
<li>思维链 Chain-of-thought: Let’s think step by step.</li>
<li>指令微调 请写一段XXX的描述</li>
<li>草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6&#x3D;11，进位1”</li>
</ul>
</li>
</ul>
</li>
<li>这些增强语言模型能力的方法都有一定程度的涌现</li>
<li>联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？</li>
</ul>
</li>
</ul>
</li>
<li>讨论<ul>
<li><strong>Emergent 现象的解释</strong><ul>
<li><strong>多步能力说</strong><ul>
<li>每个子能力达到 90%  -&gt; 一无是处</li>
<li>每个子能力达到 95% -&gt; 能完成一些任务了</li>
</ul>
</li>
<li>指标缺陷说</li>
<li>奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降</li>
</ul>
</li>
<li><strong>Emergent 的阈值可能会越来越小</strong><ul>
<li>更干净的数据，更好的训练技巧，更优秀的模型结构都可以是  Emergent阈值变小</li>
</ul>
</li>
<li>未来方向：<ul>
<li>继续扩大 model scale，远未达到上限</li>
<li>一些新结构的 scaling</li>
<li>数据的 scaling</li>
<li>理解 prompt 机制</li>
<li>更前沿的 task，用来指导 emergent</li>
<li>理解 emergence</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>第一次正式提出 emergent 实验</li>
<li><strong>做了充分的实验表明该现象在各种数据集上广泛存在</strong></li>
<li>甚至验证了一些“方法”的涌现</li>
<li>提出了一些解释该现象的观点，并提出质疑</li>
</ul>
</li>
<li>改进点<ul>
<li><strong>还是不知道为啥 emerge</strong></li>
<li>实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1qX4y1i78J/">清华博士带你思考大语言模型LLM的涌现现象（Emergent）</a>  有脑图<br> Emergent Abilities of Large Language Models （<a href="https://arxiv.org/abs/2206.07682%EF%BC%89">https://arxiv.org/abs/2206.07682）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399147&idx=1&sn=6e6d416db50d9708c900ee3b5416bba3">再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Emergent</category>
      </categories>
      <tags>
        <tag>Emergent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)继续Pre-Training</title>
    <url>/www6vHomeAIGC/2023/02/03/gptContinualPretraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="继续-预训练-continual-pre-training-1">继续-预训练 continual pre-training [1]</span><a href="#继续-预训练-continual-pre-training-1" class="header-anchor">#</a></h1><ul>
<li><p>继续预训练的目的<br>为了得到<strong>适应不同行业&#x2F;任务领域</strong>的预训练模型，<strong>提升下游任务的效果</strong></p>
</li>
<li><p>什么时候需要继续预训练？<br><strong>预训练(pre-train)的语料与下游任务(finetune)语料的【数据分布&#x2F;领域差异】大时</strong></p>
</li>
</ul>
<h1><span id="千帆llama-2中文增强技术介绍-postpretrain2">千帆Llama 2中文增强技术介绍-Postpretrain[2]</span><a href="#千帆llama-2中文增强技术介绍-postpretrain2" class="header-anchor">#</a></h1><ul>
<li><p>中文词表构建 +Tokenizer<br>中文词表扩增 29k -&gt; 59k</p>
</li>
<li><p>Embedding<br>在原有Embedding矩阵后追加中文embedding映射</p>
</li>
<li><p>数据配比<br> 中文：英文约1:1</p>
</li>
<li><p>pipeline</p>
<ul>
<li>原始数据集</li>
<li><strong>异常清洗</strong></li>
<li><strong>数据过滤</strong></li>
<li><strong>去重</strong></li>
<li>隐私匿名化</li>
</ul>
</li>
</ul>
<blockquote>
<p>开源大模型预训练语料预处理流程总结： 基于基础规则处理为主 + 基于模型的质量过滤逐步成为趋势</p>
</blockquote>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/545092184">浅谈一下「继续预训练」</a></li>
<li>&lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;<br>1xx. <a href="https://zhuanlan.zhihu.com/p/654463331">如何更好地继续预训练（Continue PreTraining）</a><br>warmup  +  学习率<br>1xx. <a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/120695507">Don’t stop pretraining，继续预训练！</a></li>
</ol>
<p>1xx. 《Investigating Continual Pretraining in Large Language Models: Insights and Implications》<br>    <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583&chksm=83839096b4f41980e8277f34650c2029a45e853adfc2b412ea386952751e44d29e75f0048d12&scene=178&cur_album_id=3343133676745932807#rd">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理-lmdeploy</title>
    <url>/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lmdeploy-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2-10">lmdeploy-推理部署 [10]</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">模型转换</a></li>
<li><a href="#turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">TurboMind 推理+命令行本地对话</a></li>
<li><a href="#turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TurboMind推理+API服务</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="lmdeploy-推理部署-10">lmdeploy-推理部署 [10]</span><a href="#lmdeploy-推理部署-10" class="header-anchor">#</a></h1><h3><span id="模型转换">模型转换</span><a href="#模型转换" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/convert.png" class>

<h3><span id="turbomind-推理命令行本地对话">TurboMind 推理+命令行本地对话</span><a href="#turbomind-推理命令行本地对话" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer.png" class>

<h3><span id="turbomind推理api服务">TurboMind推理+API服务</span><a href="#turbomind推理api服务" class="header-anchor">#</a></h3><ul>
<li>启动服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api.png" class></li>
<li>Client访问服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api-client.png" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="10">
<li><a href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md">lmdeploy 量化部署</a><br>  <a href="https://www.bilibili.com/video/BV1iW4y1A77P/">(5)LMDeploy 大模型量化部署实践</a> V</li>
</ol>
<p>1xx. <a href="https://github.com/www6v/llm-action/tree/main/inference">llm-action  inference</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent-Tools</title>
    <url>/www6vHomeAIGC/2023/01/27/gptAgentTool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E5%88%86%E7%B1%BB1">分类[1]</a><ul>
<li><a href="#tool-augmented-vs-tool-oriented-kimi-%E6%80%BB%E7%BB%93">Tool-augmented vs. Tool-oriented [kimi 总结]</a></li>
<li><a href="#tool-augmented-learning">Tool-augmented Learning</a></li>
<li><a href="#tool-oriented-learning">Tool-oriented Learning</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><a href="https://arxiv.org/pdf/2304.08354.pdf">Tool Learning with Foundation Models</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/thunlp/ToolLearningPapers">ToolLearningPapers</a> git</p>
</li>
</ul>
<h1><span id="分类1">分类[1]</span><a href="#分类1" class="header-anchor">#</a></h1><h3><span id="tool-augmented-vs-tool-oriented-kimi-总结">Tool-augmented vs. Tool-oriented [kimi 总结]</span><a href="#tool-augmented-vs-tool-oriented-kimi-总结" class="header-anchor">#</a></h3><ol>
<li><p>Tool-augmented Learning（工具增强学习）:</p>
<ul>
<li>这种学习方式指的是在基础模型（如大型预训练语言模型）的基础上，<strong>通过引入外部工具来增强模型的能力</strong>。这些工具可以是任何可以被模型通过某种接口调用的系统或服务，例如搜索引擎、数据库、API等。</li>
<li>工具增强学习的核心在于模型利用这些工具来获取额外的信息或执行特定的任务，从而弥补模型自身知识和能力的不足。</li>
<li>例如，<strong>一个语言模型可能通过调用天气API来获取最新的天气信息，或者通过搜索引擎来找到相关问题的答案</strong>。</li>
</ul>
</li>
<li><p>Tool-oriented Learning（面向工具的学习）:</p>
<ul>
<li>面向工具的学习则更多地关注于模型如何学习和理解如何使用这些工具。这不仅仅是调用工具API那么简单，而是<strong>涉及到模型对工具的深入理解和策略性使用</strong>。</li>
<li>在这种学习模式下，模型可能需要<strong>学习如何组合使用多个工具</strong>，或者在复杂任务中动态调整对工具的使用策略，以实现更高效的问题解决。</li>
<li>例如，模型可能需要学习如何在<strong>规划一次旅行</strong>时，先后调用地图API、航班搜索API和酒店预订API，同时还要根据用户反馈和环境变化动态调整计划。</li>
</ul>
</li>
</ol>
<p>总的来说，Tool-augmented Learning 强调的是通过外部工具来扩展模型的能力，而 Tool-oriented Learning 则更侧重于模型对工具使用的学习和优化。两者都是工具学习（Tool Learning）的重要组成部分，但在实际应用中可能会有不同的实现方式和关注点。</p>
<h3><span id="tool-augmented-learning">Tool-augmented Learning</span><a href="#tool-augmented-learning" class="header-anchor">#</a></h3><ul>
<li>Toolformer   <a href="/www6vHomeAIGC/2023/02/03/gptAgentToolformer/" title="(原理)Toolformer">(原理)Toolformer</a></li>
</ul>
<h3><span id="tool-oriented-learning">Tool-oriented Learning</span><a href="#tool-oriented-learning" class="header-anchor">#</a></h3><ul>
<li>ToolMaker[10]</li>
<li>CREATOR[11]</li>
<li>ToolLLM [12]</li>
<li>Visual ChatGPT[13]</li>
<li>HuggingGPT[13]</li>
<li>Gorilla <a href="/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/" title="(原理)Gorilla">(原理)Gorilla</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/624459759">大模型工具学习权威综述，BMTools 背后的论文！</a></li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/xixiaoyaoww/article/details/130278978">清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/yZYGqAKIqDfGYF2YUckiiw">回顾大模型在工具使用上的技术总结：兼看图检索增强生成方案-GRAG </a><br>   《Tool Learning with Large Language Models: A Survey》<br>   问题2:关于大模型使用工具的调研整理</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/pPkrHHkmVC29e_c2U8YEGg">一篇大模型Agent工具使用全面研究综述</a><br>    《Tool Learning with Large Language Models: A Survey》</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://zhuanlan.zhihu.com/p/633654195">LLM能够自己制作工具了：详解Large Language Models as Tool Makers</a>  </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1EN4y1q7Zn/">THUNLP成员领读EMNLP大模型工具创造新框架“CREATOR”</a> V 有思维导图 </p>
</li>
<li><p>《TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS》<br><a href="https://zhuanlan.zhihu.com/p/647899563">TOOLLLM：让大型语言模型掌握真实世界的API</a><br><a href="https://github.com/OpenBMB/ToolBench">ToolBench </a> git<br><a href="https://blog.csdn.net/Dbox_boom/article/details/134815624">论文阅读：ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a></p>
</li>
<li><a href="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/" title="Agent 多模态">Agent 多模态</a> self</li>
</ol>
<h3><span id="others">Others</span><a href="#others" class="header-anchor">#</a></h3><p>《Augmented Language Models》<br>1xx. <a href="https://blog.csdn.net/qq_39388410/article/details/130798125">Augmented Language Models（增强语言模型）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/611492200">增强语言模型（ALM）之综述篇</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PromptTuning</title>
    <url>/www6vHomeAIGC/2023/01/25/gptPromptTuningPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/646748939">大模型参数高效微调技术实战（二）-Prompt Tuning</a><br><a href="https://zhuanlan.zhihu.com/p/635686756">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a><br><a href="https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb">peft_prompt_tuning_clm.ipynb</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)不可能三角</title>
    <url>/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%921">不可能三角[1]</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">不可能三角</a></li>
<li><a href="#%E5%BC%A5%E8%A1%A5%E6%96%B9%E6%B3%95">弥补方法</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96-%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">其他 不可能三角</a><ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F">分布式系统</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8">分布式存储</a></li>
</ul>
</li>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a><ul>
<li><a href="#pretrain-finetune-%E8%8C%83%E5%BC%8F3">pretrain, finetune 范式[3]</a></li>
<li><a href="#pretrain-prompt-predict-%E8%8C%83%E5%BC%8F3">pretrain, prompt, predict 范式[3]</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#scaling-law10">Scaling Law[10]</a><ul>
<li><a href="#scaling-law">Scaling Law</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F">参数量 vs 数据量</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F-1">参数量 vs 数据量</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92-1">不可能三角</a></li>
<li><a href="#scaling-law-1">Scaling Law</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="不可能三角1">不可能三角[1]</span><a href="#不可能三角1" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/impossibleTriangle.JPG" class>

<ul>
<li>预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果</li>
<li>而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的<strong>少样本效果依旧比不过中等模型的精调</strong></li>
</ul>
<h3><span id="弥补方法">弥补方法</span><a href="#弥补方法" class="header-anchor">#</a></h3><ul>
<li><strong>优化size</strong><ul>
<li>对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低</li>
</ul>
</li>
<li><strong>优化few-shot</strong><ul>
<li>对于提升少样本表现，<strong>数据增强</strong>是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限</li>
</ul>
</li>
<li><strong>fine-tuning</strong><ul>
<li>对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA</li>
</ul>
</li>
</ul>
<h1><span id="其他-不可能三角">其他 不可能三角</span><a href="#其他-不可能三角" class="header-anchor">#</a></h1><h3><span id="分布式系统">分布式系统</span><a href="#分布式系统" class="header-anchor">#</a></h3><ul>
<li>CAP理论<ul>
<li>C 一致性</li>
<li>A 可用性</li>
<li>P 分区</li>
</ul>
</li>
</ul>
<h3><span id="分布式存储">分布式存储</span><a href="#分布式存储" class="header-anchor">#</a></h3><ul>
<li>RUM猜想<ul>
<li>Read-overhead </li>
<li>Update-overhead </li>
<li>Memory-overhead</li>
</ul>
</li>
</ul>
<h1><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h1><h3><span id="pretrain-finetune-范式3">pretrain, finetune 范式[3]</span><a href="#pretrain-finetune-范式3" class="header-anchor">#</a></h3><p>第三阶段范式</p>
<h3><span id="pretrain-prompt-predict-范式3">pretrain, prompt, predict 范式[3]</span><a href="#pretrain-prompt-predict-范式3" class="header-anchor">#</a></h3><p>第四阶段范式</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响</p>
<h1><span id="scaling-law10">Scaling Law[10]</span><a href="#scaling-law10" class="header-anchor">#</a></h1><h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/scalingLaw.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/paramVSdataSize.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/computeVSDatasize.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/501381510">预训练模型的下一步？突破Impossible Triangle</a></li>
<li><a href="https://arxiv.org/pdf/2204.06130.pdf">Impossible Triangle: What’s Next for Pre-trained Language Models?</a></li>
<li><a href="https://blog.csdn.net/zandaoguang/article/details/124395479">微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」</a></li>
<li><a href="/www6vHomeAIGC/2023/01/06/gptPromptTuning/" title="(原理)Prompt Tuning">(原理)Prompt Tuning</a> self</li>
</ol>
<h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/667489780">解析大模型中的Scaling Law</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663296750">论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models</a><br>2xx. <a href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/">Training Compute-Optimal Large Language Models 简读 </a></li>
</ol>
<p>2xx. <a href="https://zhuanlan.zhihu.com/p/536053110">【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！</a><br>《Scaling Laws for Neural Language Models》<br>《Training Compute-Optimal Large Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Multi-Agents</title>
    <url>/www6vHomeAIGC/2023/01/21/gptMultiAgents/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8D%8F%E4%BD%9C%E5%9E%8B%E7%9A%84-multi-agent-%E7%B3%BB%E7%BB%9F12">协作型的 multi-agent 系统[1][2]</a><ul>
<li><a href="#%E6%97%A0%E5%BA%8F%E5%90%88%E4%BD%9C">无序合作</a></li>
<li><a href="#%E6%9C%89%E5%BA%8F%E5%90%88%E4%BD%9C">有序合作</a></li>
</ul>
</li>
<li><a href="#%E7%AB%9E%E4%BA%89%E5%9E%8B%E7%9A%84-multi-agent-%E7%B3%BB%E7%BB%9F12">竞争型的 multi-agent 系统[1][2]</a></li>
<li><a href="#%E7%AB%9E%E4%BA%89%E5%9E%8B-vs-%E5%8D%8F%E4%BD%9C%E5%9E%8B">竞争型 vs 协作型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
<li><a href="#xxx">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="协作型的-multi-agent-系统12">协作型的 multi-agent 系统[1][2]</span><a href="#协作型的-multi-agent-系统12" class="header-anchor">#</a></h1><h3><span id="无序合作">无序合作</span><a href="#无序合作" class="header-anchor">#</a></h3><p>当系统中有三个或三个以上的Agent时，每个Agent都可以自由地公开表达自己的观点和意见。他们可以提供反馈和建议，以修改与当前任务相关的反应。<strong>整个讨论过程不受控制，没有特定的顺序，也没有引入标准化的协作工作流程</strong>。我们把这种多Agent合作称为<strong>无序合作</strong>。</p>
<p>multi-Agent系统中引入一个专门的<strong>协调Agent</strong>，负责整合和组织所有Agent的响应，从而更新最终答案。</p>
<blockquote>
<p><strong>ChatLLM 网络</strong>是这一概念的典范代表</p>
</blockquote>
<h3><span id="有序合作">有序合作</span><a href="#有序合作" class="header-anchor">#</a></h3><p>当系统中的Agent遵守特定规则时，例如按顺序逐一发表意见，下游Agent只需关注上游的产出。这样，任务完成效率就会大大提高，整个讨论过程也会变得井然有序。</p>
<blockquote>
<p><strong>CAMEL</strong> 是<strong>双Agent</strong>合作系统的成功实施案例。<br><strong>MetaGPT</strong> 从软件开发中的<strong>经典瀑布模型</strong>中汲取灵感，<strong>将Agent的输入&#x2F;输出标准化为工程文档</strong>。通过将先进的人类流程管理经验编码到Agent提示中，多个Agent之间的合作变得更有条理。然而，在 MetaGPT 的实践探索中，我们发现了Multi-Agent合作的潜在威胁。<strong>如果不制定相应的规则，多个Agent之间的频繁互动会无限放大轻微的幻觉</strong>。</p>
</blockquote>
<h1><span id="竞争型的-multi-agent-系统12">竞争型的 multi-agent 系统[1][2]</span><a href="#竞争型的-multi-agent-系统12" class="header-anchor">#</a></h1><p>当多个Agent在 “针锋相对”的状态下表达自己的论点时，一个<strong>Agent可以从其他Agent那里获得大量外部反馈，从而纠正自己扭曲的想法</strong>。</p>
<blockquote>
<p><strong>ChatEval</strong>建立了一个基于角色扮演的多Agent裁判团队。</p>
</blockquote>
<h1><span id="竞争型-vs-协作型">竞争型 vs 协作型</span><a href="#竞争型-vs-协作型" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th>协作型</th>
<th>竞争型</th>
</tr>
</thead>
<tbody><tr>
<td>系统目标</td>
<td>整体</td>
<td>个体</td>
</tr>
<tr>
<td>主流结构</td>
<td>中心化</td>
<td>去中心化</td>
</tr>
<tr>
<td>agent 功能</td>
<td>相对分散</td>
<td>相对同质</td>
</tr>
<tr>
<td>agent 关系</td>
<td>相互依赖</td>
<td>相互独立</td>
</tr>
<tr>
<td>是否自运行</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>系统资源</td>
<td>通常不共享</td>
<td>共享</td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2023/01/21/gptMultiAgents/multi-agents.webp" class>
<p>基于 LLM 的多个代理的交互场景。在合作互动中，代理以无序或有序的方式进行协作，以实现共同目标。在对抗式交互中，代理以针锋相对的方式展开竞争，以提高各自的性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/685286305">基于大语言模型多智体的综述：进步和挑战</a> 综述<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488353&idx=2&sn=374e8671df71ce7c60d2570aacc9fcf6">万字综述：大语言模型多智能体(LLM Multi-Agents)进展与挑战</a></p>
<p>1xx. <a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">LLM-Agent-Paper-List</a> ***  git</p>
<ol start="2">
<li><a href="https://zhuanlan.zhihu.com/p/656676717">《综述：全新大语言模型驱动的Agent》</a>  *** 4.2</li>
</ol>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/665644399">NLP（廿二）：LLM 时代的 multi-agent 系统</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247487550&idx=1&sn=28c8147920595f385bec3d3b05911ae7">MetaGPT-ICLR2024: 高效人类工作流(SOPs)融入多Agent协作，显著提升软件工程效率！</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/696039197">多Agent系统与任务规划、记忆管理和区块链</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agents</category>
      </categories>
      <tags>
        <tag>Agents</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 论文</title>
    <url>/www6vHomeAIGC/2023/01/20/gptStudyPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#paper">Paper</a></li>
<li><a href="#gpt%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%911">GPT研究方向[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="paper">Paper</span><a href="#paper" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/www6v/paper-reading">paper-reading</a> 李牧大神</p>
<ul>
<li>Transformer  *** <ul>
<li>GPT-4</li>
<li>Instruct GPT *** </li>
<li>GPT, GPT-2, GPT-3 精读  ***</li>
</ul>
</li>
<li>多模态<ul>
<li>CLIP</li>
<li>ViLT</li>
</ul>
</li>
<li>Chain of Thought  ***</li>
</ul>
</li>
<li><p><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a><br>基础篇 - 论文 *** </p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/129508065">LLM&#x2F;ChatGPT与多模态必读论文150篇(已更至第101篇)</a> </p>
</li>
<li><p><a href="https://github.com/zjunlp/LLMAgentPapers">LLMAgentPapers</a> 浙江大学</p>
</li>
<li><p><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a> 浙江大学</p>
</li>
</ul>
<h1><span id="gpt研究方向1">GPT研究方向[1]</span><a href="#gpt研究方向1" class="header-anchor">#</a></h1><ul>
<li>Efficient (PEFT)</li>
<li>Existing stuff(pretrained model)  -应用<br>New directions</li>
<li>Plug-and-play<br> 通用模块组件，能用在各个领域， baseline</li>
<li>Dataset,  evaluation and survey</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1oX4y1d7X6">大模型时代下做科研的四个思路【论文精读·52】</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态</title>
    <url>/www6vHomeAIGC/2023/01/18/gptMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB-1">基础模型分类 [1]</a><ul>
<li><a href="#textually-prompted-models">textually prompted models</a></li>
<li><a href="#visually-prompted-models">visually prompted models</a></li>
<li><a href="#heterogeneous-models">heterogeneous models</a></li>
</ul>
</li>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87-1">论文</a></li>
<li><a href="#model-architecture2">Model Architecture[2]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87-2">论文</a></li>
<li><a href="#arch-32">Arch [3.2]</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B31">类型[3.1]</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
<li><a href="#chat">chat</a></li>
<li><a href="#other">other</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》大学</li>
</ul>
<h1><span id="基础模型分类-1">基础模型分类 [1]</span><a href="#基础模型分类-1" class="header-anchor">#</a></h1><ul>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern.webp" class>
</li>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern1.webp" class></li>
</ul>
<h3><span id="textually-prompted-models">textually prompted models</span><a href="#textually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>contrastive<br>CLIP  双塔</li>
<li>generative<br>Flamingo </li>
<li>hybrid<br>BLIP</li>
<li>conversational<br>GPT-4， miniGPT4, LLaVa</li>
</ul>
<p>传统上，视觉语言模型主要用于需要同时理解视觉和文本模态的任务。然而，随着CLIP展示出的卓越性能，基于<strong>语言监督的模型</strong>在显著上升，并成为主流方法。在本节中，我们专注于探索依赖<strong>语言作为主要监督来源</strong>的方法。这些以文本为提示的模型可以广泛分为三种主要类型：对比、生成和混合方法。</p>
<h3><span id="visually-prompted-models">visually prompted models</span><a href="#visually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>Foundational<br>SAM</li>
</ul>
<h3><span id="heterogeneous-models">heterogeneous  models</span><a href="#heterogeneous-models" class="header-anchor">#</a></h3><h1><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch.webp" class>

<hr>
<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《MM-LLMs: Recent Advances in MultiModal Large Language Models》  腾讯</p>
</li>
<li><p>开源地址<br><a href="https://mm-llms.github.io/archives/">mm-llms</a> 腾讯</p>
</li>
</ul>
<h1><span id="model-architecture2">Model Architecture[2]</span><a href="#model-architecture2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/multimodalArach1.jpg" class>

<ul>
<li><p>Modality Encoder模态编码器<br>  对于图像，通常有四种可选编码器:NFNet-F6 (Brock等人，2021)、ViT (Dosovitskiy等人，2020)、CLIP ViT (Radford等人，2021)和Eva-CLIP ViT (Fang等人，2023)。</p>
<ul>
<li><strong>NFNet-F6</strong>是一种无归一化器的ResNet (He et al.， 2016)，展示了一种自适应梯度裁剪技术，允许在广泛增强的数据集上进行训练，同时实现SOTA级别的图像识别。</li>
<li><strong>ViT</strong>将Transformer (Vaswani et al.， 2017)应用于图像，首先将图像划分为小patch。然后进行线性投影使patch展平，然后通过多个Transformer块进行编码。</li>
<li><strong>CLIP ViT</strong>在文本和图像之间建立连接，包括一个ViT和一个文本编码器。它利用大量的文本-图像对，通过对比学习来优化ViT，将配对的文本和图像视为正样本，其他为负样本。</li>
<li>它的<strong>Eva版本</strong>稳定了大规模CLIP的训练和优化过程，为扩展和加速昂贵的多模态基础模型训练提供了新的方向。对于视频，可以均匀采样到5帧，并经过与图像相同的预处理。</li>
</ul>
</li>
<li><p>Input Projector输入投影器<br>  <strong>输入投影器</strong>可以直接通过线性投影器或多层感知器(MLP)来实现，即交替使用几个线性投影器和非线性激活函数。<br>  还有更复杂的实现，如交叉注意Cross-attention、Q-Former (Li et al.， 2023c)或P-Former (Jian et al.， 2023)。</p>
<ul>
<li><strong>Cross-attention</strong>使用一组可训练向量作为查询，并使用编码特征FX作为键将特征序列压缩到固定长度。然后将压缩后的表示直接输入LLM (Bai等人，2023b)或进一步用于X-text交叉注意融合(Alayrac等人，2022)。 </li>
<li><strong>Q-Former</strong>从FX中提取相关特征，然后将选中的特征作为提示PX。</li>
<li>同时，P-Former生成“参考提示”，对Q-Former生成的提示施加对齐约束。然而，Q-和P-Former都需要单独的PT进程进行初始化。</li>
</ul>
<table>
<thead>
<tr>
<th>Input Projector输入投影器</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>Cross-attention</td>
<td>Flamingo, Owl, Qwen-VL</td>
</tr>
<tr>
<td>Q-Former</td>
<td>BLIP2, InstructBLIP, MiniGPT-4, MiniGPT-5</td>
</tr>
<tr>
<td>MLP</td>
<td>CogVLM , LLaVa1.5</td>
</tr>
<tr>
<td>Linear Project</td>
<td>LLaVa, PaLI-x,  MiniGPT-v2</td>
</tr>
</tbody></table>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/multimodalArch.jpg" class>

<hr>
<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/pdf/2306.13549v1">A Survey on Multimodal Large Language Models</a><br> <a href="https://arxiv.org/abs/2306.13549">A Survey on Multimodal Large Language Models</a> 中国科学技术大学   腾讯</p>
</li>
<li><p>开源地址<br><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Repo</a></p>
</li>
</ul>
<h1><span id="arch-32">Arch [3.2]</span><a href="#arch-32" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch2.png" class>

<h1><span id="类型31">类型[3.1]</span><a href="#类型31" class="header-anchor">#</a></h1><ul>
<li>本文将最近具有代表性的MLLM分为4种主要类型：<ul>
<li><strong>多模态指令调整（MIT）</strong></li>
<li>多模态上下文学习（M-ICL）</li>
<li>多模态思想链（M-CoT）</li>
<li><strong>LLM辅助视觉推理（LAVR）</strong>【类似agent】</li>
</ul>
</li>
</ul>
<hr>
<h1><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h1><ul>
<li><p>对比</p>
<ul>
<li>[CNN  更深的网络]</li>
<li>[transformer 没有局限]</li>
</ul>
</li>
<li><p>CV任务</p>
<ul>
<li>分类（Classification）</li>
<li>检测（Detection）</li>
<li>分割（Segmentation）</li>
<li>跟踪（Tracking）</li>
<li>行为识别（Action Recognition）</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol>
<li><p>《Foundational Models Defining a New Era in Vision: A Survey and Outlook》<br> <a href="https://blog.csdn.net/qq_45368632/article/details/132180645">视觉大模型的全面解析</a><br> <a href="https://zhuanlan.zhihu.com/p/655135848">基础模型定义视觉的新时代：综述和展望</a><br> <a href="https://zhuanlan.zhihu.com/p/648578542">万字长文带你全面解读视觉大模型</a></p>
</li>
<li><p>《MM-LLMs: Recent Advances in MultiModal Large Language Models》<br><a href="https://blog.csdn.net/qq_41185868/article/details/135877268">AI之MLM：《MM-LLMs: Recent Advances in MultiModal Large Language Models多模态大语言模型的最新进展》翻译与解读</a> 翻译<br><a href="https://zhuanlan.zhihu.com/p/680487634">腾讯发布的多模态大模型（MM-LLM）的最新综述、从26个最新的多模态大模型中归纳最佳实践</a><br><a href="https://zhuanlan.zhihu.com/p/680955430">多模态大模型最新完整综述 MM-LLMs</a></p>
</li>
<li><p>《A Survey on Multimodal Large Language Models》  v1 v2版本<br>3.1 <a href="https://cloud.tencent.com/developer/article/2322835">MLLM首篇综述 | 一文全览多模态大模型的前世、今生和未来</a>  v1版本<br>3.2 <a href="https://mp.weixin.qq.com/s/V5aiWUYh14q00jAn2O6VKA">多模态大语言模型全面综述：架构，训练，数据，评估，扩展，应用，挑战，机遇</a>  v2版本</p>
</li>
</ol>
<h3><span id="chat">chat</span><a href="#chat" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/670821058">[论文阅读] 双子座：一个功能强大的多模态模型系列，Gemini: A Family of Highly Capable Multimodal Models</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/663655741">166页超长论文阅读，大多模态模型的黎明：GPT-4V的初步探索，The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) [上]</a></p>
<h3><span id="other">other</span><a href="#other" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/511517344">DeepMind出手！多模态小样本打败精调</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/669757416">大模型系列04 -文本图像生成</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Training</title>
    <url>/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/636270877">【LLM】从零开始训练大模型</a> ***  未<br>     <a href="https://www.bilibili.com/video/BV1a14y1o7fr/">从零开始训练大模型</a> V<br>1xx. <a href="http://arthurchiao.art/blog/how-to-train-a-gpt-assistant-zh/">[译] 如何训练一个企业级 GPT 助手（OpenAI，2023）</a> 未<br>1xx. <a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb">chatgpt2 训练</a>  10.5   10.6</p>
<h3><span id="小模型训练-poc">小模型训练 PoC</span><a href="#小模型训练-poc" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/660759033">LLM从0开始预训练系列：1、大模型训练踩坑</a><br>1xx. <a href="http://arthurchiao.art/blog/gpt-as-a-finite-state-markov-chain-zh/">[译] GPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）</a>  未</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain  Agent</title>
    <url>/www6vHomeAIGC/2023/01/11/gptLangchainAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="langchain-agent">Langchain Agent</span><a href="#langchain-agent" class="header-anchor">#</a></h1><ul>
<li>Conversational</li>
<li>OpenAI assistants</li>
<li>OpenAI functions</li>
<li>OpenAI Multi Functions Agent</li>
<li>OpenAI tools<br>OpenAI parallel function calling (a.k.a. tool calling)</li>
<li>ReAct<br>ZeroShotReactAgent</li>
<li>Self-ask with search</li>
<li>Structured tool chat</li>
</ul>
<h1><span id="langchain-apps">Langchain Apps</span><a href="#langchain-apps" class="header-anchor">#</a></h1><h3><span id="rag-chroma-private-2">rag-chroma-private [2]</span><a href="#rag-chroma-private-2" class="header-anchor">#</a></h3><p><strong>本地 部署</strong><br>This template performs RAG with no reliance on external APIs.<br>It utilizes <strong>Ollama the LLM, GPT4All for embeddings, and Chroma for the vectorstore</strong>.</p>
<h3><span id="research-assistant-34">research-assistant [3][4]</span><a href="#research-assistant-34" class="header-anchor">#</a></h3><p>This template implements a version of<br>“GPT Researcher” that you can use as a starting point for a <strong>research agent</strong>.</p>
<h1><span id="langgraph5">LangGraph[5]</span><a href="#langgraph5" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/langchain-app">Langchain Apps</a> Project Code</li>
<li><a href="https://www.bilibili.com/video/BV1JV411F7Yj/">LangChain Agents 保姆级教程 | 动画演示 讲清 核心模块 Agents | Code 讲解 | Demo 演示</a></li>
<li><a href="https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/">“Research Assistant”: Exploring UXs Besides Chat</a></li>
<li><a href="https://www.youtube.com/watch?v=DjuXACWYkkU">Building a Research Assistant from Scratch</a> </li>
<li><a href="https://blog.langchain.dev/langgraph/">LangGraph</a></li>
<li><a href="https://github.com/www6v/gpt-researcher/">gpt-researcher</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(list)数据集</title>
    <url>/www6vHomeAIGC/2023/01/08/gptDataSet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dataset">DataSet</span><a href="#dataset" class="header-anchor">#</a></h1><ul>
<li><p>综合[平台] </p>
<ul>
<li><a href="http://opendatalab.com/">OpenDataLab</a> [1]<br>上海人工智能实验室<br><strong>数据描述语言  DSDL</strong> + 平台标准数据集</li>
<li><a href="https://www.luge.ai/#/">千言数据集</a><br>百度</li>
</ul>
</li>
<li><p>评测数据集</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405040&idx=1&sn=ad45944e78b5742337158cff80dbd9b3">再看领域微调大模型的主流基座和评测数据集：项目地址及论文指引</a></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18m4y1h7zW/">大模型时代的数据变革 - 如何设计大模型的数据配方、智能数据采集、标注、ETL</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatGLM</title>
    <url>/www6vHomeAIGC/2023/01/06/gptChatGLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://www.bilibili.com/video/BV1ju411T74Y/">第十一课：ChatGLM</a> V<br><a href="https://blog.csdn.net/v_JULY_v/article/details/129880836">ChatGLM两代的部署&#x2F;微调&#x2F;实现：从基座GLM、ChatGLM的LoRA&#x2F;P-Tuning微调、6B源码解读到ChatGLM2的微调与实现</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/625468667">【Instruction Tuning】ChatGLM 微调实战（附源码）</a></p>
<p><a href="https://github.com/www6v/transformers_tasks/blob/main/LLM/chatglm_finetune/readme.md">Finetune ChatGLM-6B</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401516&idx=1&sn=80b3cfecc9f4338b87fcd9bc91ef2465">也看支持32K上下文的ChatGLM2-6B模型：优化点简读及现有开源模型主流训练优化点概述 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ChatGLM</category>
      </categories>
      <tags>
        <tag>ChatGLM</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Instruct Tuning</title>
    <url>/www6vHomeAIGC/2023/01/06/gptInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#in-context-learning-icl-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0">In Context Learning ( ICL ) 上下文学习</a></li>
<li><a href="#instruction-learning-1">Instruction Learning [1]</a><ul>
<li><a href="#instruct-tuning-">Instruct Tuning-</a></li>
<li><a href="#instructgpt">instructGPT</a></li>
<li><a href="#chatgpt">chatGPT</a></li>
</ul>
</li>
<li><a href="#instruction-tuning">Instruction Tuning</a></li>
<li><a href="#limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="in-context-learning-icl-上下文学习">In Context Learning ( ICL ) 上下文学习</span><a href="#in-context-learning-icl-上下文学习" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/ICL.webp" class>

<ul>
<li><strong>in context learning</strong>，大意是在<strong>prompt learning的基础上，将少量有标签样本融入prompt</strong>。</li>
<li>上图的ICL模型可以理解成<strong>有监督、无训练</strong>的<strong>小样本学习</strong>。</li>
<li>但<strong>并非所有ICL都不训练</strong>。比如下图右上角的<strong>FLAN</strong>就是用instruction tuning<strong>训练参数</strong>的。</li>
</ul>
<img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/ICL-tech.webp" class>
<ul>
<li><strong>FLAN</strong>，<strong>既属于 in context learning，也属于 instruction learning</strong></li>
</ul>
<h1><span id="instruction-learning-1">Instruction Learning [1]</span><a href="#instruction-learning-1" class="header-anchor">#</a></h1><h3><span id="instruct-tuning-">Instruct Tuning-</span><a href="#instruct-tuning-" class="header-anchor">#</a></h3><pre><code>FLANv1, FLANv2
</code></pre>
<h3><span id="instructgpt">instructGPT</span><a href="#instructgpt" class="header-anchor">#</a></h3><h3><span id="chatgpt">chatGPT</span><a href="#chatgpt" class="header-anchor">#</a></h3><h1><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/instructTuning.webp" class>

<ul>
<li><p>对于已有的预训练模型，继续在多项任务（B、C、D等）上做训练，在其他任务（A）上做预测。<strong>虽然依然没见过任务A，但是根据对B、C、D等的训练，对A的效果有所提升；</strong> [1]</p>
</li>
<li><p><strong>Instruct Tuning 本质上也是Prompt Tuning</strong> [2]</p>
</li>
<li><p>研究了缩放对指令微调的影响 [3]<br>  与微调指令的任务数量有关，<strong>任务数量越多效果越好</strong><br>  与模型的大小有关，<strong>模型越大效果越好</strong></p>
</li>
<li><p>Prompt vs. Instruction Tuning  [4]<br>  Prompt是去激发语言模型的<strong>补全能力</strong>，比如给出上半句生成下半句、或者做完形填空，都还是像在做language model任务.<br>  而Instruction Tuning则是激发语言模型的<strong>理解能力</strong>，通过给出更明显的指令&#x2F;指示，让模型去理解并做出正确的action<br>  <strong>Prompt tuning</strong>都是针对<strong>一个任务</strong>的，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务，而经过<strong>Instruction Tuning多任务</strong>精调后，可以用于其他任务的zero-shot</p>
</li>
<li><p>Instruction Tuning 指令微调  [4]</p>
<ul>
<li>Self Instruction<ul>
<li>Alpaca &#x3D; LLaMA + Intruction Tuning [2]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</span><a href="#limitation-of-instruction-finetuning-2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/limitation.JPG" class>
<p>问题1.  开放性问题<br>问题2.  看图</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/619406727">各种tuning的简单逻辑解释</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1cm4y1e7Cc/">第九课：Instruct Tuning</a> *** V</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/646136859">FLANv2：大模型指令微调必看论文</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/408166011">Instruction Tuning｜谷歌Quoc V.Le团队提出又一精调范式</a></p>
</li>
</ol>
<p>1xx. <a href="https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2">June 2023, A Stage Review of Instruction Tuning</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/629461665">【LLM系列之FLAN-T5&#x2F;PaLM】Scaling Instruction-Finetuned Language Models</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/597036814">如何优化大模型的In-Context Learning效果？</a></p>
<p>1xx. <a href="https://nakaizura.blog.csdn.net/article/details/128265846">Instruction Tuning（FLAN、instructGPT、chatGPT）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Instruct-Tuning</category>
      </categories>
      <tags>
        <tag>Instruct-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Prompt Tuning</title>
    <url>/www6vHomeAIGC/2023/01/06/gptPromptTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="npl范式-1">NPL范式 [1]</span><a href="#npl范式-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptPromptTuning/npl4Paragiam.jpg" class title="4种范式">


<h1><span id="prompt-tuning-2">Prompt Tuning [2]</span><a href="#prompt-tuning-2" class="header-anchor">#</a></h1><ul>
<li>🔔 Prompt Tuning<ul>
<li>🔗 文章：The Power of Scale for Parameter-Efficient Prompt Tuning (EMNLP 2021) <a href="https://aclanthology.org/2021.emnlp-main.243/">https://aclanthology.org/2021.emnlp-main.243/</a></li>
<li>🔑关键词和摘要<ul>
<li>Keywords: Large-scale PLMs, Parameter-efficient Tuning, Prompt Tuning</li>
<li>摘要<ul>
<li>Prompt变成可学习的向量，固定PLM，微调Prompt来适配下游任务</li>
<li>PLM参数规模越大，Prompt Tuning的性能和全参数微调越接近</li>
<li>这种基于<strong>Soft Prompt</strong>的Prompt Tuning方法可以看作是<strong>Prefix Tuning的简化版本</strong>（只加在输入上）</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>方法   <ul>
<li>模型示意图：xxx</li>
<li>模型基本思路：<ul>
<li>经典分类：P(Y | X; θ)<ul>
<li>Hard Prompt: P(Y | [P;X] ; θ)<ul>
<li>Soft Prompt: P(Y | [P;X] ; θ; Δ)</li>
</ul>
</li>
</ul>
</li>
<li>Pre-Training<ul>
<li>Fine-Tuning<ul>
<li>Prompt Tuning</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实现细节：<ul>
<li>模型参数量<ul>
<li>参数量：T5 ~ T5-XXL(10B)</li>
<li>预训练：LM Adaptation</li>
</ul>
</li>
<li>Prompt长度：xxx<ul>
<li>1、5、20、100、150</li>
</ul>
</li>
<li>初始化方法：xxx<ul>
<li>随机初始化</li>
<li>使用预设文本的词向量初始化，类似于设计hard prompt，然后将hard prompt转化为soft prompt</li>
<li>使用类别词向量初始化，类似于提供选项</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实验<ul>
<li>数据集：SuperGLUE</li>
<li>xxx<ul>
<li>Prompt的规模越大，性能相对而言会越好</li>
</ul>
</li>
<li>xxx<ul>
<li>基于语义信息的初始化比随机初始化要好</li>
</ul>
</li>
<li>xxx<ul>
<li>LM Adaptation 对性能提升显著</li>
<li>Prompt Tuning还是需要大模型有较好的文本生成能力</li>
</ul>
</li>
<li>xxx<ul>
<li>模型参数规模越大，Prompt Tuning效果越好</li>
<li>10B参数时与全参数微调性能接近</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点（计算友好）<ul>
<li>大模型的<strong>微调新范式</strong></li>
<li><strong>一个中心模型服务多个下游任务</strong>，<strong>节省参数存储量</strong></li>
<li><strong>无需优化模型参数</strong>，节省优化器的计算量和存储量</li>
<li><strong>只在输入层进行操作</strong>，适合多任务场景下的计算合并</li>
</ul>
</li>
<li>缺点（性能和收敛性存在问题）<ul>
<li>Prompt Tuning的<strong>收敛速度很慢</strong></li>
<li>Prompt Tuning的模型<strong>性能不稳定</strong></li>
<li>Few-shot场景上表现不佳</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="prompt-tuning3">Prompt Tuning[3]</span><a href="#prompt-tuning3" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptPromptTuning/promptTuning.JPG" class>

<ul>
<li>Allow an <strong>additional k tunable tokens</strong> per downstream task to <strong>be prepended to the input text</strong></li>
<li>No intermediate-layer prefixes or task-specific output layers</li>
<li><strong>Freeze the entire pre-trained model</strong> and <strong>only optimize the embedding layer</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/396098543">[综述]鹏飞大神的Pre-train, Prompt, and Predict [1]</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV18P411E7VK/">清华博后带你轻松吃透Prompt Tuning顶会大模型论文</a> V</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Wg4y1K77R/">第七课：Prompt Tuning</a> ***  V  有ppt</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/395115779">近代自然语言处理技术发展的“第四范式”</a>  Prompt Learning</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/396971490">Prompt范式的缘起｜Pattern-Exploiting Training</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/400790006">Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning</a></p>
<h3><span id="p-tuning-v2">P-tuning v2</span><a href="#p-tuning-v2" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/423306405">清华P-tuning v2、谷歌SPoT｜Prompt可以超过精调了吗？</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)分布式并行Training</title>
    <url>/www6vHomeAIGC/2023/01/06/gptTrainParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C-1">分布式并行 [1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-4">数据并行 [4]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C">模型并行</a><ul>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C-3">张量并行 [3]</a></li>
<li><a href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C-3">流水线并行 [3]</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E7%BB%B4%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C5">多维混合并行[5]</a><ul>
<li><a href="#dp-pp">DP + PP</a></li>
<li><a href="#3d-%E5%B9%B6%E8%A1%8Cdp-pp-tp">3D 并行（DP + PP + TP）</a></li>
<li><a href="#zero-dp-pp-tp">ZeRO-DP + PP + TP</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%A1%86%E6%9E%B6">框架</a></li>
<li><a href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8Cpipeline-parallelism-pp4kimi">流水线并行（Pipeline Parallelism, PP）[4][kimi]</a><ul>
<li><a href="#%E5%8F%AF%E8%83%BD%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">可能存在的问题</a></li>
<li><a href="#%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96">如何优化</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="分布式并行-1">分布式并行 [1]</span><a href="#分布式并行-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/pararllelTraining.jpg" class>

<h3><span id="数据并行-4">数据并行 [4]</span><a href="#数据并行-4" class="header-anchor">#</a></h3><p>数据并行可以分为<strong>中心化方式</strong>的和<strong>无中心化方式</strong>的，对应于pytorch里面的<strong>DataParallel</strong>和<strong>DistributedDataParallel(DDP)</strong></p>
<h3><span id="模型并行">模型并行</span><a href="#模型并行" class="header-anchor">#</a></h3><p><strong>张量并行</strong>与<strong>流水线并行</strong>都属于<strong>模型并行</strong>，<br>区别在于对模型参数的切分“方向”不同：<br><strong>张量并行</strong>把模型的<strong>每层进行切分 (intra-layer)<strong>，而</strong>流水线并行</strong>则<strong>按层进行切分 (inter-layer) 并在不同设备处理</strong>。[2]</p>
<h5><span id="张量并行-3">张量并行 [3]</span><a href="#张量并行-3" class="header-anchor">#</a></h5> <img src="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/tensor.png" class>

<ul>
<li>Megatron-LM（1D）</li>
<li>Colossal-AI（2D、2.5D、3D）</li>
</ul>
<h5><span id="流水线并行-3">流水线并行 [3]</span><a href="#流水线并行-3" class="header-anchor">#</a></h5><img src="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/pipeline.png" class>

<ul>
<li>GPipe</li>
<li>PipeDream</li>
</ul>
<h3><span id="多维混合并行5">多维混合并行[5]</span><a href="#多维混合并行5" class="header-anchor">#</a></h3><h5><span id="dp-pp">DP + PP</span><a href="#dp-pp" class="header-anchor">#</a></h5><h5><span id="3d-并行dp-pp-tp">3D 并行（DP + PP + TP）</span><a href="#3d-并行dp-pp-tp" class="header-anchor">#</a></h5><h5><span id="zero-dp-pp-tp">ZeRO-DP + PP + TP</span><a href="#zero-dp-pp-tp" class="header-anchor">#</a></h5><h1><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h1><ul>
<li>Megatron-LM（张量并行）</li>
<li>DeepSpeed（Zero-DP）</li>
<li>Colossal-AI（高维模型并行，如2D、2.5D、3D）</li>
<li>Alpa（自动并行）</li>
</ul>
<h1><span id="流水线并行pipeline-parallelism-pp4kimi">流水线并行（Pipeline Parallelism, PP）[4][kimi]</span><a href="#流水线并行pipeline-parallelism-pp4kimi" class="header-anchor">#</a></h1><p>引入流水线并行（Pipeline Parallelism, PP）后可能会存在以下<strong>问题</strong>以及相应的<strong>优化方法</strong>：</p>
<h3><span id="可能存在的问题">可能存在的问题</span><a href="#可能存在的问题" class="header-anchor">#</a></h3><ol>
<li><p><strong>理论上界与朴素串行方式的差异</strong>：在理想情况下，不需要流水线并行，直接对mini-batch的样本做前向和反向操作。<strong>朴素串行方式会导致硬件利用率低，因为每个micro-batch串行逐个做前向和反向，导致大量计算资源闲置</strong>。</p>
</li>
<li><p><strong>Gpipe流水线并行的等待时间</strong>：<strong>Gpipe流水线并行需要等所有micro-batch都计算完才能执行反向过程</strong>，这会导致额外的等待时间，增加了总耗时。</p>
</li>
<li><p><strong>硬件资源的浪费</strong>：由于不同阶段的计算吞吐不同，可能会导致硬件资源的浪费。</p>
</li>
</ol>
<h3><span id="如何优化">如何优化</span><a href="#如何优化" class="header-anchor">#</a></h3><ol>
<li><p><strong>Bubble Ratio分析</strong>：定义<strong>bubble ratio</strong>来衡量流水线算法对硬件的浪费程度，值越小说明流水线效率越高。通过调整micro-batch的大小，可以减少气泡空腔的面积，提高硬件利用率。</p>
</li>
<li><p><strong>Micro-batch大小分析</strong>：通过调整micro-batch的大小b，可以使得流水线并行的额外耗时尽可能小。这通常需要通过实际测试来进行性能分析。</p>
</li>
<li><p><strong>PipeDream (Non-Interleaved 1F1B)<strong>：</strong>通过解耦同一个mini-batch的不同micro-batch，允许它们独立地进行前向和反向计算</strong>，从而减少显存的使用，并提高硬件资源的利用率。</p>
</li>
<li><p><strong>Interleaved 1F1B</strong>：将流水线切分更细，使得每个设备可以分配更多的算力，减少了每个layer的计算时间，从而减少了总耗时。</p>
</li>
<li><p><strong>Re-materialization（Checkpointing）</strong>：使用checkpoint技术，即只保留每个stage的输入activation，并在backward时从stage开头重新计算，以减少显存占用。</p>
</li>
<li><p><strong>优化通信策略</strong>：对于跨mesh的通信，使用优化策略，如scatter-gather或all-gather，以减少通信开销。</p>
</li>
<li><p><strong>动态规划求解</strong>：使用动态规划来优化子图和计算资源的划分，减少计算图中的非密集型算子，以降低搜索空间。</p>
</li>
<li><p><strong>Alpa自动化搜索</strong>：Alpa通过数学建模和优化来寻找接近最优的并行策略，可以自动化地搜索并行策略，减少人工设计的工作量。</p>
</li>
</ol>
<p>通过上述方法，可以在一定程度上优化流水线并行带来的问题，提高模型训练的效率和硬件资源的利用率。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id></span><a href="#" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">How to Train Really Large Models on Many GPUs? </a></p>
</li>
<li><p><a href="https://finisky.github.io/how-to-train-large-language-model/">大模型分布式训练的并行策略</a> *</p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/132462452">大模型并行训练指南：通俗理解Megatron-DeepSpeed之模型并行与数据并行</a>  ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/664604792">[Transformer 101系列] LLM分布式训练面面观</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/661279318">大模型分布式训练并行技术（六）-多维混合并行</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/601594836/answer/3032763174">上半年大模型遍地开花，大模型发展中有哪些经验和教训？</a> 分布式训练框架</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/598714869">大模型分布式训练并行技术（一）-概述</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/465967735">分布式训练硬核技术——通信原语</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/613196255">图解大模型训练之：流水线并行（Pipeline Parallelism），以Gpipe为例</a>  系列文章 </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/622212228">图解大模型训练之：张量模型并行(TP)，Megatron-LM</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/450854172">全网最全-超大模型+分布式训练架构和经典论文</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/350707888">大规模训练系列之技术挑战</a></p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/636488690">大模型流水线并行（Pipeline）实战</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://techdiylife.github.io/big-model-training/deepspeed/deepspeed-chat.html">第1章：DeepSpeed-Chat 模型训练实战</a>  Bili<br>      <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat">DeepSpeed-Chat</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT Lora</title>
    <url>/www6vHomeAIGC/2023/01/05/gptPEFTLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8Ellama%E7%9A%84sft">基于LLaMA的SFT</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Ebloom%E7%9A%84%E5%BE%AE%E8%B0%83">基于bloom的微调</a></li>
<li><a href="#lora-%E5%8F%82%E6%95%B0">Lora 参数</a></li>
<li><a href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">最佳实践</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#bloom">bloom</a></li>
<li><a href="#llama">LLaMA</a></li>
<li><a href="#chatglm">ChatGLM</a></li>
<li><a href="#others">others</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于llama的sft">基于LLaMA的SFT</span><a href="#基于llama的sft" class="header-anchor">#</a></h1><ul>
<li><p>版本</p>
<ul>
<li>deepspeed的版本  [3.1]</li>
<li>AutoGPTQ的版本  0.6.0 -&gt; git下载到本地安装</li>
</ul>
</li>
<li><p>代码错误</p>
<ul>
<li>use_flash_attention_2 相关的错误 [3.2]</li>
</ul>
</li>
<li><p>脚本 [3.3]</p>
<ul>
<li>modescope 下载 shakechen&#x2F;Llama-2-7b-chat-hf</li>
<li>单卡训练<br>1个epoch 差不多7小时</li>
</ul>
</li>
<li><p>checkpoint 生成文件</p>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/01/05/gptPEFTLora/llama-lora.png" class>

<img src="/www6vHomeAIGC/2023/01/05/gptPEFTLora/llama-lora1.png" class>

<ul>
<li>模型生成文件</li>
</ul>
<img src="/www6vHomeAIGC/2023/01/05/gptPEFTLora/model1.png" class>

<img src="/www6vHomeAIGC/2023/01/05/gptPEFTLora/model2.png" class>



<h1><span id="基于bloom的微调">基于bloom的微调</span><a href="#基于bloom的微调" class="header-anchor">#</a></h1><ul>
<li><p>简单基础  [2]</p>
<ul>
<li>基座模型<br>Langboat&#x2F;bloom-1b4-zh </li>
<li>数据集<br>shibing624&#x2F;alpaca-zh</li>
</ul>
</li>
<li><p>稍复杂[1]</p>
<ul>
<li>基座模型<br>bloomz-560m </li>
<li>数据集<br>ought&#x2F;raft</li>
</ul>
</li>
</ul>
<h1><span id="lora-参数">Lora 参数</span><a href="#lora-参数" class="header-anchor">#</a></h1><ul>
<li><p>LoraConfig [2]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LoraConfig( </span><br><span class="line">base_model_name_or_path=<span class="string">&#x27;Langboat/bloom-1b4-zh&#x27;</span>, </span><br><span class="line">task_type=&lt;TaskType.CAUSAL_LM: <span class="string">&#x27;CAUSAL_LM&#x27;</span>&gt;, </span><br><span class="line">inference_mode=<span class="literal">False</span>, </span><br><span class="line">r=<span class="number">8</span>, </span><br><span class="line">target_modules=&#123;<span class="string">&#x27;query_key_value&#x27;</span>&#125;, </span><br><span class="line">lora_alpha=<span class="number">32</span>, </span><br><span class="line">lora_dropout=<span class="number">0.1</span>, </span><br><span class="line">modules_to_save=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数说明 [1]</p>
<ul>
<li>task_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。</li>
<li>inference_mode：是否在推理模式下使用Peft模型。</li>
<li>r： LoRA低秩矩阵的维数。关于秩的选择，通常，使用4，8，16即可。</li>
<li>lora_alpha： LoRA低秩矩阵的缩放系数，为一个常数超参，调整alpha与调整学习率类似。</li>
<li>lora_dropout：LoRA 层的丢弃（dropout）率，取值范围为[0, 1)。</li>
<li>target_modules：要替换为 LoRA 的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样.</li>
</ul>
</li>
<li><p>target_modules [1]<br>在 PEFT 中支持的模型默认的模块名如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = &#123;</span><br><span class="line">    <span class="string">&quot;t5&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>],</span><br><span class="line">    <span class="string">&quot;mt5&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>],</span><br><span class="line">    <span class="string">&quot;bart&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt2&quot;</span>: [<span class="string">&quot;c_attn&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;bloom&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;blip-2&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>, <span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;opt&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gptj&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt_neox&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt_neo&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;bert&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;roberta&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;xlm-roberta&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;electra&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;deberta-v2&quot;</span>: [<span class="string">&quot;query_proj&quot;</span>, <span class="string">&quot;value_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;deberta&quot;</span>: [<span class="string">&quot;in_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;layoutlm&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;llama&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],  <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;chatglm&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>],  <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;gpt_bigcode&quot;</span>: [<span class="string">&quot;c_attn&quot;</span>],</span><br><span class="line">    <span class="string">&quot;mpt&quot;</span>: [<span class="string">&quot;Wqkv&quot;</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="最佳实践">最佳实践</span><a href="#最佳实践" class="header-anchor">#</a></h1><ul>
<li>秩r的大小[卢老师]<ul>
<li>模型如果是垂直类的大模型<br>eg. 私有数据<br><strong>r设置大点</strong></li>
<li>模型如果是通用类的大模型<br>eg. 运维大模型<br><strong>r设置小点</strong></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="bloom">bloom</span><a href="#bloom" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/649315197">大模型参数高效微调技术实战（五）-LoRA</a><br><a href="https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_lora_clm.ipynb">bloom Lora</a> git</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13w411y7fq/">【手把手带你实战HuggingFace Transformers-高效微调篇】LoRA 原理与实战</a> V<br> <a href="https://github.com/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora.ipynb">bloom Lora-origin</a>  <a href="https://colab.research.google.com/github/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora.ipynb">bloom Lora-origin</a> git   origin运行有问题<br> <a href="https://github.com/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora%5Bworkable%5D.ipynb">bloom Lora-modify</a>  <a href="https://colab.research.google.com/drive/1SNy35_CJOobe4AxAecMZJo4LX1TjXvTm">bloom Lora-modify</a> 修改过可以在colab运行的代码</p>
</li>
</ol>
<h3><span id="llama">LLaMA</span><a href="#llama" class="header-anchor">#</a></h3><ol start="3">
<li><a href="https://github.com/www6v/Llama2-Chinese/tree/ww-workable">Llama2-Chinese</a> 模型微调-&gt; lora SFT<br>3.1 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/requirements.txt">requirements.txt</a><br>3.2 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/train/sft/finetune_clm_lora.py#L460C18-L460C19">finetune_clm_lora.py</a>  注释掉第360行<br>3.3 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/train/sft/finetune_lora.sh">train&#x2F;sft&#x2F;finetune_lora.sh</a></li>
</ol>
<p>1xx. <a href="https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/">Using LangSmith to Support Fine-tuning</a><br>    <a href="https://colab.research.google.com/drive/1tpywvzwOS74YndNXhI8NUaEfPeqOc7ub?usp=sharing&ref=blog.langchain.dev#scrollTo=v1tOYeVGtQKJ">LangSmith + LLaMA Fine-tuning Guide</a></p>
<h3><span id="chatglm">ChatGLM</span><a href="#chatglm" class="header-anchor">#</a></h3><p>1xx. 《13-基于 ChatGLM2的 Fine-tuning 实战》 AI 大模型全栈工程师培养计划  2期<br>    <a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_lora.sh">train_lora.sh</a>  基于法律文本的chatglm的lora<br>    <a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_lora.sh">train_lora.sh</a>  基于法律文本的chatglm-2的lora<br>    <a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb">课件</a><br>    bili有相关的总结的视频</p>
<p>1xx. <a href="https://github.com/mymusise/ChatGLM-Tuning">ChatGLM-Tuning</a> 卢老师推荐</p>
<h3><span id="others">others</span><a href="#others" class="header-anchor">#</a></h3><p>1xx. <a href="https://lightning.ai/pages/community/lora-insights/">Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments</a> ***<br>     <a href="https://www.bilibili.com/video/BV16u4y1a7MH/">几百次大模型LoRA和QLoRA 微调实践的经验分享</a> V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型 排行榜</title>
    <url>/www6vHomeAIGC/2023/01/04/gptLeaderBoard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%8E%92%E8%A1%8C%E6%A6%9C">排行榜</a></li>
<li><a href="#%E4%B8%AD%E5%9B%BD%E6%8E%92%E8%A1%8C%E6%A6%9C">中国排行榜</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="排行榜">排行榜</span><a href="#排行榜" class="header-anchor">#</a></h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">HuggingFaceH 大模型排行榜</a></p>
<p><a href="https://www.promptingguide.ai/models/collection">LLM Collection</a></p>
<h3><span id="中国排行榜">中国排行榜</span><a href="#中国排行榜" class="header-anchor">#</a></h3><p><a href="https://github.com/www6v/awesome-LLMs-In-China">中国大模型 </a></p>
<ul>
<li>通用 39</li>
<li>金融 25</li>
<li>司法 8</li>
<li>法律 6</li>
<li>医学 13</li>
<li>医疗 24</li>
<li>教育 13</li>
<li>科研 17</li>
<li>工业 23</li>
<li>政务 12</li>
<li>运维 7</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>leaderBoard</category>
      </categories>
      <tags>
        <tag>leaderBoard</tag>
      </tags>
  </entry>
  <entry>
    <title>垂类大模型</title>
    <url>/www6vHomeAIGC/2023/01/04/gptDomain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#domain-specialization-0">Domain Specialization [0]</a><ul>
<li><a href="#%E6%80%BB%E7%BB%93-chatmind">总结 [chatmind]</a></li>
</ul>
</li>
<li><a href="#%E9%A2%86%E5%9F%9F%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B-1">领域微调模型 [1]</a></li>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B9-2">关注点 [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="domain-specialization-0">Domain Specialization [0]</span><a href="#domain-specialization-0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/04/gptDomain/domain.JPG" class>

<p>LLM（Large Language Models）的领域专业化可以理解为将广泛训练的通用LLM调整到特定领域内以实现最佳操作。为了应对第1节中提到的领域专业化的三个挑战，LLM领域专业化的方法可以分为三类：外部增强、提示设计和模型微调。这些类别对应了对LLM的不同访问级别的假设，即无访问（黑盒）、部分访问（灰盒）和完全访问（白盒）。黑盒假设通常表示我们只能访问模型API（例如ChatGPT），除了生成的输出之外不知道任何信息；灰盒假设表示我们有限的信息（例如GPT-3 API中生成的标记的概率），这些信息可以指导我们设计和微调一个合适的提示，以更好地引出领域知识；白盒假设表示我们完全可以访问LLM（例如LLaMA及其变体），包括参数设置、训练数据和模型架构。</p>
<p>除了基于LLM可访问性的分类法之外，根据使用的训练策略，可以将LLM领域专业化方法分类为以下几种：使用领域特定数据对现有模型进行微调，从头开始为特定领域训练模型，或者使用混合训练策略。另一个分类法可以基于干预级别：预训练干预涉及修改预训练过程以鼓励领域特定知识，微调干预涉及在微调阶段进行调整，推理时干预涉及修改模型在实际应用中的行为以生成更多领域特定的输出。此外，也可以基于评估和反馈机制来建立分类法：固定评估设置了一个恒定的基准，动态评估涉及使用不断变化的基准进行持续性能评估，基于用户反馈的评估使用直接用户输入作为调整模型响应的信号。</p>
<p>在这份调查中，我们根据LLM的可访问性对现有方法进行分类，并在图2中概述了每种方法。具体来说，1）外部增强（黑盒）不一定需要访问LLM的内部参数空间，使得对资源有限的用户（如计算资源、领域特定数据）最易于访问。如图2（b）所示，通过使用外部资源或工具，将领域特定知识合并到输入提示、生成的输出或两者中，有效地调整LLM的性能而不修改其内部结构。2）提示设计（灰盒）通过访问LLM的梯度或损失值设计各种类型的提示，从而对模型的行为进行更精细的控制。3）模型微调（白盒）需要最多的访问和资源，因为它涉及更新LLM的参数，直接将领域特定知识纳入模型中（图2（d））。</p>
<p>不同类别方法之间的关系：<br>• 不同的专业化水平：每种方法在不同的专业化水平上操作（即黑盒、灰盒和白盒）。使用外部知识进行增强提供了领域特定信息的集中注入，而提示工程则在输入层面上塑造模型的推理过程。模型微调修改了LLM的内部参数，导致模型行为发生更深刻的变化。</p>
<p>• 权衡：这些方法在计算成本、实施便利性和泛化能力方面存在不同的权衡。使用外部信息进行增强和设计特定任务的指令通常比LLM的知识更新计算成本低，但可能无法达到相同水平的性能改进。模型微调和神经适配器可以提供更实质性的性能提升，但在实施上可能更具挑战性，并且如果出现过拟合，可能会导致泛化能力降低。</p>
<p>• 互补性：这三种方法可以独立使用或组合使用，以在领域特定任务上实现更好的性能。例如，可以将外部知识与经过微调的LLM结合起来，充分利用专业化知识和优化的参数。同样，精心设计的提示可以与神经适配器一起使用，引导模型的输出同时利用新学习的领域特定知识。</p>
<h3><span id="总结-chatmind">总结 [chatmind]</span><a href="#总结-chatmind" class="header-anchor">#</a></h3><ul>
<li>LLM（Large Language Models）领域专业化<ul>
<li>领域专业化的定义<ul>
<li>将广泛训练的通用LLM调整到特定领域内以实现最佳操作</li>
</ul>
</li>
<li>领域专业化的挑战<ul>
<li>挑战一：领域专业化的需求</li>
<li>挑战二：领域专业化的数据稀缺性</li>
<li>挑战三：领域专业化的知识缺乏</li>
</ul>
</li>
<li>LLM领域专业化的方法<ul>
<li>方法一：<strong>外部增强</strong>  黑盒 <ul>
<li>利用外部资源来增强LLM的领域专业化</li>
<li>例如，使用特定领域的数据集进行预训练</li>
</ul>
</li>
<li>方法二：<strong>提示设计</strong> 灰盒<ul>
<li>设计适当的提示来引导LLM生成符合特定领域需求的内容</li>
<li>根据领域知识和需求，精心设计提示</li>
</ul>
</li>
<li>方法三：<strong>模型微调</strong> 白盒<ul>
<li>对预训练的LLM进行微调，使其更适应特定领域的任务和数据</li>
<li>通过在特定领域的数据集上进行迭代训练，优化模型性能</li>
</ul>
</li>
</ul>
</li>
<li>LLM访问级别的假设<ul>
<li><strong>无访问（黑盒）</strong>假设<ul>
<li>只能访问模型API，无法获取其他信息</li>
<li>仅通过生成的输出来进行操作</li>
</ul>
</li>
<li><strong>部分访问（灰盒）</strong>假设<ul>
<li>拥有有限的信息，如生成的标记概率</li>
<li>可根据这些信息设计提示来引导LLM生成符合领域知识的内容</li>
</ul>
</li>
<li><strong>完全访问（白盒）</strong>假设<ul>
<li>可以完全访问LLM，获取其所有信息</li>
<li>可以根据需要进行修改和优化</li>
</ul>
</li>
<li>不同访问级别的假设对应不同的方法和操作方式</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="领域微调模型-1">领域微调模型 [1]</span><a href="#领域微调模型-1" class="header-anchor">#</a></h1><ul>
<li>注入领域知识，分成三种：<ul>
<li>继续预训练注入</li>
<li>微调注入以及</li>
<li>外挂注入</li>
</ul>
</li>
</ul>
<h1><span id="关注点-2">关注点 [2]</span><a href="#关注点-2" class="header-anchor">#</a></h1><ul>
<li><strong>领域相关数据</strong>  是Continue PreTrain的关键</li>
<li><strong>混合通用数据</strong>以<strong>缓解模型遗忘通用能力</strong></li>
<li><strong>领域模型Continue PreTrain</strong>时可以同步加入<strong>SFT数据</strong>，即MIP，Multi-Task Instruction PreTraining</li>
<li>仅用SFT做领域模型时，<strong>资源有限</strong>就用在<strong>Chat模型基础上训练</strong>，<strong>资源充足</strong>就在<strong>Base模型上训练</strong>。（<strong>资源&#x3D;数据+显卡</strong>）<br>+在Chat模型上进行SFT时，请一定<strong>遵循Chat模型原有的系统指令&amp;数据输入格式</strong>。</li>
<li>领域评测集时必要内容，建议有两份，一份选择题形式自动评测、一份开放形式人工评测。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p>《Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey》<br><a href="https://zhuanlan.zhihu.com/p/635480023">目前有哪些方式训练一个领域的大语言模型？ Beyond One-Model-Fits-All A Survey of Domain Specialization LLM</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401405&idx=1&sn=59baf4a22d9a9abeb42599ac91e11a79">领域微调大模型入局的自我和解：领域微调大模型若一定要做，则务必想的若干个前提条件 </a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/648798461">领域大模型-训练Trick&amp;落地思考</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642611747">垂直领域大模型的一些思考及开源模型汇总</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403459&idx=2&sn=0219fc098c208e36cd32940e71089fd2">层出不穷的垂域微调大模型非最全汇总：12大领域、57个领域微调模型概述及对垂直行业问答的一些讨论 </a> 领域模型集合<br>    <a href="https://github.com/www6v/Awesome-Domain-LLM">Awesome-Domain-LLM</a><br>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/131550529?spm=1001.2014.3001.5502">医疗金融法律大模型：从ChatDoctor到BloombergGPT&#x2F;FinGPT&#x2F;FinBERT、ChatLaw&#x2F;LawGPT_zh</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400666&idx=1&sn=bc47e8c4eca6fc4baaded42fa3c6bd77">再谈垂直领域大模型及今日前沿速递：金融领域FinBERT、BloombergGPT以及法律领域微调模型LawGPT_zh</a></p>
<p>1xx. <a href="/www6vHomeAIGC/2023/01/04/gptLeaderBoard/" title="大模型 排行榜">大模型 排行榜</a> self<br>1xx. <a href="https://github.com/www6v/Awesome-Domain-LLM">Awesome-Domain-LLM</a>  git 全</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/jgyIOOzRWAgilcW4HfufNQ">行业大模型落地的一些有趣调研总结：兼看大模型RAG问答四大技术综述 </a>  腾讯 行业大模型调研报告 ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>NL2SQL</title>
    <url>/www6vHomeAIGC/2023/01/03/gptNL2SQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://github.com/www6v/NL2SQL">https://github.com/www6v/NL2SQL</a><br>1xx. <a href="https://github.com/www6v/nl2sql-">https://github.com/www6v/nl2sql-</a><br>1xx. <a href="https://blog.langchain.dev/llms-and-sql/">LLMs and SQL</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/640580808">大模型与数据科学：从Text-to-SQL 开始（一）</a> 多款产品</p>
<p>百度千帆-ppt<br>QCon-ppt</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUxNzk5MTU3OQ==&mid=2247487028&idx=1&sn=7b6767878b7f6b891fc69e408f248ef1">语义解析 (Text-to-SQL) 技术研究及应用 上篇 </a><br>1xx. <a href="https://mp.weixin.qq.com/s/5lTLW5OOuRMo2zjbzMxr_Q">语义解析 (Text-to-SQL) 技术研究及应用 下篇 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/670509396">LLM在中文Text2SQL的实践</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/673474672">LLM在中文Text2SQL任务上的优化V2.0</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/670913902">LLM在中文Text2SQL任务上的优化V1.0</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402400&idx=1&sn=fe122657b35f27090aaca9c144d1d23b">也看大模型与数据库查询分析的落地结合：C3 Text2SQL方案及Data-Copilot数据自动化编排机制的实现思想阅读 </a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/668557045">C3: Zero-shot Text-to-SQL with ChatGPT笔记</a><br>1xx. <a href="https://github.com/bigbigwatermalon/C3SQL">C3SQL  </a> git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/Ffm8ooH8je2553IcLkJBmw">大模型Text2SQL主流数据集及可用实践项目：兼看利用大模型进行5W1H新闻要素提取 </a><br>   问题2:关于Text2sql当前的可用项目及数据集<br>   <a href="https://github.com/www6v/Awesome-Text2SQL">https://github.com/www6v/Awesome-Text2SQL</a></p>
<h3><span id="project">Project</span><a href="#project" class="header-anchor">#</a></h3><p><a href="https://github.com/eosphoros-ai/DB-GPT">DB-GPT Repo</a> git<br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487442&idx=1&sn=7889420553058119506bd677298e69d4">开源 Text-to-SQL 工具哪家强？Vanna 让 SQL 小白也能轻松玩转数据分析！</a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487419&idx=1&sn=31fb6947c97793bc58b645444a1b587c">YC孵化的Text-to-SQL未来之星：Defog开源 SQLCoder模型，打造企业级数据分析利器 </a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487328&idx=1&sn=dd20578ab376aaa11fee526d4787ce58">WrenAI：开源Text-to-SQL引擎让 SQL触手可及，数据分析的“GPT”时刻来了？</a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487260&idx=1&sn=53b5d036c9f2e039d4fc26c8053355b8">Dataherald 核心 Text-to-SQL 引擎全面开源！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NL2SQL</category>
      </categories>
      <tags>
        <tag>NL2SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)推理优化</title>
    <url>/www6vHomeAIGC/2023/01/01/gptInference/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E4%BC%98%E5%8C%96">推理 优化</a><ul>
<li><a href="#overview2">overview[2]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-1">模型压缩 [1]</a></li>
<li><a href="#kv-cache">KV Cache</a></li>
</ul>
</li>
<li><a href="#%E6%8E%A8%E7%90%86-10">推理 [10]</a><ul>
<li><a href="#llm-algorithmiceval-survey">LLM Algorithmic&#x2F;Eval Survey</a></li>
<li><a href="#llm-traininference-framework">LLM Train&#x2F;Inference Framework</a></li>
<li><a href="#continuousin-flight-batching">Continuous&#x2F;In-flight Batching</a></li>
<li><a href="#weightactivation-quantizecompress">Weight&#x2F;Activation Quantize&#x2F;Compress</a></li>
<li><a href="#ioflops-awaresparse-attention">IO&#x2F;FLOPs-Aware&#x2F;Sparse Attention</a></li>
<li><a href="#kv-cache-schedulingquantizedropping">KV Cache Scheduling&#x2F;Quantize&#x2F;Dropping</a></li>
<li><a href="#promptcontext-compression">Prompt&#x2F;Context Compression</a></li>
<li><a href="#long-context-attentionkv-cache-optimization">Long Context Attention&#x2F;KV Cache Optimization</a></li>
<li><a href="#parallel-decodingsampling">Parallel Decoding&#x2F;Sampling</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
<li><a href="#awesome-llm-inference">Awesome-LLM-Inference</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-优化">推理 优化</span><a href="#推理-优化" class="header-anchor">#</a></h1><h3><span id="overview2">overview[2]</span><a href="#overview2" class="header-anchor">#</a></h3><p>有几种方法可以在内存中<strong>降低推理成本</strong>或&#x2F;和<strong>加快推理速度</strong>。</p>
<ul>
<li>应用各种<strong>并行处理方式</strong>，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。</li>
<li><strong>内存卸载</strong>，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。</li>
<li><strong>智能批处理策略</strong>；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。</li>
<li><strong>网络压缩技术</strong>，如<strong>修剪、量化、蒸馏</strong>。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。</li>
<li>针对目标模型架构的特定改进。许多<strong>架构变化</strong>，特别是针对注意力层的变化，有助于提高Transformer解码速度。</li>
</ul>
<h3><span id="模型压缩-1">模型压缩 [1]</span><a href="#模型压缩-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/01/gptInference/compress.png" class>

<ul>
<li>剪枝（Pruning）</li>
<li>知识蒸馏（Knowledge Distillation，KD）</li>
<li>量化（Quantization）</li>
<li>低秩分解（Low-Rank Factorization）</li>
</ul>
<h3><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h3><h1><span id="推理-10">推理 [10]</span><a href="#推理-10" class="header-anchor">#</a></h1><h3><span id="llm-algorithmicx2feval-survey">LLM Algorithmic&#x2F;Eval Survey</span><a href="#llm-algorithmicx2feval-survey" class="header-anchor">#</a></h3><p><a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Efficient Large Language Models: A Survey</a></p>
<h3><span id="llm-trainx2finference-framework">LLM Train&#x2F;Inference Framework</span><a href="#llm-trainx2finference-framework" class="header-anchor">#</a></h3><p>Megatron-LM<br>vLLM ***<br>TensorRT-LLM ***<br>DeepSpeed-FastGen 2x vLLM?  ***<br>PETALS</p>
<h3><span id="continuousx2fin-flight-batching">Continuous&#x2F;In-flight Batching</span><a href="#continuousx2fin-flight-batching" class="header-anchor">#</a></h3><p>Continuous Batching<br>In-flight Batching</p>
<h3><span id="weightx2factivation-quantizex2fcompress">Weight&#x2F;Activation Quantize&#x2F;Compress</span><a href="#weightx2factivation-quantizex2fcompress" class="header-anchor">#</a></h3><p>ZeroQuant<br>GPTQ<br>SmoothQuant<br>AWQ<br>SparQ</p>
<h3><span id="iox2fflops-awarex2fsparse-attention">IO&#x2F;FLOPs-Aware&#x2F;Sparse Attention</span><a href="#iox2fflops-awarex2fsparse-attention" class="header-anchor">#</a></h3><p>MQA<br>FlashAttention ***<br>GQA</p>
<h3><span id="kv-cache-schedulingx2fquantizex2fdropping">KV Cache Scheduling&#x2F;Quantize&#x2F;Dropping</span><a href="#kv-cache-schedulingx2fquantizex2fdropping" class="header-anchor">#</a></h3><p>PagedAttention ***<br>TensorRT-LLM KV Cache FP8</p>
<h3><span id="promptx2fcontext-compression">Prompt&#x2F;Context Compression</span><a href="#promptx2fcontext-compression" class="header-anchor">#</a></h3><p>Selective-Context<br>LLMLingua ***<br>LongLLMLingua ***</p>
<h3><span id="long-context-attentionx2fkv-cache-optimization">Long Context Attention&#x2F;KV Cache Optimization</span><a href="#long-context-attentionx2fkv-cache-optimization" class="header-anchor">#</a></h3><p>RingAttention<br>KVQuant<br>RAGCache<br>KCache</p>
<h3><span id="parallel-decodingx2fsampling">Parallel Decoding&#x2F;Sampling</span><a href="#parallel-decodingx2fsampling" class="header-anchor">#</a></h3><p>Speculative Sampling ***</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://mp.weixin.qq.com/s/glPPSqHjsnDjC0DZSuuPzA">一文探秘LLM应用开发(13)-模型部署与推理(优化理论) </a> </p>
</li>
<li><p><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization </a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/656485997">大语言模型推理性能优化综述</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a> *** </p>
<p>1xx. 《A Survey on Efficient Inference for Large Language Models》<br><a href="https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg">3万字详细解析清华大学最新综述工作：大模型高效推理综述</a> 翻译<br><a href="https://mp.weixin.qq.com/s/V1dXQquifP7uHGU7QxP-Lg">无穹Paper | 如何加速大模型推理？一图读懂大语言模型高效推理技术</a> </p>
<h3><span id="awesome-llm-inference">Awesome-LLM-Inference</span><a href="#awesome-llm-inference" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://github.com/DefTruth/Awesome-LLM-Inference">Awesome-LLM-Inference Repo</a> git<br>1xx. <a href="https://zhuanlan.zhihu.com/p/669777159">[Awesome-LLM-Inference]🔥第三期：30篇，LLM推理论文集-500页PDF</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/658091768">[Awesome-LLM-Inference]🔥第二期: 20篇，LLM推理论文集-300页PDF</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/693680304">[LLM推理优化]🔥100+篇: 大模型推理各方向新发展整理</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA</title>
    <url>/www6vHomeAIGC/2023/01/01/gptLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#llama-%E6%9E%B6%E6%9E%84architecture1">LLaMA 架构（Architecture）[1]</a><ul>
<li><a href="#%E9%A2%84%E5%BD%92%E4%B8%80%E5%8C%96pre-normalization%E5%8F%97-gpt3-%E5%90%AF%E5%8F%91">预归一化（Pre-normalization）：受 GPT3 启发</a></li>
<li><a href="#swiglu-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%8F%97-palm-%E5%90%AF%E5%8F%91">SwiGLU 激活函数：受 PaLM 启发</a></li>
<li><a href="#%E6%97%8B%E8%BD%AC%E5%B5%8C%E5%85%A5rotary-embeddings%E5%8F%97-gptneo-%E5%90%AF%E5%8F%91">旋转嵌入（Rotary Embeddings）：受 GPTNeo 启发</a></li>
</ul>
</li>
<li><a href="#llama2">LLaMA2</a><ul>
<li><a href="#%E6%AF%94%E8%BE%83-21">比较 [21]</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E7%9A%84%E6%9E%B6%E6%9E%8410">训练的架构[10]</a></li>
<li><a href="#%E4%B8%8Ellama%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB11">与LLaMA主要区别：[11]</a></li>
</ul>
</li>
<li><a href="#llama3">LLaMA3</a><ul>
<li><a href="#%E6%8A%80%E6%9C%AF%E7%82%B9-20">技术点 [20]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#llama">LLaMA</a></li>
<li><a href="#llama2-1">LLaMA2</a></li>
</ul>
</li>
<li><a href="#llama3-1">LLaMA3</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="llama-架构architecture1">LLaMA 架构（Architecture）[1]</span><a href="#llama-架构architecture1" class="header-anchor">#</a></h1><h3><span id="预归一化pre-normalization受-gpt3-启发">预归一化（Pre-normalization）：受 GPT3 启发</span><a href="#预归一化pre-normalization受-gpt3-启发" class="header-anchor">#</a></h3><p>为了提高<strong>训练稳定性</strong>，我们对每个 Transformer sub-layer 的<strong>输入</strong>进行归一化，而不是对<strong>输出</strong>进行归一化。 这里使用由 Zhang 和 Sennrich（2019）提出的 RMSNorm 归一化函数。</p>
<h3><span id="swiglu-激活函数受-palm-启发">SwiGLU 激活函数：受 PaLM 启发</span><a href="#swiglu-激活函数受-palm-启发" class="header-anchor">#</a></h3><p>用 SwiGLU 激活函数替换 ReLU 非线性，该函数由 Shazeer（2020）提出，目的是<strong>提升性能</strong>。 但我们使用的维度是 <code>2/3 * 4d</code>，而不是 PaLM 中的 <code>4d</code>。</p>
<h3><span id="旋转嵌入rotary-embeddings受-gptneo-启发">旋转嵌入（Rotary Embeddings）：受 GPTNeo 启发</span><a href="#旋转嵌入rotary-embeddings受-gptneo-启发" class="header-anchor">#</a></h3><p>去掉了绝对位置嵌入（absolute positional embeddings），并在每个网络层中添加旋转位置嵌入（rotary positional embeddings，RoPE）。 RoPE 由 Su 等（2021）提出。</p>
<h1><span id="llama2">LLaMA2</span><a href="#llama2" class="header-anchor">#</a></h1><h3><span id="比较-21">比较 [21]</span><a href="#比较-21" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/01/gptLlama/llama2.png" class>

<h3><span id="训练的架构10">训练的架构[10]</span><a href="#训练的架构10" class="header-anchor">#</a></h3><p>Llama2采用了Llama 1中的大部分预训练设置和模型架构。使用RMSNorm应用预归一化，使用SwiGLU激活函数和旋转位置嵌入。</p>
<p>具体的，使用AdamW优化器进行训练，使用余弦学习率方式来动态调整学习率，预热2000步，并将最终学习率衰减到峰值学习率的10%，并使用0.1的权重衰减和1.0的梯度裁剪。</p>
<p>与Llama 1的主要架构差异包括增加了<strong>上下文长度【两倍关系】</strong>和**分组查询关注(GQA)**。</p>
<p>分词器方面，使用与Llama 1相同的标记器，采用字节对编码(BPE)算法，使用sentencepece的实现。值得注意的是，<strong>与Llama 1一样，将所有数字拆分为单个数字，并使用字节来分解未知的UTF-8字符。总词汇表大小为32k。</strong></p>
<h3><span id="与llama主要区别11">与LLaMA主要区别：[11]</span><a href="#与llama主要区别11" class="header-anchor">#</a></h3><ul>
<li>更多的训练数据<br> 1.4T -&gt; 2T</li>
<li>更⻓的上下文窗口<br> 2k-&gt; 4k</li>
<li>GQA技术</li>
<li>完整的RLHF链条</li>
</ul>
<h1><span id="llama3">LLaMA3</span><a href="#llama3" class="header-anchor">#</a></h1><h3><span id="技术点-20">技术点 [20]</span><a href="#技术点-20" class="header-anchor">#</a></h3><ul>
<li><p>GQA<br>  引入了<strong>Grouped Query Attention (GQA)<strong>，这可以</strong>减少推理过程中的KV缓存大小</strong>，增加推理效率<br>  性能提升主要来自于<strong>kv cache的size减小</strong>，那么kv cache占用的显存就变小，那么我们LLM serving可以处理的请求数量就<strong>更多</strong>，batchsize<strong>更大</strong>，吞吐量就<strong>变大</strong></p>
</li>
<li><p>编码词表 BBPE<br>BBPE：将句子构建为<strong>UTF-8编码字节序</strong>列而非字符序列，再进行<strong>BPE</strong>进行学习<br>Token词典从LLAMA-2的32K拓展到了<strong>128K</strong>，以增加编码效率</p>
</li>
<li><p>数据合成</p>
<ul>
<li>用 llama2 <strong>半自动地合成</strong>一些数据<br>例如生成给定话题下面的一些样例数据，或者加一个系统提示让它对给定的文本在某些维度上打分, 然后训练一些<strong>分类&#x2F;打分器</strong>，去过滤预训练语料。</li>
</ul>
</li>
<li><p>模型的次优化</p>
<ul>
<li>Sub-optimal Chinchilla Law<br>.一个是<strong>固定住模型大小</strong>，持续增加训练数据，模型效果会持续变好<br>另外一个是<strong>固定住训练数据量</strong>，那么你持续放大模型参数规模，同样的，模型效果也会越来愈好</li>
</ul>
</li>
<li><p>DPO训练方法</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="llama">LLaMA</span><a href="#llama" class="header-anchor">#</a></h3><ol>
<li><a href="http://arthurchiao.art/blog/llama-paper-zh/">[译][论文] LLaMA：开放和高效的基础语言模型集</a><br>1xx. <a href="https://www.bilibili.com/video/BV1nN41157a9/">第十五课：LLaMA</a>  *** 华为  V<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399298&idx=1&sn=dd83c4f42c68a89f8199f990e7570586">Meta最新语言模型LLaMA论文研读：小参数+大数据的开放、高效基础语言模型阅读笔记 </a></li>
</ol>
<h3><span id="llama2">LLaMA2</span><a href="#llama2" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401927&idx=1&sn=3dddcb5c1d8b3c246a8b7529502fdcf0">也谈凌晨刷屏的Llama2开源可商用模型：从其数据构造、模型架构和评估方式等方面的一些总结与发现 </a></li>
<li><a href="https://www.bilibili.com/video/BV1qQ4y1t7Aj/">【对话引擎应用】千帆中文增强Llama2提升大模型对话指令遵循能力</a>  百度<br>1xx. <a href="https://zhuanlan.zhihu.com/p/649756898">Llama 2详解</a>  ***<br><a href="https://www.bilibili.com/video/BV12h4y1N7C8/">Llama 2 模型结构解析</a> *** V<br>1xx. <a href="https://www.bilibili.com/video/BV1Me411z7ZV/">第十六课：LLaMA2</a> *** 华为  V<br>1xx. <a href="http://arthurchiao.art/blog/llama2-paper-zh/">[译][论文] LLaMA 2：开放基础和微调聊天模型</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/644671690">Llama2技术细节&amp;开源影响</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401959&idx=1&sn=582fa45cd00035bac621336f47cce252">再看Llama2的实际体验与民间评测：从现有公开在线测试地址到几个测试例子看初步效果分析 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402689&idx=1&sn=a1847ea36cde32bd386f85f41cf197b9">关于大模型行业问答落地的技术方案再思考：兼看Llama2中文汉化的成立性、实现路线以及代表项目</a></li>
</ol>
<h1><span id="llama3">LLaMA3</span><a href="#llama3" class="header-anchor">#</a></h1><ol start="20">
<li>《LLAMA3的一些改造点》 卢老师</li>
<li><a href="https://mp.weixin.qq.com/s/4FtVlX4wlRVwti7OVs3Zlg">从Llama-1到Llama-3 </a><br>1xx.  <a href="https://mp.weixin.qq.com/s/Qrjye1ZAe0NJzWYLFa4plA">大模型之战的新序幕-从Llama3之后</a></li>
</ol>
<p>1xx. <a href="https://llama.family/">Llama中文社区</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Agent</title>
    <url>/www6vHomeAIGC/2023/01/01/gptAgentPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E5%BE%AE%E8%B0%83%E7%9A%84agent-function-call12">基于微调的Agent-function call[1][2]</a></li>
<li><a href="#assistant-api-3">Assistant API [3]</a><ul>
<li><a href="#assistant-api%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D">Assistant API功能介绍</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
<li><a href="#lagent-agentlego4">Lagent &amp; AgentLego[4]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于微调的agent-function-call12">基于微调的Agent-function call[1][2]</span><a href="#基于微调的agent-function-call12" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/01/gptAgentPractice/dirs.JPG" class>

<img src="/www6vHomeAIGC/2023/01/01/gptAgentPractice/xtuner-agent.png" class>

<h1><span id="assistant-api-3">Assistant API [3]</span><a href="#assistant-api-3" class="header-anchor">#</a></h1><h3><span id="assistant-api功能介绍">Assistant API功能介绍</span><a href="#assistant-api功能介绍" class="header-anchor">#</a></h3><p>从功能实现层面来说，Assistant API是截至目前最完整、性能最强大的AI应用开发API，具体功能如下：</p>
<ul>
<li>首先，Assistant API前所未有的能够<strong>调用OpenAI各模型的各项能力</strong>，包括可以调用Chat系列模型（即GPT系列模型）完成文本对话、调用DALL·E 3进行绘图、调用GPT-4-vision进行图像识别、以及调用Text-to-Speech模型进行语音转文字等，并且支持在一轮对话中调用不同模型；</li>
<li>其次，Assistant API还<strong>内置了代码解释器功能（Code interpreter）和海量文本信息提取功能（Knowledge retrieval）</strong>同时也一如既往支持借助<strong>Function calling</strong>进行模型功能层面拓展，此外，非常重要的是，Assistant API还支持在一轮对话中调用多个工具；</li>
<li>其三，此外对于开发者非常友好的一点是，Assistant API最小运行单元为持久化的线程对象（persistent Threads），因此在实际运行Assistant API时，不仅能可以精确控制每一步的执行过程，同时persistent Threads也会保留每轮对话的核心信息，并且当超出模型接收信息最大上下文限制时能够自动删除早期信息，从而实现对模型短期记忆的合理管理；</li>
<li>其四，Assistant API还能够直<strong>接连接OpenAI在线文档库</strong>，即如果用户将外部文档保存在OpenAI云空间内，则可以在调用Assistant API时实时访问文档库中的任意文件，甚至可以在不同线程中调用不同的文档。而在借助Assistant API的Knowledge retrieval功能，则可以让大模型实时获取这些文件信息，并且合理管理短期记忆；</li>
</ul>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><h1><span id="lagent-amp-agentlego4">Lagent &amp; AgentLego[4]</span><a href="#lagent-amp-agentlego4" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/InternLM/tutorial/blob/main/xtuner/README.md">xtuner 实战</a><br>4【补充】用 MS-Agent 数据集 赋予 LLM 以 Agent 能力</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1yK4y1B75J/">(4)XTuner 大模型单卡低成本微调实战</a> V</p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/tree/master/basic/%E4%B9%9D%E5%A4%A9Hector/Assistant%20API%E8%AF%A6%E8%A7%A3%E4%B8%8EAgent%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98">Assistant API详解与Agent开发实战-九天Hector</a></p>
</li>
<li><p><a href="https://github.com/InternLM/Tutorial/tree/camp2/agent">Lagent &amp; AgentLego 智能体应用搭建</a><br><a href="https://github.com/InternLM/Tutorial/blob/camp2/agent/lagent.md">Lagent：轻量级智能体框架</a><br><a href="https://github.com/InternLM/Tutorial/blob/camp2/agent/agentlego.md">AgentLego：组装智能体“乐高”</a></p>
</li>
</ol>
<p>1xx. <a href="https://qwenlm.github.io/zh/blog/qwen-agent-2405/">使用Qwen-Agent将上下文记忆扩展到百万量级</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)RAG</title>
    <url>/www6vHomeAIGC/2022/12/31/gptRAGPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#data-processing17">Data processing[17]</a></li>
<li><a href="#%E5%8C%BB%E7%96%97%E9%97%AE%E7%AD%94rag20">医疗问答RAG[20]</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#chuck">chuck</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F">数据格式</a></li>
<li><a href="#%E6%94%B9%E5%86%99query">改写query</a></li>
<li><a href="#%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B">召回模型</a></li>
<li><a href="#%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B">排序模型</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F">索引方式</a></li>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#%E5%8C%BB%E7%96%97%E9%97%AE%E7%AD%94">医疗问答</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="data-processing17">Data processing[17]</span><a href="#data-processing17" class="header-anchor">#</a></h1><p>长文本   变成   QA pair</p>
<ul>
<li>规则匹配</li>
<li><strong>利用LLM抽取</strong></li>
<li>人工处理</li>
</ul>
<h1><span id="医疗问答rag20">医疗问答RAG[20]</span><a href="#医疗问答rag20" class="header-anchor">#</a></h1><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/31/gptRAGPractice/arch.JPG" class>

<h3><span id="chuck">chuck</span><a href="#chuck" class="header-anchor">#</a></h3><p><strong>段落</strong><br>句子<br>token</p>
<h3><span id="数据格式">数据格式</span><a href="#数据格式" class="header-anchor">#</a></h3><p>{“id”: xxx, “病情描述”: “xxx”,  “治疗方案”: “xxx” }</p>
<h3><span id="改写query">改写query</span><a href="#改写query" class="header-anchor">#</a></h3><ul>
<li>HyDE</li>
<li>RAG Fusion -&gt; Generate Similar query<br>用户的查询不精准，要扩充query, 用大模型改写</li>
</ul>
<h3><span id="召回模型">召回模型</span><a href="#召回模型" class="header-anchor">#</a></h3><ul>
<li><p>bert模型</p>
<ul>
<li><strong>sbert</strong><ul>
<li><strong>2个bert模型</strong>，共享参数，s1,s2向量化后做<strong>相似度</strong>计算</li>
<li><strong>速度快</strong></li>
<li>相似度<br>欧式距离</li>
</ul>
</li>
<li>在百万语料上训练<ul>
<li><strong>语料格式</strong><br>[s1][s2] 0 - 无关<br>[s1][s2] 1-类似</li>
</ul>
</li>
</ul>
</li>
<li><p>根据query, 召回id和value整条记录</p>
</li>
</ul>
<h3><span id="排序模型">排序模型</span><a href="#排序模型" class="header-anchor">#</a></h3><ul>
<li>bert模型<ul>
<li>1个bert模型</li>
<li><strong>速度慢</strong></li>
<li><strong>格式</strong><ul>
<li>query[sep]s2  -&gt; 经过softmax，产生2分类，0-1</li>
</ul>
</li>
<li>也要训练<ul>
<li>同<strong>召回模型训练方式</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="索引方式">索引方式</span><a href="#索引方式" class="header-anchor">#</a></h3><ul>
<li>树索引</li>
<li>知识图谱的索引</li>
</ul>
<h3><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h3><ul>
<li>综合归纳的作用</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="17">
<li>&lt;&lt;大模型结合 RAG 构建客服场景自动问答系统&gt;&gt;  NVIDIA大模型日系列活动</li>
</ol>
<h3><span id="医疗问答">医疗问答</span><a href="#医疗问答" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1fW421P7u6?p=5">基于百万语料的医疗RAG项目</a> v</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Retrievers</title>
    <url>/www6vHomeAIGC/2022/12/31/gptRetrievers/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#langchain-retrievers10">Langchain Retrievers[10]</a><ul>
<li><a href="#multiqueryretriever">MultiQueryRetriever</a></li>
<li><a href="#contextual-compression">Contextual compression</a></li>
<li><a href="#ensemble-retriever">Ensemble Retriever</a></li>
<li><a href="#multivector-retriever">MultiVector Retriever</a></li>
<li><a href="#parent-document-retriever">Parent Document Retriever</a></li>
<li><a href="#self-querying">Self-querying</a></li>
</ul>
</li>
<li><a href="#langchian-retriever10">Langchian Retriever[10]</a></li>
<li><a href="#langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="langchain-retrievers10">Langchain Retrievers[10]</span><a href="#langchain-retrievers10" class="header-anchor">#</a></h1><h3><span id="multiqueryretriever">MultiQueryRetriever</span><a href="#multiqueryretriever" class="header-anchor">#</a></h3><p>The MultiQueryRetriever automates the process of prompt tuning by using an LLM to <strong>generate multiple queries from different perspectives for a given user input query</strong>. </p>
<h3><span id="contextual-compression">Contextual compression</span><a href="#contextual-compression" class="header-anchor">#</a></h3><h3><span id="ensemble-retriever">Ensemble Retriever</span><a href="#ensemble-retriever" class="header-anchor">#</a></h3><p>The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and <strong>rerank the results based on the Reciprocal Rank Fusion algorithm</strong>.<br>The most common pattern is to <strong>combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity)</strong>, because their strengths are complementary. It is also known as “hybrid search”.</p>
<h3><span id="multivector-retriever">MultiVector Retriever</span><a href="#multivector-retriever" class="header-anchor">#</a></h3><p>The methods to create multiple vectors per document include:<br>    - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).<br>    - Summary: create a summary for each document, embed that along with (or instead of) the document.<br>    - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.</p>
<h3><span id="parent-document-retriever">Parent Document Retriever</span><a href="#parent-document-retriever" class="header-anchor">#</a></h3><p>chunks of data</p>
<h3><span id="self-querying">Self-querying</span><a href="#self-querying" class="header-anchor">#</a></h3><p>This allows the retriever to not only use the user-input query for <strong>semantic similarity comparison</strong> with the contents of stored documents but to also extract filters from the user query on <strong>the metadata</strong> of stored documents and to execute those filters.</p>
<h1><span id="langchian-retriever10">Langchian Retriever[10]</span><a href="#langchian-retriever10" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Name</th>
<th>Index Type</th>
<th>Uses an LLM</th>
<th>When to Use</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore">Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you are just getting started and looking for something quick and easy.</td>
<td>This is the <strong>simplest method</strong> and the one that is easiest to get started with. It involves creating embeddings for each piece of text.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever">ParentDocument</a></td>
<td>Vectorstore + Document Store</td>
<td>No</td>
<td>If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.</td>
<td>This involves indexing <strong>multiple chunks</strong> for each document. Then you find the  chunks that are most similar in embedding space, but you retrieve the  <strong>whole parent</strong> document and <strong>return</strong> that (rather than individual chunks).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector">Multi Vector</a></td>
<td>Vectorstore + Document Store</td>
<td>Sometimes during indexing</td>
<td>If you are able to extract information from documents that you think is more relevant to index than the text itself.</td>
<td>This involves creating multiple vectors for each document. Each vector could be created in a <strong>myriad of ways</strong> - examples include <strong>summaries of the text</strong> and <strong>hypothetical questions</strong>.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query">Self Query</a></td>
<td>Vectorstore</td>
<td>Yes</td>
<td>If users are asking questions that are better answered by fetching  documents based on metadata rather than similarity with the text.</td>
<td>This uses an LLM to transform user input into two things: (1) a string to  look up semantically, (2) a <strong>metadata filer</strong> to go along with it. This is  useful because oftentimes questions are about the METADATA of documents  (not the content itself).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression">Contextual Compression</a></td>
<td>Any</td>
<td>Sometimes</td>
<td>If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.</td>
<td>This puts a <strong>post-processing step</strong> on top of another retriever and extracts  only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/time_weighted_vectorstore">Time-Weighted Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you have timestamps associated with your documents, and you want to retrieve the most recent ones</td>
<td>This fetches documents based on a combination of semantic similarity (as in  normal vector retrieval) and recency (looking at timestamps of indexed  documents)</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever">Multi-Query Retriever</a></td>
<td>Any</td>
<td>Yes</td>
<td>If users are asking questions that are complex and require multiple pieces of distinct information to respond</td>
<td>This uses an LLM to <strong>generate multiple queries</strong> from the original one. This is useful when the original query needs pieces of information about  multiple topics to be properly answered. By generating multiple queries, we can then fetch documents for each of them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble">Ensemble</a></td>
<td>Any</td>
<td>No</td>
<td>If you have multiple retrieval methods and want to try combining them.</td>
<td>This fetches documents from <strong>multiple retrievers</strong> and then <strong>combines</strong> them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder">Long-Context Reorder</a></td>
<td>Any</td>
<td>No</td>
<td>If you are working with a long-context model and noticing that it’s not  paying attention to information in the middle of retrieved documents.</td>
<td>This fetches documents from an underlying retriever, and then reorders them  so that the most similar are near the beginning and end. This is useful  because it’s been shown that for longer context models they sometimes  don’t pay attention to information in the middle of the context window.</td>
</tr>
</tbody></table>
<h1><span id="langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</span><a href="#langchain-vs-llamaindex-1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>langchain</th>
<th>llamaindex</th>
</tr>
</thead>
<tbody><tr>
<td>Ensemble</td>
<td>Hybrid Fusion</td>
</tr>
<tr>
<td>Rewrite-Retrieve-Read</td>
<td>Query Rewriting</td>
</tr>
<tr>
<td></td>
<td>AutoMerging</td>
</tr>
<tr>
<td>ParentDocumentRetrieval</td>
<td>Small-to-Big Retrieval</td>
</tr>
<tr>
<td></td>
<td>Sentence Window Retrieval</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1qe411r78b/">【高级RAG || 原理介绍】Llamaindex 5种高级RAG方法</a> V </li>
<li><a href="https://python.langchain.com/docs/modules/data_connection/retrievers">retrievers</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Retrievers</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>LLMOps</title>
    <url>/www6vHomeAIGC/2022/12/28/gptLLMOps/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li><a href="https://drive.google.com/file/d/1LZXTrRdrloIqAJT6xaNTl4WQd6y95o7K/view">LLMOps: Deployment and Learning in Production</a><br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/">LLMOps: Deployment and Learning in Production</a><br><a href="https://zhuanlan.zhihu.com/p/629589593">[必读] LLM 应用开发全栈指南</a> LLMOps</li>
<li><a href="https://zhuanlan.zhihu.com/p/632026876">了解一下新领域 LLMOps: 大模型运维</a><br><a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations">Understanding LLMOps: Large Language Model Operations</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Fine-Tuning 时机</title>
    <url>/www6vHomeAIGC/2022/12/28/gptFineTuningWhen/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BD%95%E6%97%B6%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%831">何时进行微调[1]</a></li>
<li><a href="#what-4">what [4]</a></li>
<li><a href="#common-use-cases2">Common use cases[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="何时进行微调1">何时进行微调[1]</span><a href="#何时进行微调1" class="header-anchor">#</a></h1><p>语言模型（LLM）可以通过至少两种方式学习新知识：权重更新（例如预训练或微调）或提示（例如检索增强生成，RAG）。模型的权重就像长期记忆，而提示就像短期记忆。这个OpenAI Cookbook给出了一个有用的比喻：当你对模型进行微调时，就像是在离考试还有一周的时候准备复习。当你通过提示（例如检索）向提示中插入知识时，就像是在有开放笔记的考试中。</p>
<p>基于这一点，<strong>不建议使用微调来教授LLM新的知识或事实回忆</strong>；OpenAI的John Schulman在一次讲话中指出，微调可能会<strong>增加虚构</strong>。微调<strong>更适合教授专门的任务</strong>，但应与提示或RAG相对比。正如这里所讨论的，对于具有丰富示例和&#x2F;或缺乏上下文学习能力的LLM来说，微调对于定义明确的任务可能是有帮助的。这篇Anyscale博客很好地总结了这些观点：<strong>微调是为形式而非事实</strong>[3]。</p>
<h1><span id="what-4">what [4]</span><a href="#what-4" class="header-anchor">#</a></h1><p>这是一个很好的问题。我大致将微调类比为人的专业知识：</p>
<ul>
<li><strong>用文字描述一个任务 ~&#x3D; 零样本提示</strong></li>
<li><strong>给出解决任务的示例 ~&#x3D; 少样本提示</strong></li>
<li><strong>允许人们练习任务 ~&#x3D; 微调</strong></li>
</ul>
<p>考虑到这个比喻，令人惊奇的是我们有了可以仅通过提示就能在许多任务上达到高水平准确性的模型，但我也预计达到顶级性能可能需要微调，特别是在具有明确定义的具体任务的应用中，在这些任务中我们可以收集大量数据并在其上进行“练习”。</p>
<p>这可能是一个需要牢记的<strong>粗略图景</strong>。<strong>小型模型</strong>无法进行上下文学习，并且从提示工程中受益甚少，但根据任务的难度，<strong>仍然有可能将它们微调为表现良好的专家</strong>。</p>
<p>需要注意的是，所有这些都还是非常新颖的。</p>


<h1><span id="common-use-cases2">Common use cases[2]</span><a href="#common-use-cases2" class="header-anchor">#</a></h1><p>微调可以改善结果的一些常见<strong>用例</strong>包括：</p>
<ul>
<li><strong>设定风格、语气、格式或其他定性因素</strong></li>
<li><strong>提高生成所需输出的可靠性</strong></li>
<li><strong>纠正无法按照复杂提示要求执行的问题</strong></li>
<li>以特定方式处理许多边缘情况</li>
<li><strong>执行难以用提示清晰表达的新技能或任务</strong></li>
</ul>
<p>从较高层面来看，这些情况下微调更容易实现“<strong>展示而非告诉</strong>”的效果。在接下来的部分中，我们将探讨如何为微调设置数据以及各种示例，这些示例中微调改善了基线模型的性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/">Using LangSmith to Support Fine-tuning</a><br>  <a href="https://colab.research.google.com/drive/1tpywvzwOS74YndNXhI8NUaEfPeqOc7ub?usp=sharing&ref=blog.langchain.dev">colab</a>   LANGCHAIN_API_KEY</p>
</li>
<li><p><a href="https://platform.openai.com/docs/guides/fine-tuning">Fine-tuning</a>  openai *** </p>
</li>
<li><p><a href="https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts">Fine tuning is for form, not facts</a> ***</p>
</li>
<li><p><a href="https://twitter.com/karpathy/status/1655994367033884672">Andrej Karpathy twitter</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)RAG OpenAI案例</title>
    <url>/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#openai-rag-%E6%A1%88%E4%BE%8B3">OpenAI RAG 案例[3]</a><ul>
<li><a href="#query-transformations5">Query Transformations[5]</a></li>
<li><a href="#query-construction-4">Query Construction [4]</a></li>
</ul>
</li>
<li><a href="#advanced-rag">Advanced RAG</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="openai-rag-案例3">OpenAI RAG 案例[3]</span><a href="#openai-rag-案例3" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/openai-rag.jpg" class>

<ol>
<li>retrieval with consine similarity</li>
<li><strong>HyDE retrieval</strong> [5]<br>Fine-tune Embeddings<br><strong>Chunk&#x2F;embedding experiments</strong></li>
<li><strong>Reranking</strong> [6][8]<br>Classification step</li>
<li>Prompt engineering<br><strong>Tool use</strong><br><strong>Query expansion</strong>[5]</li>
</ol>
<h3><span id="query-transformations5">Query Transformations[5]</span><a href="#query-transformations5" class="header-anchor">#</a></h3><ul>
<li><strong>Query expansion</strong><br>Multi-query retriever </li>
<li><strong>HyDE</strong></li>
<li>Step back prompting<br> [抽象prompting]</li>
<li>Rewrite-Retrieve-Read</li>
</ul>
<h3><span id="query-construction-4">Query Construction [4]</span><a href="#query-construction-4" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/structured_data_stacks.jpg" class>

<table>
<thead>
<tr>
<th><strong>Examples</strong></th>
<th><strong>Data source</strong></th>
<th><strong>References</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Text-to-metadata-filter</strong></td>
<td>Vectorstores</td>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/?ref=blog.langchain.dev#constructing-from-scratch-with-lcel"><strong>Docs</strong></a></td>
</tr>
<tr>
<td><strong>Text-to-SQL</strong></td>
<td>SQL DB</td>
<td><a href="https://python.langchain.com/docs/use_cases/qa_structured/sql?ref=blog.langchain.dev"><strong>Docs</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/llms-and-sql/"><strong>blog</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/"><strong>blog</strong></a></td>
</tr>
</tbody></table>
<ul>
<li>Text-to-metadata-filter [7]</li>
</ul>
<p>A <strong>self-querying</strong> retriever is one that, as the name suggests, has the  ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a <strong>structured query</strong> and then applies that structured query to its underlying  VectorStore. This allows the retriever to not only use the user-input  query for semantic similarity comparison with the contents of stored  documents but to also <strong>extract filters from the user query on the  metadata of stored documents and to execute those filters</strong>.</p>
<h1><span id="advanced-rag">Advanced RAG</span><a href="#advanced-rag" class="header-anchor">#</a></h1><h3><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h3><ul>
<li>离线 index</li>
<li>在线 查询</li>
</ul>
<img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/rag.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/deconstructing-rag/">Deconstructing RAG</a> ***</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://blog.langchain.dev/applying-openai-rag/">Applying OpenAI’s RAG Strategies</a>   *** </p>
</li>
<li><p><a href="https://blog.langchain.dev/query-construction/">Query Construction</a> ***</p>
</li>
<li><p><a href="https://blog.langchain.dev/query-transformations/">Query Transformations</a></p>
</li>
<li><p><a href="https://txt.cohere.com/rerank/">Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here</a><br><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-pinecone-rerank">Rerank</a><br><a href="https://python.langchain.com/docs/integrations/retrievers/cohere-reranker">Cohere Reranker</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/docs/docs/modules/data_connection/retrievers/self_query.ipynb">self_query</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb">RAG Fusion</a><br><a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1">Forget RAG, the Future is RAG-Fusion</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT 概述</title>
    <url>/www6vHomeAIGC/2022/12/20/gptFineTuningPEFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="huggingface-peft中的任务1">Huggingface  PEFT中的任务[1]</span><a href="#huggingface-peft中的任务1" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class TaskType(str, enum.Enum):</span><br><span class="line">    SEQ_CLS = &quot;SEQ_CLS&quot;  # 3. 序列分类任务</span><br><span class="line">    SEQ_2_SEQ_LM = &quot;SEQ_2_SEQ_LM&quot;  # 2. 条件生成任务</span><br><span class="line">    CAUSAL_LM = &quot;CAUSAL_LM&quot;  #  1. 因果语言建模任务</span><br><span class="line">    TOKEN_CLS = &quot;TOKEN_CLS&quot;  #  4. Token 分类任务</span><br><span class="line">    QUESTION_ANS = &quot;QUESTION_ANS&quot;</span><br><span class="line">    FEATURE_EXTRACTION = &quot;FEATURE_EXTRACTION&quot;</span><br></pre></td></tr></table></figure>

<h3><span id="1-因果语言建模任务causal-language-modeling">1. 因果语言建模任务（Causal Language Modeling）</span><a href="#1-因果语言建模任务causal-language-modeling" class="header-anchor">#</a></h3><p>  因果语言建模任务（CLM），在这种建模方法中，模型试图预测给定上下文中的下一个单词，该上下文通常包括在当前单词之前的所有单词。</p>
<h3><span id="2-条件生成任务conditional-generation">2. 条件生成任务（Conditional Generation）</span><a href="#2-条件生成任务conditional-generation" class="header-anchor">#</a></h3><p>  条件生成任务（Conditional Generation），根据给定的输入（可能是文本、图片等）生成符合条件的输出。<br>  条件生成的应用包括但不限于机器翻译、文本摘要、图像描述等。这些任务通常需要模型在输入和输出之间建立复杂的映射关系。</p>
<blockquote>
<p>因果语言建模任务  vs.  条件生成任务<br>  因果语言建模主要关注于生成连贯、自然的文本，而条件生成关注于生成满足特定条件或任务要求的文本。这两种建模方法在某些场景下可能会互相使用和结合，以实现更复杂的自然语言处理任务。</p>
</blockquote>
<h3><span id="3-序列分类任务sequence-classification">3. 序列分类任务（Sequence Classification）</span><a href="#3-序列分类任务sequence-classification" class="header-anchor">#</a></h3><p>  序列分类（Sequence Classification），对整个句子进行分类。如: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关等</p>
<h3><span id="4-token-分类任务token-classification">4. Token 分类任务（Token Classification）</span><a href="#4-token-分类任务token-classification" class="header-anchor">#</a></h3><p>  Token 分类任务（Token Classification），对句子中的每个词进行分类。如: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/651744834">大模型参数高效微调技术实战（一）-PEFT概述</a></li>
<li><a href="https://github.com/www6v/llm-action#llm%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98">LLM微调实战</a> 李国东</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 系列</title>
    <url>/www6vHomeAIGC/2022/12/11/gptFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%9B%E5%8C%96%E6%97%B6%E9%97%B4%E7%BA%BF">进化时间线</a></li>
<li><a href="#gpt1-1">GPT1 [1]</a></li>
<li><a href="#gpt2-1">GPT2 [1]</a><ul>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">核心思想</a></li>
<li><a href="#gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</a></li>
</ul>
</li>
<li><a href="#gpt3-1">GPT3 [1]</a><ul>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">下游任务评估方法</a></li>
<li><a href="#few-shot-vs-fine-tuning">Few-shot vs fine-tuning</a></li>
<li><a href="#gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</a></li>
</ul>
</li>
<li><a href="#instructgpt-1">InstructGPT [1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88">技术方案</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="#chatgpt-%E8%AE%AD%E7%BB%83-3">ChatGPT 训练  [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="进化时间线">进化时间线</span><a href="#进化时间线" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/12/11/gptFamily/family.jpg" class>

<h1><span id="gpt1-1">GPT1 [1]</span><a href="#gpt1-1" class="header-anchor">#</a></h1><ol>
<li>它是最早一批提出在 NLP 任务上使用 <strong>pre-train + fine-tuning 范式</strong>的工作。</li>
<li>GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</li>
<li><strong>预训练模型具有 zero-shot 的能力</strong>，并且能随着预训练的进行不断增强</li>
</ol>
<h1><span id="gpt2-1">GPT2 [1]</span><a href="#gpt2-1" class="header-anchor">#</a></h1><h3><span id="核心思想">核心思想</span><a href="#核心思想" class="header-anchor">#</a></h3><p>当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，<strong>不需要在下游任务微调</strong>。</p>
<h3><span id="gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</span><a href="#gpt-2-vs-gpt-1" class="header-anchor">#</a></h3><ol>
<li><strong>主推 zero-shot</strong>，而 GPT-1 为 pre-train + fine-tuning；</li>
<li>训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；</li>
<li>模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；</li>
<li>模型结构调整，层归一化和参数初始化方式；</li>
<li>训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；</li>
</ol>
<h1><span id="gpt3-1">GPT3 [1]</span><a href="#gpt3-1" class="header-anchor">#</a></h1><h3><span id="下游任务评估方法">下游任务评估方法</span><a href="#下游任务评估方法" class="header-anchor">#</a></h3><p>GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：<br><strong>Zero-shot</strong>：仅使用当前任务的自然语言描述，不进行任何梯度更新；<br><strong>One-shot</strong>：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；<br><strong>Few-shot</strong>：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；</p>
<ul>
<li>Shot[2]<ul>
<li>One-shot</li>
<li>Few-Shot</li>
<li>Zero-Shot</li>
</ul>
</li>
</ul>
<h3><span id="few-shot-vs-fine-tuning">Few-shot vs fine-tuning</span><a href="#few-shot-vs-fine-tuning" class="header-anchor">#</a></h3><p>其中 <strong>Few-shot</strong> 也被称为 <strong>in-context learning</strong>，虽然它与 fine-tuning 一样都需要一些<strong>有监督标注数据</strong>，但是两者的区别是：<br>【本质区别】<br><strong>fine-tuning</strong> 基于标注数据<strong>对模型参数进行更新</strong><br>而<strong>in-context learning</strong>使用标注数据时不做任何的梯度回传, <strong>模型参数不更新</strong></p>
<h3><span id="gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</span><a href="#gpt-3-vs-gpt-2" class="header-anchor">#</a></h3><ol>
<li>效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章；</li>
<li><strong>主推 few-shot</strong>，相比于 GPT-2 的 zero-shot，具有很强的创新性；</li>
<li>模型结构略微变化，采用 <strong>sparse attention</strong> 模块；</li>
<li>海量训练语料 <strong>45TB</strong>（清洗后 570GB），相比于 GPT-2 的 40GB；</li>
<li>海量模型参数，最大模型为 <strong>1750 亿</strong>，GPT-2 最大为 15 亿参数；</li>
</ol>
<h1><span id="instructgpt-1">InstructGPT [1]</span><a href="#instructgpt-1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>有监督微调，</li>
<li>奖励模型训练，</li>
<li>强化学习训练</li>
</ul>
<h3><span id="技术方案">技术方案</span><a href="#技术方案" class="header-anchor">#</a></h3><ul>
<li><p>有监督微调（SFT）<br>本质上来说，<strong>SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3</strong>。但是值得一提的是，这里<strong>标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别</strong>。<br>InstructGPT 在 SFT 中标注的数据，正是为了<strong>消除这种模型预测与用户表达习惯之间的 gap</strong>。在标注过程中，他们<strong>从 GPT-3 的用户真实请求中采样</strong>大量下游任务的描述，然后让<strong>标注人员对任务描述进行续写</strong>，从而得到该问题的高质量回答。</p>
</li>
<li><p>基于人类反馈的强化学习（RLHF）</p>
<img src="/www6vHomeAIGC/2022/12/11/gptFamily/instructGPT.jpg" class></li>
</ul>
<h3><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h3><ol>
<li>解决 GPT-3 的<strong>输出与人类意图</strong>之间的<strong>Align问题</strong>；</li>
<li>让具备丰富世界知识的大模型，<strong>学习“人类偏好”</strong>；</li>
<li>标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠；</li>
<li>InstructGPT 在<strong>真实性</strong>，<strong>丰富度</strong>上表现更好；</li>
<li>InstructGPT 对有害结果的生成控制的更好，但是对于<strong>“偏见”没有明显改善</strong>；</li>
</ol>
<h1><span id="chatgpt-训练-3">ChatGPT 训练  [3]</span><a href="#chatgpt-训练-3" class="header-anchor">#</a></h1><ul>
<li>基于人类反馈的强化学习微调技术 RLHF<ul>
<li>使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型<ul>
<li>Supervised fine-tuning (SFT)<br>&#x3D; Instruction Tuning</li>
</ul>
</li>
<li>训练奖励模型 Reward Model（RM）</li>
<li>使用强化学习算法微调语言模型<ul>
<li>RLHF<br>[本质  基于强化学习, 强化学习算法]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/609716668">GPT &#x2F; GPT-2 &#x2F; GPT-3 &#x2F; InstructGPT 进化之路</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/624793654">Few-Shot, Zero-Shot &amp; One-shot 的通俗理解</a></p>
</li>
<li><p><a href="https://shimo.im/docs/KlkKv4XQDouwWRqd/read">AI 大模型微调训练营大纲</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/VYv8BRgGnp9ZTuXxaSuFwg">万字拆解！追溯ChatGPT各项能力的起源 </a> 符尧</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642282717">[Transformer 101系列] ChatGPT是怎么炼成的?</a> 未</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Advanced RAG</title>
    <url>/www6vHomeAIGC/2022/12/07/gptRAGPerformance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a><ul>
<li><a href="#advanced-rag-2">Advanced RAG [2]</a></li>
<li><a href="#advanced-rag-1">Advanced RAG [1]</a></li>
</ul>
</li>
<li><a href="#embedding">Embedding</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95">索引</a><ul>
<li><a href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F">索引方式</a><ul>
<li><a href="#%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8%E7%B4%A2%E5%BC%951">向量存储索引[1]</a></li>
<li><a href="#%E5%B1%82%E6%AC%A1%E7%B4%A2%E5%BC%951">层次索引[1]</a></li>
<li><a href="#%E5%81%87%E8%AE%BE%E9%97%AE%E9%A2%98%E5%92%8Chyde1">假设问题和HyDE[1]</a></li>
<li><a href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%B0%E5%AF%8C1">上下文丰富[1]</a></li>
<li><a href="#%E8%9E%8D%E5%90%88%E6%A3%80%E7%B4%A2%E6%88%96%E6%B7%B7%E5%90%88%E6%90%9C%E7%B4%A21">融合检索或混合搜索[1]</a></li>
</ul>
</li>
<li><a href="#%E5%88%86%E5%9D%97-1">分块 [1]</a></li>
</ul>
</li>
<li><a href="#pre-retrival%E9%98%B6%E6%AE%B5">pre-retrival阶段</a><ul>
<li><a href="#query-transformer-%E6%9F%A5%E8%AF%A2%E8%BD%AC%E6%8D%A2-301">query transformer 查询转换 [30][1]</a></li>
<li><a href="#query-routing-%E6%9F%A5%E8%AF%A2%E8%B7%AF%E7%94%B1-311">query-routing 查询路由  [31][1]</a></li>
</ul>
</li>
<li><a href="#retrieval">Retrieval</a><ul>
<li><a href="#%E6%A3%80%E7%B4%A2%E5%99%A8-retriever">检索器 Retriever</a></li>
</ul>
</li>
<li><a href="#post-retrieval">Post-Retrieval</a><ul>
<li><a href="#reranker20">Reranker[20]</a></li>
<li><a href="#fusion23">Fusion[23]</a></li>
</ul>
</li>
<li><a href="#encoder-and-llm-fine-tuning">Encoder and LLM fine-tuning</a><ul>
<li><a href="#encoder-fine-tuning40">Encoder fine-tuning[40]</a></li>
<li><a href="#ranker-fine-tuning41">Ranker fine-tuning[41]</a></li>
<li><a href="#llm-fine-tuning42">LLM fine-tuning[42]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#post-retrieval-1">Post-Retrieval</a></li>
</ul>
</li>
<li><a href="#pre-retrival">pre-retrival</a><ul>
<li><a href="#fine-tuning">fine-tuning</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><h3><span id="advanced-rag-2">Advanced RAG [2]</span><a href="#advanced-rag-2" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/07/gptRAGPerformance/advanced-rag.JPG" class>

<h3><span id="advanced-rag-1">Advanced RAG [1]</span><a href="#advanced-rag-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/07/gptRAGPerformance/advaced.png" class>


<h1><span id="embedding">Embedding</span><a href="#embedding" class="header-anchor">#</a></h1><blockquote>
<p>最佳实践<br><strong>BGE</strong> 优于 OpenAI ADA02</p>
</blockquote>
<h1><span id="索引">索引</span><a href="#索引" class="header-anchor">#</a></h1><h2><span id="索引方式">索引方式</span><a href="#索引方式" class="header-anchor">#</a></h2><h3><span id="向量存储索引1">向量存储索引[1]</span><a href="#向量存储索引1" class="header-anchor">#</a></h3><ul>
<li>近似最近邻实现（如聚类、树或HNSW算法）<br>Faiss、nmslib、annoy</li>
<li>LlamaIndex</li>
</ul>
<h3><span id="层次索引1">层次索引[1]</span><a href="#层次索引1" class="header-anchor">#</a></h3><ul>
<li>创建<strong>两个索引</strong>—一个由摘要组成，另一个由文档块组成，并分两步检索，首先通过摘要过滤掉相关文档，然后只在这个相关组内检索。</li>
</ul>
<h3><span id="假设问题和hyde1">假设问题和HyDE[1]</span><a href="#假设问题和hyde1" class="header-anchor">#</a></h3><h3><span id="上下文丰富1">上下文丰富[1]</span><a href="#上下文丰富1" class="header-anchor">#</a></h3><ul>
<li>句子窗口检索<br>为了在获取最相关的单个句子后更好地对找到的上下文进行推理，我们<strong>将上下文窗口在检索到的句子之前和之后扩展了k个句子，然后将此扩展的上下文发送给LLM</strong>。</li>
<li>自动合并检索器（又名<strong>父文档检索器</strong>）</li>
</ul>
<h3><span id="融合检索或混合搜索1">融合检索或混合搜索[1]</span><a href="#融合检索或混合搜索1" class="header-anchor">#</a></h3><p>基于关键字的老式搜索—稀疏检索算法（如tf-idf或搜索行业标准BM25）和现代语义或向量搜索，并将其组合到一个检索结果中。</p>
<ul>
<li>Reciprocal Rank Fusion (RRF)算法<br>对检索到的结果进行重新排序以获得最终输出</li>
<li>实现<br>LangChain  - Ensemble Retriever类</li>
</ul>
<h2><span id="分块-1">分块 [1]</span><a href="#分块-1" class="header-anchor">#</a></h2><h1><span id="pre-retrival阶段">pre-retrival阶段</span><a href="#pre-retrival阶段" class="header-anchor">#</a></h1><h3><span id="query-transformer-查询转换-301">query transformer 查询转换 [30][1]</span><a href="#query-transformer-查询转换-301" class="header-anchor">#</a></h3><h3><span id="query-routing-查询路由-311">query-routing 查询路由  [31][1]</span><a href="#query-routing-查询路由-311" class="header-anchor">#</a></h3><p>LlamaIndex和LangChain都支持查询路由器</p>
<h1><span id="retrieval">Retrieval</span><a href="#retrieval" class="header-anchor">#</a></h1><h3><span id="检索器-retriever">检索器 Retriever</span><a href="#检索器-retriever" class="header-anchor">#</a></h3><ul>
<li>Ensemble Retriever<br>最常见的模式是将<strong>稀疏检索器（如BM25）</strong>与<strong>密集检索器（如嵌入相似度）</strong>结合起来，因为它们的优势是互补的。这也被称为“混合搜索”。<strong>稀疏检索器</strong>擅长基于<strong>关键词查找</strong>相关文档，而<strong>密集检索器</strong>擅长基于<strong>语义相似性查找</strong>相关文档。</li>
</ul>
<blockquote>
<p>最佳实践<br><strong>BM25+FAAIS   好于 FAAIS相似度搜索</strong><br><strong>FAAIS相似度搜索 好于 HyDE和上下文压缩</strong></p>
</blockquote>
<h1><span id="post-retrieval">Post-Retrieval</span><a href="#post-retrieval" class="header-anchor">#</a></h1><h2><span id="reranker20">Reranker[20]</span><a href="#reranker20" class="header-anchor">#</a></h2><h2><span id="fusion23">Fusion[23]</span><a href="#fusion23" class="header-anchor">#</a></h2><p>其思想在于通过生成多个用户查询和重新排序结果来解决RAG固有的约束；利用倒数排序融合（RRF）和自定义向量评分加权，生成全面准确的结果。</p>
<h1><span id="encoder-and-llm-fine-tuning">Encoder and LLM fine-tuning</span><a href="#encoder-and-llm-fine-tuning" class="header-anchor">#</a></h1><h3><span id="encoder-fine-tuning40">Encoder fine-tuning[40]</span><a href="#encoder-fine-tuning40" class="header-anchor">#</a></h3><h3><span id="ranker-fine-tuning41">Ranker fine-tuning[41]</span><a href="#ranker-fine-tuning41" class="header-anchor">#</a></h3><p>它的工作方式如下：将查询和检索到的前k个文本块传递给交叉编码器，并用SEP令牌分隔，并将其微调为输出1表示相关块，输出0表示不相关。</p>
<h3><span id="llm-fine-tuning42">LLM fine-tuning[42]</span><a href="#llm-fine-tuning42" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced RAG Techniques: an Illustrated Overview</a>  ***<br><a href="https://mp.weixin.qq.com/s/CO7hMv4RW7OE6zwUmVfp5A">最全的RAG技术概览 </a><br><a href="https://zhuanlan.zhihu.com/p/673922981">高级检索增强生成技术(RAG)全面指南：原理、分块、编码、索引、微调、Agent、展望</a> </p>
</li>
<li><p>《Retrieval-Augmented Generation for Large Language Models: A Survey》</p>
</li>
</ol>
<p>1xx. <a href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">A Cheat Sheet and Some Recipes For Building Advanced RAG</a><br>     <a href="https://mp.weixin.qq.com/s/KM8c3PUww1SOK1dbLjn1Tw">LlamaIndex官方年度巨献：高清大图纵览高级 RAG技术，强烈推荐收藏 </a> *** 看图<br>     <a href="https://baoyu.io/translations/rag/a-cheat-sheet-and-some-recipes-for-building-advanced-rag">构建高级 RAG 的指南和技巧 [译]</a></p>
<h3><span id="post-retrieval">Post-Retrieval</span><a href="#post-retrieval" class="header-anchor">#</a></h3><ol start="20">
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRerank/" title="(原理|实战)RAG Rerank">(原理|实战)RAG Rerank</a> self</li>
<li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb">RAG Fusion</a> git<br>  <a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1">Forget RAG, the Future is RAG-Fusion</a><br>  <a href="https://blog.csdn.net/lichunericli/article/details/135451681">忘记RAG，未来是RAG-Fusion</a><br>  <a href="https://mp.weixin.qq.com/s/NFjn8pUsQaSx85nhBphORA">再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/xu6z0zRQxQF2iOEvG-Sysg">传统RAG破局者：混合检索助力新纪元</a><br>     四、混合检索的原理   六、何时使用混合检索？</p>
<h1><span id="pre-retrival">pre-retrival</span><a href="#pre-retrival" class="header-anchor">#</a></h1><ol start="30">
<li><a href="/www6vHomeAIGC/2023/04/20/gptQueryTransformation/" title="(原理|实战)Query Transformation">(原理|实战)Query Transformation</a>  self</li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRouting/" title="(原理|实战)Query Routing">(原理|实战)Query Routing</a>  self</li>
</ol>
<h3><span id="fine-tuning">fine-tuning</span><a href="#fine-tuning" class="header-anchor">#</a></h3><ol start="40">
<li><p><a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding/">Finetune Embeddings</a> notebook</p>
</li>
<li><p><a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/cross_encoder_finetuning/cross_encoder_finetuning/">How to Finetune a cross-encoder using LLamaIndex</a> notebook</p>
</li>
<li><p><a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/openai_fine_tuning/">Fine Tuning GPT-3.5-Turbo</a> notebook</p>
</li>
</ol>
<p>1xx. <a href="https://www.youtube.com/watch?v=ahnGLM-RC1Y">A Survey of Techniques for Maximizing LLM Performance</a>  *** V<br>    <a href="https://zhuanlan.zhihu.com/p/670880685">A Survey of Techniques for Maximizing LLM Performance梳理</a> </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406795&idx=1&sn=00ea4aab819eed3d622287fa1d32816f">大模型RAG问答技术架构及核心模块回顾：从Embedding、prompt-embedding到Reranker </a> ***</p>
<p>1xx. CON<br>   <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406194&idx=1&sn=aafe667fa5a73bd89a00272c5598c98e">引入COT缓解大模型RAG问答的上下文区分问题：兼看Langchain的表格检索思路及GPTBIAS评估框架 </a> CON<br>   <a href="https://cobusgreyling.medium.com/chain-of-note-con-retrieval-for-llms-763ead1ae5c5">Chain-Of-Note (CoN) Retrieval For LLMs</a><br>   <a href="https://praveengovindaraj.com/cutting-through-the-noise-chain-of-notes-con-robust-approach-to-super-power-your-rag-pipelines-0df5f1ce7952">Cutting Through the Noise: Chain-of-Note’s (CoN) Robust Approach to super power your RAG pipelines</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Transformer</title>
    <url>/www6vHomeAIGC/2022/11/30/gptTransformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a></li>
<li><a href="#attention-3">Attention [3]</a><ul>
<li><a href="#%E4%BC%98%E5%8C%964">优化[4]</a></li>
</ul>
</li>
<li><a href="#transformer-2">Transformer [2]</a><ul>
<li><a href="#encoder-decoder%E6%9E%B6%E6%9E%84-1">Encoder-Decoder架构 [1]</a></li>
<li><a href="#self-attention">Self-attention</a></li>
<li><a href="#multi-head-attentionmha">Multi-Head Attention(MHA)</a></li>
<li><a href="#positional-encoding-3031">Positional Encoding  [30][31]</a></li>
<li><a href="#normalization-20">Normalization [20]</a></li>
</ul>
</li>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84-67">架构 [6][7]</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E7%82%B9">优化点</a></li>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B95">关注点[5]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#normalization">Normalization</a></li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">位置编码</a></li>
<li><a href="#attention">Attention</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="神经网络">神经网络</span><a href="#神经网络" class="header-anchor">#</a></h1><ul>
<li><p>正向传播<br>损失函数  </p>
</li>
<li><p>反相传播<br>梯度</p>
</li>
</ul>
<h1><span id="attention-3">Attention [3]</span><a href="#attention-3" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/30/gptTransformer/self-attention.jpg" class>

<h3><span id="优化4">优化[4]</span><a href="#优化4" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/30/gptTransformer/attentions.jpg" class>

<h1><span id="transformer-2">Transformer [2]</span><a href="#transformer-2" class="header-anchor">#</a></h1><h3><span id="encoder-decoder架构-1">Encoder-Decoder架构 [1]</span><a href="#encoder-decoder架构-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/30/gptTransformer/Transformer_decoder.jpg" class>
<img src="/www6vHomeAIGC/2022/11/30/gptTransformer/transformer_resideual_layer_norm_3.jpg" class>

<p>transfomer 架构在GPU上的并行</p>
<h3><span id="self-attention">Self-attention</span><a href="#self-attention" class="header-anchor">#</a></h3><p>Q&#x3D;K&#x3D;V<br>aligment</p>
<h3><span id="multi-head-attentionmha">Multi-Head Attention(MHA)</span><a href="#multi-head-attentionmha" class="header-anchor">#</a></h3><h3><span id="positional-encoding-3031">Positional Encoding  [30][31]</span><a href="#positional-encoding-3031" class="header-anchor">#</a></h3><h3><span id="normalization-20">Normalization [20]</span><a href="#normalization-20" class="header-anchor">#</a></h3><ul>
<li><p>batch normalization</p>
</li>
<li><p>Layer Normalization</p>
<ul>
<li>Post-LN</li>
<li>Pre-LN</li>
<li>Sandwich-LN<br>layerNorm是针对序列数据提出的一种归一化方法，主要在layer维度进行归一化，即对整个序列进行归一化。</li>
</ul>
</li>
<li><p>RMS Norm</p>
</li>
</ul>
<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="架构-67">架构 [6][7]</span><a href="#架构-67" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/30/gptTransformer/bigModelArch.jpg" class>

<img src="/www6vHomeAIGC/2022/11/30/gptTransformer/bigModelArch1.jpg" class>

<h3><span id="优化点">优化点</span><a href="#优化点" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/30/gptTransformer/transformers.jpg" class>

<h3><span id="关注点5">关注点[5]</span><a href="#关注点5" class="header-anchor">#</a></h3><ul>
<li><p><strong>Mask attention 的策略不同</strong></p>
<ul>
<li>bert  [双向都能看到]</li>
<li>chatgpt  [只能看到单项的]</li>
<li>chatglm  [左边像bert, 右边像gpt]</li>
</ul>
</li>
<li><p><strong>训练任务目标不同</strong></p>
<ul>
<li>bert [mask掉一个次, 在原位置把它预测出来]</li>
<li>gpt [预测下一个词]</li>
<li>chatglm [用gpt的方式来做bert的任务]</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a> ***<br><a href="https://baoyu.io/translations/llm/illustrated-transformer">图解 Transformer [译]</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/311156298">Transformer - Attention is all you need</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/410776234">超详细图解Self-Attention</a> ***</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a>  *** [架构]+训练+微调</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1gY4y1d7nk/">基于ChatGLM对话系统实战</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV16h4y1W7us/">第一课：Transformer</a> ***  华为<br>1xx. <a href="https://bbycroft.net/llm">LLM Visualization</a> ***  可视化<br>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/127411638">Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT</a> ***<br>1xx. <a href="https://baoyu.io/translations/llm/the-random-transformer">深入解析随机 Transformer [译]</a> ***<br>1xx. <a href="https://e2eml.school/transformers.html">Transformers from Scratch</a></p>
<h3><span id="normalization">Normalization</span><a href="#normalization" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1tk4y1F7b6/">Normalization归一化：batch normalization vs layer nomalization</a></li>
</ol>
<h3><span id="位置编码">位置编码</span><a href="#位置编码" class="header-anchor">#</a></h3><ol start="30">
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/134085503">一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long</a></li>
<li><a href="https://www.bilibili.com/video/BV1Xa4y1d7YY/">【NLP入门】Transformer基本结构：Embedding与位置编码</a>  V</li>
</ol>
<h3><span id="attention">Attention</span><a href="#attention" class="header-anchor">#</a></h3><p>1xx. <a href="https://blog.csdn.net/kkm09/article/details/120855658">李宏毅《深度学习》- Self-attention 自注意力机制</a><br>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/134228287">一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/640784855">[Transformer 101系列] 初探LLM基座模型</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/664046612">LLM从0开始预训练系列：2、大模型技术报告总结（GPT&#x2F;PaLM&#x2F;GLM&#x2F;LLaMA&#x2F;Skywork）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>向量数据库</title>
    <url>/www6vHomeAIGC/2022/11/27/gptVectorStore/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93">向量数据库</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F-7">向量数据库-索引方式 [7]</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%953">向量的相似度算法[3]</a><ul>
<li><a href="#%E6%AF%94%E8%BE%834">比较[4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


  
<h1><span id="向量数据库">向量数据库</span><a href="#向量数据库" class="header-anchor">#</a></h1><ul>
<li><p>国产</p>
<ul>
<li>Milvus</li>
<li>Tencent </li>
<li>zilliz cloud</li>
</ul>
</li>
<li><p>国外</p>
<ul>
<li>Pinecone</li>
<li>FAISS<br>[ANN]</li>
<li>Chroma</li>
<li>Weaviate</li>
</ul>
</li>
</ul>
<h1><span id="向量数据库-索引方式-7">向量数据库-索引方式 [7]</span><a href="#向量数据库-索引方式-7" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/27/gptVectorStore/index.jpg" class>

<h1><span id="向量的相似度算法3">向量的相似度算法[3]</span><a href="#向量的相似度算法3" class="header-anchor">#</a></h1><ul>
<li>Cosine Similarity *<br>余弦</li>
<li>Dot Product *</li>
<li>Squared Euclidean (L2-Squared) *<br>欧式距离</li>
<li>Manhattan (L1 Norm or Taxicab Distance) *</li>
<li>Hamming *</li>
<li>ANN</li>
</ul>
<h3><span id="比较4">比较[4]</span><a href="#比较4" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>Similarity Metric</th>
<th>Vector properties considered</th>
</tr>
</thead>
<tbody><tr>
<td>Euclidean distance</td>
<td>Magnitudes and direction</td>
</tr>
<tr>
<td>Cosine similarity</td>
<td>Only direction</td>
</tr>
<tr>
<td>Dot product similarity</td>
<td>Magnitudes and direction</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/476025527">云原生向量数据库Milvus扫盲，看完这篇就够了</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/477231485">云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema</a></p>
</li>
<li><p><a href="https://weaviate.io/blog/distance-metrics-in-vector-search?ref=blog.langchain.dev">Distance Metrics in Vector Search</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/vector-similarity/">Vector Similarity Explained</a></p>
</li>
<li><p>xxx</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://www.modb.pro/db/1694527960317513728">向量数据库（第 1 部分）：每个数据库有何不同？</a></p>
</li>
</ol>
<p>1xx. <a href="https://cloud.tencent.com/developer/article/2352088">微信向量检索分析一体化数仓探索：OLAP For Embedding</a> *** </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/646832642">Meta向量数据库Faiss介绍</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>向量数据库</category>
      </categories>
      <tags>
        <tag>向量数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>金融大模型</title>
    <url>/www6vHomeAIGC/2022/11/24/gptDomainFinance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%8A%80%E6%9C%AF1">金融大模型 技术[1]</a></li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B">金融大模型</a><ul>
<li><a href="#fingpt-%E5%93%A5%E5%A4%A7-2">FinGPT 哥大 [2]</a></li>
<li><a href="#disc-finllm-4">DISC-FinLLM [4]</a></li>
<li><a href="#%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-3">轩辕金融大模型 [3]</a></li>
<li><a href="#bloomberggpt">BloombergGPT</a></li>
<li><a href="#finbert">FinBERT</a></li>
</ul>
</li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%9C%BA%E6%99%AF">金融场景</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="金融大模型-技术1">金融大模型 技术[1]</span><a href="#金融大模型-技术1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/24/gptDomainFinance/finance.png" class>
<p>A,B  先忽略<br>C - BloombergGPT<br>D - FinGPT</p>
<h1><span id="金融大模型">金融大模型</span><a href="#金融大模型" class="header-anchor">#</a></h1><h3><span id="fingpt-哥大-2">FinGPT   哥大 [2]</span><a href="#fingpt-哥大-2" class="header-anchor">#</a></h3><ul>
<li><p>Resource</p>
<ul>
<li><a href="https://github.com/www6v/FinGPT">Github Repo</a></li>
<li><a href="https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster">FinGPT-Forecaster</a></li>
<li><a href="https://huggingface.co/FinGPT">huggingface</a></li>
<li>五篇paper</li>
</ul>
</li>
<li><p><a href="https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99">Training</a></p>
</li>
</ul>
<h3><span id="disc-finllm-4">DISC-FinLLM [4]</span><a href="#disc-finllm-4" class="header-anchor">#</a></h3><h3><span id="轩辕金融大模型-3">轩辕金融大模型 [3]</span><a href="#轩辕金融大模型-3" class="header-anchor">#</a></h3><ul>
<li>Resource<ul>
<li><a href="https://github.com/Duxiaoman-DI/XuanYuan">XuanYuan</a> git </li>
<li><a href="https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus">数据集</a></li>
</ul>
</li>
</ul>
<p>【大模型(基于BLOOM-176B)转向小模型（XuanYuan-13B-Chat）】</p>
<h3><span id="bloomberggpt">BloombergGPT</span><a href="#bloomberggpt" class="header-anchor">#</a></h3><p>未开源</p>
<h3><span id="finbert">FinBERT</span><a href="#finbert" class="header-anchor">#</a></h3><h1><span id="金融场景">金融场景</span><a href="#金融场景" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《A Survey of Large Language Models in Finance (FinLLMs)》<br> <a href="https://github.com/adlnlp/FinLLMs">FinLLMs</a></li>
<li><a href="https://www.bilibili.com/video/BV1R64y1j76H/">FinGPT开源金融垂类大模型</a> V</li>
<li>&lt;&lt;06【脱敏版】金融行业实战：度小满轩辕金融大模型应用探索与开发实践&gt;&gt;<br> <a href="https://www.bilibili.com/video/BV1G64y1j7Zj/">金融行业实战：度小满轩辕金融大模型应用探索与开发实践</a> V</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404800&idx=2&sn=9c1ad9d8aa8b0725dd6289bc15e177c9">本周大模型代表进展解析:ChatGLM3的特性认识及LoRA专家模组形式的金融领域微调模型实现策略</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400799&idx=1&sn=fb3778d1914849d3b41b047b33ce32a9">ChatGPT能否预测股价走势？大模型应用于金融预测与今日大模型前沿速递</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>金融大模型</category>
      </categories>
      <tags>
        <tag>金融大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Training</title>
    <url>/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#training-pipeline0">Training Pipeline[0]</a><ul>
<li><a href="#%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0-2">设置训练参数 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%87%8F-2">参数量 vs 训练数据量 [2]</a></li>
</ul>
</li>
<li><a href="#pre-training">Pre-training</a><ul>
<li><a href="#pre-training-4">Pre-training [4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="training-pipeline0">Training Pipeline[0]</span><a href="#training-pipeline0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/bigModelTrainingPipeline.jpg" class>

<p><strong>模型训练分为四个阶段</strong> [2]</p>
<ul>
<li>预训练（Pretraining） –&gt;Base model  <ul>
<li>预训练技术<br>预训练本质上是⼀个⽆监督学习过程</li>
</ul>
</li>
<li>监督微调（Supervised Finetuning） –&gt; SFT model<br>核⼼原因还是在于需要“赋予”⼤模型更加定制化的功能</li>
<li>奖励建模（Reward Modeling）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<p><strong>三个角度解析</strong> [2]</p>
<ul>
<li>数据量：<strong>预训练</strong>阶段所需的<strong>数据量很大</strong>，但<strong>质量要求不高</strong>；而<strong>后面的三个阶段</strong>恰恰相反，需要的<strong>数据质量较高</strong>。</li>
<li>训练方法：<strong>预训练和监督微调</strong>的训练方法相同，都是<strong>预测下一个单词</strong>。奖励模型和强化学习的训练方法则不同。<strong>奖励模型</strong>是<strong>二元分类学习</strong>，而<strong>强化学习</strong>则鼓励模型生成奖励模型评分较高的回答。</li>
<li>训练所需资源：预训练阶段的资源消耗巨大，使用数千颗GPU，花费<strong>数月</strong>时间，占总训练时间的99%。后面的三个阶段只需使用数十颗GPU，训练时间约<strong>数天</strong>。</li>
</ul>
<h3><span id="设置训练参数-2">设置训练参数 [2]</span><a href="#设置训练参数-2" class="header-anchor">#</a></h3><p>设置训练参数，如batch-size、learning rate等</p>
<ul>
<li>预训练阶段的<strong>Batch Size非常大</strong>，范围在0.5M到4M之间。</li>
<li><strong>Learning rate设定较小</strong>，且随着网络规模的增大，Learning rate越来越小。</li>
</ul>
<h3><span id="参数量-vs-训练数据量-2">参数量 vs 训练数据量 [2]</span><a href="#参数量-vs-训练数据量-2" class="header-anchor">#</a></h3><p><strong>参数量并不是衡量模型能力的唯一标准，训练数据量也是一个非常重要的因素。</strong><br>LLaMA模型，尽管它的参数量只有650亿，但其性能与参数量为1750亿的GPT-3模型相比也非常优秀。主要原因在于，LLaMA模型的训练数据量达到了1.4万亿，而GPT-3只有3000亿。</p>
<h1><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor">#</a></h1><h3><span id="pre-training-4">Pre-training [4]</span><a href="#pre-training-4" class="header-anchor">#</a></h3><ul>
<li><p>⾃回归与⽣成式</p>
<ul>
<li><strong>⾃回归模型</strong>是⼀种序列模型，它在预测下⼀个输出时，会将之前的所有输出作为输⼊，然后<strong>根据统计规律、结合已经输⼊的样本</strong>，预测下个位置各单词出现的概率，然后输出概率最⼤的单词，类似于完形填空；</li>
<li><strong>⽣成式模型</strong>的预测过程和⾃回归模型类似，都是根据统<br>计规律预测下个单词的概率，所不同的是，<strong>⽣成式模型可以根据之前的样本的<br>概率分布⽣成下⼀个词，⽣成式模型预测时会存在⼀定的随机性；</strong></li>
</ul>
</li>
<li><p>GPT来说，就是⼀个⾃回归⽣成式模型 [4]<br>⼀个⾃回归⽣成式模型在进⾏预测的时候，<strong>会⾸先根据⾃回归模型，在参考到⽬前为⽌<br>已经⽣成的词的情况下确定下⼀个词的概率分布，然后再根据⽣成式的⽅式来根据这个<br>分布⽣成下⼀个词</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a>  from ppt</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://techdiylife.github.io/big-model-training/deepspeed/LLM-state-of-GPT.html">大模型训练入门实战</a>  ***<br><a href="https://karpathy.ai/stateofgpt.pdf">State of GPT</a><br><a href="https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg">State of GPT：大神Andrej揭秘OpenAI大模型原理和训练过程 </a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
<li><p>大模型入门必看教程  九天Hector</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399532&idx=1&sn=31b7bc5a4f3114d8215da0edc2559e47">语言模型预训练基础知识总结：标准数据流pipleline、tokenizer的认识以及常见编码模型范式 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/651316650">从头预训练大模型实践经验</a>  ***<br><a href="https://wandb.ai/site/wp-content/uploads/2023/09/Current-Best-Practices-for-Training-LLMs-from-Scratch-Final.pdf">Current Best Practices for Training LLMs from Scratch</a>  原文</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)PEFT</title>
    <url>/www6vHomeAIGC/2022/11/18/gptFineTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E7%B1%BB">分类</a></li>
<li><a href="#peft-%E5%88%86%E7%B1%BB-11">PEFT 分类 [1.1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="分类">分类</span><a href="#分类" class="header-anchor">#</a></h1><ul>
<li><p>全量微调</p>
</li>
<li><p>局部微调</p>
<ul>
<li>PEFT(Parameter-Efficient Fine-Tuning)  PEFT</li>
</ul>
</li>
</ul>
<h1><span id="peft-分类-11">PEFT 分类 [1.1]</span><a href="#peft-分类-11" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/18/gptFineTuning/category.png" class>

<p>高效微调技术可以粗略分为以下三大类：增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R）。而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。</p>
<ul>
<li><p>PEFT</p>
<ul>
<li>[本质   基于有监督学习]</li>
</ul>
</li>
<li><p>PEFT(Parameter-Efficient Fine-Tuning)  PEFT</p>
<ul>
<li><p><strong>引入重参数化（R）</strong>    </p>
<ul>
<li><strong>LoRA</strong> [2021 Microsoft]<br>Low-Rank Adaptation of LLMs<br>LoRA   【 并联方式的外挂】 [效果比较好]</li>
<li>QLoRA [2023 University of Washington]<br> Efficient Finetuning of Quantized LLMs</li>
<li>AdaLoRA [2023 Microsoft]<br> Adaptive Budget Allocation for PEFT</li>
</ul>
</li>
<li><p>增加额外参数（A）</p>
<ul>
<li><strong>软提示（Soft prompts）</strong> <ul>
<li>Prefix Tuning[2021 Stanford]<br>增加一个可被训练的Embedding层<br>【难实现】</li>
<li><strong>Prompt Tuning</strong> [2021 Google]<br>【简化版本的Prefix Tuning】</li>
<li><strong>P-Turning v1</strong> [2021 Tsinghua]</li>
<li><strong>P-Turning v2</strong> [2022 Tsinghua]</li>
</ul>
</li>
<li>Adapter-Tuning[2019 Google]<br>【 串联方式的外挂】</li>
</ul>
</li>
<li><p>选取一部分参数更新（S）</p>
<ul>
<li>BitFit</li>
</ul>
</li>
<li><p>additive</p>
<ul>
<li>IA3</li>
</ul>
</li>
</ul>
</li>
<li><p>统一微调框架<br>  UniPELT</p>
</li>
<li><p>总结[3]</p>
<img src="/www6vHomeAIGC/2022/11/18/gptFineTuning/overview.jpg" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://github.com/www6v/llm-action#llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86">llm微调技术原理</a>  李国东<br>1.1 <a href="https://zhuanlan.zhihu.com/p/635152813">大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介</a><br>1.2  <a href="https://zhuanlan.zhihu.com/p/649755252">大模型参数高效微调技术原理综述（七）-最佳实践、总结</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1t8411D7v4?p=8">大模型干货教程看这一个就够了~2023年全网最硬核最全面的大模型公开课|大模型微调 | ChatGLM | LangChain</a> V ***</p>
</li>
<li><p><a href="https://aicarrier.feishu.cn/file/H1YvbRyacopEs6xzgZ8c9DDcnIh">大模型参数高效微调技术原理及实践</a> pdf<br><a href="https://www.bilibili.com/video/BV1qw411c7Hd/">如何高效微调大模型？技术原理与最佳实践揭秘！</a> V ***</p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/132116949">LLM高效参数微调方法：从Prefix Tuning、Prompt Tuning、P-Tuning V1&#x2F;V2到LoRA、QLoRA(含对模型量化的解释)</a> **</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402136&idx=1&sn=554331e397015c4da95fb0d0929f5aa1">7月末关于大模型微调数据工程与评估的技术综述：从数据构造方案到模型评估范式的论文梳理指引 </a> 对齐-论文集<br>   <a href="https://github.com/GaryYufei/AlignLLMHumanSurvey">AlignLLMHumanSurvey</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) OpenAI Function Call</title>
    <url>/www6vHomeAIGC/2022/11/16/gptFunctionCall/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#function-call">Function Call</a><ul>
<li><a href="#%E8%B0%83%E7%94%A8%E9%A1%BA%E5%BA%8F-0-12">调用顺序 [0] [1][2]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-2">代码 [2]</a></li>
<li><a href="#goal">goal</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><h3><span id="调用顺序-0-12">调用顺序  [0] [1][2]</span><a href="#调用顺序-0-12" class="header-anchor">#</a></h3><ul>
<li>Function Calling 整个功能的调用顺序大致如下<ul>
<li>声明函数：定义当前函数的名称，描述，以及对应的参数信息，并请求对应的接口；</li>
<li>解析函数参数：接受对应的接口返回，并解析对应的函数参数信息；</li>
<li>执行函数：根据对应的参数信息调用本地函数；</li>
<li>上报结果：将本地函数执行的结果上报给 Chat 接口；</li>
</ul>
</li>
</ul>
<img src="/www6vHomeAIGC/2022/11/16/gptFunctionCall/functioncall1.png" class>

<h3><span id="代码-2">代码 [2]</span><a href="#代码-2" class="header-anchor">#</a></h3><h3><span id="goal">goal</span><a href="#goal" class="header-anchor">#</a></h3><p> The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="http://lihuaxi.xjx100.cn/news/1382737.html">大模型开发(十一)：Chat Completions模型的Function calling功能详解</a> </p>
</li>
<li><p><a href="https://www.duidaima.com/Group/Topic/OtherTools/13709">如何使用Chat Completions接口的函数调用功能</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131892482">OpenAI开发系列（十一）：Function calling功能的实际应用流程与案例解析</a>   代码  流程图<br><a href="https://github.com/www6v/AIGC/tree/master/basic/Function-calling">代码</a>  git</p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131933871">OpenAI开发系列（十三）：利用Function calling功能开发基于大模型的实时天气查询助手</a> 未</p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131912170">OpenAI开发系列（十二）：Function calling功能的流程优化与多轮对话实现</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Function Call</category>
      </categories>
      <tags>
        <tag>Function Call</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Prompt Engineering</title>
    <url>/www6vHomeAIGC/2022/11/10/gptPromptEngineering/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#basic-prompting-2">Basic Prompting [2]</a><ul>
<li><a href="#zero-shot-prompting-3">Zero-Shot Prompting [3]</a></li>
<li><a href="#few-shot-prompting-3">Few-Shot Prompting [3]</a></li>
</ul>
</li>
<li><a href="#cot-2">CoT [2]</a><ul>
<li><a href="#chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</a></li>
<li><a href="#self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</a></li>
<li><a href="#tree-of-thoughts-tot">Tree of Thoughts (ToT)</a></li>
<li><a href="#cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</a></li>
<li><a href="#tips-and-extensions-2">Tips and Extensions   [2]</a></li>
</ul>
</li>
<li><a href="#automatic-prompt-design-2">Automatic Prompt Design [2]</a></li>
<li><a href="#six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</a><ul>
<li><a href="#write-clear-instructions">Write clear instructions</a></li>
<li><a href="#provide-reference-text">Provide reference text</a></li>
<li><a href="#split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</a></li>
<li><a href="#give-the-model-time-to-think">Give the model time to “think”</a></li>
<li><a href="#use-external-tools">Use external tools</a></li>
<li><a href="#test-changes-systematically">Test changes systematically</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E7%82%B9vs-%E7%BC%BA%E7%82%B9">优点vs 缺点</a><ul>
<li><a href="#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="basic-prompting-2">Basic Prompting [2]</span><a href="#basic-prompting-2" class="header-anchor">#</a></h1><h3><span id="zero-shot-prompting-3">Zero-Shot Prompting [3]</span><a href="#zero-shot-prompting-3" class="header-anchor">#</a></h3><h3><span id="few-shot-prompting-3">Few-Shot Prompting [3]</span><a href="#few-shot-prompting-3" class="header-anchor">#</a></h3><h1><span id="cot-2">CoT [2]</span><a href="#cot-2" class="header-anchor">#</a></h1><h3><span id="chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</span><a href="#chain-of-thought-promptingcot-3" class="header-anchor">#</a></h3><ul>
<li>Few-shot CoT</li>
<li>Zero-shot COT<br><strong>“Let’s think step by step”</strong></li>
</ul>
<h3><span id="self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</span><a href="#self-consistencycot-sc-3" class="header-anchor">#</a></h3><p>The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to <strong>select</strong> the most consistent answer.  </p>
<h3><span id="tree-of-thoughts-tot">Tree of Thoughts (ToT)</span><a href="#tree-of-thoughts-tot" class="header-anchor">#</a></h3><h3><span id="cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</span><a href="#cot-vs-cot-sc-vs-tot-3" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/10/gptPromptEngineering/TOT.jpg" class>

<h3><span id="tips-and-extensions-2">Tips and Extensions   [2]</span><a href="#tips-and-extensions-2" class="header-anchor">#</a></h3><p>Self-Ask </p>
<h1><span id="automatic-prompt-design-2">Automatic Prompt Design [2]</span><a href="#automatic-prompt-design-2" class="header-anchor">#</a></h1><ul>
<li>Automatic Chain-of-Thought (Auto-CoT) [3]</li>
</ul>
<h1><span id="six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</span><a href="#six-strategies-for-getting-better-results1" class="header-anchor">#</a></h1><h3><span id="write-clear-instructions">Write clear instructions</span><a href="#write-clear-instructions" class="header-anchor">#</a></h3><p>   清晰的指令</p>
<h3><span id="provide-reference-text">Provide reference text</span><a href="#provide-reference-text" class="header-anchor">#</a></h3><h3><span id="split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</span><a href="#split-complex-tasks-into-simpler-subtasks" class="header-anchor">#</a></h3><pre><code>复杂任务简单化
</code></pre>
<h3><span id="give-the-model-time-to-think">Give the model time to “think”</span><a href="#give-the-model-time-to-think" class="header-anchor">#</a></h3><p>   给模型时间去思考</p>
<h3><span id="use-external-tools">Use external tools</span><a href="#use-external-tools" class="header-anchor">#</a></h3><p>   使用外部工具</p>
<h3><span id="test-changes-systematically">Test changes systematically</span><a href="#test-changes-systematically" class="header-anchor">#</a></h3><h1><span id="优点vs-缺点">优点vs 缺点</span><a href="#优点vs-缺点" class="header-anchor">#</a></h1><h3><span id="优点">优点</span><a href="#优点" class="header-anchor">#</a></h3><p>简单  容易上手</p>
<h3><span id="缺点">缺点</span><a href="#缺点" class="header-anchor">#</a></h3><ul>
<li>上限有限  </li>
<li>模型适配<br>prompt要适配每个模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">Prompt engineering</a>  openai</li>
<li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">Prompt Engineering </a> paper</li>
<li><a href="https://www.promptingguide.ai/techniques">Prompt Engineering Guide</a> guide<br><a href="https://github.com/www6v/Prompt-Engineering-Guide">Prompt-Engineering-Guide </a> *** git</li>
</ol>
<p>1xx.   【社区第十三讲】 老刘说NLP线上交流  *** 很全 </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/682352630">[论文阅读] Prompt Engineering综述</a></p>
<p>1xx. <a href="https://blog.langchain.dev/the-prompt-landscape/">The Prompt Landscape</a>  langchain</p>
<p>1xx. <a href="https://colab.research.google.com/github/comet-ml/comet-llm/blob/main/examples/CometLLM_Prompts.ipynb">CometLLM - suite of LLMOps tools - track and visualize LLM prompts and chains</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671915693">大模型 PUA 指南：来自 Google Meta Microsoft 等大厂</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/632369186">NLP（十三）：Prompt Engineering 面面观</a></p>
<p>1xx. <a href="https://github.com/brexhq/prompt-engineering?tab=readme-ov-file"> prompt-engineering</a> git</p>
<p>1xx. <a href="https://finisky.github.io/chain-of-thought-prompting-summary/">Chain-of-Thought Prompting 简读 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399405&idx=1&sn=75cc058ff83467aa6bf107cf69335e71">ChatGPT应用端的Prompt解析：从概念、基本构成、常见任务、构造策略到开源工具与数据集 </a></p>
<p>1xx. <a href="https://github.com/Eladlev/AutoPrompt">AutoPrompt Repo</a> git</p>
<h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="200">
<li><a href="https://mp.weixin.qq.com/s/nXoZJ4xfgihA2mnBQ8EdIQ">运维大模型探索之 Text2PromQL 问答机器人 </a>     架构图， 最后两个重点总结   未</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent</title>
    <url>/www6vHomeAIGC/2022/11/02/gptAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%8D%E5%90%8C%E8%A7%86%E8%A7%9211">不同视角[11]</a></li>
<li><a href="#%E8%A7%86%E8%A7%921">视角1</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84%E5%9B%BE">架构图</a></li>
<li><a href="#%E7%BB%84%E4%BB%B6-1">组件 [1]</a></li>
<li><a href="#planning-1">Planning [1]</a></li>
<li><a href="#memory-1">Memory [1]</a></li>
<li><a href="#tool-use-1">Tool Use [1]</a></li>
</ul>
</li>
<li><a href="#agentic-reasoning-design-patterns20">Agentic Reasoning Design Patterns[20]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#xxx">xxx</a></li>
<li><a href="#xxx-1">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="不同视角11">不同视角[11]</span><a href="#不同视角11" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Source</th>
<th>作者</th>
<th>Agent &#x3D; LLM + ?</th>
</tr>
</thead>
<tbody><tr>
<td>1. LLM Powered Autonomous Agents</td>
<td>Lilian Weng</td>
<td><strong>Plan</strong> + <strong>Memory</strong> + Tools</td>
</tr>
<tr>
<td>2. The Rise and Potential of Large Language ModelBased Agents: A Survey</td>
<td>FudanNLP</td>
<td>Perception + <strong>Brain</strong> + Action</td>
</tr>
<tr>
<td>3. A Survey on Large Language Model based Autonomous Agents</td>
<td>中国人民大学高瓴人工智能学院</td>
<td>Profile + <strong>Memory</strong> + <strong>Planning</strong> + Action</td>
</tr>
</tbody></table>
<h1><span id="视角1">视角1</span><a href="#视角1" class="header-anchor">#</a></h1><h3><span id="架构图">架构图</span><a href="#架构图" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/02/gptAgent/agent-overview.jpg" class>

<h3><span id="组件-1">组件  [1]</span><a href="#组件-1" class="header-anchor">#</a></h3><p>Agent &#x3D; LLM + plan[规划能力] + memory[记忆能力] +Tools[工具使用能力] </p>
<h3><span id="planning-1">Planning [1]</span><a href="#planning-1" class="header-anchor">#</a></h3><ul>
<li><p>Task Decomposition</p>
<ul>
<li>CoT </li>
<li>ToT</li>
</ul>
</li>
<li><p>Self-Reflection</p>
<ul>
<li>ReAct </li>
<li>Reflexion </li>
<li>Chain of Hindsight</li>
</ul>
</li>
</ul>
<h3><span id="memory-1">Memory [1]</span><a href="#memory-1" class="header-anchor">#</a></h3><ul>
<li>Types of Memory<ul>
<li><strong>Sensory memory</strong> as learning <strong>embedding representations for raw inputs, including text, image or other modalities</strong>;</li>
<li><strong>Short-term memory</strong> as <strong>in-context learning</strong>. It is short and finite, as it is restricted by the finite context window length of Transformer.</li>
<li><strong>Long-term memory</strong> as the external <strong>vector store</strong> that the agent can attend to at query time, accessible via fast retrieval.</li>
</ul>
</li>
</ul>
<h3><span id="tool-use-1">Tool Use [1]</span><a href="#tool-use-1" class="header-anchor">#</a></h3><ul>
<li><p>让 agent 选择合适的工具 [10]</p>
<ul>
<li>可以 retrieve 相关示例来做 <strong>few-shot prompt</strong>。</li>
<li>也可以进一步 <strong>fine tune 特定模型</strong>，例如之前的 Toolformer。</li>
</ul>
</li>
<li><p>Research</p>
<ul>
<li><strong>TALM</strong> (Tool Augmented Language Models; Parisi et al. 2022) [1]</li>
<li><strong>Toolformer</strong> (Schick et al. 2023)   [1]</li>
<li><strong>Gorilla</strong> [10]</li>
</ul>
</li>
<li><p>Production  [1]</p>
<ul>
<li>ChatGPT <strong>Plugins</strong> </li>
<li>OpenAI API <strong>function calling</strong></li>
</ul>
</li>
</ul>
<h1><span id="agentic-reasoning-design-patterns20">Agentic Reasoning Design Patterns[20]</span><a href="#agentic-reasoning-design-patterns20" class="header-anchor">#</a></h1><ul>
<li>robust technology<ul>
<li>Reflection：让 Agent 审视和修正自己生成的输出；</li>
<li>Tool Use：LLM 生成代码、调用 API 等进行实际操作；</li>
</ul>
</li>
<li>emerging technology<ul>
<li>Planning：让 Agent 分解复杂任务并按计划执行；</li>
<li>Multiagent Collaboration：多个 Agent 扮演不同角色合作完成任务；</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ol>
<li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents </a> paper</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/656676717">《综述：全新大语言模型驱动的Agent》</a>  ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/648376562">LLM-based Agents survey 基于大语言模型多智能代理简单综述及展望</a> ***</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://zhuanlan.zhihu.com/p/633033220">LLM 全栈开发指南补遗</a>  Agents  ***<br>   <a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/">Harrison Chase: Agents</a>  ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/679032270">LLM Agent 现状和一些思考 （202401）</a><br>   agent的三种视角</p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/135868163?spm=1001.2014.3001.5502">智能体AI Agent的极速入门：从ReAct、AutoGPT到AutoGen、QwenAgent、XAgent、MetaGPT</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/622947810">AutoGPT与LLM Agent解析</a> *** </p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://mp.weixin.qq.com/s/4ky_OSLrHh2MxdT3AjqW1Q">吴恩达红杉美国 AI 峰会谈 Agent Workflow 以及 4 种主流设计模式，相比 LLM 更强调迭代与对话 </a><br>1xx. <a href="https://mp.weixin.qq.com/s/DyXv9nxFQJYUrAFr22BCCA">深度｜盘点 3 种 OpenAI 等硅谷 AI 大厂在研 Agent 类型</a><br>1xx. <a href="https://mp.weixin.qq.com/s/8k2Qo5vIJ2Gvm9QLFtZA4Q">Agent落地范式本质上是工程及产品设计上的花活：兼看文档图表理解的几个关键问题</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain</title>
    <url>/www6vHomeAIGC/2022/11/02/gptLangchain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#modules">Modules</a><ul>
<li><a href="#main-modules">main modules</a><ul>
<li><a href="#model-io">Model I&#x2F;O</a></li>
<li><a href="#retrieval">Retrieval</a></li>
<li><a href="#agent">Agent</a></li>
</ul>
</li>
<li><a href="#additional-modules">Additional modules</a><ul>
<li><a href="#chains">Chains</a></li>
<li><a href="#memory-10">Memory [10]</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#function-call">Function Call</a></li>
<li><a href="#%E5%BA%94%E7%94%A84">应用[4]</a></li>
<li><a href="#chains-1-89">Chains [1] [8][9]</a></li>
<li><a href="#templates7">Templates[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="modules">Modules</span><a href="#modules" class="header-anchor">#</a></h1><h2><span id="main-modules">main modules</span><a href="#main-modules" class="header-anchor">#</a></h2><h3><span id="model-ix2fo">Model I&#x2F;O</span><a href="#model-ix2fo" class="header-anchor">#</a></h3><ul>
<li>Language models  [10]        <ul>
<li>LLM</li>
<li>Chat Model</li>
<li><strong>Embedding</strong></li>
</ul>
</li>
<li>Prompts <ul>
<li>Prompt Template</li>
<li>Few-shot example</li>
<li>Example Selectors [类比选择]<br>关键字  相似度  长度</li>
</ul>
</li>
<li>Output parsers</li>
<li><strong>function call</strong>[2]</li>
</ul>
<h3><span id="retrieval">Retrieval</span><a href="#retrieval" class="header-anchor">#</a></h3><ul>
<li>Document Loaders</li>
<li>Text Splitters</li>
<li><strong>Retrievers</strong>[10]</li>
<li>VectorStores</li>
<li>index</li>
</ul>
<h3><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h3><ul>
<li>Plan-and-execute agents</li>
</ul>
<h2><span id="additional-modules">Additional modules</span><a href="#additional-modules" class="header-anchor">#</a></h2><h3><span id="chains">Chains</span><a href="#chains" class="header-anchor">#</a></h3><ul>
<li>2大类<ul>
<li>Chain interface[Legacy]</li>
<li>LangChain Expression Language (LCEL)<br>LCEL is a declarative way to compose chains.</li>
</ul>
</li>
<li>Foundational<ul>
<li>LLM</li>
<li>Sequential- SequentialChain</li>
<li><strong>Router</strong></li>
<li>Transformation</li>
</ul>
</li>
</ul>
<h3><span id="memory-10">Memory [10]</span><a href="#memory-10" class="header-anchor">#</a></h3><ul>
<li>帮语言模型补充上下文</li>
<li>ConversationBufferMemory</li>
<li>ConversationBufferWindowMemory<br>窗口</li>
<li>ConversationSummaryMemory</li>
<li>VectorStoreRetrieverMemory</li>
</ul>
<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.base <span class="keyword">import</span> (</span><br><span class="line">    create_openai_fn_chain,</span><br><span class="line">    create_structured_output_chain,[<span class="number">2</span>]</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.citation_fuzzy_match <span class="keyword">import</span> (</span><br><span class="line">    create_citation_fuzzy_match_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.extraction <span class="keyword">import</span> (</span><br><span class="line">    create_extraction_chain,</span><br><span class="line">    create_extraction_chain_pydantic,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.qa_with_structure <span class="keyword">import</span> (</span><br><span class="line">    create_qa_with_sources_chain,</span><br><span class="line">    create_qa_with_structure_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.tagging <span class="keyword">import</span> (</span><br><span class="line">    create_tagging_chain,</span><br><span class="line">    create_tagging_chain_pydantic,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h1><span id="应用4">应用[4]</span><a href="#应用4" class="header-anchor">#</a></h1><ul>
<li>Question &amp; Answering Using Documents As Context[3]</li>
<li>Extraction[Kor]</li>
<li>Evaluation</li>
<li>Querying Tabular Data[sqlite]</li>
<li>Code Understanding</li>
<li>Interacting with APIs</li>
<li>Chatbots</li>
</ul>
<h1><span id="chains-1-89">Chains [1] [8][9]</span><a href="#chains-1-89" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;stuff&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;map_reduce&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;refine&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_qa_chain(llm, chain_type=<span class="string">&quot;map_rerank&quot;</span>, verbose=<span class="literal">True</span>, return_intermediate_steps=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>链类型</th>
<th>整合方法</th>
<th>优缺点</th>
</tr>
</thead>
<tbody><tr>
<td>stuff</td>
<td>将所有内容放入一个提示中，输入LLM</td>
<td>简单、廉价、效果好&#x2F; 对输入文本有一定token限制</td>
</tr>
<tr>
<td>Map_reduce</td>
<td>每个问题和文本块单独给语言模型，并将答案汇总生成最终结果</td>
<td>输入任意数量文本，且并行处理&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Refine</td>
<td>迭代处理多个文本，基于前一个文档答案构建下一个答案</td>
<td>用于组合信息，依次构建答案&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Map_rerank</td>
<td>每个文档单独调用LLM,并要求返回一个得分，然后选择最高的得分</td>
<td>需要告诉模型评分的规则&#x2F; 费token</td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2022/11/02/gptLangchain/chains-type.jpg" class>


<h1><span id="templates7">Templates[7]</span><a href="#templates7" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/gkamradt/langchain-tutorials">https://github.com/gkamradt/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-functioncall.py">functioncall</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-qaOnDoc.py">qaOnDoc</a></p>
</li>
<li><p><a href="https://github.com/www6v/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb">LangChain Cookbook Part 2: Use Cases</a><br> 10.公开课</p>
</li>
<li><p><a href="https://github.com/kyrolabs/awesome-langchain">https://github.com/kyrolabs/awesome-langchain</a></p>
</li>
<li><p><a href="https://github.com/Crossme0809/langchain-tutorials">https://github.com/Crossme0809/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/templates/docs/INDEX.md">Templates</a> *** docs<br><a href="https://templates.langchain.com/">templates</a> webui</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/666656208">吴恩达短课_LangChain</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651216604">精华笔记：吴恩达 x LangChain 《使用LangChain构建与数据对话的聊天机器人》（下）</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2313918">一文入门最热的LLM应用开发框架LangChain</a> 未</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2331337">大模型LangChain框架基础与使用示例</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)RAG</title>
    <url>/www6vHomeAIGC/2022/11/02/gptRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rag-overview2">RAG Overview[2]</a></li>
<li><a href="#rag-vs-ft-2">RAG vs FT [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
<li><a href="#%E8%AF%84%E4%BC%B0">评估</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="rag-overview2">RAG Overview[2]</span><a href="#rag-overview2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/02/gptRAG/rag-overview.jpg" class>


<h1><span id="rag-vs-ft-2">RAG vs FT [2]</span><a href="#rag-vs-ft-2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/02/gptRAG/rag-vs-ft.jpg" class>
<p>todo:  有中文翻译的图片</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol start="2">
<li><p>《Retrieval-Augmented Generation for Large Language Models: A Survey》<br><a href="https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey">面向大语言模型的检索增强生成技术：综述 [译]</a>  翻译<br><a href="https://zhuanlan.zhihu.com/p/673910600">LLM之RAG理论（二）| RAG综述论文详解</a><br><a href="https://cloud.tencent.com/developer/article/2373340">同济大学发布最新检索增强(RAG)的LLM生成技术综述</a><br><a href="https://mp.weixin.qq.com/s/JjcN6OoxNK7tddmIOpvr2g">面向大模型的检索增强生成（RAG）综述 </a><br><a href="https://www.promptingguide.ai/zh/research/rag">大语言模型的检索增强生成 (RAG) 方法</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> ***</p>
</li>
</ol>
<p>1xx. 《Retrieval-Augmented Generation for AI-Generated Content: A Survey》<br><a href="https://mp.weixin.qq.com/s/FKv9glaGZD0qbLmB2zg6bg">北大最新综述精读：RAG在AIGC中的前世今生，覆盖300篇论文！</a><br>   <a href="https://mp.weixin.qq.com/s?__biz=MzkzODMxNTkzNg==&mid=2247484337&idx=1&sn=484db46f6a974cb26b7659096b31cdd8">最新RAG综述来了！北京大学发布AIGC的检索增强技术综述</a></p>
<p>1xx.  《A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models》<br><a href="https://mp.weixin.qq.com/s/AqBPzewJm8dKwjz5BBu-Ag">百度最新大模型RAG综述 | RAG 与 LLM 的融合调查：面向检索增强型大型语言模型</a><br><a href="https://mp.weixin.qq.com/s/h8z4eXsemPMeL2oI_8VnvQ">一文看懂RAG的各种套路 | 综述：当RAG遇到大语言模型</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/jgyIOOzRWAgilcW4HfufNQ">行业大模型落地的一些有趣调研总结：兼看大模型RAG问答四大技术综述</a> 四大综述</p>
<h3><span id="评估">评估</span><a href="#评估" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404511&idx=2&sn=fefb78c1d920cb5b437f2e3da9935637">再看大模型RAG检索增强如何评估：RAGAS开源自动化评估框架</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404476&idx=2&sn=d07b27dc9162ab0aaec3108004e4cfbe">大模型RAG检索增强问答如何评估：噪声、拒答、反事实、信息整合四大能力评测任务探索 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)大模型</title>
    <url>/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#llms%E7%9A%84%E8%83%8C%E6%99%AF1">LLMs的背景[1]</a><ul>
<li><a href="#scaling-law-of-llms">Scaling law of LLMs</a></li>
<li><a href="#llms%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B">LLMs的涌现能力</a></li>
<li><a href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF">大语言模型的关键技术 ***</a></li>
</ul>
</li>
<li><a href="#pre-training1">Pre-training[1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86">数据收集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">模型训练 ***</a></li>
</ul>
</li>
<li><a href="#adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</a><ul>
<li><a href="#%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98">指令调优 ***</a><ul>
<li><a href="#%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%9E%84%E5%BB%BA">格式化实例的构建</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%AD%96%E7%95%A5">指令微调策略</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%9A%84%E6%95%88%E6%9E%9C">指令微调的效果</a></li>
</ul>
</li>
<li><a href="#%E5%AF%B9%E9%BD%90%E8%B0%83%E4%BC%98">对齐调优</a></li>
<li><a href="#%E9%AB%98%E6%95%88%E8%B0%83%E4%BC%98">高效调优</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="llms的背景1">LLMs的背景[1]</span><a href="#llms的背景1" class="header-anchor">#</a></h1><h3><span id="scaling-law-of-llms">Scaling law of LLMs</span><a href="#scaling-law-of-llms" class="header-anchor">#</a></h3><ul>
<li>KM scaling law</li>
<li>Chinchilla Scaling law</li>
</ul>
<h3><span id="llms的涌现能力">LLMs的涌现能力</span><a href="#llms的涌现能力" class="header-anchor">#</a></h3><ul>
<li>in-context learning</li>
<li>instruction following</li>
<li>step-by-step reasoning</li>
</ul>
<h3><span id="大语言模型的关键技术">大语言模型的关键技术 ***</span><a href="#大语言模型的关键技术" class="header-anchor">#</a></h3><ul>
<li>Scaling</li>
<li>Training</li>
<li>Ability Eliciting</li>
<li>Alignment Tuning</li>
<li>Tool Manipulation</li>
</ul>
<h1><span id="pre-training1">Pre-training[1]</span><a href="#pre-training1" class="header-anchor">#</a></h1><h3><span id="数据收集">数据收集</span><a href="#数据收集" class="header-anchor">#</a></h3><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><h3><span id="模型训练">模型训练 ***</span><a href="#模型训练" class="header-anchor">#</a></h3><ul>
<li><p>优化设置</p>
<ul>
<li>Batch Training</li>
<li>Learning Rate</li>
<li>Optimizer</li>
<li>Stabilizing the Training</li>
</ul>
</li>
<li><p>可扩展的训练技巧</p>
<ul>
<li>3D并行<br>数据并行 +  流水线并行 + 张量并行</li>
<li>ZeRO</li>
<li>混合精度训练</li>
<li>总体训练建议</li>
</ul>
</li>
</ul>
<h1><span id="adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</span><a href="#adaptation-tuning-of-llms1" class="header-anchor">#</a></h1><h3><span id="指令调优">指令调优 ***</span><a href="#指令调优" class="header-anchor">#</a></h3><p>本质上，指令微调是在<strong>自然语言格式的实例（instance）集合上</strong>微调预训练后的 LLM 的方法 [62]。</p>
<p>指令微调后，LLM 可以展现出<strong>泛化到未见过任务</strong>的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。</p>
<h5><span id="格式化实例的构建">格式化实例的构建</span><a href="#格式化实例的构建" class="header-anchor">#</a></h5><ul>
<li>格式化已有数据集</li>
<li>格式化人类需求</li>
<li>构建实例的关键因素<ul>
<li><strong>增加指令</strong></li>
<li><strong>设计格式</strong></li>
</ul>
</li>
</ul>
<p>总的来说，指令<strong>多样性似乎比实例数量更重要</strong></p>
<h5><span id="指令微调策略">指令微调策略</span><a href="#指令微调策略" class="header-anchor">#</a></h5><ul>
<li><p><strong>平衡数据分布</strong><br>一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。<br>此外，根据最近的研究发现 [64, 99]，<strong>提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例</strong>通常可以带来<strong>性能提升</strong>。</p>
</li>
<li><p>结合指令微调和预训练<br>为了使微调过程更加有效和稳定，OPT-IML [99] 在<strong>指令微调期间加入了预训练数据</strong>，这可以看作是对模型的正则化（regularization）。</p>
</li>
</ul>
<p>具体而言，GLM-130B [97] 和 Galactica [34] 将<strong>指令格式数据集作为预训练语料库的一小部分来预训练 LLM</strong>，这有可能同时获得预训练和指令微调的优势。</p>
<h5><span id="指令微调的效果">指令微调的效果</span><a href="#指令微调的效果" class="header-anchor">#</a></h5><ul>
<li>性能改进<br>最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，**表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]**。 【普适性】</li>
</ul>
<p>此外，**经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]**。</p>
<ul>
<li>任务泛化性<br>todo</li>
</ul>
<h3><span id="对齐调优">对齐调优</span><a href="#对齐调优" class="header-anchor">#</a></h3><h3><span id="高效调优">高效调优</span><a href="#高效调优" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://aibox.ruc.edu.cn/docs/2023-08/cb9badcb213f4c8b89d00d579eed4a4c.pdf">大语言模型综述</a> 中文  v10<br>  <a href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf">大语言模型综述</a> 中文<br>  <a href="https://github.com/www6v/LLMSurvey">LLMSurvey Repo</a>  git<br>  <a href="https://zhuanlan.zhihu.com/p/630203554">[论文]大语言模型综述</a><br>  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400817&idx=1&sn=c1ed1c9c87bf2526e02d21d84429c5cf">详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结</a><br>  <a href="https://zhuanlan.zhihu.com/p/662673023">大模型综述-A Survey of Large Language Models</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408221&idx=1&sn=2874583ed668ae0b89889c81a4ab8d79">值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/381282229">43页预训练模型综述（清华、复旦、人大）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT  学习资源</title>
    <url>/www6vHomeAIGC/2022/08/01/gptStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E7%A8%8B">工程</a></li>
<li><a href="#%E8%AF%BE%E7%A8%8B">课程</a><ul>
<li><a href="#%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4">极客时间</a></li>
<li><a href="#%E7%9F%A5%E4%B9%8E">知乎</a></li>
<li><a href="#%E6%B8%85%E5%8D%8E">清华</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6">百度</a></li>
<li><a href="#%E4%B9%9D%E5%A4%A9">九天</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E4%B8%9A%E7%95%8C">工业界</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="工程">工程</span><a href="#工程" class="header-anchor">#</a></h1><ul>
<li><a href="https://github.com/www6v/openai-cookbook">openai-cookbook</a><br><a href="https://cookbook.openai.com/">cookbook.openai</a></li>
</ul>
<h1><span id="课程">课程</span><a href="#课程" class="header-anchor">#</a></h1><h3><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h3><ul>
<li><a href="https://shimo.im/docs/47kgM6NewnSO613V">尚硅谷×极客时间《AI 大模型实战训练营》大纲</a> </li>
<li><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a> </li>
<li><a href="https://w.1yb.co/KqBR58E">《AI 大模型微调训练营》大纲</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541101">ChatGPT 从 0 到 1</a>  基础</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541201">ChatGPT 和预训练模型实战课</a></li>
</ul>
<h3><span id="知乎">知乎</span><a href="#知乎" class="header-anchor">#</a></h3><ul>
<li><a href="https://agiclass.feishu.cn/docx/DDzxdQZBooXw9Jx4DdWcLZjLnHd">《AI 大模型全栈工程师》课程表（第 02 期） </a>  </li>
<li><a href="https://www.zhihu.com/people/dou-hong-jian-44/posts">AI Box专栏</a>  中国人大  AI ***<br>大模型survey</li>
</ul>
<h3><span id="清华">清华</span><a href="#清华" class="header-anchor">#</a></h3><ul>
<li><a href="https://www.zhihu.com/education/video-course/1545850719483392000">【清华 NLP X OpenBMB】大模型公开课｜带你从入门到实战</a>  V ***</li>
</ul>
<h3><span id="百度">百度</span><a href="#百度" class="header-anchor">#</a></h3><p><a href="https://cloud.baidu.com/qianfandev/topic/267956">《大模型应用实践》实训营</a></p>
<h3><span id="九天">九天</span><a href="#九天" class="header-anchor">#</a></h3><p><a href="https://appze9inzwc2314.pc.xiaoe-tech.com/p/t_pc/goods_pc_detail/goods_detail/p_64467371e4b0cf39e6c0c026?fromH5=true&entry_type=2002&share_type=5&type=3&entry=2">大模型技术实战课 </a></p>
<h1><span id="工业界">工业界</span><a href="#工业界" class="header-anchor">#</a></h1><p><a href="https://rocketmq-learning.com/">rocketmq-learning 社区</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning</title>
    <url>/www6vHomeAIGC/2022/06/11/aiDeepLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#deeplearning-1">DeepLearning [1]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%832">模型训练[2]</a></li>
<li><a href="#%E6%B1%82%E8%A7%A3%E5%99%A82">求解器[2]</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B02">常用的损失函数[2]</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B6%85%E5%8F%822">常用的超参[2]</a><ul>
<li><a href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88">过拟合与欠拟合</a></li>
<li><a href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E7%AD%96%E7%95%A5">学习率调整策略</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%B8%B8%E8%A7%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%843">自然语言处理常见的网络结构[3]</a><ul>
<li><a href="#%E6%96%87%E6%9C%AC%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-textcnn">文本卷积神经网络 TextCNN</a></li>
<li><a href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-rnn">循环神经网络 RNN</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="deeplearning-1">DeepLearning [1]</span><a href="#deeplearning-1" class="header-anchor">#</a></h1><ul>
<li><p>Feedforward 前向传播</p>
<ul>
<li>training<br>Get some “ground truth” labeled data, a set of   (𝒙, 𝒚)    i.e. training data</li>
<li>Feedforward:   𝒚′&#x3D; 𝒇(𝒙),  calculate loss: 𝑳(𝒚′, 𝒚)</li>
<li>Gradient Descent</li>
</ul>
</li>
<li><p>Backward</p>
<ul>
<li>Backpropagation 反向传播<br>算出每个权重的梯度</li>
</ul>
</li>
<li><p>parameters learning<br>当我们要去训练一个神经网络的时候我们要做的事情就是先feedforward的前向传播,<br>然后根据这个前向传播的结果算出所有权重的梯度，然后再把这个梯度呢 转换成一个update的值，去update每个权重。</p>
</li>
</ul>
<h1><span id="模型训练2">模型训练[2]</span><a href="#模型训练2" class="header-anchor">#</a></h1><ul>
<li><p>两个要素</p>
<ul>
<li>一个<strong>数据集</strong></li>
<li>一个<strong>损失函数</strong></li>
</ul>
</li>
<li><p>模型训练</p>
<ul>
<li>本质上是一个求解最优化问题的过程</li>
<li>怎么求解<ul>
<li>梯度下降与凸问题<br>梯度决定了函数变化的方向，每次迭代更新我们会<strong>收敛到一个极值</strong></li>
<li>mini-batch梯度下降<br>条件允许的情况下，<strong>Batch Size尽量大些</strong>      </li>
<li><strong>学习率</strong><ul>
<li>学习率也很关键，甚至需要<strong>动态调整</strong><br>适当调整学习率（Learning Rate），<strong>避免陷入很差的局部解或者跳过了好的解</strong>  </li>
<li>学习率，它和梯度的模数共同决定了<strong>每步走多远</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="求解器2">求解器[2]</span><a href="#求解器2" class="header-anchor">#</a></h1><ul>
<li>最常用的就是 <strong>Adam</strong> 或者 <strong>AdamW</strong></li>
</ul>
<h1><span id="常用的损失函数2">常用的损失函数[2]</span><a href="#常用的损失函数2" class="header-anchor">#</a></h1><ul>
<li>两个数值的差距</li>
<li>两个向量之间的（欧式）距离</li>
<li>两个向量之间的夹角（余弦距离）</li>
<li>两个概率分布之间的差异，<strong>交叉熵</strong></li>
</ul>
<h1><span id="常用的超参2">常用的超参[2]</span><a href="#常用的超参2" class="header-anchor">#</a></h1><h3><span id="过拟合与欠拟合">过拟合与欠拟合</span><a href="#过拟合与欠拟合" class="header-anchor">#</a></h3><ul>
<li>防止过拟合的方法（1）：<strong>Weight Decay</strong><br>惩罚参数的复杂性</li>
<li>防止过拟合的方法（2）：<strong>Dropout</strong><br> 在前向传播的时候，概率性的（临时）删除一部分神经元，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征</li>
</ul>
<h3><span id="学习率调整策略">学习率调整策略</span><a href="#学习率调整策略" class="header-anchor">#</a></h3><ul>
<li><p>开始时学习率<strong>大</strong>些：快速到达最优解附近</p>
</li>
<li><p>逐渐<strong>减小</strong>学习率：避免跳过最优解</p>
</li>
<li><p>NLP 任务的损失函数有很多“悬崖峭壁”，自适应学习率更能处理这种极端情况，<strong>避免梯度爆炸</strong>。</p>
</li>
<li><p><strong>防止过拟合的方法（3）</strong>：<strong>学习率 Warm Up</strong></p>
<ul>
<li>先从一个很小的学习率逐渐上升到正常学习率，在稳步减小学习率</li>
</ul>
</li>
</ul>
<h1><span id="自然语言处理常见的网络结构3">自然语言处理常见的网络结构[3]</span><a href="#自然语言处理常见的网络结构3" class="header-anchor">#</a></h1><h3><span id="文本卷积神经网络-textcnn">文本卷积神经网络 TextCNN</span><a href="#文本卷积神经网络-textcnn" class="header-anchor">#</a></h3><ul>
<li><p>how</p>
<ul>
<li>一个窗口的卷积和Pooling过程</li>
</ul>
</li>
<li><p>why</p>
<ul>
<li>参数量较少、好训练、算力要求低</li>
<li>适合<strong>文本分类</strong>问题</li>
<li>善于表示<strong>局部特征（卷积窗口）</strong>，不擅长表示长上下文依赖关系</li>
</ul>
</li>
</ul>
<h3><span id="循环神经网络-rnn">循环神经网络 RNN</span><a href="#循环神经网络-rnn" class="header-anchor">#</a></h3><ul>
<li>简易 RNN<br>最大问题是随着序列长度增加，<strong>梯度消失或爆炸</strong></li>
<li>LSTM 和 GRU<br>通过「门」来控制上文的状态被记住还是遗忘，同时防止梯度消失或爆炸</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1GA41157mJ/">系统论文阅读研讨会week9：机器学习系统（一）</a> V ***<br> <a href="https://learn-sys.github.io/cn/reading/">W9：机器学习系统（一）</a> ***  对应的PPT</p>
</li>
<li><p>《11-机器学习基础-上》AI 大模型全栈工程师培养计划_2<br><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/index.ipynb">课件</a> </p>
</li>
<li><p>《12-机器学习基础-下》AI 大模型全栈工程师培养计划_2<br> <a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/index.ipynb">课件</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/461925341">吴恩达：28张图全解深度学习知识</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/www6vHomeAIGC/2022/06/07/aiMachineLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="机器学习算法">机器学习算法</span><a href="#机器学习算法" class="header-anchor">#</a></h2><ul>
<li><p>监督式学习</p>
<ul>
<li><p>Linear Models</p>
<ul>
<li>逻辑回归 (Logistic Regression)<br><strong>离散</strong><br>逻辑回归其实是一个分类算法而不是回归算法。</li>
<li>线性回归 (Linear Regression)<br><strong>连续</strong></li>
</ul>
</li>
<li><p>Nearest Neighbors</p>
<ul>
<li>K邻近算法，KNN</li>
</ul>
</li>
<li><p>决策树 Decision Trees</p>
</li>
<li><p>Support Vector Machines, SVM [2]</p>
<ul>
<li>可分 <ul>
<li>线性可分</li>
<li>线性不可分</li>
</ul>
</li>
<li>超平面<ul>
<li>低纬升到高纬</li>
</ul>
</li>
</ul>
</li>
<li><p>Naive Bayes</p>
</li>
<li><p>随机森林</p>
</li>
</ul>
</li>
<li><p>无监督式学习</p>
<ul>
<li>关联规则 </li>
<li>K-means聚类算法<br>质心（centroids），距离</li>
</ul>
</li>
<li><p>强化学习</p>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>Classification<br>Identifying which category an object belongs to.</li>
<li>Regression<br>  Predicting a continuous-valued attribute associated with an object.</li>
<li>Clustering<br>  Automatic grouping of similar objects into sets.  </li>
<li>Dimensionality reduction<br>  Reducing the number of random variables to consider.</li>
</ul>
<img src="/www6vHomeAIGC/2022/06/07/aiMachineLearning/scikit-learn.png" class title="scikit-learn overview">



<h2><span id="按学习模型划分-4">按学习模型划分 [4]</span><a href="#按学习模型划分-4" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/479973669">【机器学习算法】10种常见机器学习算法+Python代码</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/b8227eac1fa6">机器学习–有监督–支持向量机SVM</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/supervised_learning.html">Supervised learning</a></p>
</li>
<li><p><a href="https://blog.csdn.net/hustlei/article/details/121803226">人工智能导论(6)——机器学习(Machine Learning)</a> ***</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-工具和应用</title>
    <url>/www6vHomeAIGC/2022/05/09/gpt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#platform">Platform</a></li>
<li><a href="#tools-mix">Tools &amp; Mix</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE">思维导图</a></li>
<li><a href="#%E8%A7%86%E9%A2%91">视频</a></li>
<li><a href="#%E8%8B%B1%E8%AF%AD">英语</a></li>
</ul>
</li>
<li><a href="#%E5%AE%A2%E6%88%B7%E7%AB%AF">客户端</a></li>
<li><a href="#chrome-plugin">Chrome plugin</a></li>
<li><a href="#%E5%88%9B%E4%B8%9A">创业</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="platform">Platform</span><a href="#platform" class="header-anchor">#</a></h1><ul>
<li>国外<br><a href="https://poe.com/ChatGPT">Poe</a> ***</li>
<li>国内<br><a href="https://saas.edu360.cn/system/chatgpt">实战云</a> gpt3.5  gpt4<br><a href="https://www.feijix.com/n/y0BnXI">ChatGPT使用指南！</a>   ***<br><a href="https://www.1888ai.com/base/chat">灵犀百通</a>  gpt3.5<br><a href="https://gpt.91chat-ai.cn/chat">ChatGpt PLUS</a><br><a href="https://yiyan.baidu.com/">文心一言</a></li>
</ul>
<h1><span id="tools-amp-mix">Tools &amp; Mix</span><a href="#tools-amp-mix" class="header-anchor">#</a></h1><ul>
<li><p>GPT学习宝典</p>
<ul>
<li>聚合<ul>
<li><a href="https://gpt.candobear.com/toolbox">GPT  工具箱</a></li>
</ul>
</li>
<li>教程<ul>
<li><a href="https://gpt.candobear.com/courses">学习资料</a></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe">极客时间 AIGC 知识库</a> *** </p>
<ul>
<li>聚合<ul>
<li><a href="https://gp477l8icq.feishu.cn/wiki/M1uCwFNjkiAGC7k30TaclZqknPh">AI工具大全</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/RpabwPG9niFEu9kwJAQcAGxenDg">AI主流工具精选</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/VJ9ewqfOgiyrbQksbyLcrODtnkb">AI经典项目</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/QVV6w3XstiR7hlkK53Bc8f9DnMf">AI导航站</a></li>
</ul>
</li>
<li><a href="https://longalong.feishu.cn/wiki/wikcneAKpN3u473N7J9EAC4Ga0b">应用与变现案例</a></li>
</ul>
</li>
<li><p><a href="https://www.ailookme.com/">AI 工具箱</a>  *** </p>
</li>
<li><p><a href="https://gptdoc.sparkai.chat/">ChatGPT Tutorial 101</a></p>
</li>
</ul>
<h1><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h1><h3><span id="思维导图">思维导图</span><a href="#思维导图" class="header-anchor">#</a></h3><p><a href="https://albus.org/">albus</a></p>
<h3><span id="视频">视频</span><a href="#视频" class="header-anchor">#</a></h3><p><a href="https://b.jimmylv.cn/">BibiGPT</a><br><a href="https://crucible.docnavigator.in/">Youtube tools</a></p>
<h3><span id="英语">英语</span><a href="#英语" class="header-anchor">#</a></h3><p><a href="https://callannie.ai/signin">callannie</a></p>
<h1><span id="客户端">客户端</span><a href="#客户端" class="header-anchor">#</a></h1><ul>
<li>ChatGPT 客户端<br> windows， mac</li>
</ul>
<h1><span id="chrome-plugin">Chrome plugin</span><a href="#chrome-plugin" class="header-anchor">#</a></h1><ul>
<li><p>WebChatGPT[instatlled]</p>
</li>
<li><p>AIPRM for ChatGPT[instatlled]</p>
</li>
<li><p>ChatGPT Sidebar<br>要注册账号, 需要api token</p>
</li>
<li><p>ChatHub  [instatlled]<br> chatgpt + new bing<br><a href="https://github.com/chathub-dev/chathub">ChatHub </a></p>
</li>
<li><p>OpenAI Translator<br><a href="https://github.com/yetone/openai-translator">openai-translator</a><br>要注册账号, 需要api token</p>
</li>
</ul>
<h1><span id="创业">创业</span><a href="#创业" class="header-anchor">#</a></h1><ul>
<li><a href="https://gpt3demo.com/map">GPT-3 Demo</a><ul>
<li>聊天机器人</li>
<li>代码辅助</li>
<li>写作应用</li>
<li>游戏</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能-学习资源</title>
    <url>/www6vHomeAIGC/2022/01/22/aiStudyResouce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="书籍">书籍</span><a href="#书籍" class="header-anchor">#</a></h2><ul>
<li>理论<ul>
<li>《人工智能的数据基础》</li>
<li>《统计学习方法》 v2</li>
<li>The Hundred-Page Machine Learning Book - 入门</li>
<li>西瓜书 ***</li>
<li>花书</li>
</ul>
</li>
<li>框架<ul>
<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, v3 - 入门</li>
<li>Deep Learning with PyTorch - 入门</li>
<li>Deep Learning with Python - v2 - keras库</li>
</ul>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>浙大 - 吴浩基   ***</li>
<li>周志华 《机器学习初步》 *** </li>
<li>Coursera吴恩达- 《machine learning》+ 笔记  入门  *** </li>
<li><a href="https://www.bilibili.com/video/av79340208/">Machine Learning A-Z Hands-On Python &amp; R In Data Science</a>  Udemy 入门  ***</li>
<li><a href="https://www.bilibili.com/video/BV1KB4y1E73v">【人工智能系列】【中文】机器学习A-Z Machine Learning in Chinese(前7部分)</a></li>
<li><a href="https://www.bilibili.com/video/BV1jF411A7VF/">[Coursera公开课] [机器学习专项课程1&#x2F;4] 机器学习基础：案例研究</a>  ***</li>
<li><a href="https://www.bilibili.com/video/BV1Bg411Z77N">聚类算法：层次聚类、k-means 聚类、k-medoids 聚类、密度聚类</a>  ***</li>
</ul>
<h2><span id="深度学习">深度学习</span><a href="#深度学习" class="header-anchor">#</a></h2><ul>
<li>《动手学深度学习- 第二版 -pyTorch》  ***<br>b站有视频课<br><a href="http://zh.d2l.ai/index.html">《动手学深度学习》</a> 在线<br>电子书+jupternote代码</li>
<li>《神经网络和深度学习 》 复旦  </li>
<li>Coursera吴恩达《深度学习》 + 笔记  ***</li>
<li>老唐 ***</li>
<li>莫烦Python </li>
<li>北京大学 TensorFlow 2.0</li>
<li>算法可视化  ***</li>
<li>李宏毅 台湾</li>
</ul>
<h2><span id="nlp-amp-大模型">NLP &amp; 大模型</span><a href="#nlp-amp-大模型" class="header-anchor">#</a></h2><ul>
<li><p><a href="https://www.zhihu.com/education/video-course/1546509363711614976">沈向洋带你读论文——CV &amp; NLP 专题</a> V </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1C14y147dp">2022年首发！B站讲的最好的【NLP自然语言处理】保姆级教程！</a>  V  有实践  *** </p>
</li>
<li><p>MLNLP第六期学术研讨会开始报名</p>
</li>
</ul>
<h2><span id="知识图谱">知识图谱</span><a href="#知识图谱" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1VT411G7Y6?p=6">【国家级精品课】浙江大学教授（新全44集）知识图谱公开课分享</a>  ***</li>
</ul>
<h2><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h2><ul>
<li>极客时间<ul>
<li>《AI 技术内参》  洪亮劼   全 ***</li>
<li>《机器学习 40 讲》  王天一 </li>
<li>《人工智能基础课》  王天一<br> 机器学习，深度学习</li>
<li>《成为AI产品经理》  刘海丰 京东   </li>
<li>《零基础实战机器学习》 黄佳  ***</li>
<li>《PyTorch深度学习实战》方远 大厂</li>
<li><a href="https://time.geekbang.org/course/intro/100023001?tab=catalog">TensorFlow 快速入门与实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/315">TensorFlow 2 项目进阶实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/100046401">NLP 实战高手课</a></li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>深度学习在CTR预估的应用   张俊林</li>
<li>深度学习在图像理解中的应用  熊鹏飞</li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>知识图谱技术实践  邵蓥侠</li>
</ul>
</li>
<li>极客训练营<br>-《机器学习训练营1期》  视频课</li>
</ul>
<h2><span id="中国大学mooc">中国大学MOOC</span><a href="#中国大学mooc" class="header-anchor">#</a></h2><ul>
<li>中国大学MOOC <a href="https://www.icourse163.org/learn/HIT-1206320802?tid=1468208513#/learn/announce">深度学习基础</a>   哈尔滨工业大学</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/FUDAN-1205806833">深度学习及其应用</a>   复旦</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/ZUCC-1206146808">深度学习应用开发-TensorFlow实践</a>  浙大城市学院</li>
</ul>
<h2><span id="培训">培训</span><a href="#培训" class="header-anchor">#</a></h2><ul>
<li><a href="http://bit.baidu.com/">百度技术培训中心</a>  *** 认证， 自动驾驶，人工智能培训  </li>
<li><a href="http://bit.baidu.com/courseRouteDetail?id=111">人工智能学习路线</a>  百度技术培训中心</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能 知识点</title>
    <url>/www6vHomeAIGC/2022/01/22/aiOverview/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="人工智能引论知识点">人工智能引论知识点</span><a href="#人工智能引论知识点" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2022/01/22/aiOverview/ai-overview.png" class>

<blockquote>
<p>62个知识点，9个高阶知识点(研究生课程)</p>
</blockquote>
<h2><span id="美国k12-ai知识点">美国K12 AI知识点</span><a href="#美国k12-ai知识点" class="header-anchor">#</a></h2><ul>
<li>智能感知</li>
<li>表示和推理</li>
<li>机器学习</li>
<li>自然交互能力</li>
<li>对社会的影响</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1Wa41157U4?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">吴飞教授解读：人工智能知识点全景图：迈向智能+时代蓝皮书</a> video<br><a href="https://www.163.com/dy/article/HFAFUJPM051193U6.html">人工智能知识点全景图：迈向智能+时代蓝皮书</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>AI 应用场景</title>
    <url>/www6vHomeAIGC/2021/08/11/ai/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E4%B8%8E%E8%A1%8C%E4%B8%9A-2">应用与行业 [2]</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-4">计算机视觉 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">应用场景</a></li>
<li><a href="#%E8%AF%AD%E9%9F%B3%E6%8A%80%E6%9C%AF-4">语音技术 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1">应用场景</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-4">自然语言处理 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2">应用场景</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2021/08/11/ai/ai.png" class title="AI">

<ul>
<li>人工智能的三个层面<ul>
<li>计算智能<br>能算能存</li>
<li>感知智能<br>能听会说， 能看会认</li>
<li>认知智能<br>能理解，会思考</li>
</ul>
</li>
</ul>
<h2><span id="应用与行业-2">应用与行业 [2]</span><a href="#应用与行业-2" class="header-anchor">#</a></h2><ul>
<li>健康码<img src="/www6vHomeAIGC/2021/08/11/ai/ai-hangye.png" class title="行业"></li>
</ul>
<h2><span id="计算机视觉-4">计算机视觉 [4]</span><a href="#计算机视觉-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>图像分类<ul>
<li>计算机视觉的核心问题<ul>
<li>细粒度图像分类</li>
</ul>
</li>
<li>人脸识别<ul>
<li>身份确认</li>
<li>身份查找</li>
</ul>
</li>
</ul>
</li>
<li>图像重建</li>
<li>目标检测<ul>
<li>物体定位</li>
<li>热门方向，领域<ul>
<li>在无人驾驶领域很重要</li>
<li>机器人导航</li>
<li>智能视频监控</li>
<li>工业检查</li>
</ul>
</li>
<li>关键问题<ul>
<li>小目标 高精度检测</li>
<li>多类别物体检测</li>
</ul>
</li>
</ul>
</li>
<li>图像搜索</li>
<li>图像分割<ul>
<li>核心问题</li>
<li>三类(逐层递进)<ul>
<li>语义分割</li>
<li>实例分割</li>
<li>全景分割</li>
</ul>
</li>
<li>应用场景</li>
</ul>
</li>
<li>目标跟踪</li>
</ul>
<h2><span id="语音技术-4">语音技术 [4]</span><a href="#语音技术-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>语音识别<ul>
<li>语音转文字</li>
<li>应用<ul>
<li>智能音响</li>
<li>语音输入发</li>
</ul>
</li>
</ul>
</li>
<li>语音合成 <ul>
<li>文字转语音 TTS</li>
<li>应用<ul>
<li>人机交互</li>
<li>语音客服</li>
<li>虚拟偶像-腾讯AI主播 艾灵</li>
</ul>
</li>
</ul>
</li>
<li>声纹识别<ul>
<li>微信的声音锁功能</li>
</ul>
</li>
</ul>
<h2><span id="自然语言处理-4">自然语言处理 [4]</span><a href="#自然语言处理-4" class="header-anchor">#</a></h2><ul>
<li>人工智能的最高境界</li>
</ul>
<h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>文本分类<ul>
<li>新闻分类  </li>
<li>邮件自动回复，垃圾邮件</li>
<li>客服聊天情感分析</li>
<li>内容审核</li>
</ul>
</li>
<li>机器翻译<ul>
<li>在线多语言翻译</li>
<li>会议中的语音同传</li>
<li>翻译机</li>
<li>跨语言检索</li>
</ul>
</li>
<li>知识图谱<ul>
<li>认知智能</li>
</ul>
</li>
<li>对话系统<ul>
<li>任务导向 - 问答系统</li>
<li>非任务导向 - 聊天机器人</li>
</ul>
</li>
<li>信息检索</li>
<li>文本生成<ul>
<li>写作机器人</li>
</ul>
</li>
</ul>
<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li>机器可以看   -  计算机视觉 </li>
<li>机器可以听   -  语音技术</li>
<li>机器可以理解 -  自然语言处理</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://cloud.tencent.com/edu/learning/course-3460-61199">腾讯云人工智能从业者认证线上培训课程</a> </p>
<ol>
<li>1.1  </li>
<li>1.2 </li>
<li>1.4 未</li>
<li>1.5</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>应用场景</category>
      </categories>
      <tags>
        <tag>应用场景</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-Code</title>
    <url>/www6vHomeAIGC/2021/05/28/gptPromptCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E5%85%B7">工具</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%AE%80%E5%8D%95%E4%BB%BB%E5%8A%A1-1">代码相关-简单任务 [1]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%B9%81%E7%90%90%E5%B7%A5%E4%BD%9C-1">代码相关- 繁琐工作 [1]</a></li>
<li><a href="#%E8%BF%90%E7%BB%B4-ops-4">运维 Ops [4]</a><ul>
<li><a href="#%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80-vs-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80">编程语言 vs 自然语言</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="工具">工具</span><a href="#工具" class="header-anchor">#</a></h1><ul>
<li>Copilot *** - 收费</li>
<li>AWS CodeWhispter</li>
<li>Cursor ***</li>
<li>tabnine 免费</li>
<li>Code Llama  - 开源</li>
</ul>
<h1><span id="代码相关-简单任务-1">代码相关-简单任务 [1]</span><a href="#代码相关-简单任务-1" class="header-anchor">#</a></h1><ul>
<li><p><strong>注释</strong><br> 你作为一名程序员，请解释一下下面这段代码</p>
</li>
<li><p><strong>防御性编程</strong><br> 请为这段代码增加防御性编程的功能</p>
</li>
<li><p>写单元测试 </p>
</li>
<li><p><strong>时间复杂度  time complexity</strong><br> 这段代码的时间复杂度是多少</p>
</li>
<li><p>流程图<br> 画出redis master和slave之间同步的流程图</p>
</li>
<li><p>Writing shell script</p>
</li>
<li><p>Writing git commands<br>一个分支中的代码合并到另一个分支中</p>
</li>
<li><p><strong>Improve code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  How do i improve this code?</span><br><span class="line">  fruits = [<span class="string">&quot;apple&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;cherry&quot;</span>]</span><br><span class="line">  newlist = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> fruits:</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&quot;a&quot;</span> <span class="keyword">in</span> x:</span><br><span class="line">    newlist.append(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(newlist)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Translating Code</strong> 代码转换</p>
<ul>
<li>Convert this Python code to Javascript    </li>
<li>请把下面这段python代码转换成Java代码</li>
</ul>
</li>
</ul>
<h1><span id="代码相关-繁琐工作-1">代码相关- 繁琐工作 [1]</span><a href="#代码相关-繁琐工作-1" class="header-anchor">#</a></h1><ul>
<li><p>Building API</p>
<ul>
<li>I need an API built with express.js to return the list of products. Each product should have attributes like ID, title, description, price and imageUrl</li>
<li>modify the code and  retrieve the products from a MongoDB database</li>
<li>use TypeScript in this code</li>
<li>Generate this API using Python and FastAPI</li>
</ul>
</li>
<li><p><strong>Generating Dummy Data</strong></p>
<ul>
<li>Generate dummy data for a table called customers. Each customer should have an ID, first name, last name and city.</li>
<li>I don’t need a Javascript. Just give the data.</li>
<li>Create a Python class for storing these objects.</li>
</ul>
</li>
<li><p><strong>SQL</strong></p>
<ul>
<li>write a SQL query to generate a table called products with these columns：<br>ID（int）<br>title（string）<br>category(int)</li>
<li>write a query to retrieve the top 5 customers in Shanghai</li>
<li>Revise this query and join the customers table with the orders table to find out how much each cumster has spent. Then pick the top 5 who have spent the most.</li>
</ul>
</li>
<li><p>正则  [2]</p>
</li>
<li><p>CronJob [2]</p>
</li>
<li><p>K8s</p>
</li>
</ul>
<h1><span id="运维-ops-4">运维 Ops [4]</span><a href="#运维-ops-4" class="header-anchor">#</a></h1><h2><span id="编程语言-vs-自然语言">编程语言 vs 自然语言</span><a href="#编程语言-vs-自然语言" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>语言类型</th>
<th>执行原理</th>
</tr>
</thead>
<tbody><tr>
<td>C++语言</td>
<td>C++语言 –&gt; 编译器&#x2F;链接器 –&gt; 既定任务</td>
</tr>
<tr>
<td>Java语言</td>
<td>Java语言 –&gt; 编译器&#x2F;虚拟机 –&gt; 既定任务</td>
</tr>
<tr>
<td>Python语言</td>
<td>Python语言 –&gt; 解释器 –&gt; 既定任务</td>
</tr>
<tr>
<td>人类自然语言</td>
<td>人类自然语言 –&gt; LLMs –&gt; 各种后端组件 –&gt; 既定任务</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1Z84y1G7nY/">【ChatGPT】面向程序员的ChatGPT使用教程38种方式来提升生产力</a> V</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a><br>03, 04, 06<br><a href="https://github.com/www6v/AICoder">AICoder</a> git</li>
<li><a href="https://cloud.tencent.com/developer/article/2207540">ChatGPT 帮我跑了一个完整的 DevOps 流水线，离了个大谱…</a><br><a href="https://github.com/www6v/AICoder/tree/master/Cursor/">Gin on K8s</a> git</li>
<li><a href="https://www.promptops.com/">PromptOps</a>    </li>
<li><a href="https://www.geeksforgeeks.org/chatgpt-prompts-for-software-developers/">Top 20 ChatGPT Prompts For Software Developers</a> 未</li>
</ol>
<pre><code>

</code></pre>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-How to use</title>
    <url>/www6vHomeAIGC/2021/05/26/gptPrompt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B9%94%E5%93%88%E9%87%8C%E6%B2%9F%E9%80%9A%E8%A7%86%E7%AA%97-4-%E8%B1%A1%E9%99%90">乔哈里沟通视窗-4 象限</a><ul>
<li><a href="#%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93gpt%E7%9F%A5%E9%81%93">你不知道，GPT知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B9%9F%E7%9F%A5%E9%81%93">你知道，GPT也知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B8%8D%E7%9F%A5%E9%81%93">你知道，GPT不知道</a></li>
<li><a href="#%E4%BD%A0%E5%92%8Cgpt%E9%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93">你和GPT都不知道</a></li>
</ul>
</li>
<li><a href="#%E8%BE%BE%E5%85%8B%E6%95%88%E5%BA%94">达克效应</a><ul>
<li><a href="#%E6%A3%80%E9%AA%8C%E8%87%AA%E5%B7%B1%E8%AE%A4%E7%9F%A5%E8%83%BD%E5%8A%9B%E6%B0%B4%E5%B9%B3%E6%8F%90%E9%97%AE%E5%8F%A5%E5%BC%8F">检验自己认知&#x2F;能力水平提问句式</a></li>
</ul>
</li>
<li><a href="#%E7%9F%A5%E9%81%93%E5%81%9A%E5%88%B0">知道做到</a></li>
<li><a href="#%E8%A7%92%E8%89%B2%E5%85%B3%E7%B3%BB">角色关系</a></li>
<li><a href="#%E9%80%9A%E7%94%A8">通用</a><ul>
<li><a href="#%E6%B2%9F%E9%80%9A%E6%A8%A1%E5%BC%8F">沟通模式</a></li>
<li><a href="#%E5%BD%92%E7%BA%B3">归纳</a></li>
<li><a href="#%E6%80%9D%E7%BB%B4%E9%93%BE">思维链</a></li>
<li><a href="#%E6%B2%A1%E5%95%A5%E7%94%A8">没啥用</a></li>
</ul>
</li>
<li><a href="#prompt-how-to-use">Prompt - How to use</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="乔哈里沟通视窗-4-象限">乔哈里沟通视窗-4 象限</span><a href="#乔哈里沟通视窗-4-象限" class="header-anchor">#</a></h1><h3><span id="你不知道gpt知道">你不知道，GPT知道</span><a href="#你不知道gpt知道" class="header-anchor">#</a></h3><p>1、元问题：我想了解xxxx，我应该向你问哪些问题？<br>2、请给我列出xxx领域&#x2F;行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。<br>3、请详细介绍一下elon musk的主要生平事迹。请详细介绍一下tesla这家企业的发展历程。</p>
<h3><span id="你知道gpt也知道">你知道，GPT也知道</span><a href="#你知道gpt也知道" class="header-anchor">#</a></h3><p>检验认知：<br>1、对于xxx主题&#x2F;技能，你认为哪些是我必须理解和掌握的核心要点？<br>2、我理解的xxx是这样的，你觉得我的理解对吗？<br>3、我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？<br>4、我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？</p>
<p>扩充认知：<br>1、我知道xxx的概念，我想知道更多关于xxx的信息。<br>2、我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？<br>3、我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？<br>4、我想要在xxx领域有所创新，你能提供一些启发或想法吗？<br>5、我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？<br>6、我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？<br>7、（背景信息xxx），我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。<br>8、我是一个xx新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？</p>
<h3><span id="你知道gpt不知道">你知道，GPT不知道</span><a href="#你知道gpt不知道" class="header-anchor">#</a></h3><p>介绍背景现象之后可以向gpt发问，你怎么看待这种现象？可能的原因有哪些？这会对xxx产生什么样的影响？你觉得xxx应该怎么做？</p>
<h3><span id="你和gpt都不知道">你和GPT都不知道</span><a href="#你和gpt都不知道" class="header-anchor">#</a></h3><p>如果xxx，这对社会会产生什么影响？</p>
<h1><span id="达克效应">达克效应</span><a href="#达克效应" class="header-anchor">#</a></h1><h3><span id="检验自己认知x2f能力水平提问句式">检验自己认知&#x2F;能力水平提问句式</span><a href="#检验自己认知x2f能力水平提问句式" class="header-anchor">#</a></h3><p>1、为了测试我对xxx的理解程度，你会问我什么问题来检验我的水平，最少10个。<br>2、我是xx领域的专家，你会问我哪些问题来检验我的专业水平？<br>3、追问一句，这些我都懂，还有更专业更细更深的问题吗？<br>4、你问我答的游戏</p>
<p>扩展自己能力边界的提问句式我已经很精通xxx了，我想知道我是否还有需要学习的地方？然后不停的问，还有呢还有呢？</p>
<h1><span id="知道做到">知道做到</span><a href="#知道做到" class="header-anchor">#</a></h1><p>让GPT完成具体任务<br>1、我想做xxx，你能给我提供什么帮助？<br>2、我想要你做xxx，我应该给你输入什么信息？<br>3、直接下指令</p>
<h1><span id="角色关系">角色关系</span><a href="#角色关系" class="header-anchor">#</a></h1><ul>
<li>模拟虚拟人物</li>
<li>模拟名人</li>
<li>模拟一段关系</li>
<li>模拟多个具体的人</li>
<li>模拟多类人</li>
</ul>
<h1><span id="通用">通用</span><a href="#通用" class="header-anchor">#</a></h1><h3><span id="沟通模式">沟通模式</span><a href="#沟通模式" class="header-anchor">#</a></h3><p>prompt &#x3D;  定义角色+背景信息+任务目标+输出要求</p>
<h3><span id="归纳">归纳</span><a href="#归纳" class="header-anchor">#</a></h3><ul>
<li>使用markdown格式写富爸爸穷爸爸的思维导图，以代码格式输出</li>
<li>以脑图的方式归纳上文</li>
<li>请用提纲的方式来归纳上文</li>
<li>请用简练的列提纲的方式来归纳上文</li>
</ul>
<h3><span id="思维链">思维链</span><a href="#思维链" class="header-anchor">#</a></h3><h3><span id="没啥用">没啥用</span><a href="#没啥用" class="header-anchor">#</a></h3><p>请用简单的语句来归纳上文，归纳的语句可以生成脑图<br>请用金字塔思维的方式来简单的归纳上文</p>
<h1><span id="prompt-how-to-use">Prompt - How to use</span><a href="#prompt-how-to-use" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://learnprompting.org/zh-Hans/docs/intro">Learn Prompting</a> *** **</p>
</li>
<li><p><a href="https://www.aishort.top/">Chatgpt ShortCut</a><br><a href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT Shortcut </a></p>
</li>
<li><p><a href="https://prompts.chat/"> Awesome ChatGPT Prompts</a>  </p>
</li>
<li><p><a href="https://snackprompt.com/">snackprompt.com</a> ***</p>
</li>
<li><p><a href="https://flowgpt.com/">flowgpt</a> ***</p>
</li>
<li><p><a href="https://prompthero.com/">prompthero</a></p>
</li>
<li><p><a href="https://publicprompts.art/">publicprompts</a></p>
</li>
<li><p><a href="https://learningprompt.wiki/">https://learningprompt.wiki/</a> prompt 学习教程</p>
</li>
<li><p><a href="https://gpt.candobear.com/prompt">Prompt 大全</a></p>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/ZYkUwbXgzi5eqYkHl0MceNrSnhb">提示指令库</a>  汇总</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://www.bilibili.com/video/BV1Lg4y1c7fk/">学完这个视频，简历加一条：熟练掌握ChatGPT解决复杂问题｜ChatGPT使用教程</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
</search>
