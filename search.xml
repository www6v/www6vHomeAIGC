<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AIGC 汇总</title>
    <url>/www6vHomeHexo/2022/11/16/gptSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/06/11/aiDeepLearning/" title="Deep Learning">Deep Learning</a></li>
<li><a href="/www6vHomeHexo/2022/06/07/aiMachineLearning/" title="Machine Learning">Machine Learning</a></li>
<li><a href="/www6vHomeHexo/2021/08/11/ai/" title="AI 应用场景">AI 应用场景</a> </li>
<li><a href="/www6vHomeHexo/2022/01/22/aiOverview/" title="人工智能 知识点">人工智能 知识点</a></li>
<li><a href="/www6vHomeHexo/2023/02/05/gptNLPTask/" title="NLP 任务">NLP 任务</a></li>
</ul>
<h2><span id="model">Model</span><a href="#model" class="header-anchor">#</a></h2><ul>
<li>基础<ul>
<li><a href="/www6vHomeHexo/2022/10/30/gptLargeModel/" title="大模型">大模型</a></li>
<li><a href="/www6vHomeHexo/2022/11/30/gptTransformer/" title="Transformer">Transformer</a></li>
</ul>
</li>
<li>基座模型<ul>
<li><a href="/www6vHomeHexo/2022/12/11/gptFamily/" title="GPT 系列">GPT 系列</a>  </li>
<li><a href="/www6vHomeHexo/2023/01/01/gptLlama/" title="LLaMA">LLaMA</a>   </li>
<li><a href="/www6vHomeHexo/2023/01/06/gptChatGLM/" title="ChatGLM">ChatGLM</a>   </li>
<li><a href="/www6vHomeHexo/2023/01/04/gptLeaderBoard/" title="排行榜">排行榜</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2023/01/18/gptMultimodal/" title="多模态">多模态</a>  </li>
<li><a href="/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/" title="不可能三角">不可能三角</a> </li>
<li><a href="/www6vHomeHexo/2023/02/03/gptEmergent/" title="涌现现象（Emergent）">涌现现象（Emergent）</a></li>
</ul>
<h2><span id="training-amp-inference">Training &amp; Inference</span><a href="#training-amp-inference" class="header-anchor">#</a></h2><ul>
<li>训练<ul>
<li><a href="/www6vHomeHexo/2022/11/19/gptLargeModelTraining/" title="训练">训练</a></li>
<li><a href="/www6vHomeHexo/2023/01/15/gptLargeModelTrainingPractice/" title="训练Train-实战">训练Train-实战</a> </li>
<li><a href="/www6vHomeHexo/2023/01/06/gptTrainParallelism/" title="训练-并行">训练-并行</a></li>
<li><a href="/www6vHomeHexo/2023/02/03/gptContinualPretraining/" title="继续-预训练">继续-预训练</a>  </li>
<li><a href="/www6vHomeHexo/2024/02/01/gptPrecision/" title="混合精度">混合精度</a></li>
</ul>
</li>
<li>推理 <ul>
<li><a href="/www6vHomeHexo/2023/02/02/gptInferenceFramework/" title="推理-框架">推理-框架</a> </li>
<li><a href="/www6vHomeHexo/2023/01/01/gptInference/" title="推理-优化">推理-优化</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2023/01/08/gptDataSet/" title="数据集">数据集</a> </li>
<li><a href="/www6vHomeHexo/2023/02/05/gptDataProcess/" title="数据处理">数据处理</a></li>
</ul>
<h2><span id="finetuning">FineTuning</span><a href="#finetuning" class="header-anchor">#</a></h2><ul>
<li>基础<ul>
<li><a href="/www6vHomeHexo/2022/11/18/gptFineTuning/" title="Fine-Tuning 原理">Fine-Tuning 原理</a> </li>
<li><a href="/www6vHomeHexo/2022/12/28/gptFineTuningWhen/" title="Fine-Tuning 时机">Fine-Tuning 时机</a>  </li>
<li><a href="/www6vHomeHexo/2023/01/06/gptPromptTuning/" title="Prompt Tuning">Prompt Tuning</a> </li>
<li><a href="/www6vHomeHexo/2023/01/06/gptInstructTuning/" title="Instruct Tuning">Instruct Tuning</a></li>
</ul>
</li>
<li>实战<ul>
<li><a href="/www6vHomeHexo/2022/12/20/gptFineTuningPEFT/" title="PEFT 实战">PEFT 实战</a>  
<ul>
<li><a href="/www6vHomeHexo/2023/01/05/gptPEFTLora/" title="PEFT Lora 实战">PEFT Lora 实战</a> </li>
<li><a href="/www6vHomeHexo/2024/01/12/gptPEFTQLora/" title="PEFT QLoRA 实战">PEFT QLoRA 实战</a> </li>
<li><a href="/www6vHomeHexo/2023/01/25/gptPromptTuningPractice/" title="PromptTuning 实战">PromptTuning 实战</a>    </li>
<li><a href="/www6vHomeHexo/2024/01/28/gptPEFTPtuning/" title="PEFT P-Tuning">PEFT P-Tuning</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2024/01/26/gptFineTuningBert/" title="Fine Tuning-Bert">Fine Tuning-Bert</a></li>
</ul>
</li>
</ul>
<h2><span id="prompt">Prompt</span><a href="#prompt" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/11/10/gptPromptEngineering/" title="Prompt Engineering">Prompt Engineering</a></li>
<li><a href="/www6vHomeHexo/2022/07/11/gptPromptResearch/" title="Prompt-学术研究">Prompt-学术研究</a></li>
<li><a href="/www6vHomeHexo/2021/05/28/gptPromptCode/" title="Prompt-Code">Prompt-Code</a></li>
<li><a href="/www6vHomeHexo/2021/05/26/gptPrompt/" title="Prompt-How to use">Prompt-How to use</a></li>
</ul>
<h2><span id="rag">RAG</span><a href="#rag" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/11/02/gptRAG/" title="RAG 原理">RAG 原理</a></li>
<li><a href="/www6vHomeHexo/2022/12/31/gptRAGPractice/" title="RAG 实践">RAG 实践</a> </li>
<li><a href="/www6vHomeHexo/2022/12/07/gptRAGPerformance/" title="RAG 性能">RAG 性能</a></li>
<li><a href="/www6vHomeHexo/2022/12/27/gptRAGPerformanceOpenAI/" title="RAG 性能-OpenAI案例">RAG 性能-OpenAI案例</a></li>
</ul>
<h2><span id="langchain">Langchain</span><a href="#langchain" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/11/02/gptLangchain/" title="Langchain">Langchain</a></li>
<li><a href="/www6vHomeHexo/2022/12/31/gptRetrievers/" title="Retrievers">Retrievers</a> </li>
<li><a href="/www6vHomeHexo/2023/01/11/gptLangchainAgent/" title="Langchain  Agent">Langchain  Agent</a></li>
</ul>
<h2><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/11/02/gptAgent/" title="Agent 原理">Agent 原理</a></li>
<li><a href="/www6vHomeHexo/2023/01/21/gptMultiAgents/" title="Multi-Agents">Multi-Agents</a>  </li>
<li><a href="/www6vHomeHexo/2023/01/01/gptAgentPractice/" title="Agent 实践">Agent 实践</a> </li>
<li>Tools<ul>
<li><a href="/www6vHomeHexo/2022/11/16/gptFunctionCall/" title="Function Call">Function Call</a> </li>
<li><a href="/www6vHomeHexo/2023/01/27/gptAgentTool/" title="Agent-Tools">Agent-Tools</a>  </li>
<li><a href="/www6vHomeHexo/2023/02/03/gptToolformer/" title="Toolformer">Toolformer</a></li>
</ul>
</li>
</ul>
<h2><span id="application">Application</span><a href="#application" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/09/gpt/" title="GPT-工具和应用">GPT-工具和应用</a></li>
<li><a href="/www6vHomeHexo/2022/12/28/gptLLMOps/" title="LLMOps">LLMOps</a> </li>
<li><a href="/www6vHomeHexo/2022/11/27/gptVectorStore/" title="向量数据库">向量数据库</a></li>
<li><a href="/www6vHomeHexo/2023/01/03/gptNL2SQL/" title="NL2SQL">NL2SQL</a> </li>
<li><a href="/www6vHomeHexo/2023/01/04/gptLargeModelDomain/" title="垂类大模型">垂类大模型</a> </li>
<li><a href="/www6vHomeHexo/2022/11/24/gptFinGPT/" title="FinGTP">FinGTP</a></li>
</ul>
<h2><span id="study">Study</span><a href="#study" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/08/01/gptStudy/" title="GPT  学习资源">GPT  学习资源</a></li>
<li><a href="/www6vHomeHexo/2023/01/20/gptStudyPaper/" title="GPT 论文">GPT 论文</a></li>
<li><a href="/www6vHomeHexo/2022/01/22/aiStudyResouce/" title="人工智能-学习资源">人工智能-学习资源</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据  汇总</title>
    <url>/www6vHomeHexo/2022/09/19/bigDataSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="存储">存储</span><a href="#存储" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/09/07/clickhouse/" title="Clickhouse">Clickhouse</a>
</li>
<li><a href="/www6vHomeHexo/2022/01/08/elasticsearchModel/" title="Elasticsearch 数据模型Model">Elasticsearch 数据模型Model</a></li>
<li><a href="/www6vHomeHexo/2019/08/03/elasticsearchDistributed/" title="Elasticsearch 分布式集群">Elasticsearch 分布式集群</a></li>
<li><a href="/www6vHomeHexo/2019/08/02/elasticsearch/" title="Elasticsearch基础(ES)">Elasticsearch基础(ES)</a>
</li>
<li><a href="/www6vHomeHexo/2023/04/02/hbaselsmTree/" title="HBase - LSM-Tree">HBase - LSM-Tree</a></li>
<li><a href="/www6vHomeHexo/2021/06/07/hbaseHotkey/" title="HBase Hotkey-预分区和Rowkey设计">HBase Hotkey-预分区和Rowkey设计</a></li>
<li><a href="/www6vHomeHexo/2020/09/04/hbase/" title="HBase总结">HBase总结</a>
</li>
<li><a href="/www6vHomeHexo/2018/06/07/hdfs/" title="HDFS NameNode HA 解决方案">HDFS NameNode HA 解决方案</a>
</li>
<li><a href="/www6vHomeHexo/2022/09/01/iceberg/" title="Iceberg">Iceberg</a>
</li>
<li><a href="/www6vHomeHexo/2022/09/08/bigDataMetaMgt/" title="大数据 元数据管理">大数据 元数据管理</a>
</li>
<li><a href="/www6vHomeHexo/2019/09/15/bigDataStorage/" title="大数据存储">大数据存储</a></li>
</ul>
<h1><span id="平台">平台</span><a href="#平台" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/10/16/dataMiddlePlatform/" title="数据中台">数据中台</a>
</li>
<li><a href="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/" title="实时数仓">实时数仓</a>
</li>
<li><a href="/www6vHomeHexo/2022/08/04/streamingBatchIntegration/" title="批流一体">批流一体</a>
</li>
<li><a href="/www6vHomeHexo/2022/09/15/userBehaviorAnalysis/" title="用户行为分析">用户行为分析</a>
</li>
<li><a href="/www6vHomeHexo/2021/06/15/olap/" title="OLAP 在线分析处理">OLAP 在线分析处理</a>
</li>
<li><a href="/www6vHomeHexo/2022/09/21/personProflie/" title="用户画像">用户画像</a></li>
</ul>
<h1><span id="计算">计算</span><a href="#计算" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/06/17/bigDataComputing/" title="大数据 计算Computing">大数据 计算Computing</a>
</li>
<li><a href="/www6vHomeHexo/2022/09/08/bigDataSchedule/" title="大数据 调度">大数据 调度</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>流处理 汇总</title>
    <url>/www6vHomeHexo/2022/04/22/streamingSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#flink">Flink</a><ul>
<li><a href="#basic">Basic</a></li>
<li><a href="#checkpoint">Checkpoint</a></li>
<li><a href="#table">Table</a></li>
</ul>
</li>
<li><a href="#spark">Spark</a></li>
<li><a href="#beam">Beam</a></li>
<li><a href="#%E6%89%B9%E6%B5%81%E4%B8%80%E4%BD%93">批流一体</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#streaming-system%E7%BF%BB%E8%AF%91">《Streaming System》翻译</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="flink">Flink</span><a href="#flink" class="header-anchor">#</a></h1><h3><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2019/07/29/streamingFlink/" title="Flink 总结">Flink 总结</a>
</li>
<li><a href="/www6vHomeHexo/2022/03/31/streamingFlinkWindow/" title="Flink-Window">Flink-Window</a>
</li>
<li><a href="/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/" title="Flink-Watermark &amp; Window">Flink-Watermark &amp; Window</a>
</li>
<li><a href="/www6vHomeHexo/2022/03/31/streamingFlinkDeploy/" title="Flink-部署方式">Flink-部署方式</a></li>
</ul>
<h3><span id="checkpoint">Checkpoint</span><a href="#checkpoint" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/02/03/streamingFlinkCheckpoint/" title="Flink Checkpoint-分布式快照方法">Flink Checkpoint-分布式快照方法</a></li>
<li><a href="/www6vHomeHexo/2022/02/01/streamingFlinkExactlyOnce/" title="Flink 端到端Exactly-once">Flink 端到端Exactly-once</a></li>
</ul>
<h3><span id="table">Table</span><a href="#table" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/07/18/flinkSQL/" title="Flink SQL">Flink SQL</a></li>
<li><a href="/www6vHomeHexo/2022/01/22/streamingFlinkJoin/" title="Flink 双流Join">Flink 双流Join</a></li>
</ul>
<h1><span id="spark">Spark</span><a href="#spark" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2019/03/09/streamingSpark/" title="Spark 总结">Spark 总结</a></li>
<li><a href="/www6vHomeHexo/2022/05/19/streamingSparkPerformance/" title="Spark 性能优化">Spark 性能优化</a></li>
<li><a href="/www6vHomeHexo/2019/03/10/streamingSparkTrain/" title="Spark公司内部培训">Spark公司内部培训</a></li>
</ul>
<h1><span id="beam">Beam</span><a href="#beam" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/04/21/streamingBeam/" title="流计算-Beam">流计算-Beam</a></li>
</ul>
<h1><span id="批流一体">批流一体</span><a href="#批流一体" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2022/08/04/streamingBatchIntegration/" title="批流一体">批流一体</a></li>
</ul>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2019/07/19/streamComputing/" title="流式计算[Flink Beam Spark]">流式计算[Flink Beam Spark]</a></li>
</ul>
<h1><span id="streaming-system翻译">《Streaming System》翻译</span><a href="#streaming-system翻译" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/" title="《Streaming System》- 第一章：流处理入门[完整]">《Streaming System》- 第一章：流处理入门[完整]</a></li>
<li><a href="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/" title="《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行[完整]">《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行[完整]</a></li>
<li><a href="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/" title="《Streaming System》-第三章：水位线[完整]">《Streaming System》-第三章：水位线[完整]</a></li>
<li><a href="/www6vHomeHexo/2000/03/14/streamingSystemChapter4/" title="《Streaming System》-第四章：高级窗口 [完整]">《Streaming System》-第四章：高级窗口 [完整]</a></li>
<li><a href="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/" title="《Streaming System》-第五章：精确一次和副作用 [完整]">《Streaming System》-第五章：精确一次和副作用 [完整]</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>流处理</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算 汇总</title>
    <url>/www6vHomeHexo/2022/04/09/cloudComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8E%82%E5%95%86">厂商</a><br>- <a href="#%E5%88%86%E7%B1%BB">分类</a><br>- <a href="#%E6%A8%AA%E5%90%91%E6%AF%94%E8%BE%83">横向比较</a></li>
<li><a href="#%E9%80%9A%E7%94%A8%E6%80%BB%E7%BB%93">通用总结</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="厂商">厂商</span><a href="#厂商" class="header-anchor">#</a></h2><h5><span id="分类">分类</span><a href="#分类" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2018/10/04/awsSummary/" title="AWS 汇总">AWS 汇总</a></li>
<li><a href="/www6vHomeHexo/2022/05/16/aliyunSummary/" title="阿里云 汇总">阿里云 汇总</a></li>
<li><a href="/www6vHomeHexo/2022/06/30/tencentTCPSummary/" title="腾讯云TCP-汇总">腾讯云TCP-汇总</a></li>
</ul>
<h5><span id="横向比较">横向比较</span><a href="#横向比较" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2022/04/30/cloudProduct/" title="云计算产品-计算">云计算产品-计算</a></li>
<li><a href="/www6vHomeHexo/2022/05/22/cloudProduct-Network/" title="云计算产品-网络Network">云计算产品-网络Network</a></li>
<li><a href="/www6vHomeHexo/2022/05/22/cloudProduct-Storage/" title="云计算产品-存储Storage">云计算产品-存储Storage</a></li>
</ul>
<h2><span id="通用总结">通用总结</span><a href="#通用总结" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>类型</th>
<th>总结</th>
</tr>
</thead>
<tbody><tr>
<td>Overview<br></td>
<td><a href="../../../../2019/02/07/xaas/">云计算中的Xaas</a></td>
</tr>
<tr>
<td>计算 <br></td>
<td><a href="../../../../2020/07/29/vm/">云计算中的虚拟机vm</a><br><a href="../../../../2019/10/10/serverless/">Serverless</a> <br><a href="../../../../2022/06/03/serverlessOptimize/">Serverless 优化</a> <br><a href="../../../../2022/06/25/serverlessConcern/">Serverless 关注点</a></td>
</tr>
<tr>
<td>网络<br></td>
<td><a href="../../../../2022/04/09/vpc/">VPC</a></td>
</tr>
<tr>
<td>存储  <br></td>
<td><a href="../../../../2019/10/08/storage/">非结构化存储</a></td>
</tr>
<tr>
<td>计费 <br></td>
<td><a href="../../../../2022/05/21/cloudComputingBilling/">云计算计费</a></td>
</tr>
<tr>
<td>迁移<br></td>
<td><a href="../../../../2022/04/11/dbMigrate/">数据库迁移</a></td>
</tr>
<tr>
<td>数据中心<br></td>
<td><a href="../../../../2019/05/15/netConnection/">IDC网络互通</a> <br><a href="../../../../2022/01/30/cloudDatacenter/">云计算-数据中心</a></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>汇总</category>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 汇总</title>
    <url>/www6vHomeHexo/2022/01/22/k8sSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%BC%96%E6%8E%92%E5%8E%9F%E7%90%86">编排原理</a></li>
<li><a href="#operator-controller">Operator &amp;&amp; Controller</a></li>
<li><a href="#container-runtime">Container Runtime</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%92%8Cdns">服务和DNS</a></li>
<li><a href="#%E5%AD%98%E5%82%A8">存储</a></li>
<li><a href="#%E8%B0%83%E5%BA%A6">调度</a></li>
<li><a href="#%E7%9B%91%E6%8E%A7%E5%92%8Cautoscale">监控和AutoScale</a></li>
<li><a href="#%E7%94%9F%E4%BA%A7%E5%8C%96">生产化</a></li>
<li><a href="#paas">PaaS</a></li>
<li><a href="#%E5%AE%89%E5%85%A8">安全</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E8%B5%B0%E8%AF%BB">代码走读</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">问题和最佳实践</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
<li><a href="#%E5%AD%A6%E4%B9%A0">学习</a></li>
<li><a href="#%E5%AE%89%E8%A3%85">安装</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="编排原理">编排原理</span><a href="#编排原理" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/04/25/k8s/" title="Kubernetes 架构">Kubernetes 架构</a> </li>
<li><a href="/www6vHomeHexo/2019/06/09/k8sResource/" title="Kubernetes Workload">Kubernetes Workload</a> </li>
<li><a href="/www6vHomeHexo/2022/02/16/k8sDeployment/" title="Kubernetes Deployment">Kubernetes Deployment</a> </li>
<li><a href="/www6vHomeHexo/2019/11/11/k8sStatefulSet/" title="Kubernetes StatefulSet原理和源码">Kubernetes StatefulSet原理和源码</a> </li>
<li><a href="/www6vHomeHexo/2019/11/14/k8sResouceModel/" title="Kubenetes资源模型">Kubenetes资源模型</a> </li>
<li><a href="/www6vHomeHexo/2022/04/03/k8sPLEG/" title="kubelet和PLEG">kubelet和PLEG</a></li>
</ul>
<h2><span id="operator-ampamp-controller">Operator &amp;&amp; Controller</span><a href="#operator-ampamp-controller" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2021/12/30/k8s-operator/" title="Kubernetes Operator-kubebuilder">Kubernetes Operator-kubebuilder</a> </li>
<li><a href="/www6vHomeHexo/2019/11/19/k8sOperator/" title="Kubernetes Operator-Etcd">Kubernetes Operator-Etcd</a> </li>
<li><a href="/www6vHomeHexo/2019/08/29/k8sDeclarativeAPI/" title="Kubernetes声明式API">Kubernetes声明式API</a> </li>
<li><a href="/www6vHomeHexo/2022/02/17/k8sOperator-redis/" title="Kubernetes Operator-Redis">Kubernetes Operator-Redis</a></li>
<li><a href="/www6vHomeHexo/2023/10/16/k8sAdmissionWebhook/" title="K8s  AdmissionWebhook">K8s  AdmissionWebhook</a></li>
</ul>
<h2><span id="container-runtime">Container Runtime</span><a href="#container-runtime" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/11/19/k8sRuntime/" title="Kubernetes Runtime">Kubernetes Runtime</a> </li>
<li><a href="/www6vHomeHexo/2021/06/01/k8sAbandonDocker/" title="K8S 弃用Docker">K8S 弃用Docker</a></li>
</ul>
<h2><span id="网络">网络</span><a href="#网络" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/08/23/k8sNetwork/" title="Kubernetes网络">Kubernetes网络</a> </li>
<li><a href="/www6vHomeHexo/2022/05/03/k8sCalico/" title="Calico">Calico</a></li>
</ul>
<h2><span id="服务和dns">服务和DNS</span><a href="#服务和dns" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/11/04/k8sService/" title="Kubernetes服务">Kubernetes服务</a> </li>
<li><a href="/www6vHomeHexo/2022/01/12/k8sDNS/" title="Kubernetes CoreDNS">Kubernetes CoreDNS</a> </li>
<li><a href="/www6vHomeHexo/2022/02/10/k8sIngressNginx/" title="Kubernetes Nginx Ingress">Kubernetes Nginx Ingress</a></li>
</ul>
<h2><span id="存储">存储</span><a href="#存储" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/09/01/k8sStorage/" title="Kubernetes存储">Kubernetes存储</a> </li>
<li><a href="/www6vHomeHexo/2022/01/12/k8sRook/" title="Kubernetes Rook">Kubernetes Rook</a> </li>
<li><a href="/www6vHomeHexo/2022/01/08/ceph/" title="Ceph 总结">Ceph 总结</a> </li>
<li><a href="/www6vHomeHexo/2022/04/06/etcd/" title="etcd 总结">etcd 总结</a></li>
</ul>
<h2><span id="调度">调度</span><a href="#调度" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/06/09/k8sScheduler/" title="Kubernetes调度器">Kubernetes调度器</a> </li>
<li><a href="/www6vHomeHexo/2022/05/27/k8sAdvancedScheduling/" title="Kubernetes 高级调度">Kubernetes 高级调度</a></li>
</ul>
<h2><span id="监控和autoscale">监控和AutoScale</span><a href="#监控和autoscale" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/11/16/k8sAutoScale/" title="Kubernetes自动伸缩和HPA">Kubernetes自动伸缩和HPA</a> </li>
<li><a href="/www6vHomeHexo/2022/04/10/observabilityPrometheus/" title="可观测性-Prometheus">可观测性-Prometheus</a> </li>
<li><a href="/www6vHomeHexo/2022/02/11/observabilityPrometheusHA/" title="可观测性-Prometheus  HA">可观测性-Prometheus  HA</a> </li>
<li><a href="/www6vHomeHexo/2022/01/30/k8sObservability/" title="可观测性-Kubernetes">可观测性-Kubernetes</a></li>
</ul>
<h2><span id="生产化">生产化</span><a href="#生产化" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/02/02/k8sAppMigrate/" title="K8S 应用迁移至K8S">K8S 应用迁移至K8S</a> </li>
<li><a href="/www6vHomeHexo/2022/01/02/k8sHA/" title="K8S高可用-控制面">K8S高可用-控制面</a> </li>
<li><a href="/www6vHomeHexo/2022/04/05/k8sAvailable/" title="K8S高可用-零停机[自主中断]">K8S高可用-零停机[自主中断]</a> </li>
<li><a href="/www6vHomeHexo/2022/10/22/k8sAvailableHealth/" title="K8S高可用-零停机[探针]">K8S高可用-零停机[探针]</a> </li>
<li><a href="/www6vHomeHexo/2020/08/16/linuxKernelParam/" title="虚拟机和容器中的内核参数 kernel">虚拟机和容器中的内核参数 kernel</a> </li>
<li><a href="/www6vHomeHexo/2022/01/16/k8sUpgrade/" title="Kubernetes 升级upgrade">Kubernetes 升级upgrade</a></li>
</ul>
<h2><span id="paas">PaaS</span><a href="#paas" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/01/12/k8sPaaS/" title="Kubernetes PaaS平台">Kubernetes PaaS平台</a> </li>
<li><a href="/www6vHomeHexo/2022/01/05/k8sOpenShift/" title="Kubernetes OpenShift">Kubernetes OpenShift</a> </li>
<li><a href="/www6vHomeHexo/2021/10/18/k8sMultiTenancy/" title="Kubernetes 多租户">Kubernetes 多租户</a> </li>
<li><a href="/www6vHomeHexo/2019/11/14/k8sRBAC/" title="Kubenetes RBAC">Kubenetes RBAC</a> </li>
<li><a href="/www6vHomeHexo/2022/05/08/k8sMultiCluster/" title="Kubernetes 多集群管理">Kubernetes 多集群管理</a> </li>
<li><a href="/www6vHomeHexo/2022/06/03/k8sVM/" title="Kubernetes和VM">Kubernetes和VM</a></li>
</ul>
<h2><span id="安全">安全</span><a href="#安全" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/22/k8sSecurity/" title="Kubernetes安全-Security">Kubernetes安全-Security</a> </li>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCKS/" title="Kubernetes CKS">Kubernetes CKS</a> </li>
<li><a href="/www6vHomeHexo/2022/01/16/k8sSecurityPractice/" title="Kubernetes 安全实践">Kubernetes 安全实践</a></li>
</ul>
<h2><span id="代码走读">代码走读</span><a href="#代码走读" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCodeOfApiServer/" title="Kubernetes ApiServer 代码走读">Kubernetes ApiServer 代码走读</a> </li>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCodeOfInformerFramework/" title="Kubernetes Informer Framework 代码走读">Kubernetes Informer Framework 代码走读</a> </li>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCodeOfKubelet/" title="Kubernetes Kubelet 代码走读">Kubernetes Kubelet 代码走读</a> </li>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCodeOfKubeScheduler/" title="Kubernetes KubeScheduler 代码走读">Kubernetes KubeScheduler 代码走读</a></li>
</ul>
<h2><span id="问题和最佳实践">问题和最佳实践</span><a href="#问题和最佳实践" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/04/03/k8sProblem/" title="Kubernetes问题排查-troubleshooting">Kubernetes问题排查-troubleshooting</a> </li>
<li><a href="/www6vHomeHexo/2022/01/23/k8sBestPractice/" title="Kubernetes最佳实践-BestPractice">Kubernetes最佳实践-BestPractice</a> </li>
<li><a href="/www6vHomeHexo/2022/06/08/k8sTroubleshoot/" title="kubernetes 问题Troubleshoot">kubernetes 问题Troubleshoot</a></li>
</ul>
<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/11/03/k8sSkill/" title="Kubernetes技能图谱">Kubernetes技能图谱</a> </li>
<li><a href="/www6vHomeHexo/2019/11/13/k8sPattern/" title="Kubernetes模式">Kubernetes模式</a> </li>
<li><a href="/www6vHomeHexo/2019/08/11/k8sInterface/" title="Kubernetes开放接口">Kubernetes开放接口</a></li>
</ul>
<h2><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/06/09/k8sCommand/" title="Kubernetes命令">Kubernetes命令</a> </li>
<li><a href="/www6vHomeHexo/2020/05/26/k8sDeclarativeManage/" title="k8s声明式应用管理">k8s声明式应用管理</a></li>
</ul>
<h2><span id="学习">学习</span><a href="#学习" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/21/k8sStudy/" title="Kubernetes 学习资源">Kubernetes 学习资源</a> </li>
<li><a href="/www6vHomeHexo/2020/06/14/cloudNativeResource/" title="云原生-学习资源">云原生-学习资源</a></li>
</ul>
<h2><span id="安装">安装</span><a href="#安装" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/06/03/k8sSetupSummary/" title="Kubernetes 安装方式">Kubernetes 安装方式</a> </li>
<li><a href="/www6vHomeHexo/2021/06/02/k8sDeploy/" title="Kubernetes安装-kubeasz">Kubernetes安装-kubeasz</a> </li>
<li><a href="/www6vHomeHexo/2019/01/17/k8sSetup/" title="Kubernetes集群搭建(二进制)">Kubernetes集群搭建(二进制)</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Service Mesh 汇总</title>
    <url>/www6vHomeHexo/2021/06/07/istioSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2019/07/02/istio/">istio</a></li>
<li><a href="../../../../2022/01/14/istioControlPanel/">istio 控制面ControlPanel</a> </li>
<li><a href="../../../../2019/11/21/istioTrafficManagement/">Istio流量管理</a></li>
<li><a href="../../../../2019/11/21/istioDataplane/">istio数据面</a> </li>
<li><a href="/www6vHomeHexo/2022/07/02/istioDataplaneAmbient/" title="istio 数据平面-ambient 模式">istio 数据平面-ambient 模式</a></li>
<li><a href="../../../../2019/07/20/istio-k8s-service/">Istio、Kubernetes和Spring Cloud中服务的比对</a>    </li>
<li><a href="../../../../2019/11/18/istioKnowledgeMap/">Istio知识图谱</a></li>
</ul>
<h2><span id="实践">实践</span><a href="#实践" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2019/07/15/istioCommand/">istio常用命令</a></li>
<li><a href="../../../../2019/07/02/istioSetup-bookinfo/">istio安装 + Bookinfo示例</a></li>
<li><a href="../../../../2022/01/06/istioMigrateFromSpringCloud/">SpringCloud迁移到istio</a></li>
<li><a href="../../../../2022/02/06/istioServiceFailover/">Istio Service Failover</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 汇总</title>
    <url>/www6vHomeHexo/2021/05/27/linuxSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>关键词: linux, 计算机组成</p>
<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84">总体架构</a></li>
<li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F3">操作系统[3]</a></li>
<li><a href="#cpu-and-cache1">CPU and Cache[1]</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#file-block">File， block</a></li>
<li><a href="#network">Network</a></li>
<li><a href="#frameworks-and-tools">Frameworks and Tools</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="总体架构">总体架构</span><a href="#总体架构" class="header-anchor">#</a></h2><ul>
<li>北桥<br>主桥 - 处理高速信号 </li>
<li>南桥<br>IO桥 - [低速]</li>
</ul>
<h2><span id="操作系统3">操作系统[3]</span><a href="#操作系统3" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/01/30/linuxKernel/" title="Linux Kernel总结">内核</a>  self</li>
<li>栈<br>用函数和寄存器的方式记录了线程的执行历史</li>
<li>中断</li>
<li><a href="/www6vHomeHexo/2019/08/22/linuxProcess/" title="Linux 进程">进程</a>  self</li>
<li><a href="/www6vHomeHexo/2022/01/30/linuxSystemCall/" title="Linux 系统调用SystemCall">系统调用</a>  self</li>
<li>虚拟内存</li>
<li><a href="/www6vHomeHexo/2019/08/23/linuxMemory/" title="Linux内存管理">内存管理</a>  self</li>
<li><a href="/www6vHomeHexo/2022/05/29/linuxSceduling/" title="Linux 调度">调度器</a>  self
<ul>
<li>cpu调度级别</li>
<li>io调度级别<br>deadline， CFQ</li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2019/08/24/linuxFile/" title="Linux文件系统">文件系统</a> self</li>
<li>网络</li>
</ul>
<h2><span id="cpu-and-cache1">CPU and Cache[1]</span><a href="#cpu-and-cache1" class="header-anchor">#</a></h2><ul>
<li>CPU架构<ul>
<li>SMP UMA<br>系统总线成了系统瓶颈，应运而生了NUMA</li>
<li>NUMA</li>
<li>Bus 总线<br>[cpu和内存之间的通道]</li>
</ul>
</li>
<li>CPU Cache<ul>
<li>L1 的存取速度：4 个CPU时钟周期</li>
<li>L2 的存取速度：11 个CPU时钟周期</li>
<li>L3 的存取速度：39 个CPU时钟周期</li>
<li>RAM内存的存取速度：107 个CPU时钟周期</li>
</ul>
</li>
<li>cacheline<br>Cache Line是最小单位（64Bytes）<br>eg. L1有32KB，32KB&#x2F;64B &#x3D; 512个Cache Line  </li>
<li>缓存的一致性<ul>
<li>cache line对齐</li>
<li>一致性协议  <ul>
<li>Directory 协议</li>
<li>Snoopy 协议<br> MESI协议, cache line的4个状态</li>
</ul>
</li>
</ul>
</li>
<li>Cache 预取    </li>
<li>TLB(Translation Lookaside Buffer)  <ul>
<li>快表，解决MMU查找page慢的问题</li>
<li>专门用于cache内存中的页表项</li>
</ul>
</li>
</ul>
<h2><span id="memory">Memory</span><a href="#memory" class="header-anchor">#</a></h2><ul>
<li>MMU(Memory Management Unit)</li>
<li>page - 4K<ul>
<li>Hugepage: 2M, 1G</li>
</ul>
</li>
<li>虚拟内存</li>
<li>overcommit</li>
<li>SLUB slab</li>
<li>buffer &amp;&amp; cache<br>page cache - 文件<br>buffer - 磁盘</li>
</ul>
<h2><span id="file-block">File， block</span><a href="#file-block" class="header-anchor">#</a></h2><ul>
<li>POSIX</li>
<li>io_uring<br>linux 5.1引入的异步io接口，适合io密集型应用</li>
</ul>
<h2><span id="network">Network</span><a href="#network" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/08/07/tcpUdpControlCongestion/" title="TCP流控和拥塞控制">TCP 阻塞</a>  self</li>
<li><a href="/www6vHomeHexo/2022/01/30/linuxNetwork/" title="Linux 协议栈">Linux 协议栈</a> self</li>
<li>epoll</li>
</ul>
<h2><span id="frameworks-and-tools">Frameworks and Tools</span><a href="#frameworks-and-tools" class="header-anchor">#</a></h2><ul>
<li>data plan <ul>
<li><a href="/www6vHomeHexo/2022/01/25/linuxDPDK/" title="DPDK">DPDK</a>  self        </li>
<li><a href="https://spdk.io/">SPDK</a> 官网<ul>
<li>用户态的TCP&#x2F;IP协议栈 libuns</li>
</ul>
</li>
<li>VPP</li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2022/05/22/linux-ebpf/" title="eBPF">eBPF</a>  self
<ul>
<li>LLVM</li>
<li>bcc</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《深入浅出DPDK》</li>
<li><a href="https://coolshell.cn/articles/20793.html">与程序员相关的CPU缓存知识</a>   ***   有代码</li>
<li>&lt;&lt;性能之巅&gt;&gt;</li>
</ol>
]]></content>
      <categories>
        <category>汇总</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>故障模型 &amp;&amp; 故障排查</title>
    <url>/www6vHomeHexo/2021/05/23/troubleshootingSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5">故障模型 &amp;&amp; 故障排查</a><br>- <a href="#java-core-%E5%BA%94%E7%94%A8">Java Core &amp;&amp; 应用</a><br>- <a href="#%E4%B8%AD%E9%97%B4%E4%BB%B6">中间件</a><br>- <a href="#infra">Infra</a><br>- <a href="#k8s">K8s</a><br>- <a href="#basic">Basic</a></li>
</ul>
<!-- tocstop -->

</div>


<h2><span id="故障模型-ampamp-故障排查">故障模型 &amp;&amp; 故障排查</span><a href="#故障模型-ampamp-故障排查" class="header-anchor">#</a></h2><h5><span id="java-core-ampamp-应用">Java Core &amp;&amp; 应用</span><a href="#java-core-ampamp-应用" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2018/10/27/faultModel1/" title="故障模型-应用层">故障模型-应用层</a>
<ul>
<li><a href="/www6vHomeHexo/2014/02/02/javaMemoryLeak/" title="Java内存泄漏的案例和解决方案">Java内存泄漏的案例和解决方案</a> </li>
<li><a href="/www6vHomeHexo/2014/07/21/twoGCcase/" title="两个GC案例">两个GC案例</a></li>
<li><a href="/www6vHomeHexo/2017/08/09/interrupted/" title="关于任务取消相关异常的排查">关于任务取消相关异常的排查</a></li>
<li><a href="/www6vHomeHexo/2014/09/06/classloader/" title="Classloader相关的故障排查">Classloader相关的故障排查</a></li>
<li><a href="/www6vHomeHexo/2021/05/04/multiAgent/" title="多个Java Agent同时使用的类增强冲突问题">多个Java Agent同时使用的类增强冲突问题</a>  ***</li>
<li><a href="/www6vHomeHexo/2021/05/17/lostTraceId/" title="TraceId 丢失问题">TraceId 丢失问题</a>  ***</li>
<li><a href="/www6vHomeHexo/2022/07/20/springTransactionInvalid/" title="Spring  Transaction  失效">Spring  Transaction  失效</a></li>
</ul>
</li>
</ul>
<h5><span id="中间件">中间件</span><a href="#中间件" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2018/05/03/faultModel2/" title="故障模型-中间件层">故障模型-中间件层</a>
<ul>
<li>微服务<ul>
<li><a href="/www6vHomeHexo/2019/07/06/findProblem/" title="故障排查的流程和方法">故障排查的流程和方法</a></li>
<li><a href="/www6vHomeHexo/2017/10/17/slowRT/" title="服务慢响应超时排查">服务慢响应超时排查</a> </li>
<li><a href="/www6vHomeHexo/2017/07/28/zookeeperBug/" title="zookeeper未通知到服务客户端的异常排查">zookeeper未通知到服务客户端的异常排查</a>  ***</li>
</ul>
</li>
<li>Redis<ul>
<li><a href="/www6vHomeHexo/2021/05/21/redisBigKey/" title="Redis 大Key">Redis 大Key</a> ***</li>
<li><a href="/www6vHomeHexo/2022/05/23/redisNodeId/" title="Redis 集群扩容时NodeId问题">Redis 集群扩容时NodeId问题</a> ***</li>
</ul>
</li>
<li>Kafka<ul>
<li><a href="/www6vHomeHexo/2021/05/18/kafkaGracefulDown/" title="Kafka  单副本缩容问题排查">Kafka  单副本缩容问题排查</a>  ***</li>
</ul>
</li>
<li>多线程 - 死锁<ul>
<li><a href="/www6vHomeHexo/2017/09/25/mybatisBug/" title="线上不能下单问题排查">线上不能下单问题排查</a></li>
</ul>
</li>
<li>数据库<ul>
<li><a href="/www6vHomeHexo/2021/06/05/tidbTroubleshooting/" title="TiDB  故障排查">TiDB  故障排查</a>   </li>
<li><a href="/www6vHomeHexo/2020/06/21/mysqlBestPractice/" title="使用MySQL的性能问题和紧急处理手段">使用MySQL的性能问题和紧急处理手段</a></li>
<li><a href="/www6vHomeHexo/2023/08/15/mysqlDeadLock/" title="MySQL  锁和死锁">MySQL  锁和死锁</a>  ***</li>
<li><a href="/www6vHomeHexo/2022/08/16/mysqlMasterSlaveDelay/" title="MySQL 主从延迟">MySQL 主从延迟</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2017/02/19/splitBrain/" title="Split Brain">Split Brain</a></li>
</ul>
</li>
</ul>
<h5><span id="infra">Infra</span><a href="#infra" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2018/05/03/faultModel3/" title="故障模型-基础设施层">故障模型-基础设施层</a>
<ul>
<li><a href="/www6vHomeHexo/2022/01/11/haproxyTcpdump/" title="HAProxy抓包">HAProxy抓包</a>   网络</li>
<li><a href="/www6vHomeHexo/2020/08/09/tcpTimewait/" title="TIME_WAIT和优化">TIME_WAIT和优化</a> ***</li>
<li><a href="/www6vHomeHexo/2020/08/16/linuxPerformance-cpu/" title="Linux性能优化 之 cpu优化">Linux性能优化 之 cpu优化</a></li>
</ul>
</li>
</ul>
<h5><span id="k8s">K8s</span><a href="#k8s" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2022/06/08/k8sTroubleshoot/" title="kubernetes 问题Troubleshoot">kubernetes 问题Troubleshoot</a>  </li>
<li><a href="/www6vHomeHexo/2022/04/03/k8sProblem/" title="Kubernetes问题排查-troubleshooting">Kubernetes问题排查-troubleshooting</a>      ***</li>
<li><a href="/www6vHomeHexo/2022/01/23/k8sBestPractice/" title="Kubernetes最佳实践-BestPractice">Kubernetes最佳实践-BestPractice</a>   ***</li>
</ul>
<h5><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2020/08/08/tcpFault/" title="TCP故障模式">TCP故障模式</a></li>
<li><a href="/www6vHomeHexo/2019/10/12/crashDetect/" title="宕机检测-Lease、心跳">宕机检测-Lease、心跳</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>故障排查</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>架构 汇总</title>
    <url>/www6vHomeHexo/2021/03/20/archSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="应用架构">应用架构</span><a href="#应用架构" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/11/01/apiDesign/" title="OpenAPI 设计">OpenAPI 设计</a></li>
<li><a href="/www6vHomeHexo/2018/02/25/cqrs/" title="CQRS 简介和案例分析">CQRS 简介和案例分析</a></li>
<li><a href="/www6vHomeHexo/2020/05/22/ddd/" title="DDD  领域驱动设计">DDD  领域驱动设计</a></li>
<li><a href="/www6vHomeHexo/2023/07/06/dddPractice/" title="DDD-落地实战 Practice">DDD-落地实战 Practice</a> </li>
<li><a href="/www6vHomeHexo/2018/04/07/EAI/" title="应用集成方式">应用集成方式</a></li>
<li><a href="/www6vHomeHexo/2018/03/17/DomainLogicAndSQL/" title="领域逻辑和SQL">领域逻辑和SQL</a></li>
</ul>
<h2><span id="系统架构">系统架构</span><a href="#系统架构" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/05/02/middleStage/" title="中台战略">中台战略</a></li>
<li><a href="/www6vHomeHexo/2022/02/02/disaggregationOfComputeAndStorage/" title="存算分离-数据应用">存算分离-数据应用</a></li>
<li><a href="/www6vHomeHexo/2019/04/28/haSummary/" title="高可用+容灾  汇总">高可用+容灾  汇总</a>
<ul>
<li><a href="/www6vHomeHexo/2022/06/26/available/" title="高可用 Available">高可用 Available</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2023/05/13/unifyModel/" title="统一模型">统一模型</a> </li>
<li><a href="/www6vHomeHexo/2017/06/17/multiLive/" title="异地多活 总结">异地多活 总结</a></li>
</ul>
<h2><span id="系统设计">系统设计</span><a href="#系统设计" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/09/13/feed/" title="Feed流 总结">Feed流 总结</a></li>
<li><a href="/www6vHomeHexo/2022/03/02/systemDesign/" title="系统设计 总结">系统设计 总结</a></li>
<li><a href="/www6vHomeHexo/2018/05/21/secKillSummary/" title="秒杀系统总结">秒杀系统总结</a></li>
<li><a href="/www6vHomeHexo/2018/05/06/seckill/" title="秒杀系统和商品详情页系统(培训讲义)">秒杀系统和商品详情页系统(培训讲义)</a></li>
</ul>
<h2><span id="设计原则">设计原则</span><a href="#设计原则" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2018/09/28/designPrinciple/" title="设计原则">设计原则</a></li>
<li><a href="/www6vHomeHexo/2023/04/02/designOCPspi/" title="开闭原则 - SPI">开闭原则 - SPI</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>架构</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>服务治理  汇总</title>
    <url>/www6vHomeHexo/2021/03/03/serviceGovernanceSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86-%E6%B1%87%E6%80%BB">服务治理 汇总</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#api-gateway">API Gateway</a></li>
<li><a href="#config-discovery">Config &amp; Discovery</a></li>
<li><a href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1">负载均衡</a></li>
<li><a href="#%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B">线程模型</a></li>
<li><a href="#%E5%AE%B9%E9%94%99%E9%99%90%E6%B5%81">容错&amp;限流</a></li>
</ul>
<ul>
<li><a href="#%E5%AE%89%E5%85%A8">安全</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="服务治理-汇总">服务治理  汇总</span><a href="#服务治理-汇总" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2019/09/09/microservice/" title="微服务 总结">微服务 总结</a></li>
<li><a href="/www6vHomeHexo/2015/05/07/soaFeature/" title="分布式服务框架功能">分布式服务框架功能</a></li>
</ul>
<h3><span id="api-gateway">API Gateway</span><a href="#api-gateway" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/01/21/apiGateway/" title="API Gateway网关">API Gateway网关</a></li>
<li><a href="/www6vHomeHexo/2022/03/22/apiGatawayApisix/" title="API 网关-apisix">API 网关-apisix</a></li>
<li><a href="/www6vHomeHexo/2022/03/22/apiGatawaySpringGateway/" title="API 网关-SpringCloud Gateway">API 网关-SpringCloud Gateway</a></li>
<li><a href="/www6vHomeHexo/2022/03/23/apiGatewayGray/" title="API 网关-灰度发布">API 网关-灰度发布</a></li>
</ul>
<h3><span id="config-amp-discovery">Config &amp; Discovery</span><a href="#config-amp-discovery" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2020/07/27/config/" title="服务治理-分布式配置">服务治理-分布式配置</a></li>
<li><a href="/www6vHomeHexo/2022/08/14/soaDiscovery/" title="服务发现">服务发现</a></li>
</ul>
<h3><span id="负载均衡">负载均衡</span><a href="#负载均衡" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/05/06/loadBalance/" title="负载均衡-算法">负载均衡-算法</a></li>
<li><a href="/www6vHomeHexo/2019/08/22/nginx/" title="Nginx总结">Nginx总结</a></li>
<li><a href="/www6vHomeHexo/2020/03/26/nginxOptimize/" title="Nginx优化">Nginx优化</a></li>
</ul>
<h3><span id="线程模型">线程模型</span><a href="#线程模型" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2015/07/09/jsfThreadModel/" title="京东服务框架JSF服务提供者线程模型">京东服务框架JSF服务提供者线程模型</a></li>
</ul>
<h3><span id="容错amp限流">容错&amp;限流</span><a href="#容错amp限流" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2015/06/17/soaTolerate/" title="分布式服务框架 容错机制">分布式服务框架 容错机制</a></li>
<li><a href="/www6vHomeHexo/2016/10/07/soaTolerateFramework/" title="容错框架">容错框架</a></li>
<li><a href="/www6vHomeHexo/2016/09/26/ratelimit/" title="限流-总结">限流-总结</a></li>
<li><a href="/www6vHomeHexo/2022/03/28/ratelimitSentinel/" title="流量治理-Sentinel">流量治理-Sentinel</a></li>
<li><a href="/www6vHomeHexo/2016/01/17/soaTimeout/" title="超时和重试 总结">超时和重试 总结</a></li>
<li><a href="/www6vHomeHexo/2022/08/14/soaGracefulStart/" title="优雅启动">优雅启动</a> </li>
<li><a href="/www6vHomeHexo/2022/08/14/soaGracefulClose/" title="优雅关闭">优雅关闭</a></li>
</ul>
<h2><span id="安全">安全</span><a href="#安全" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/08/10/soaAuth/" title="服务治理-鉴权">服务治理-鉴权</a></li>
<li><a href="/www6vHomeHexo/2020/03/20/securityOAuth2/" title="安全-OAuth2">安全-OAuth2</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>service</category>
      </categories>
      <tags>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title>消息系统 汇总</title>
    <url>/www6vHomeHexo/2021/02/11/mqSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2016/04/19/mq/" title="消息中间件总结">消息中间件总结</a></li>
<li><a href="/www6vHomeHexo/2022/05/12/mqCompare/" title="MQ总结(Kafka, Rocketmq, Rabbitmq)">MQ总结(Kafka, Rocketmq, Rabbitmq)</a></li>
<li><a href="/www6vHomeHexo/2021/05/19/mqOrdering/" title="消息系统 顺序消息">消息系统 顺序消息</a></li>
</ul>
<h2><span id="kafka">Kafka</span><a href="#kafka" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2016/05/11/kafka/" title="Kafka总结">Kafka总结</a></li>
<li><a href="/www6vHomeHexo/2022/05/15/kafkaProducer/" title="Kafka Producer生产者">Kafka Producer生产者</a></li>
<li><a href="/www6vHomeHexo/2016/06/25/kafkaConsumer/" title="Kafka消费者总结">Kafka消费者总结</a></li>
<li><a href="/www6vHomeHexo/2022/05/11/kafkaRebalance/" title="Kafka消费者-Rebalance机制">Kafka消费者-Rebalance机制</a></li>
<li><a href="/www6vHomeHexo/2016/07/05/kafkaReliability/" title="Kafka 可靠性总结">Kafka 可靠性总结</a></li>
<li><a href="/www6vHomeHexo/2021/05/15/kafkaIndex/" title="Kafka 索引">Kafka 索引</a>   #1</li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaZeroCopy/" title="Kafka-ZeroCopy">Kafka-ZeroCopy</a> </li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaController/" title="Kafka Controller-控制器">Kafka Controller-控制器</a></li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaReplica/" title="Kafka Replication-副本机制">Kafka Replication-副本机制</a></li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaElection/" title="Kafka 中的选主">Kafka 中的选主</a></li>
<li><a href="/www6vHomeHexo/2019/05/15/kafkaQ-A/" title="Kafka  Q&amp;A">Kafka  Q&amp;A</a></li>
<li><a href="/www6vHomeHexo/2022/05/04/kafkaTransaction/" title="Kafka 幂等性和事务">Kafka 幂等性和事务</a>  #2</li>
</ul>
<h2><span id="rocketmq">RocketMQ</span><a href="#rocketmq" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/06/18/mqRocketmq/" title="RocketMQ总结">RocketMQ总结</a>  </li>
<li><a href="/www6vHomeHexo/2020/08/12/mqRocketmqTransaction/" title="Rocketmq中的事务">Rocketmq中的事务</a>   #2 </li>
<li><a href="/www6vHomeHexo/2023/02/26/mqRocketmqStorage/" title="RocketMQ 文件系统">RocketMQ 文件系统</a>  #1</li>
</ul>
<h2><span id="pulsar">Pulsar</span><a href="#pulsar" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/31/mqPulsar/" title="Pulsar">Pulsar</a>  </li>
<li><a href="/www6vHomeHexo/2022/06/18/mqPulsarSync/" title="Pulsar-数据同步">Pulsar-数据同步</a>  </li>
<li><a href="/www6vHomeHexo/2022/06/10/mqComparePulsarVsKafka/" title="Pulsar vs. Kafka">Pulsar vs. Kafka</a></li>
</ul>
<h2><span id="resource">Resource</span><a href="#resource" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2021/05/19/mqStudy/" title="MQ 学习资料&amp;案例">MQ 学习资料&amp;案例</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>MQ</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>关系型数据库 汇总</title>
    <url>/www6vHomeHexo/2021/02/09/rmdbSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="单机">单机</span><a href="#单机" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2020/08/14/mysqlTransactionAndLock/" title="MySQL 事务-隔离性">MySQL 事务-隔离性</a></li>
<li><a href="/www6vHomeHexo/2020/06/26/mysqlUpdate/" title="MySQL中的SQL更新语句">MySQL中的SQL更新语句</a></li>
<li><a href="/www6vHomeHexo/2020/06/21/mysqlReliability/" title="MySQL的主从 高可用 容灾">MySQL的主从 高可用 容灾</a></li>
<li><a href="/www6vHomeHexo/2019/09/10/mysqlIndex/" title="MySQL的索引和优化">MySQL的索引和优化</a></li>
<li><a href="/www6vHomeHexo/2015/02/21/mysqlTransaction/" title="MySQL事务-总结">MySQL事务-总结</a>   </li>
<li><a href="/www6vHomeHexo/2022/02/27/mysqlLog/" title="MySQL Logs">MySQL Logs</a>   </li>
<li><a href="/www6vHomeHexo/2023/08/15/mysqlDeadLock/" title="MySQL  锁和死锁">MySQL  锁和死锁</a>  </li>
<li><a href="/www6vHomeHexo/2022/08/16/mysqlMasterSlaveDelay/" title="MySQL 主从延迟">MySQL 主从延迟</a></li>
</ul>
<h2><span id="分布式">分布式</span><a href="#分布式" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/02/09/distributedDatabase/" title="分布式 数据库 总结">分布式 数据库 总结</a>
</li>
<li><a href="/www6vHomeHexo/2022/03/11/distributedDatabaseCompare/" title="分布式 数据库 比较">分布式 数据库 比较</a>
</li>
<li><a href="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/" title="TiKV Transaction-MVCC+TSO">TiKV Transaction-MVCC+TSO</a>
</li>
<li><a href="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/" title="分布式数据库-全局时钟">分布式数据库-全局时钟</a>  
</li>
<li><a href="/www6vHomeHexo/2021/05/30/distributedDatabaseJoinQuery/" title="数据库  关联查询">数据库  关联查询</a>
</li>
<li><a href="/www6vHomeHexo/2023/06/05/globalSecondaryIndex/" title="全局二级索引-GSI">全局二级索引-GSI</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>关系型数据库</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis+缓存 汇总</title>
    <url>/www6vHomeHexo/2021/02/07/nosqlRedisSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#redis-%E5%9F%BA%E7%A1%80">Redis 基础</a><ul>
<li><a href="#overviw">overviw</a></li>
<li><a href="#arch-redis-cluster">Arch &amp; Redis Cluster</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">数据类型</a></li>
<li><a href="#%E4%BA%8B%E5%8A%A1">事务</a></li>
<li><a href="#%E6%8C%81%E4%B9%85%E5%8C%96">持久化</a></li>
<li><a href="#%E8%B5%84%E6%BA%90%E5%9B%9E%E6%94%B6">资源回收</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
<li><a href="#redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96">Redis 故障&amp;优化</a></li>
<li><a href="#redis-%E5%BA%94%E7%94%A8">Redis 应用</a></li>
<li><a href="#redis-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90">Redis 学习资源</a></li>
<li><a href="#%E7%BC%93%E5%AD%98">缓存</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="redis-基础">Redis 基础</span><a href="#redis-基础" class="header-anchor">#</a></h1><h3><span id="overviw">overviw</span><a href="#overviw" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2016/11/12/redis/" title="Redis 总结">Redis 总结</a></li>
<li><a href="/www6vHomeHexo/2022/01/18/redisIO/" title="Redis 的IO模型">Redis 的IO模型</a></li>
</ul>
<h3><span id="arch-amp-redis-cluster">Arch &amp; Redis Cluster</span><a href="#arch-amp-redis-cluster" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2021/05/25/redisArch/" title="Redis 架构">Redis 架构</a></li>
<li><a href="/www6vHomeHexo/2022/03/28/redisError/" title="Redis Error-MOVED和ASK指令">Redis Error-MOVED和ASK指令</a></li>
<li><a href="/www6vHomeHexo/2022/07/11/redisCluster/" title="Redis Cluster">Redis Cluster</a>    </li>
<li><a href="/www6vHomeHexo/2022/07/11/redisClusterSpec/" title="Redis Cluster Spec">Redis Cluster Spec</a>  </li>
<li><a href="/www6vHomeHexo/2022/07/31/redisHA/" title="Redis 集群  容灾（同城多活）">Redis 集群  容灾（同城多活）</a></li>
</ul>
<h3><span id="数据类型">数据类型</span><a href="#数据类型" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2021/06/20/redisRehash/" title="Redis Rehash机制">Redis Rehash机制</a></li>
<li><a href="/www6vHomeHexo/2021/05/05/redisDataStructure/" title="Redis数据结构">Redis数据结构</a></li>
</ul>
<h3><span id="事务">事务</span><a href="#事务" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/01/18/redisTransaction/" title="Redis 事务">Redis 事务</a></li>
</ul>
<h3><span id="持久化">持久化</span><a href="#持久化" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2021/01/02/redisRDB/" title="Redis RDB">Redis RDB</a></li>
<li><a href="/www6vHomeHexo/2021/12/08/rdb/" title="Redis RDB源码">Redis RDB源码</a></li>
<li><a href="/www6vHomeHexo/2022/01/18/redisAOF/" title="Redis AOF">Redis AOF</a></li>
<li><a href="/www6vHomeHexo/2021/11/21/aofRewrite/" title="Redis AOF Rewrite">Redis AOF Rewrite</a></li>
<li><a href="/www6vHomeHexo/2023/07/10/redisBothAofAndRDB/" title="Redis 混合持久化">Redis 混合持久化</a></li>
<li><a href="/www6vHomeHexo/2022/07/10/redisReplica/" title="Redis 主从复制">Redis 主从复制</a></li>
</ul>
<h3><span id="资源回收">资源回收</span><a href="#资源回收" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/06/01/redisLazyFree/" title="Redis LazyFree">Redis LazyFree</a></li>
<li><a href="/www6vHomeHexo/2021/06/02/redisDelete/" title="Redis 回收策略">Redis 回收策略</a></li>
<li><a href="/www6vHomeHexo/2022/07/16/redisLRU/" title="Redis LRU算法">Redis LRU算法</a></li>
</ul>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/05/14/redisNVM/" title="Redis NVM">Redis NVM</a></li>
</ul>
<h1><span id="redis-故障amp优化">Redis 故障&amp;优化</span><a href="#redis-故障amp优化" class="header-anchor">#</a></h1><ul>
<li><p>常见问题</p>
<ul>
<li><a href="/www6vHomeHexo/2021/05/21/redisBigKey/" title="Redis 大Key">Redis 大Key</a> </li>
<li><a href="/www6vHomeHexo/2021/05/24/redisOptimize/" title="Redis 优化建议">Redis 优化建议</a></li>
<li><a href="/www6vHomeHexo/2022/03/28/redisReliability-1/" title="Redis雪崩、击穿、穿透">Redis雪崩、击穿、穿透</a></li>
<li><a href="/www6vHomeHexo/2022/02/21/redisDbConsistent/" title="Redis和数据库之间的一致性">Redis和数据库之间的一致性</a></li>
<li><a href="/www6vHomeHexo/2022/03/25/redisSlowResponse/" title="Redis 慢查询排查">Redis 慢查询排查</a> </li>
<li><a href="/www6vHomeHexo/2022/06/03/redisHotkey/" title="Redis 热点Hotkey">Redis 热点Hotkey</a></li>
<li><a href="/www6vHomeHexo/2022/07/31/redisHitRate/" title="Redis  命中率">Redis  命中率</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2022/05/23/redisNodeId/" title="Redis 集群扩容时NodeId问题">Redis 集群扩容时NodeId问题</a></li>
</ul>
<h1><span id="redis-应用">Redis 应用</span><a href="#redis-应用" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2021/06/29/redisUseCase/" title="Redis 使用场景UseCase">Redis 使用场景UseCase</a></li>
<li><a href="/www6vHomeHexo/2022/05/05/redisDistKey/" title="Redis 分布式锁">Redis 分布式锁</a></li>
<li><a href="/www6vHomeHexo/2022/04/27/redisSLO/" title="Redis SLO">Redis SLO</a></li>
</ul>
<h1><span id="redis-学习资源">Redis 学习资源</span><a href="#redis-学习资源" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2021/05/24/redisStudy/" title="Redis 学习资源">Redis 学习资源</a></li>
</ul>
<h1><span id="缓存">缓存</span><a href="#缓存" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeHexo/2021/05/25/cacheConsistent/" title="缓存 一致性">缓存 一致性</a></li>
<li><a href="/www6vHomeHexo/2019/05/25/cacheMultiLayer/" title="多级缓存(cache)">多级缓存(cache)</a></li>
<li><a href="/www6vHomeHexo/2018/01/21/cacheSummary/" title="缓存(cache)总结">缓存(cache)总结</a></li>
<li><a href="/www6vHomeHexo/2017/12/07/cache/" title="缓存(cache)机制">缓存(cache)机制</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>可观察性  汇总</title>
    <url>/www6vHomeHexo/2020/07/17/observabilitySummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2019/08/31/observability/" title="可观测性 总结">可观测性 总结</a></li>
<li><a href="/www6vHomeHexo/2022/08/04/observabilityBuilding/" title="可观测性-系统构建">可观测性-系统构建</a></li>
</ul>
<h3><span id="统一模型">统一模型</span><a href="#统一模型" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/01/29/observabilityOpenTelemetry/" title="可观测性-OpenTelemetry">可观测性-OpenTelemetry</a></li>
</ul>
<h3><span id="tracing">Tracing</span><a href="#tracing" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2023/01/28/observabilityTracing/" title="可观测性-Tracing">可观测性-Tracing</a></li>
<li><a href="/www6vHomeHexo/2022/03/18/observabilitySkywalking/" title="可观测性-Skywalking">可观测性-Skywalking</a></li>
</ul>
<h3><span id="metric">Metric</span><a href="#metric" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/04/10/observabilityPrometheus/" title="可观测性-Prometheus">可观测性-Prometheus</a></li>
<li><a href="/www6vHomeHexo/2022/02/11/observabilityPrometheusHA/" title="可观测性-Prometheus  HA">可观测性-Prometheus  HA</a></li>
<li><a href="/www6vHomeHexo/2022/07/17/observabilityPrometheusBiz/" title="可观测性-Prometheus业务监控">可观测性-Prometheus业务监控</a></li>
</ul>
<h3><span id="log">Log</span><a href="#log" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2023/01/28/observabilityLog/" title="可观测性-Log">可观测性-Log</a></li>
</ul>
<h3><span id="k8s">K8s</span><a href="#k8s" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/01/30/k8sObservability/" title="可观测性-Kubernetes">可观测性-Kubernetes</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>可观察性</category>
      </categories>
      <tags>
        <tag>可观察性</tag>
      </tags>
  </entry>
  <entry>
    <title>LSM-Tree 汇总</title>
    <url>/www6vHomeHexo/2020/04/06/lsmTreeSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="lsm-tree-存储引擎">LSM-Tree 存储引擎</span><a href="#lsm-tree-存储引擎" class="header-anchor">#</a></h2><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ul>
<li>paperlsmTreeSurvey  todo</li>
<li><a href="/www6vHomeHexo/2022/06/05/lsmTreeKeyValueSeparation/" title="LSM-Tree  KV分离">LSM-Tree  KV分离</a>   优化</li>
<li><a href="/www6vHomeHexo/2022/01/08/lsmTreeCompaction/" title="LSM-Tree Compaction压缩策略">LSM-Tree Compaction压缩策略</a>   优化</li>
</ul>
<h3><span id="工程">工程</span><a href="#工程" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2023/04/02/hbaselsmTree/" title="HBase - LSM-Tree">HBase - LSM-Tree</a> </li>
<li><a href="/www6vHomeHexo/2022/01/29/rocksdb/" title="RocksDB 总结">RocksDB 总结</a> </li>
<li><a href="/www6vHomeHexo/2022/04/05/rocksdbLsm/" title="RocksDB- LSM-Tree">RocksDB- LSM-Tree</a>  </li>
<li><a href="/www6vHomeHexo/2023/04/06/rocksdbSST/" title="Rocksdb-SST">Rocksdb-SST</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>LSM-Tree</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式系统 Handbook</title>
    <url>/www6vHomeHexo/2019/10/23/distributedSystemHandbook/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p><a href="https://www6v.github.io/www6vBook/">分布式系统 Handbook</a>  Wang Wei(www6v)</p>
]]></content>
      <categories>
        <category>汇总</category>
        <category>handbook</category>
      </categories>
      <tags>
        <tag>handbook</tag>
      </tags>
  </entry>
  <entry>
    <title>性能和优化 汇总</title>
    <url>/www6vHomeHexo/2019/09/22/performanceOptimizeSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2018/11/21/performance/" title="性能优化总结">性能优化总结</a>  ***</li>
<li><a href="/www6vHomeHexo/2022/06/16/performanceAnalysis/" title="性能分析">性能分析</a></li>
<li><a href="/www6vHomeHexo/2022/06/15/performanceTest/" title="性能测试">性能测试</a></li>
</ul>
<h2><span id="linux">Linux</span><a href="#linux" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/08/08/linuxPerformance/" title="Linux性能优化">Linux性能优化</a>   cpu， memory， io</li>
<li><a href="/www6vHomeHexo/2018/12/26/linuxProfile/" title="Linux性能分析">Linux性能分析</a>   method and tools</li>
<li><a href="/www6vHomeHexo/2020/08/16/linuxPerformance-cpu/" title="Linux性能优化 之 cpu优化">Linux性能优化 之 cpu优化</a>  ***</li>
<li><a href="/www6vHomeHexo/2020/08/16/linuxKernelParam/" title="虚拟机和容器中的内核参数 kernel">虚拟机和容器中的内核参数 kernel</a></li>
<li><a href="/www6vHomeHexo/2022/01/25/linuxDPDK/" title="DPDK">DPDK</a></li>
</ul>
<h2><span id="网络-net">网络 Net</span><a href="#网络-net" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2020/08/09/tcpTimewait/" title="TIME_WAIT和优化">TIME_WAIT和优化</a>  *** </li>
<li><a href="/www6vHomeHexo/2019/08/07/tcpUdpControlCongestion/" title="TCP流控和拥塞控制">TCP流控和拥塞控制</a></li>
</ul>
<h2><span id="java和应用">Java和应用</span><a href="#java和应用" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2017/11/27/optimize/" title="JVM性能调优">JVM性能调优</a>  内存 *** </li>
<li><a href="/www6vHomeHexo/2015/12/05/async/" title="异步化 总结">异步化 总结</a>  ***</li>
<li><a href="/www6vHomeHexo/2014/07/02/threadNum/" title="线程池最佳线程数">线程池最佳线程数</a>  线程  ***</li>
<li><a href="/www6vHomeHexo/2017/12/07/cache/" title="缓存(cache)机制">缓存(cache)机制</a>  内存</li>
<li><a href="/www6vHomeHexo/2014/03/05/falseSharing/" title="伪共享 FalseSharing">伪共享 FalseSharing</a>  内存</li>
<li><a href="/www6vHomeHexo/2022/07/14/performancePool/" title="性能优化-池化Pool">性能优化-池化Pool</a></li>
</ul>
<h2><span id="mysql">MySQL</span><a href="#mysql" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2019/09/10/mysqlIndex/" title="MySQL的索引和优化">MySQL的索引和优化</a>  ***</li>
<li><a href="/www6vHomeHexo/2020/06/21/mysqlBestPractice/" title="使用MySQL的性能问题和紧急处理手段">使用MySQL的性能问题和紧急处理手段</a>  ***</li>
</ul>
<h2><span id="spark">Spark</span><a href="#spark" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/19/streamingSparkPerformance/" title="Spark 性能优化">Spark 性能优化</a>   ***</li>
<li><a href="/www6vHomeHexo/2019/03/10/streamingSparkTrain/" title="Spark公司内部培训">Spark公司内部培训</a></li>
</ul>
<h2><span id="中间件">中间件</span><a href="#中间件" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2021/05/24/redisOptimize/" title="Redis 优化建议">Redis 优化建议</a></li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaZeroCopy/" title="Kafka-ZeroCopy">Kafka-ZeroCopy</a></li>
<li><a href="/www6vHomeHexo/2020/03/26/nginxOptimize/" title="Nginx优化">Nginx优化</a></li>
</ul>
<h2><span id="serverless">Serverless</span><a href="#serverless" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/06/03/serverlessOptimize/" title="Serverless 优化">Serverless 优化</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>性能</category>
      </categories>
      <tags>
        <tag>性能</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>稳定性 汇总</title>
    <url>/www6vHomeHexo/2019/09/22/stableSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a><ul>
<li><a href="#%E7%A8%B3%E5%AE%9A%E6%80%A7">稳定性</a></li>
<li><a href="#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84">系统架构</a></li>
</ul>
</li>
<li><a href="#sre">SRE</a><ul>
<li><a href="#sre-1">SRE</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B">故障模型</a></li>
<li><a href="#%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C">容量保障</a></li>
<li><a href="#%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B">混沌工程</a></li>
<li><a href="#%E5%8F%AF%E8%A7%82%E6%B5%8B">可观测</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><h3><span id="稳定性">稳定性</span><a href="#稳定性" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a></li>
</ul>
<h3><span id="系统架构">系统架构</span><a href="#系统架构" class="header-anchor">#</a></h3><ul>
<li><a href="../../../../categories/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/">系统架构</a>  category<ul>
<li><a href="/www6vHomeHexo/2022/06/26/available/" title="高可用 Available">高可用 Available</a> </li>
<li><a href="/www6vHomeHexo/2017/06/17/multiLive/" title="异地多活 总结">异地多活 总结</a></li>
</ul>
</li>
</ul>
<h1><span id="sre">SRE</span><a href="#sre" class="header-anchor">#</a></h1><h3><span id="sre">SRE</span><a href="#sre" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/03/13/sre/" title="SRE 总结">SRE 总结</a>

</li>
<li><a href="/www6vHomeHexo/2023/02/01/sreWorkbook/" title="《SRE 工作手册》">《SRE 工作手册》</a>
<ul>
<li><a href="/www6vHomeHexo/2022/04/27/sreWorkbookBasic/" title="SRE 五大根基">SRE 五大根基</a> </li>
<li><a href="/www6vHomeHexo/2022/05/02/sreWorkbookBasicSLO/" title="SRE 五大根基-SLO">SRE 五大根基-SLO</a> </li>
<li><a href="/www6vHomeHexo/2022/05/02/sreWorkbookBasicAlert/" title="SRE 五大根基-报警">SRE 五大根基-报警</a></li>
</ul>
</li>
</ul>
<h3><span id="故障模型">故障模型</span><a href="#故障模型" class="header-anchor">#</a></h3><p> <a href="../../../../2018/10/27/fault/">故障模型</a></p>
<h3><span id="容量保障">容量保障</span><a href="#容量保障" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/03/13/capacityGuarantee/" title="容量保障与全链路压测">容量保障与全链路压测</a></li>
</ul>
<h3><span id="混沌工程">混沌工程</span><a href="#混沌工程" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2019/09/24/chaosEngineering/" title="混沌工程">混沌工程</a></li>
</ul>
<h3><span id="可观测">可观测</span><a href="#可观测" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2020/07/17/observabilitySummary/" title="可观察性  汇总">可观察性  汇总</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>稳定性</category>
      </categories>
      <tags>
        <tag>稳定性</tag>
      </tags>
  </entry>
  <entry>
    <title>网络/内存/存储 汇总</title>
    <url>/www6vHomeHexo/2019/08/31/brief/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93">网络总结</a><br>- <a href="#linux%E7%BD%91%E7%BB%9C">Linux网络</a><br>- <a href="#%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-vpc">容器网络、VPC</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%B1%82">应用层</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E6%80%BB%E7%BB%93">内存总结</a><br>- <a href="#linux">Linux</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%B1%82-1">应用层</a></li>
<li><a href="#%E6%96%87%E4%BB%B6%E5%92%8C%E5%AD%98%E5%82%A8">文件和存储</a><br>- <a href="#linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F20190824linuxfile">Linux文件系统</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%B1%82-2">应用层</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="网络总结">网络总结</span><a href="#网络总结" class="header-anchor">#</a></h2><h5><span id="linux网络">Linux网络</span><a href="#linux网络" class="header-anchor">#</a></h5><ul>
<li><a href="../../../../2019/08/19/iptables/">iptables总结</a></li>
<li><a href="../../../../2019/08/07/tcpUdpControlCongestion/">TCP流控和拥塞控制</a></li>
<li><a href="../../../../2015/04/25/tcp/">TCP总结</a></li>
<li><a href="../../../../2019/08/25/linux-socket/">Socket总结</a></li>
</ul>
<h5><span id="容器网络-vpc">容器网络、VPC</span><a href="#容器网络-vpc" class="header-anchor">#</a></h5><ul>
<li><a href="../../../../2019/08/04/docker-network/">Docker网络</a></li>
<li><a href="../../../../2019/08/11/k8sInterface/">Kubernetes开放接口</a>   </li>
<li><a href="../../../../2019/05/15/netConnection/">IDC网络互通</a></li>
</ul>
<h5><span id="应用层">应用层</span><a href="#应用层" class="header-anchor">#</a></h5><ul>
<li><a href="../../../../2015/08/23/nettySummary/">Netty总结</a></li>
<li><a href="../../../../2015/10/03/nettyEpollEventLoop/">Netty EpollEventLoop</a></li>
<li><a href="../../../../2015/09/06/nettyEventLoop-Accept/">Netty中NioEventLoop的accept过程</a></li>
<li><a href="../../../../2019/08/14/https/">HTTPS总结</a></li>
</ul>
<h2><span id="内存总结">内存总结</span><a href="#内存总结" class="header-anchor">#</a></h2><h5><span id="linux">Linux</span><a href="#linux" class="header-anchor">#</a></h5><ul>
<li><a href="../../../../2019/08/23/linuxMemory/">Linux内存管理</a>  </li>
<li><a href="../../../../2019/09/14/zeroCopy/">Linux zero copy</a>   （todo: kafka zero-copy）</li>
</ul>
<h5><span id="应用层">应用层</span><a href="#应用层" class="header-anchor">#</a></h5><ul>
<li><a href="../../../../2014/01/03/memoryModel/">Java内存模型</a></li>
<li>golang内存分配<br><a href="https://mp.weixin.qq.com/s/7bTGxhl7RXBmw5bxaR7Cnw">图解Go语言内存分配</a><br><a href="https://www.infoq.cn/article/IEhRLwmmIM7-11RYaLHR">图解 Go 内存分配器</a></li>
<li><a href="http://arthurchiao.art/blog/memory-models-underlie-programming-languages-zh/">[译] 编程语言中的 6 种内存模型（2016）</a></li>
<li>Redis[todo]</li>
</ul>
<h2><span id="文件和存储">文件和存储</span><a href="#文件和存储" class="header-anchor">#</a></h2><h5><span id="linux文件系统"></span><a href="#linux文件系统" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>系统</th>
<th>组件</th>
<th>缓存</th>
</tr>
</thead>
<tbody><tr>
<td>虚拟文件系统</td>
<td>dentry， inode(索引文件)</td>
<td>page cache</td>
</tr>
<tr>
<td>块设备</td>
<td>dev</td>
<td>buffer</td>
</tr>
</tbody></table>
<h5><span id="应用层">应用层</span><a href="#应用层" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>总结</th>
<th>知识点</th>
</tr>
</thead>
<tbody><tr>
<td><a href="../../../../2022/01/08/ceph/">Ceph</a></td>
<td>Block, File, Object</td>
</tr>
<tr>
<td><a href="../../../../2017/04/23/fileIO/">文件IO总结</a></td>
<td>mmap，NIO(FileChannel)</td>
</tr>
<tr>
<td><a href="../../../../2016/05/11/kafka/">Kafka总结</a></td>
<td>Partition的Segment中的index文件，data文件</td>
</tr>
<tr>
<td><a href="../../../../2019/06/18/mqRocketmq/">RocketMQ总结</a></td>
<td>CommitLog， ComsumeQueue， 索引文件(todo mmap优化)</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>编程语言Examples 汇总</title>
    <url>/www6vHomeHexo/2019/06/19/languageDemo/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><a href="https://github.com/www6v/myExamples.git">myExamples</a></p>
]]></content>
      <categories>
        <category>汇总</category>
        <category>语言</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>高可用+容灾  汇总</title>
    <url>/www6vHomeHexo/2019/04/28/haSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8">高可用</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E4%B8%AD%E9%97%B4%E4%BB%B6">中间件</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1">数据服务</a></li>
<li><a href="#k8s-%E9%AB%98%E5%8F%AF%E7%94%A8">K8S 高可用</a></li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E7%81%BE%E5%A4%9A%E6%B4%BB">容灾&amp;多活</a><ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%BA%94%E7%94%A8%E5%AE%B9%E7%81%BE">服务&amp;应用容灾</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%AE%B9%E7%81%BE">数据容灾</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E5%AE%B9%E7%81%BE">环境容灾</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="高可用">高可用</span><a href="#高可用" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/06/26/available/" title="高可用 Available">高可用 Available</a>
<ul>
<li><a href="/www6vHomeHexo/2017/02/19/splitBrain/" title="Split Brain">Split Brain</a></li>
</ul>
</li>
<li><a href="/www6vHomeHexo/2022/01/25/tencentTCP3/" title="腾讯云TCP3-构建腾讯云上高可用架构">腾讯云TCP3-构建腾讯云上高可用架构</a></li>
</ul>
<h3><span id="中间件">中间件</span><a href="#中间件" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2016/07/05/kafkaReliability/" title="Kafka 可靠性总结">Kafka 可靠性总结</a></li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaElection/" title="Kafka 中的选主">Kafka 中的选主</a></li>
<li><a href="/www6vHomeHexo/2021/05/16/kafkaController/" title="Kafka Controller-控制器">Kafka Controller-控制器</a></li>
</ul>
<h3><span id="数据服务">数据服务</span><a href="#数据服务" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2020/06/21/mysqlReliability/" title="MySQL的主从 高可用 容灾">MySQL的主从 高可用 容灾</a></li>
<li>dbTradeoff</li>
</ul>
<h3><span id="k8s-高可用">K8S  高可用</span><a href="#k8s-高可用" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/01/02/k8sHA/" title="K8S高可用-控制面">K8S高可用-控制面</a></li>
<li><a href="/www6vHomeHexo/2022/04/05/k8sAvailable/" title="K8S高可用-零停机[自主中断]">K8S高可用-零停机[自主中断]</a></li>
</ul>
<h1><span id="容灾amp多活">容灾&amp;多活</span><a href="#容灾amp多活" class="header-anchor">#</a></h1><h3><span id="服务amp应用容灾">服务&amp;应用容灾</span><a href="#服务amp应用容灾" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2017/06/17/multiLive/" title="异地多活 总结">异地多活 总结</a> </li>
<li><a href="/www6vHomeHexo/2019/07/20/istio-k8s-service/" title="Istio、Kubernetes和Spring Cloud中服务的比对">Istio、Kubernetes和Spring Cloud中服务的比对</a></li>
</ul>
<h3><span id="数据容灾">数据容灾</span><a href="#数据容灾" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/07/13/aliyunDB/" title="阿里云 数据库">阿里云 数据库</a> </li>
<li><a href="/www6vHomeHexo/2022/07/31/redisHA/" title="Redis 集群  容灾（同城多活）">Redis 集群  容灾（同城多活）</a></li>
</ul>
<h3><span id="环境容灾">环境容灾</span><a href="#环境容灾" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeHexo/2022/06/26/aliyunDisasterRecovery/" title="阿里云-容灾恢复DR">阿里云-容灾恢复DR</a></li>
<li><a href="/www6vHomeHexo/2022/01/04/aliyunHybridCloud/" title="阿里云-混合云HybridCloud">阿里云-混合云HybridCloud</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>HA</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>安全 汇总</title>
    <url>/www6vHomeHexo/2019/03/12/securitySummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="网络安全">网络安全</span><a href="#网络安全" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/10/23/cyberSecurity/" title="网络空间安全-Cyber Security">网络空间安全-Cyber Security</a></li>
<li><a href="/www6vHomeHexo/2022/03/12/cyberSecurityTool/" title="安全产品">安全产品</a></li>
<li><a href="/www6vHomeHexo/2020/03/20/securityOAuth2/" title="安全-OAuth2">安全-OAuth2</a></li>
</ul>
<h2><span id="云计算安全">云计算安全</span><a href="#云计算安全" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2022/10/01/awsSecurity">aws 安全</a></li>
<li><a href="/www6vHomeHexo/2022/01/11/tencentTCP5/" title="腾讯云TCP5-云上信息安全">腾讯云TCP5-云上信息安全</a></li>
</ul>
<h2><span id="容器安全">容器安全</span><a href="#容器安全" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeHexo/2022/05/22/k8sSecurity/" title="Kubernetes安全-Security">Kubernetes安全-Security</a></li>
<li><a href="/www6vHomeHexo/2022/01/15/k8sCKS/" title="Kubernetes CKS">Kubernetes CKS</a></li>
<li><a href="/www6vHomeHexo/2022/01/16/k8sSecurityPractice/" title="Kubernetes 安全实践">Kubernetes 安全实践</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>安全</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>学习资源-汇总</title>
    <url>/www6vHomeHexo/2018/12/04/studyResouceSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="comprehensive">Comprehensive</span><a href="#comprehensive" class="header-anchor">#</a></h2><p><a href="../../../../2022/01/27/categoryOfGithub/">github资源分类</a> ***<br><a href="../../../../2022/11/30/paperStudy/">Paper 学习资源</a></p>
<h2><span id="dev">Dev</span><a href="#dev" class="header-anchor">#</a></h2><h5><span id="cloud-native">cloud native</span><a href="#cloud-native" class="header-anchor">#</a></h5><p><a href="../../../../2022/10/01/awsStudyResource/">AWS 学习资源</a><br><a href="../../../../2022/05/21/k8sStudy/">Kubernetes 学习资源</a><br><a href="../../../../2020/06/14/cloudNativeResource/">云原生-学习资源</a></p>
<h5><span id="distributed">distributed</span><a href="#distributed" class="header-anchor">#</a></h5><p><a href="../../../../2022/06/25/devopsStudyResource/">Devops 学习资源</a><br><a href="../../../../2022/05/30/linuxStudy/">Linux-学习资源</a><br><a href="../../../../2019/10/13/distributedStudy/">分布式系统学习资源-个人</a><br><a href="../../../../2019/01/21/distributedStudyTeam/">分布式系统学习资源-团队</a></p>
<h5><span id="language">language</span><a href="#language" class="header-anchor">#</a></h5><p><a href="../../../../2022/09/09/golangStudy/">Golang 学习资源</a><br><a href="../../../../2022/08/25/languageStudy/">语言 学习资源</a></p>
<h2><span id="data">Data</span><a href="#data" class="header-anchor">#</a></h2><p><a href="../../../../2022/01/22/aiStudyResouce/">人工智能-学习资源</a><br><a href="../../../../2022/05/28/bigDataStudy/">大数据 学习资源</a></p>
<p><a href="../../../../2019/09/10/others/">资料收集</a> *</p>
]]></content>
      <categories>
        <category>汇总</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>混合精度</title>
    <url>/www6vHomeHexo/2024/02/01/gptPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="混合精度1">混合精度[1]</span><a href="#混合精度1" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2024/02/01/gptPrecision/solution.png" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1R94y1g78L?p=6">混合精度</a>  *** V<br>1xx. <a href="https://zhuanlan.zhihu.com/p/441591808">全网最全-混合精度训练原理</a>  ***<br>1xx. <a href="https://www.bilibili.com/video/BV1CB4y1R78v/">半精度训练与LLaMA2训练实战</a> 有代码<br>1xx. <a href="https://www.bilibili.com/video/BV1y34y1M7t1/">低精度训练与大模型下载</a> 有代码</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Precision</category>
      </categories>
      <tags>
        <tag>Precision</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT P-Tuning</title>
    <url>/www6vHomeHexo/2024/01/28/gptPEFTPtuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="最佳实践1">最佳实践[1]</span><a href="#最佳实践1" class="header-anchor">#</a></h3><ul>
<li>要看losss, 也要看业务的loss</li>
<li>生成模型常用的评价方法<ul>
<li>BLEU 能评估流畅度</li>
<li>结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）</li>
</ul>
</li>
<li>垂直模型<ul>
<li>stf之后失去通用能力</li>
<li>要有通用能力, 需要pre-train和STF中都融入通用的语料</li>
</ul>
</li>
<li>每个模型的学习率lr不一样<ul>
<li>chatglm的学习率<br>LR&#x3D;2e-2</li>
</ul>
</li>
</ul>
<h3><span id="学习率">学习率</span><a href="#学习率" class="header-anchor">#</a></h3><ul>
<li>改的特别大<br>模型训练的时候会震荡</li>
<li>改的特别小<br> 模型训练的时候会收敛非常慢</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm的p-tuning<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm-2的P-tuning v2<br><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb">十一、小参数量微调</a><br>bili有相关的总结的视频</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Fine Tuning-Bert</title>
    <url>/www6vHomeHexo/2024/01/26/gptFineTuningBert/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="基于bert的二分类">基于bert的二分类</span><a href="#基于bert的二分类" class="header-anchor">#</a></h1><ul>
<li>代码 - 全参FT,非PEFT<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">SEED=<span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ALBERT是一种压缩过的BERT</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;albert-base-v2&quot;</span></span><br><span class="line">DATASET_NAME = <span class="string">&quot;glue&quot;</span> <span class="comment"># 一组NLP评测任务</span></span><br><span class="line">DATASET_TASK = <span class="string">&quot;mrpc&quot;</span> <span class="comment"># MRPC 是其中一个子任务 -- Microsoft Research Paraphrase Corpus</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Bert的基础上加了一个线性分类器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClassifier</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.bert_encoder = backbone</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="keyword">return</span> loss_fct(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask,labels=<span class="literal">None</span></span>):</span><br><span class="line">        output = self.bert_encoder(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        output = output.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = self.compute_loss(output, labels)</span><br><span class="line">            <span class="keyword">return</span> loss, output</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集对应的评估方法</span></span><br><span class="line">glue_metric = datasets.load_metric(DATASET_NAME, DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> glue_metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">raw_datasets = load_dataset(DATASET_NAME,DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="comment"># 验证集</span></span><br><span class="line">raw_valid_dataset = raw_datasets[<span class="string">&quot;validation&quot;</span>]</span><br><span class="line"></span><br><span class="line">columns = raw_train_dataset.column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">transformers.set_seed(SEED)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_fn</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = tokenizer(examples[<span class="string">&quot;sentence1&quot;</span>], examples[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;attention_mask&quot;</span>] = inputs[<span class="string">&quot;attention_mask&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;labels&quot;</span>] = examples[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_train_dataset = raw_train_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenized_valid_dataset = raw_valid_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据校准器（自动生成batch）</span></span><br><span class="line">collater = DataCollatorWithPadding(</span><br><span class="line">    tokenizer=tokenizer, return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型 -- 其实Transformer可以直接用AutoModelForSequenceClassification</span></span><br><span class="line"><span class="comment">#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我手工写了分类器层，为了方便大家理解什么叫在Transformer上面做分类任务</span></span><br><span class="line">backbone = AutoModel.from_pretrained(MODEL_NAME)</span><br><span class="line">model = MyClassifier(backbone)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./output&quot;</span>,        <span class="comment"># checkpoint保存路径</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;steps&quot;</span>,    <span class="comment"># 每N步做一次eval</span></span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,             <span class="comment"># 训练epoch数</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">8</span>,  <span class="comment"># 每张卡的batch大小</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,   <span class="comment"># 累加几个step做一次参数更新</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">8</span>,  <span class="comment"># evaluation batch size</span></span><br><span class="line">    logging_steps=<span class="number">20</span>,             <span class="comment"># 每20步eval一次</span></span><br><span class="line">    save_steps=<span class="number">20</span>,                <span class="comment"># 每20步保存一个checkpoint</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,             <span class="comment"># 学习率</span></span><br><span class="line">    warmup_ratio=<span class="number">0.1</span>,               <span class="comment"># 预热（可选）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练器</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, <span class="comment"># 待训练模型</span></span><br><span class="line">    args=training_args, <span class="comment"># 训练参数</span></span><br><span class="line">    data_collator=collater, <span class="comment"># 数据校准器</span></span><br><span class="line">    train_dataset=tokenized_train_dataset, <span class="comment"># 训练集</span></span><br><span class="line">    eval_dataset=tokenized_valid_dataset, <span class="comment"># 验证集</span></span><br><span class="line">    compute_metrics=compute_metrics, <span class="comment"># 评价指标</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用wandb（与huggingface.co同步的机制）</span></span><br><span class="line">os.environ[<span class="string">&quot;WANDB_DISABLED&quot;</span>] = <span class="string">&quot;true&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb">Bert fine-tuning 二分类</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>Fine-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT QLoRA 实战</title>
    <url>/www6vHomeHexo/2024/01/12/gptPEFTQLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86-1">技术原理 [1]</a></li>
<li><a href="#%E5%AE%9E%E6%88%981-2">实战1 [2]</a></li>
<li><a href="#%E5%AE%9E%E6%88%982-34">实战2 [3][4]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="技术原理-1">技术原理 [1]</span><a href="#技术原理-1" class="header-anchor">#</a></h1><p>使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。<br>QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。</p>
<ul>
<li><p>4bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。</p>
</li>
<li><p>双量化：对第一次量化后的那些常量再进行一次量化，减少存储空间。</p>
</li>
<li><p>分页优化器:  使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</p>
</li>
</ul>
<img src="/www6vHomeHexo/2024/01/12/gptPEFTQLora/qlora.png" class>

<p>实验证明，无论是使用16bit、8bit还是4bit的适配器方法，都能够复制16bit全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。</p>
<h1><span id="实战1-2">实战1 [2]</span><a href="#实战1-2" class="header-anchor">#</a></h1><h1><span id="实战2-34">实战2 [3][4]</span><a href="#实战2-34" class="header-anchor">#</a></h1><ul>
<li><p>Training的模型</p>
<img src="/www6vHomeHexo/2024/01/12/gptPEFTQLora/dirs.png" class>
</li>
<li><p>合并后的模型</p>
<img src="/www6vHomeHexo/2024/01/12/gptPEFTQLora/dir.png" class>
</li>
<li><p>4bit量化推理</p>
<img src="/www6vHomeHexo/2024/01/12/gptPEFTQLora/xtuner-chat.png" class></li>
</ul>
<blockquote>
<p>Training的时候要用tmux</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tmux new -s finetune</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tmux attach -t finetune</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ctcl +b , D</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>16bit量化推理慢,  要用4bit量化推理</p>
</blockquote>
<h1><span id="参数">参数</span><a href="#参数" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/636215898">大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/636644164">高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香</a><br><a href="https://github.com/www6v/llm-action/tree/main/train/qlora">qlora</a> git</p>
</li>
<li><p><a href="https://github.com/www6v/tutorial/tree/main/xtuner">internLM fine-tuning on xtuner</a>   </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1yK4y1B75J/">(4)XTuner 大模型单卡低成本微调实战</a> V</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/671089942">[大模型微调技术] LoRA、QLoRA、QA-LoRA 原理笔记</a> 未</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2375230">大模型实操 | LoRA、QLoRA微调大模型实战技巧分享，含常见QA解答！</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s  AdmissionWebhook</title>
    <url>/www6vHomeHexo/2023/10/16/k8sAdmissionWebhook/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="mutatingadmissionwebhook-是如何工作的1chat">MutatingAdmissionWebhook 是如何工作的[1][chat]</span><a href="#mutatingadmissionwebhook-是如何工作的1chat" class="header-anchor">#</a></h2><p><code>MutatingAdmissionWebhook</code>拦截与<code>MutatingWebhookConfiguration</code>中定义的规则匹配的请求，然后将其发送到Webhook服务器进行处理，然后再持久化到<a href="https://github.com/coreos/etcd">etcd</a> 中。 <code>MutatingAdmissionWebhook</code>通过向Webhook服务器发送admission请求来执行变更。Webhook服务器只是遵循<a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/pkg/apis/admission/types.go">Kubernetes的API</a>  的普通HTTP服务器。</p>
<p>以下图示了<code>MutatingAdmissionWebhook</code>的工作原理:</p>
<img src="/www6vHomeHexo/2023/10/16/k8sAdmissionWebhook/admissionWebhook.jpg" class>


<p><code>MutatingAdmissionWebhook</code>需要三个对象才能正常工作:</p>
<ol>
<li><p><strong>MutatingWebhookConfiguration</strong></p>
<p><code>MutatingAdmissionWebhook</code>需要在<code>apiserver</code>中注册，提供<code>MutatingWebhookConfiguration</code>。在注册过程中，MutatingAdmissionWebhook说明以下内容:</p>
<ul>
<li>如何连接到Webhook Admission服务器</li>
<li>如何验证Webhook Admission服务器</li>
<li>Webhook Admission服务器的URL路径</li>
<li>定义了哪些资源和操作它处理的规则</li>
<li>如何处理来自Webhook Admission服务器的无法识别的错误</li>
</ul>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: admissionregistration.k8s.io/v1beta1</span><br><span class="line">kind: MutatingWebhookConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: sidecar-injector-webhook-cfg</span><br><span class="line">  labels:</span><br><span class="line">    app: sidecar-injector</span><br><span class="line">webhooks:</span><br><span class="line">  - name: sidecar-injector.morven.me</span><br><span class="line">    clientConfig:</span><br><span class="line">      service:</span><br><span class="line">        name: sidecar-injector-webhook-svc   #2</span><br><span class="line">        namespace: default</span><br><span class="line">        path: &quot;/mutate&quot;</span><br><span class="line">      caBundle: $&#123;CA_BUNDLE&#125;</span><br><span class="line">    rules:</span><br><span class="line">      - operations: [ &quot;CREATE&quot; ]</span><br><span class="line">        apiGroups: [&quot;&quot;]</span><br><span class="line">        apiVersions: [&quot;v1&quot;]</span><br><span class="line">        resources: [&quot;pods&quot;]</span><br><span class="line">    namespaceSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        sidecar-injector: enabled</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p><strong>MutatingAdmissionWebhook本身</strong></p>
<p><code>MutatingAdmissionWebhook</code>是一种插件式的Admission控制器，可以配置到<code>apiserver</code>中。<code>MutatingAdmissionWebhook</code>插件从<code>MutatingWebhookConfiguration</code>中获取感兴趣的Admission Webhooks列表。然后，<code>MutatingAdmissionWebhook</code>观察到对apiserver的请求，并拦截与admission webhook规则匹配的请求，并并行地调用它们。</p>
</li>
<li><p><strong>Webhook Admission Server</strong></p>
<p><code>Webhook Admission服务器</code>只是一个符合Kubernetes <a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/pkg/apis/admission/types.go">API</a>的普通HTTP服务器。对于每个API server的请求，<code>MutatingAdmissionWebhook</code>将admissionReview（用于参考的<a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/pkg/apis/admission/types.go">API</a>）发送到相关的webhook admission服务器。webhook admission服务器会从admissionReview中收集信息，如<code>object</code>，<code>oldobject</code>和<code>userInfo</code>，然后返回一个admissionReview响应，其中包括填充了admission决策和可选的<code>Patch</code>以改变资源的AdmissionResponse。</p>
</li>
</ol>
<h3><span id="服务部署">服务部署</span><a href="#服务部署" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: sidecar-injector-webhook-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: sidecar-injector</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: sidecar-injector</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: sidecar-injector</span><br><span class="line">          image: morvencao/sidecar-injector:v1</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          args:</span><br><span class="line">            - -sidecarCfgFile=/etc/webhook/config/sidecarconfig.yaml  #1</span><br><span class="line">            - -tlsCertFile=/etc/webhook/certs/cert.pem</span><br><span class="line">            - -tlsKeyFile=/etc/webhook/certs/key.pem</span><br><span class="line">            - -alsologtostderr</span><br><span class="line">            - -v=4</span><br><span class="line">            - 2&gt;&amp;1</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: webhook-certs</span><br><span class="line">              mountPath: /etc/webhook/certs</span><br><span class="line">              readOnly: true</span><br><span class="line">            - name: webhook-config</span><br><span class="line">              mountPath: /etc/webhook/config</span><br><span class="line">      volumes:</span><br><span class="line">        - name: webhook-certs</span><br><span class="line">          secret:</span><br><span class="line">            secretName: sidecar-injector-webhook-certs</span><br><span class="line">        - name: webhook-config</span><br><span class="line">          configMap:</span><br><span class="line">            name: sidecar-injector-webhook-configmap</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: sidecar-injector-webhook-configmap</span><br><span class="line">data:</span><br><span class="line">  sidecarconfig.yaml: |</span><br><span class="line">    containers:</span><br><span class="line">      - name: sidecar-nginx</span><br><span class="line">        image: nginx:1.12.2</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">          - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: nginx-conf</span><br><span class="line">            mountPath: /etc/nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - name: nginx-conf</span><br><span class="line">        configMap:</span><br><span class="line">          name: nginx-configmap</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: sidecar-injector-webhook-svc  #2</span><br><span class="line">  labels:</span><br><span class="line">    app: sidecar-injector</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 443</span><br><span class="line">  selector:</span><br><span class="line">    app: sidecar-injector</span><br></pre></td></tr></table></figure>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://medium.com/ibm-cloud/diving-into-kubernetes-mutatingadmissionwebhook-6ef3c5695f74">Diving into Kubernetes MutatingAdmissionWebhook</a><br><a href="https://github.com/www6v/kube-sidecar-injector">kube-sidecar-injector</a>  git</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL  锁和死锁</title>
    <url>/www6vHomeHexo/2023/08/15/mysqlDeadLock/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%94%81">锁</a><ul>
<li><a href="#%E8%A1%8C%E9%94%81-%E9%94%81%E4%BC%98%E5%8C%96-3">行锁， 锁优化 [3]</a></li>
<li><a href="#%E9%9A%90%E5%BC%8F%E9%94%81%E5%92%8C%E6%98%BE%E7%A4%BA%E9%94%81">隐式锁和显示锁</a></li>
</ul>
</li>
<li><a href="#%E6%AD%BB%E9%94%81">死锁</a><ul>
<li><a href="#%E6%AD%BB%E9%94%81%E5%92%8C%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B-5">死锁和死锁检测 [5]</a></li>
<li><a href="#%E9%A2%84%E9%98%B2%E6%AD%BB%E9%94%81-7">预防死锁 [7]</a></li>
<li><a href="#%E6%AD%BB%E9%94%81%E7%9A%84%E6%8E%92%E6%9F%A5%E5%92%8C%E8%A7%A3%E5%86%B3-7">死锁的排查和解决 [7]</a></li>
</ul>
</li>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a><ul>
<li><a href="#case-1">Case [1]</a></li>
<li><a href="#%E5%8E%9F%E5%9B%A0">原因</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="锁">锁</span><a href="#锁" class="header-anchor">#</a></h1><h3><span id="行锁-锁优化-3">行锁， 锁优化 [3]</span><a href="#行锁-锁优化-3" class="header-anchor">#</a></h3><ul>
<li><p>在InnoDB事务中，<strong>行锁</strong>是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是<strong>两阶段锁协议</strong>。<br>知道了这个设定，对我们使用事务有什么帮助呢？那就是，<strong>如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放.</strong>[todo 加个例子]</p>
</li>
<li><p><strong>行锁是通过索引实现的</strong>，如果不通过索引条件检索数据，那么 InnoDB 将对表中所有的记录进行加锁。</p>
</li>
<li><p><strong>行锁</strong>的具体实现算法有三种：record lock、gap lock 以及 next-key lock。</p>
<ul>
<li><strong>record lock</strong>是专门对索引项加锁；</li>
<li><strong>gap lock</strong> 是对索引项之间的间隙加锁；</li>
<li><strong>next-key lock</strong> 则是前面两种的组合，对索引项以其之间的间隙加锁。<br>只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock，在Select 、Update 和 Delete 时，除了基于唯一索引的查询之外，其他索引查询时都会获取gap lock 或 next-key lock，即锁住其扫描的范围。</li>
</ul>
</li>
</ul>
<h3><span id="隐式锁和显示锁">隐式锁和显示锁</span><a href="#隐式锁和显示锁" class="header-anchor">#</a></h3><p>显示锁<br>SELECT … LOCK IN SHARE MODE(加共享锁);<br>SELECT … FOR UPDATE(加排他锁);</p>
<h1><span id="死锁">死锁</span><a href="#死锁" class="header-anchor">#</a></h1><h3><span id="死锁和死锁检测-5">死锁和死锁检测 [5]</span><a href="#死锁和死锁检测-5" class="header-anchor">#</a></h3><p>当出现死锁以后，有两种策略：</p>
<ul>
<li><p>一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数<br><strong>innodb_lock_wait_timeout</strong>来设置。<br>innodb_lock_wait_timeout的默认值是50s。 实际中不用这种策略。</p>
</li>
<li><p>另一种策略是，发起<strong>死锁检测</strong>，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事<br>务得以继续执行。将参数 <strong>innodb_deadlock_detect</strong> 设置为on，表示开启这个逻辑。</p>
<ul>
<li><p>带来的问题：每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要<strong>消耗大量的CPU资源</strong>。</p>
</li>
<li><p>一种解决思路是<strong>控制并发度</strong>：并发控制要做在数据库服务端。如果有中间件，可以考虑在中间件实现；如果-团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，<strong>对于相同行的更新，-在进入引擎之前排队</strong>。这样在InnoDB内部就不会有大量的死锁检测工作了。</p>
</li>
<li><p>另一种解决思路是<strong>在应用层上优化</strong>:你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。 比如，一个账户1条记录变10条记录。</p>
</li>
</ul>
</li>
</ul>
<h3><span id="预防死锁-7">预防死锁 [7]</span><a href="#预防死锁-7" class="header-anchor">#</a></h3><ul>
<li>减少长事务</li>
<li>大事务拆成小事务</li>
<li>保证加锁顺序一直</li>
<li>业务允许的情况下，降低隔离级别<br>RR几倍下会有间隙锁，会提高死锁发生的概率</li>
</ul>
<h3><span id="死锁的排查和解决-7">死锁的排查和解决 [7]</span><a href="#死锁的排查和解决-7" class="header-anchor">#</a></h3><ul>
<li>通过日志系统及时<strong>通知</strong>死锁事件<br> 通过ELK做通知</li>
<li>结合业务代码与<strong>死锁日志</strong> 进行分析<ul>
<li>通过 pt-deadlock-logger 监控死锁 </li>
<li>查看最近一次的死锁日志<br><code>show engine innodb status</code></li>
</ul>
</li>
</ul>
<h1><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h1><h3><span id="case-1">Case [1]</span><a href="#case-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/08/15/mysqlDeadLock/case.jpg" class>

<h3><span id="原因">原因</span><a href="#原因" class="header-anchor">#</a></h3><p>死锁是在并发环境下，两个或多个事务互相等待对方持有的资源而无法继续执行的情况。在上文中，死锁的产生是因为两个事务A和事务B都持有间隙(4,+∞）的gap锁，并且两个事务都在等待对方释放锁，导致循环等待而造成死锁。</p>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><ul>
<li><p>innodb_lock_wait_timeout 超时时间 - 通用<br>避免死锁最直观的方法就是在两个事务相互等待时，<strong>当一个事务的等待时间超过设置的某一<br>阈值，就对这个事务进行回滚，另一个事务就可以继续执行了。</strong>这种方法简单有效，在<br>InnoDB 中，参数 innodb_lock_wait_timeout 是用来设置超时时间的。</p>
</li>
<li><p>替换  幂等性校验 - 非通用<br>我们还可以<strong>使用其它的方式来代替数据库实现幂等性校验</strong>。例如，使用 Redis 以及<br>ZooKeeper 来实现，运行效率比数据库更佳。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ul>
<li><ol>
<li>《35 | 记一次线上SQL死锁事故：如何避免死锁？》 刘超</li>
</ol>
</li>
<li><ol start="3">
<li>《33 | MySQL调优之事务：高并发场景下的数据库事务调优》   刘超</li>
</ol>
</li>
<li><ol start="5">
<li>《07 | 行锁功过：怎么减少行锁对性能的影响？》 MySQL实战45讲  丁奇</li>
</ol>
</li>
<li><ol start="7">
<li><a href="https://www.bilibili.com/video/BV1V3411z7Hj/">MYSQL死锁的检测与预防</a></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 混合持久化</title>
    <url>/www6vHomeHexo/2023/07/10/redisBothAofAndRDB/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#redis-%E6%B7%B7%E5%90%88%E6%8C%81%E4%B9%85%E5%8C%96">Redis 混合持久化</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E9%A1%BA%E5%BA%8F%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%B5%81%E7%A8%8B-23">数据恢复顺序和加载流程 [2][3]</a></li>
<li><a href="#%E5%BC%80%E5%90%AF">开启</a></li>
<li><a href="#%E5%8E%9F%E7%90%86-3">原理 [3]</a></li>
</ul>
</li>
<li><a href="#aofrdb%E6%B7%B7%E5%90%881">AOF+RDB混合[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="redis-混合持久化">Redis 混合持久化</span><a href="#redis-混合持久化" class="header-anchor">#</a></h1><h3><span id="数据恢复顺序和加载流程-23">数据恢复顺序和加载流程 [2][3]</span><a href="#数据恢复顺序和加载流程-23" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/07/10/redisBothAofAndRDB/mix.jpg" class>

<p>在这种情况下，  <strong>当redis重启的时候会优先载入AOF文件来恢复原始的数据</strong>  ，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。</p>
<p>RDB和AOF共存时会优先加载AOF文件</p>
<p><strong>【主从切换   优先加载  RDB  -&gt;  速度】</strong><br><strong>【redis重启 优先加载 AOF -&gt; 数据完整性】</strong></p>
<h3><span id="开启">开启</span><a href="#开启" class="header-anchor">#</a></h3>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aof-use-rdb-preamble no -&gt; yes</span><br></pre></td></tr></table></figure>

<h3><span id="原理-3">原理 [3]</span><a href="#原理-3" class="header-anchor">#</a></h3><p>RDB镜像做全量持久化，AOF做增量持久化 先使用RDB进行快照存储，然后使用AOF持久化记录所有的写操作，当重写策略满足或手动触发重写的时候，将最新的数据存储为新的RDB记录。<br>这样的话，重启服务的时候会从RDB和AOF两部分恢复数据，既保证了数据完整性，又提高了恢复数据的性能。简单来说:混合持久化方式产生的文件一部分是RDB格式，一部分是AOF格式。</p>
<h1><span id="aofrdb混合1">AOF+RDB混合[1]</span><a href="#aofrdb混合1" class="header-anchor">#</a></h1><p>而<strong>混合使用 RDB 和 AOF</strong>，正好可以取两者之长，避两者之短，以较小的性能开销保证数据可靠性和性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《05丨内存快照：宕机后，Redis如何实现快速恢复？》 </li>
<li><a href="https://www.cnblogs.com/wiseblog/articles/13540042.html">redis++：Redis持久化 rdb &amp; aof 工作原理及流程图 （三）</a></li>
<li><a href="https://github.com/www6v/Learning-in-practice/blob/master/Redis/4.Redis%E6%8C%81%E4%B9%85%E5%8C%96/9.RDB-AOF%E6%B7%B7%E5%90%88%E6%8C%81%E4%B9%85%E5%8C%96.md">RDB-AOF混合持久化</a><br><a href="https://www.bilibili.com/video/BV13R4y1v7sP/?p=45">尚硅谷Redis零基础到进阶，最强redis7教程，阳哥亲自带练（附redis面试题）</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>DDD-落地实战 Practice</title>
    <url>/www6vHomeHexo/2023/07/06/dddPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#ddd-%E8%90%BD%E5%9C%B0">DDD 落地</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8Eddd%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A0%B8%E5%BF%83">基于DDD应用架构的核心</a></li>
<li><a href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF-4">设计思路 [4]</a></li>
<li><a href="#%E5%88%86%E5%B1%82-2">分层 [2]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E5%88%86%E5%B1%82-2">代码分层 [2]</a></li>
<li><a href="#%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%8120">项目代码[20]</a></li>
</ul>
</li>
<li><a href="#%E6%A1%86%E6%9E%B6">框架</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="ddd-落地">DDD 落地</span><a href="#ddd-落地" class="header-anchor">#</a></h1><h3><span id="基于ddd应用架构的核心">基于DDD应用架构的核心</span><a href="#基于ddd应用架构的核心" class="header-anchor">#</a></h3><p>分离业务复杂度和技术复杂度</p>
<h3><span id="设计思路-4">设计思路 [4]</span><a href="#设计思路-4" class="header-anchor">#</a></h3><ul>
<li><p><strong>贫血模型</strong></p>
<ul>
<li>实现<br><strong>业务逻辑放到Service中</strong></li>
<li>缺点 [7]<br><strong>业务逻辑被埋没在存储业务中</strong></li>
<li>贫血模型的<strong>缺陷</strong>  [21]<ul>
<li>无法保护模型对象的完整性和一致性</li>
<li>对象操作的可发现性极差</li>
<li>代码逻辑重复</li>
<li>代码的健壮性差</li>
<li>强依赖底层实现</li>
</ul>
</li>
<li><strong>99%的代码都是基于贫血模型</strong>  [21]<ul>
<li>数据库思维</li>
<li>贫血模型“简单”</li>
<li>脚本思维</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>充血模型</strong></p>
<ul>
<li>实现<br> <strong>业务逻辑放到领域对象中(实体对象中有实现方法)</strong></li>
<li>开闭原则<br> 保持了对象的封装性，使得领域模型在面临多态、继承等复杂结构时，易于变更</li>
<li>适用场景<br> 类似继承、多态的情况<br>在软件设计的过程中需要将一些类型或者编码进行转换<br>更好地表现领域对象之间的关系<br>“聚合”，也就是在真实世界中那些代表整体与部分的事</li>
</ul>
</li>
<li><p>比较</p>
<ul>
<li>贫血模型比充血模型更加<strong>简单易行</strong><ul>
<li>贫血模型<br>不需要  仓库、工厂、缓存，简单粗暴</li>
</ul>
</li>
<li>充血模型需要更强的<strong>设计与协作能力</strong><ul>
<li>充血模型<br>需要开发人员有更强的OOA&#x2F;D能力、分析业务、业务建模与设计能力<br>要有较强的团队协作能力</li>
<li>贫血模型<br>所有业务处理过程都交给Service完成</li>
</ul>
</li>
<li>贫血模型更容易应对<strong>复杂的业务处理场景</strong></li>
</ul>
</li>
</ul>
<h3><span id="分层-2">分层  [2]</span><a href="#分层-2" class="header-anchor">#</a></h3><ul>
<li>用户接口层(Controller层) </li>
<li>Application层</li>
<li>Domain层</li>
<li>Infrastructure层</li>
</ul>
<h3><span id="代码分层-2">代码分层  [2]</span><a href="#代码分层-2" class="header-anchor">#</a></h3><ul>
<li><p>Interface</p>
<ul>
<li>assembler(DTO和领域对象的互转)</li>
<li>dto</li>
<li>facade（<strong>粗粒度的调用接口</strong>，将用户请求<strong>委派</strong>给一个或多个应用服务进行处理）</li>
</ul>
</li>
<li><p>Application</p>
<ul>
<li>event（pub， sub）（<strong>事件处理相关的核心业务逻辑在领域层实现</strong>）</li>
<li>service（应用服务）</li>
</ul>
</li>
<li><p>Domain</p>
<ul>
<li>aggregate<ul>
<li>entity<ul>
<li><strong>聚合根</strong> </li>
<li><strong>实体</strong>     </li>
<li><strong>值对象</strong>   </li>
<li><strong>工厂模式（Factory）</strong></li>
</ul>
</li>
<li>event<br><strong>事件实体</strong>以及<strong>与事件活动相关的业务逻辑代码</strong></li>
<li>repository<br>所在聚合的查询或持久化领域对象的代码，通常包括仓储接口和仓储实现方法<br><strong>Data Model只存在于数据层，而Domain Model在领域层，而链接了这两层的关键对象，就是Repository</strong> [7]</li>
<li>service<br>领域服务是多个实体组合出来的一段业务逻辑</li>
</ul>
</li>
</ul>
</li>
<li><p>Infrastructure</p>
<ul>
<li>config</li>
<li>Util（开发框架、消息、数据库、缓存、文件、总线、网关、第三方类库、通用算法等基础代码，）</li>
</ul>
</li>
</ul>
<h3><span id="项目代码20">项目代码[20]</span><a href="#项目代码20" class="header-anchor">#</a></h3><h1><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h1><ul>
<li>Axon Framework</li>
<li>COLA [22]</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ul>
<li><ol start="2">
<li>《13丨代码模型（上）：如何使用DDD设计微服务代码模型？》   欧创新</li>
</ol>
</li>
<li><ol start="4">
<li>《04  领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏</li>
</ol>
</li>
<li><ol start="7">
<li>《24 直播：框架之上的业务分层》  体系课_Go高级工程师实战营(完结)</li>
</ol>
</li>
<li><ol start="20">
<li><a href="https://zhuanlan.zhihu.com/p/343388831">阿里技术专家详解DDD系列 第二讲 - 应用架构</a><br><a href="https://github.com/www6v/jExamples/tree/master/src/main/java/ddd/transactionScript">refactor 之前的Transaction Script</a> git<br><a href="https://github.com/www6v/jExamples/tree/master/src/main/java/ddd/refactor">refactor 之后的DDD</a></li>
</ol>
</li>
<li><ol start="21">
<li><a href="https://zhuanlan.zhihu.com/p/348706530">阿里技术专家详解DDD系列 第三讲 - Repository模式</a></li>
</ol>
</li>
<li><ol start="22">
<li><a href="https://blog.csdn.net/significantfrank/article/details/110934799">COLA 4.0：应用架构的最佳实践</a>  未</li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>DDD</tag>
      </tags>
  </entry>
  <entry>
    <title>Java Feature</title>
    <url>/www6vHomeHexo/2023/06/10/javaFeature/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%89%88%E6%9C%AC">版本</a></li>
<li><a href="#%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7-1">版本+特性 [1]</a><ul>
<li><a href="#java-14">Java 1.4</a></li>
<li><a href="#java-5java-15">Java 5（Java 1.5）</a></li>
<li><a href="#java-7">Java 7</a></li>
<li><a href="#java-se-8java-8-lts%E9%95%BF%E6%9C%9F%E6%94%AF%E6%8C%81%E7%89%88%E6%9C%AC">Java SE 8（Java 8）- LTS长期支持版本</a></li>
<li><a href="#java-se-9java-9">Java SE 9（Java 9）</a></li>
<li><a href="#java-se-10java-10">Java SE 10（Java 10）</a></li>
<li><a href="#java-se-11java-11-lts%E9%95%BF%E6%9C%9F%E6%94%AF%E6%8C%81%E7%89%88%E6%9C%AC">Java SE 11（Java 11）-LTS长期支持版本</a></li>
<li><a href="#java-se-12java-12">Java SE 12（Java 12）</a></li>
<li><a href="#java-se-13-java-13">Java SE 13 （Java 13）</a></li>
<li><a href="#java-se-14-java-14">Java SE 14 （Java 14）</a></li>
<li><a href="#java-se-15-java-15">Java SE 15 （Java 15）</a></li>
<li><a href="#java-se-16-java-16">Java SE 16 （Java 16）</a></li>
<li><a href="#java-se-17java-17-lts%E9%95%BF%E6%9C%9F%E6%94%AF%E6%8C%81%E7%89%88%E6%9C%AC">Java SE 17（Java 17）-LTS长期支持版本</a></li>
<li><a href="#java-se-18java-182">Java SE 18（Java 18）[2]</a></li>
<li><a href="#java-se-19java-19-3">Java SE 19（Java 19） [3]</a></li>
<li><a href="#java-se-20java-20-4">Java SE 20（Java 20） [4]</a></li>
<li><a href="#java-se-21java-21-5">Java SE 21（Java 21） [5]</a></li>
</ul>
</li>
<li><a href="#%E7%89%B9%E6%80%A7-8">特性 [8]</a><ul>
<li><a href="#%E6%96%87%E5%AD%97%E5%9D%97-text-blocks-jdk-13-jdk-15">文字块 text blocks |  JDK 13-JDK 15</a></li>
<li><a href="#record-%E6%A1%A3%E6%A1%88%E7%B1%BB-%E4%B8%8D%E5%8F%AF%E5%8F%98-jdk14-jdk16">record 档案类 [不可变] |  JDK14-JDK16</a></li>
<li><a href="#sealed-classes-%E5%B0%81%E9%97%AD%E7%B1%BB-%E6%89%A9%E5%B1%95%E6%80%A7-jdk-15-jdk-17">sealed classes 封闭类   [扩展性] |   JDK 15-JDK 17</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B%E5%8C%B9%E9%85%8D-%E5%88%87%E9%99%A4%E8%87%83%E8%82%BF%E7%9A%84%E5%BC%BA%E5%88%B6%E8%BD%AC%E6%8D%A2-jdk-14-jdk-16">类型匹配 [切除臃肿的强制转换] | JDK 14-JDK 16</a></li>
<li><a href="#switch-%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E7%AE%80%E5%8C%96%E5%A4%9A%E6%83%85%E6%99%AF%E6%93%8D%E4%BD%9C-jdk-12-jdk-14">switch 表达式 [简化多情景操作] |  JDK 12-JDK 14</a></li>
<li><a href="#switch%E5%8C%B9%E9%85%8D-%E9%80%82%E9%85%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E7%B1%BB%E5%9E%8B-jdk-17-21">switch匹配 [适配不同的类型]  |  JDK 17-21</a></li>
<li><a href="#%E5%A4%96%E9%83%A8%E5%86%85%E5%AD%98%E6%8E%A5%E5%8F%A3-jdk18-">外部内存接口 | JDK18-?</a></li>
<li><a href="#%E5%A4%96%E9%83%A8%E5%87%BD%E6%95%B0%E6%8E%A5%E5%8F%A3%E5%8F%96%E4%BB%A3java%E6%9C%AC%E5%9C%B0%E6%8E%A5%E5%8F%A3-jdk-17-">外部函数接口[取代Java本地接口] |  JDK 17-?</a></li>
<li><a href="#gc">GC</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="版本">版本</span><a href="#版本" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/06/10/javaFeature/version.jpg" class width="720" height="756">

<table>
<thead>
<tr>
<th>版本</th>
<th>发布日期</th>
</tr>
</thead>
<tbody><tr>
<td>JDK 19</td>
<td>2022&#x2F;09&#x2F;20</td>
</tr>
<tr>
<td>JDK 20</td>
<td>2023&#x2F;03&#x2F;21</td>
</tr>
</tbody></table>
<h1><span id="版本特性-1">版本+特性 [1]</span><a href="#版本特性-1" class="header-anchor">#</a></h1><h3><span id="java-14">Java 1.4</span><a href="#java-14" class="header-anchor">#</a></h3><ul>
<li>NIO（New I&#x2F;O）</li>
</ul>
<h3><span id="java-5java-15">Java 5（Java 1.5）</span><a href="#java-5java-15" class="header-anchor">#</a></h3><ul>
<li>泛型</li>
<li>自动装箱&#x2F;拆箱</li>
<li>枚举类型</li>
</ul>
<h3><span id="java-7">Java 7</span><a href="#java-7" class="header-anchor">#</a></h3><ul>
<li>Try-with-resources</li>
</ul>
<h3><span id="java-se-8java-8-lts长期支持版本">Java SE 8（Java 8）- LTS长期支持版本</span><a href="#java-se-8java-8-lts长期支持版本" class="header-anchor">#</a></h3><ul>
<li><strong>Lambda 表达式</strong>：简化函数式编程。允许以更简洁的语法编写函数式接口的实例，使代码更加简洁。</li>
<li><strong>Stream API</strong>：用于处理集合，支持函数式操作，如过滤、映射和聚合。</li>
<li>方法引用：允许直接引用现有方法或构造函数，避免了重复编写类似的代码。</li>
<li><strong>接口的默认方法</strong>：在接口中提供默认实现，提高接口的灵活性。</li>
<li>时间 API：提供了一组强大的时间操作类，简化了日期和时间的操作。</li>
<li>重复注解：允许在同一个地方多次声明同一个注解，提高了代码的可读性。</li>
<li><strong>CompletableFuture 类</strong>：简化异步编程，提供更好的错误处理和异常处理机制。</li>
<li>Nashorn 引擎：提供了一种基于 JavaScript 的解决方案，允许将 JavaScript 代码嵌入到 Java 应用程序中。</li>
<li><strong>Optional 类</strong>：减少空指针异常，提高代码可读性。</li>
</ul>
<h3><span id="java-se-9java-9">Java SE 9（Java 9）</span><a href="#java-se-9java-9" class="header-anchor">#</a></h3><ul>
<li><strong>模块系统（Project Jigsaw）</strong>：将 Java 的庞大代码库划分为可重用的模块，简化大型应用的构建和维护。</li>
<li>JShell：Java 的交互式命令行工具，用于快速尝试和测试 Java 代码片段。</li>
<li>新的集合工厂方法：方便地创建不可变集合，如 List.of()、Set.of() 和 Map.of()。</li>
</ul>
<h3><span id="java-se-10java-10">Java SE 10（Java 10）</span><a href="#java-se-10java-10" class="header-anchor">#</a></h3><ul>
<li><strong>局部变量类型推断</strong>：使用 var 关键字自动推断局部变量的类型，简化代码。</li>
<li>垃圾收集器接口改进：提高了垃圾收集器的可插拔性和灵活性。</li>
</ul>
<h3><span id="java-se-11java-11-lts长期支持版本">Java SE 11（Java 11）-LTS长期支持版本</span><a href="#java-se-11java-11-lts长期支持版本" class="header-anchor">#</a></h3><ul>
<li>新的 HTTP 客户端 API：支持 HTTP&#x2F;2 和 WebSocket，提供了更现代化的编程方式。</li>
<li>改进的垃圾收集：引入了 <strong>ZGC</strong> 和 Epsilon 垃圾收集器。</li>
<li>String 类的新方法：如 lines()、isBlank()、strip() 等。</li>
</ul>
<h3><span id="java-se-12java-12">Java SE 12（Java 12）</span><a href="#java-se-12java-12" class="header-anchor">#</a></h3><ul>
<li><strong>switch 表达式</strong>：允许在 switch 语句中使用表达式，提高了代码的可读性和简洁性。</li>
<li>改进的字符串类：提供了一些新的方法，使得字符串的操作更加方便和高效。</li>
<li>Shenandoah 垃圾回收器：提供了一种低停顿时间的垃圾回收器，适用于大型堆内存的应用程序。</li>
<li>微基准测试套件：提供了一种用于快速测试性能的微基准测试框架。</li>
<li>JDK 源代码重构：对 JDK 源代码进行了重构，提高了代码的可读性和维护性。</li>
</ul>
<h3><span id="java-se-13-java-13">Java SE 13 （Java 13）</span><a href="#java-se-13-java-13" class="header-anchor">#</a></h3><ul>
<li><strong>文本块</strong>：允许以更简洁的语法创建多行字符串，提高了代码的可读性和简洁性。</li>
<li>改进的 switch 表达式：允许在 switch 语句中使用表达式，提供更好的类型推断和更灵活的写法。</li>
<li>ZGC 垃圾回收器改进：提高了 ZGC 垃圾回收器的性能和可靠性。</li>
<li>应用程序类数据共享改进：提高了应用程序类数据共享的性能和效率。</li>
</ul>
<h3><span id="java-se-14-java-14">Java SE 14 （Java 14）</span><a href="#java-se-14-java-14" class="header-anchor">#</a></h3><p>2020年3月17日</p>
<ul>
<li>instanceof 模式匹配：允许在 instanceof 操作符中使用模式匹配，提高了代码的简洁性和可读性。</li>
<li><strong>Records 类</strong>：提供了一种更简单和安全的数据类的定义方式。</li>
<li>Switch 表达式增强：允许使用箭头操作符(-&gt;)作为 lambda 表达式的简写语法。</li>
<li>文本块增强：允许在文本块中使用嵌入式表达式，使得文本块更加灵活和强大。</li>
<li>改进的 NullPointerException 信息：提供更详细的 NullPointerException 信息。</li>
</ul>
<h3><span id="java-se-15-java-15">Java SE 15 （Java 15）</span><a href="#java-se-15-java-15" class="header-anchor">#</a></h3><ul>
<li><strong>隐式的类文件</strong>：允许在 Java 源代码中定义多个类，而不需要单独的类文件。</li>
<li>改进的文本块：允许在文本块中使用转义字符和 Unicode 转义，提高了文本块的灵活性和可读性。</li>
<li>改进的 switch 表达式：允许在 switch 语句中使用多个匹配项，提供更灵活的写法。</li>
<li><strong>Sealed 类和接口</strong>：允许控制哪些类或接口可以继承或实现该类或接口，提高了代码的安全性和可维护性。</li>
<li>其他改进：包括增强的 ZGC 垃圾回收器、改进的内存管理、新增的 Unix 域套接字 API 等。</li>
</ul>
<h3><span id="java-se-16-java-16">Java SE 16 （Java 16）</span><a href="#java-se-16-java-16" class="header-anchor">#</a></h3><ul>
<li>增强的文本块：允许在文本块中使用转义字符和嵌入式表达式。</li>
<li>移除了废弃的 ParallelScavenge 垃圾回收器。</li>
<li>改进的 ZGC 垃圾回收器：提高了性能和可靠性，增加了可配置参数。</li>
<li>Records 类的增强：允许在 records 类中添加静态方法和私有构造函数。</li>
<li><strong>Vector API</strong>：提供了一种新的 API，用于高效地执行矢量化操作。</li>
</ul>
<h3><span id="java-se-17java-17-lts长期支持版本">Java SE 17（Java 17）-LTS长期支持版本</span><a href="#java-se-17java-17-lts长期支持版本" class="header-anchor">#</a></h3><ul>
<li><strong>嵌套枚举</strong>：允许在类和接口中定义嵌套枚举，提高了代码的可读性和简洁性。</li>
<li>改进的 switch 语句：允许在 switch 语句中使用 case 标签作为表达式，提供更灵活的写法。</li>
<li>预览性功能：包括<strong>模式匹配</strong>、嵌套枚举、记录类的序列化等新特性。</li>
<li>增强的垃圾回收器：提高了性能和可靠性，增加了可配置参数。</li>
<li>其他改进：包括新的内存管理和性能优化，增强的 JIT 编译器等。</li>
</ul>
<h3><span id="java-se-18java-182">Java SE 18（Java 18）[2]</span><a href="#java-se-18java-182" class="header-anchor">#</a></h3><p>400：默认使用 UTF-8<br>408：简易 Web 服务器<br>413：Java API 文档中的代码片段<br>416：使用方法句柄重新实现核心反射<br>417：矢量 API<br>418：网络地址解析 SPI<br>419：<strong>外部函数和内存 API</strong><br>420：switch 的模式匹配<br>421：废弃对象终止机制 </p>
<h3><span id="java-se-19java-19-3">Java SE 19（Java 19） [3]</span><a href="#java-se-19java-19-3" class="header-anchor">#</a></h3><p>405: 	Record Patterns (Preview)<br>422: 	Linux&#x2F;RISC-V Port<br>424: 	Foreign Function &amp; Memory API (Preview)<br>425: 	<strong>Virtual Threads (Preview)</strong><br>426: 	Vector API (Fourth Incubator)<br>427: 	Pattern Matching for switch (Third Preview)<br>428: 	<strong>Structured Concurrency (Incubator)</strong></p>
<h3><span id="java-se-20java-20-4">Java SE 20（Java 20） [4]</span><a href="#java-se-20java-20-4" class="header-anchor">#</a></h3><p>429: 	<strong>Scoped Values (Incubator)</strong><br>432: 	Record Patterns (Second Preview)<br>433: 	Pattern Matching for switch (Fourth Preview)<br>434: 	Foreign Function &amp; Memory API (Second Preview)<br>436: 	Virtual Threads (Second Preview)<br>437: 	Structured Concurrency (Second Incubator)<br>438: 	Vector API (Fifth Incubator)</p>
<h3><span id="java-se-21java-21-5">Java SE 21（Java 21） [5]</span><a href="#java-se-21java-21-5" class="header-anchor">#</a></h3><p>430: 	String Templates (Preview)<br>431: 	Sequenced Collections<br>439: 	<strong>Generational ZGC</strong><br>440: 	Record Patterns<br>441: 	<strong>Pattern Matching for switch</strong><br>442: 	Foreign Function &amp; Memory API (Third Preview)<br>443: 	Unnamed Patterns and Variables (Preview)<br>444: 	<strong>Virtual Threads</strong> [6][7]<br>445: 	Unnamed Classes and Instance Main Methods (Preview)<br>446: 	Scoped Values (Preview)<br>448: 	Vector API (Sixth Incubator)<br>449: 	Deprecate the Windows 32-bit x86 Port for Removal<br>451: 	Prepare to Disallow the Dynamic Loading of Agents<br>452: 	Key Encapsulation Mechanism API<br>453: 	Structured Concurrency (Preview)</p>
<h1><span id="特性-8">特性 [8]</span><a href="#特性-8" class="header-anchor">#</a></h1><h3><span id="文字块-text-blocks-jdk-13-jdk-15">文字块 text blocks |  JDK 13-JDK 15</span><a href="#文字块-text-blocks-jdk-13-jdk-15" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">textBlock</span> <span class="operator">=</span> <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        &lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="string">        &lt;html&gt;</span></span><br><span class="line"><span class="string">            &lt;body&gt;</span></span><br><span class="line"><span class="string">            	&lt;h1&gt;&quot;Hello World!&quot;&lt;/h1&gt;</span></span><br><span class="line"><span class="string">            &lt;/body&gt;</span></span><br><span class="line"><span class="string">        &lt;/html&gt;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>;</span><br><span class="line">System.out.println(</span><br><span class="line">		<span class="string">&quot;Here is the text block:\n&quot;</span> + textBlock);</span><br></pre></td></tr></table></figure>

<h3><span id="record-档案类-不可变-jdk14-jdk16">record  档案类 [不可变] |  JDK14-JDK16</span><a href="#record-档案类-不可变-jdk14-jdk16" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">record</span> <span class="title class_">Circle</span><span class="params">(<span class="type">double</span> radius)</span> <span class="keyword">implements</span> <span class="title class_">Shape</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">double</span> <span class="title function_">area</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Math.PI * radius * radius;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="sealed-classes-封闭类-扩展性-jdk-15-jdk-17">sealed classes 封闭类   [扩展性] |   JDK 15-JDK 17</span><a href="#sealed-classes-封闭类-扩展性-jdk-15-jdk-17" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title class_">Shape</span> <span class="keyword">permits</span> Circle, Square &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> String id;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Shape</span><span class="params">(String id)</span> &#123;</span><br><span class="line">    	<span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="type">double</span> <span class="title function_">area</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="类型匹配-切除臃肿的强制转换-jdk-14-jdk-16">类型匹配 [切除臃肿的强制转换] | JDK 14-JDK 16</span><a href="#类型匹配-切除臃肿的强制转换-jdk-14-jdk-16" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (shape <span class="keyword">instanceof</span> Rectangle rect) &#123;</span><br><span class="line">	<span class="keyword">return</span> (rect.length == rect.width);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="switch-表达式-简化多情景操作-jdk-12-jdk-14">switch 表达式 [简化多情景操作] |  JDK 12-JDK 14</span><a href="#switch-表达式-简化多情景操作-jdk-12-jdk-14" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">daysInMonth</span> <span class="operator">=</span> <span class="keyword">switch</span> (month) &#123;</span><br><span class="line">    <span class="keyword">case</span> Calendar.JANUARY,</span><br><span class="line">        Calendar.MARCH,</span><br><span class="line">        Calendar.MAY,</span><br><span class="line">        Calendar.JULY,</span><br><span class="line">        Calendar.AUGUST,</span><br><span class="line">        Calendar.OCTOBER,</span><br><span class="line">        Calendar.DECEMBER -&gt; <span class="number">31</span>;</span><br><span class="line">    <span class="keyword">case</span> Calendar.APRIL,</span><br><span class="line">        Calendar.JUNE,</span><br><span class="line">        Calendar.SEPTEMBER,</span><br><span class="line">        Calendar.NOVEMBER -&gt; <span class="number">30</span>;</span><br><span class="line">    <span class="keyword">case</span> Calendar.FEBRUARY -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (((year % <span class="number">4</span> == <span class="number">0</span>) &amp;&amp; !(year % <span class="number">100</span> == <span class="number">0</span>))</span><br><span class="line">        		|| (year % <span class="number">400</span> == <span class="number">0</span>)) &#123;</span><br><span class="line">        	<span class="keyword">yield</span> <span class="number">29</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        	<span class="keyword">yield</span> <span class="number">28</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">default</span> -&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(</span><br><span class="line">    	<span class="string">&quot;Calendar in JDK does not work&quot;</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h3><span id="switch匹配-适配不同的类型-jdk-17-21">switch匹配 [适配不同的类型]  |  JDK 17-21</span><a href="#switch匹配-适配不同的类型-jdk-17-21" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isSquare</span><span class="params">(Shape shape)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">switch</span> (shape) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="literal">null</span>, Shape.Circle c -&gt; <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">case</span> Shape.Square s -&gt; <span class="literal">true</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="外部内存接口-jdk18-">外部内存接口 | JDK18-?</span><a href="#外部内存接口-jdk18-" class="header-anchor">#</a></h3><ul>
<li><p>ByteBuffer &amp;&amp; 零拷贝<br>  使用堆外存储最常用的办法，就是使用 ByteBuffer 这个类来分配<strong>直接存储空间（direct</strong><br>  <strong>buffer）</strong>。JVM 虚拟机会尽最大努力直接在直接存储空间上执行 IO 操作，避免数据在本<br>  地和 JVM 之间的拷贝。<br>  由于频繁的内存拷贝是性能的主要障碍之一。所以为了极致的性能，应用程序通常也会尽<br>  量避免内存的拷贝。理想的状况下，一份数据只需要一份内存空间，这就是我们常说的<strong>零<br>  拷贝</strong>。</p>
<p>  用 ByteBuffer 这个类来分配直接存储空间的方法</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> ByteBuffer <span class="title function_">allocateDirect</span><span class="params">(<span class="type">int</span> capacity)</span>;</span><br></pre></td></tr></table></figure>
<p>  第一个缺陷是没有资源释放的接口。<br>  第二个缺陷是存储空间尺寸的限制。</p>
</li>
<li><p>外部内存接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> (<span class="type">ResourceScope</span> <span class="variable">scope</span> <span class="operator">=</span> ResourceScope.newConfinedScope()) &#123;</span><br><span class="line">    <span class="type">MemorySegment</span> <span class="variable">segment</span> <span class="operator">=</span> MemorySegment.allocateNative(<span class="number">4</span>, scope);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">    	MemoryAccess.setByteAtOffset(segment, i, (<span class="type">byte</span>)<span class="string">&#x27;A&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="外部函数接口取代java本地接口-jdk-17-">外部函数接口[取代Java本地接口] |  JDK 17-?</span><a href="#外部函数接口取代java本地接口-jdk-17-" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloWorld</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">        <span class="keyword">try</span> (<span class="type">ResourceScope</span> <span class="variable">scope</span> <span class="operator">=</span> ResourceScope.newConfinedScope()) &#123;</span><br><span class="line">            <span class="type">CLinker</span> <span class="variable">cLinker</span> <span class="operator">=</span> CLinker.getInstance();</span><br><span class="line">            <span class="type">MemorySegment</span> <span class="variable">helloWorld</span> <span class="operator">=</span></span><br><span class="line">            		CLinker.toCString(<span class="string">&quot;Hello, world!\n&quot;</span>, scope);</span><br><span class="line">            <span class="type">MethodHandle</span> <span class="variable">cPrintf</span> <span class="operator">=</span> cLinker.downcallHandle(</span><br><span class="line">            		CLinker.systemLookup().lookup(<span class="string">&quot;printf&quot;</span>).get(),</span><br><span class="line">            		MethodType.methodType(<span class="type">int</span>.class, MemoryAddress.class),</span><br><span class="line">            		FunctionDescriptor.of(CLinker.C_INT, CLinker.C_POINTER));</span><br><span class="line">            cPrintf.invoke(helloWorld.address());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="gc">GC</span><a href="#gc" class="header-anchor">#</a></h3><p>G1,<br>ZGC, 分代式 ZGC </p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://justzqq.com/2023/03/23/java%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7%E6%95%B4%E7%90%86%EF%BC%81/">Java各个版本发布时间和主要特性整理！</a><br><a href="https://blog.csdn.net/wcdunf/article/details/129861244">Java 历史大版本及其详细特性介绍:</a></li>
<li><a href="https://www.bilibili.com/read/cv16596828">Java 18 的新特性</a></li>
<li><a href="https://openjdk.org/projects/jdk/19/">JDK 19</a></li>
<li><a href="https://openjdk.org/projects/jdk/20/">JDK 20</a></li>
<li><a href="https://openjdk.org/projects/jdk/21/">JDK 21</a></li>
<li><a href="https://blog.csdn.net/lewyu521/article/details/127649776">Java19 虚拟线程原理介绍及实现</a> ***</li>
<li><a href="https://www.jdon.com/63445.html">Java虚拟线程：异步编程之死</a></li>
<li>《深入剖析 Java 新特性》  范学雷<br> 02,03,04,05,06,07,08,09,12, 13</li>
<li><a href="https://www.bilibili.com/read/cv13542498">从Java 11升级到Java 17 </a> V 未</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>Feature</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>全局二级索引-GSI</title>
    <url>/www6vHomeHexo/2023/06/05/globalSecondaryIndex/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9C%BA%E6%99%AF">场景</a></li>
<li><a href="#%E5%85%A8%E5%B1%80%E7%B4%A2%E5%BC%95-vs-%E5%B1%80%E9%83%A8%E7%B4%A2%E5%BC%95-12">全局索引 vs 局部索引 [1][2]</a></li>
<li><a href="#polardb-%E5%85%A8%E5%B1%80%E7%B4%A2%E5%BC%95">PolarDB 全局索引</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="场景">场景</span><a href="#场景" class="header-anchor">#</a></h2><p>[Claude]<br>分布式数据库的全局二级索引主要有以下几个使用场景:<br>    <strong>加速多表关联查询</strong>:用全局二级索引构建外键索引,可以有效地加速涉及多表关联的查询。<br>    <strong>全表扫描查询</strong>:当需要做全表扫描时,使用全局二级索引可以跳过不需要的行,大幅降低扫描成本。<br>    频繁查询的列上:对于数据库中经常需要根据某列进行查询的情况,建立该列的全局二级索引,可以大幅提高查询效率。<br>    <strong>需要排序的查询</strong>:当需要查询结果按某个列排序时,全局二级索引可以加速排序。<br>    模糊查询优化:通过部分键值匹配,全局二级索引可以优化like查询等模式查询。<br>    <strong>分页查询优化</strong>:结合 Cursor 标记,全局二级索引可以有效的支持分页查询。</p>
<p>总的来说,对于需要高效查询、排序、混合查询等复杂查询的分布式数据库而言,引入全局二级索引可以大大提高性能。尤其是处理大规模数据时,二级索引发挥的作用更加明显。</p>
<ul>
<li><p>全局二级索引能够解决什么问题 [1]<br>如果查询的维度与逻辑表的拆分维度不同，会<strong>产生跨分片查询</strong>。跨分片查询的增加会导致查询卡慢、连接池耗尽等性能问题。<strong>GSI</strong>能够通过增加拆分维度来<strong>减少跨分片查询，消除性能瓶颈</strong>。</p>
</li>
<li><p>增加拆分维度 [4]<br>例如，对于在线商城的订单表，假设按照买家用户维度拆分，那么对于卖家查询（例如，查询某个卖家的本月所有订单）就需要扫描所有分区。但是借助全局二级索引，可以仅仅扫描相应卖家所在的索引表分区，快速找到所需的订单信息。</p>
</li>
<li><p>全局唯一约束 [4]<br>例如，假设用户表是一张分布式表，按照用户ID分区。若要求用户手机号需要全局唯一，那么本地索引无法满足，必须构建一个按手机号作为索引键（同时也是分区键）的唯一索引。</p>
</li>
</ul>
<h2><span id="全局索引-vs-局部索引-12">全局索引 vs 局部索引 [1][2]</span><a href="#全局索引-vs-局部索引-12" class="header-anchor">#</a></h2><ul>
<li><p>全局二级索引 [DDIA  基于关键词的二级索引分区]<br>数据行和对应的索引行保存在不同分片上</p>
<ul>
<li>分类 [3]<ul>
<li>全局非分区索引（Global Non-Partitioned Index）</li>
<li>全局分区索引（Global Partitioned Index）</li>
</ul>
</li>
</ul>
</li>
<li><p>局部索引  [DDIA  基于文档的二级索引分区]<br>如果数据行和对应的索引行保存在相同分片上</p>
</li>
</ul>
<h2><span id="polardb-全局索引">PolarDB 全局索引</span><a href="#polardb-全局索引" class="header-anchor">#</a></h2><ul>
<li><p>PolarDB-X [1]</p>
<ul>
<li>XA多写，保证主表与<strong>索引表</strong>数据强一致。[性能会不会慢]</li>
<li>Online Schema Change，添加GSI不锁主表。</li>
</ul>
</li>
<li><p>PolarDB-X GSI [4]<br>每个GSI对应一张分布式索引表，和其他分布式表一样，按照指定的分区规则水平拆分为多张物理表。PolarDB-X使用分布式事务维护主表和索引表之间数据强一致。</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://help.aliyun.com/document_detail/182179.html">全局二级索引</a>   PolarDB</li>
<li><a href="https://zhuanlan.zhihu.com/p/384439886">设计数据密集型应用-C6-分区和二级索引</a>  DDIA</li>
<li><a href="https://www.oceanbase.com/docs/enterprise-oceanbase-database-cn-10000000000376130">二级索引</a>  OB</li>
<li><a href="https://doc.polardbx.com/features/topics/gsi.html">全局二级索引 </a>  PolarDB</li>
<li><a href="https://zhuanlan.zhihu.com/p/572156705">PolarDB-X 全局二级索引</a> 未</li>
<li><a href="https://zhuanlan.zhihu.com/p/440801781">PolarDB-X 数据分布解读（三） ：TPCC与透明分布式</a>  未</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>二级索引</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>统一模型</title>
    <url>/www6vHomeHexo/2023/05/13/unifyModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="计算密集">计算密集</span><a href="#计算密集" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>计算密集</th>
<th>技术</th>
<th>产品</th>
</tr>
</thead>
<tbody><tr>
<td>微服务</td>
<td>RPC(2th)<br>Service Mesh(3th)<br>多运行时(4th )</td>
<td>Dubbo<br>istio proxyless<br>daper</td>
</tr>
<tr>
<td>容器</td>
<td>编排</td>
<td>K8s</td>
</tr>
<tr>
<td>Service Mesh</td>
<td>Sidecar <br>控制面， 数据面</td>
<td>Envoy xDS <br>微软SMI</td>
</tr>
<tr>
<td>可观测</td>
<td>Tracing+Metric+Logs</td>
<td>OpenTelemetry&#x3D;<br>OpenCensus+OpenTracing</td>
</tr>
<tr>
<td>Sererless</td>
<td>Sererless+ VM<br>Sererless+容器<br>Sererless+服务<br>Sererless+数据库<br></td>
<td>Ali ECS<br>Ali ECI<br>FasS<br>Aurora，TiDB Cloud<br></td>
</tr>
</tbody></table>
<h2><span id="数据密集">数据密集</span><a href="#数据密集" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>数据密集</th>
<th>技术</th>
<th>产品</th>
</tr>
</thead>
<tbody><tr>
<td>消息队列</td>
<td>CloudEvent</td>
<td>EventMesh</td>
</tr>
<tr>
<td>数据库</td>
<td>分离: 存算分离(资源伸缩)<br>融合: HTAP(模型)</td>
<td>TiDB(TiKV, TiFlash) ，PolarDB</td>
</tr>
<tr>
<td>大数据</td>
<td>流计算</td>
<td>Beam，Flink</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统架构</category>
        <category>统一模型</category>
      </categories>
      <tags>
        <tag>统一模型</tag>
      </tags>
  </entry>
  <entry>
    <title>TiKV Transaction-MVCC+TSO</title>
    <url>/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h1><h3><span id="percolator-1">Percolator  [1]</span><a href="#percolator-1" class="header-anchor">#</a></h3><p>  总体来说就是一个经过<strong>优化的 2PC</strong> 的实现，依赖一个<strong>单点的授时服务 TSO</strong> 来实现单调递增的事务编号生成，提供<strong>SI 的隔离级别</strong>。</p>
<h3><span id="tikv-的写事务分为两个阶段-1">TiKV 的写事务分为两个阶段  [1]</span><a href="#tikv-的写事务分为两个阶段-1" class="header-anchor">#</a></h3><ul>
<li><p>1、Prewrite 阶段<br>MVCC 在对应传统 2PC 的第一阶段的 prewrite 流程。<br>首先选出一个 primary row 和其他的 secondary rows，然后对 primary row 进行上锁，再对 secondary rows 进行类似的上锁流程。如果任何一步出错，都会进行回滚。完成 prewrite 阶段后，进入 commit 阶段，当前时间戳为 commitTs，TSO 会保证 commitTs &gt; startTs。</p>
</li>
<li><p>2、Commit 阶段<br>MVCC 中的 Commit 流程，包括在 primary 上写入 meta，删除 Lock 标记，以及异步提交 secondaries。如果 primary row 提交失败，则整个事务回滚。如果成功，则标志着整个事务提交成功。</p>
</li>
<li><p>Tidb乐观锁 [2]</p>
<img src="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/leguan.JPG" class>
</li>
<li><p>Tidb悲观锁 [2]</p>
<img src="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/beiguan.JPG" class></li>
</ul>
<h3><span id="tikv-的读事务-1">TiKV 的读事务  [1]</span><a href="#tikv-的读事务-1" class="header-anchor">#</a></h3><p>  在事务中进行读操作的过程。<br>  <strong>首先，需要检查行是否被锁定，如果被锁定，则需要等待或者清除锁。然后，需要读取最新的数据版本，方法是读取元数据并找到最大的时间戳。</strong> 锁分为两级，Primary和Secondary row，只有Primary row的锁被释放，事务才算提交成功。Secondary row的提交可以异步进行，但在此过程中可能需要清理锁。即使Secondary row提交失败，也可以通过锁找到Primary row，并根据元数据确定事务是回滚还是提交成功。</p>
<blockquote>
<p>TiKV 的事务默认隔离级别是 Repeatable Read（SI）, 也对外暴露显式的加锁的 API，用于为客户端实现 SELECT … FOR UPDATE 等隔离级别为 SSI 的语句。</p>
</blockquote>
<h3><span id="读写冲突处理-3">读写冲突处理 [3]</span><a href="#读写冲突处理-3" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/write-vs-read.JPG" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://cn.pingcap.com/blog/tidb-transaction-model">TiKV 事务模型概览，Google Spanner 开源实现</a>  *** </li>
<li>《13 | 隔离性：为什么使用乐观协议的分布式数据库越来越少? 》  分布式数据库30讲</li>
<li>《11｜隔离性：读写冲突时，快照是最好的办法吗？》 分布式数据库30讲</li>
<li><a href="https://zhuanlan.zhihu.com/p/149377959">percolator的理解与开源实现分析</a>   未</li>
<li><a href="https://zhuanlan.zhihu.com/p/261115166">Percolator - 分布式事务的理解与分析</a>   未</li>
<li>《云原生数据库 原理与实践》 8.1.3   未</li>
<li><a href="https://cn.pingcap.com/blog/pessimistic-transaction-the-new-features-of-tidb">TiDB 新特性漫谈：悲观事务</a> 未</li>
<li><a href="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/" title="分布式数据库-全局时钟">分布式数据库-全局时钟</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Rocksdb-SST</title>
    <url>/www6vHomeHexo/2023/04/06/rocksdbSST/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rocksdb-sst-%E7%B1%BB%E5%9E%8B12">Rocksdb SST 类型[1][2]</a></li>
<li><a href="#rocksdb-sst-blockbasedtable-5">Rocksdb SST- BlockBasedTable [5]</a><ul>
<li><a href="#%E7%B4%A2%E5%BC%95%E5%9D%97-index-block-3">索引块 Index Block [3]</a></li>
<li><a href="#sstable%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2%E8%BF%87%E7%A8%8B-6">SSTable的数据检索过程 [6]</a></li>
</ul>
</li>
</ul>
<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="rocksdb-sst-类型12">Rocksdb SST 类型[1][2]</span><a href="#rocksdb-sst-类型12" class="header-anchor">#</a></h2><ul>
<li><p>BlockBasedTable [本文重点讨论]<br>**an overview of the block-based table type in Rocksdb, which is inherited from LevelDB. **<br>Data is stored in fixed-sized blocks, which can be compressed and encoded for efficient storage. To retrieve data, the block where the record may reside is located and read into memory, and a search is performed within the block. The block cache is used to avoid frequent reads of the same block.<br><strong>Rocksdb 中基于块的表类型的概述，该类型继承自 LevelDB。</strong><br>数据存储在固定大小的块中，可以压缩和编码以实现高效存储。为了检索数据，需要定位并读入可能包含记录的块，并在块内执行搜索。块缓存用于避免频繁读取相同的块。</p>
</li>
<li><p>Plain table</p>
</li>
</ul>
<h2><span id="rocksdb-sst-blockbasedtable-5">Rocksdb SST- BlockBasedTable [5]</span><a href="#rocksdb-sst-blockbasedtable-5" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2023/04/06/rocksdbSST/rocksdbSST.jpg" class>


<table>
<thead>
<tr>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Footer</td>
<td>指出 IndexBlock 和 MetaIndexBlock 在文件中的偏移量信息，它是元信息的元信息，它位于 sstable 文件的尾部</td>
</tr>
<tr>
<td>IndexBlock</td>
<td>记录了 DataBlock 相关的元信息</td>
</tr>
<tr>
<td>MetaIndexBlock</td>
<td>各个元信息的Block，包括Filter、Properties(整个table的属性信息)、Compression dictionary、Range deletion tombstone</td>
</tr>
<tr>
<td>MetaBlock</td>
<td>存储布隆过滤器的二进制数据 及其他元信息数据</td>
</tr>
<tr>
<td>DataBlock</td>
<td>存储实际的数据即键值对内容</td>
</tr>
</tbody></table>
<h3><span id="索引块-index-block-3">索引块 Index Block [3]</span><a href="#索引块-index-block-3" class="header-anchor">#</a></h3><p>索引块用于查找包含指定key的数据块。是一种基于<strong>二分搜索</strong>的数据结构。一个文件可能包含一个索引块，也可能包含一组分区索引块，这取决于使用配置。即存在全局索引与分区索引两种索引方式。</p>
<h3><span id="sstable的数据检索过程-6">SSTable的数据检索过程 [6]</span><a href="#sstable的数据检索过程-6" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/04/06/rocksdbSST/querySST.jpg" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/facebook/rocksdb/wiki/A-Tutorial-of-RocksDB-SST-formats">A Tutorial of RocksDB SST formats</a></li>
<li><a href="https://github.com/facebook/rocksdb/wiki/Rocksdb-BlockBasedTable-Format">Rocksdb BlockBasedTable Format</a></li>
<li><a href="https://www.yii666.com/blog/334918.html">RocksDB基本架构与原理介绍</a></li>
<li><a href="https://leveldb-handbook.readthedocs.io/zh/latest/sstable.html">leveldb  sstable</a> ***  未</li>
<li><a href="https://www.jianshu.com/p/d6ce3593a69e">RocksDB block-based SST 文件详解</a> *** </li>
<li><a href="https://zhuanlan.zhihu.com/p/37633790">浅析RocksDB的SSTable格式</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>RocksDB</category>
      </categories>
      <tags>
        <tag>RocksDB</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase - LSM-Tree</title>
    <url>/www6vHomeHexo/2023/04/02/hbaselsmTree/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="column-family和lsm-tree">column family和LSM-tree</span><a href="#column-family和lsm-tree" class="header-anchor">#</a></h3><p><strong>column family本质上是一颗LSM-tree</strong>。</p>
<img src="/www6vHomeHexo/2023/04/02/hbaselsmTree/LSM-tree.JPG" class title="hbase中的LSM-tree实现">

<ul>
<li>LSM-Tree的核心思想就是将写入推迟(Defer)并转换为批量(Batch)写，首先将大量写入缓存在内存，当积攒到一定程度后，将他们批量写入文件中，这要一次I&#x2F;O可以进行多条数据的写入，充分利用每一次I&#x2F;O。</li>
<li><strong>LSM-Tree是对写操作友好的索引结构； 将写入操作全部转化为磁盘的顺序写入。一次随机IO写入转换成一次顺序IO写入（HLog顺序写）和一次内存写入（MemStore写入）。</strong></li>
<li><strong>为了提高读取效率， LSM-tree设计了异步的Compaction</strong>， 小文件合并成大文件（<strong>归并排序</strong>）。</li>
</ul>
<h3><span id="hbase的存储lsm-tree">HBase的存储[lsm-tree]</span><a href="#hbase的存储lsm-tree" class="header-anchor">#</a></h3><ul>
<li>MemStore<br>由两个ConcurrentSkipListMap实现（双缓冲）;<br>ConcurrentSkipListMap A异步flush罗盘成HFile;<br>**HDFS只允许顺序读写，MemStore在落盘生成HFile之前完成kv的排序；  **</li>
<li>HFile<br><strong>HFile Data Block（文件读取的最小单元）内的kv是按key排序的索引树，对读友好</strong>；<br>HFile Index Block的索引结构分为两种: V1 单层索引， V2 多级索引（只加载部分索引，降低内存使用）<br>HDFS的Block默认是64M，128M；HBase的Block默认是64K；<img src="/www6vHomeHexo/2023/04/02/hbaselsmTree/HFile.JPG" class title="HFile物理结构"></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Block Type</th>
<th align="center">基本介绍</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Data Block</td>
<td align="center">用户Key-Value</td>
</tr>
<tr>
<td align="center">Meta Block</td>
<td align="center">Bloom过滤器相关元数据</td>
</tr>
<tr>
<td align="center">Root Index</td>
<td align="center">HFile索引树根索引</td>
</tr>
<tr>
<td align="center">Intermediate Level Index</td>
<td align="center">HFile索引树中间层级索引</td>
</tr>
<tr>
<td align="center">Leaf Level Index</td>
<td align="center">HFile索引树叶子索引</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://kernelmaker.github.io/lsm-tree">【Paper笔记】The Log structured Merge-Tree（LSM-Tree）</a></li>
<li>《Hbase原理和实践》 胡争  范欣欣   第1,2,5,7，8章</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>开闭原则 - SPI</title>
    <url>/www6vHomeHexo/2023/04/02/designOCPspi/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="开闭原则open-closed-principle">开闭原则（Open Closed Principle）</span><a href="#开闭原则open-closed-principle" class="header-anchor">#</a></h2><p><em>open for extension, but closed for modification</em></p>
<h2><span id="开闭原则实现-spi">开闭原则实现 - SPI</span><a href="#开闭原则实现-spi" class="header-anchor">#</a></h2><ul>
<li>SPI<ul>
<li>Java SPI</li>
<li>Dubbo SPI<br><code>ExtensionLoader</code></li>
<li>Spring SPI<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@FunctionalInterface</span><br><span class="line">@Order(Ordered.LOWEST_PRECEDENCE)</span><br><span class="line">public interface MyBeanPostProcessor extends BeanPostProcessor &#123;</span><br><span class="line">   // define your methods here</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/mpyidudu/p/15808383.html">Java SPI机制以及和Dubbo&#x2F;Spring SPI对比 </a></p>
<p><a href="https://www.bilibili.com/video/BV1zp4y1q7fg/">面试官问烂的Dubbo中SPI机制的源码解析</a> *** 未<br><a href="https://zhuanlan.zhihu.com/p/580004065">源码级深度理解 Java SPI</a>  未<br><a href="https://zhuanlan.zhihu.com/p/529674338">剖析 SPI 在 Spring 中的应用</a> 未</p>
]]></content>
      <categories>
        <category>架构</category>
        <category>设计原则</category>
      </categories>
      <tags>
        <tag>设计原则</tag>
      </tags>
  </entry>
  <entry>
    <title>RocketMQ 文件系统</title>
    <url>/www6vHomeHexo/2023/02/26/mqRocketmqStorage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="rocketmq-文件系统">RocketMQ 文件系统</span><a href="#rocketmq-文件系统" class="header-anchor">#</a></h2><h5><span id="overview-8">Overview [8]</span><a href="#overview-8" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2023/02/26/mqRocketmqStorage/mqRocketmqFile.png" class title="rocketmq文件系统">

<h5><span id="逻辑存储层">逻辑存储层</span><a href="#逻辑存储层" class="header-anchor">#</a></h5><ul>
<li>Overview<img src="/www6vHomeHexo/2023/02/26/mqRocketmqStorage/rocketmq-storage.png" class width="720" height="468" title="逻辑存储层"></li>
</ul>
<img src="/www6vHomeHexo/2023/02/26/mqRocketmqStorage/rocketmq-file.png" class title="逻辑存储层">

<ul>
<li>ComsumerQueue[10]<img src="/www6vHomeHexo/2023/02/26/mqRocketmqStorage/ComsumerQueue.JPG" class title="ComsumerQueue"></li>
</ul>
<h5><span id="存储映像层8">存储映像层[8]</span><a href="#存储映像层8" class="header-anchor">#</a></h5><p>mappedByteBuffer 则是一块映射到 CommitLog 文件的内存（具体可以了解 mmap ）</p>
<h6><span id="刷盘机制38">刷盘机制[3][8]</span><a href="#刷盘机制38" class="header-anchor">#</a></h6><ul>
<li>同步刷盘</li>
<li>异步刷盘</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><h5><span id="存储">存储</span><a href="#存储" class="header-anchor">#</a></h5><ol start="3">
<li><p><a href="https://yq.aliyun.com/articles/66110?spm=a2c4e.11155435.0.0.2cb97b3fBOIG8W">RocketMQ 关键特性</a> ***</p>
</li>
<li><p><a href="https://blog.csdn.net/xxxxxx91116/article/details/50333161">《RocketMq》二、存储篇</a>  *</p>
</li>
<li><p><a href="https://blog.csdn.net/gh670011677/article/details/75095469">分布式消息队列RocketMQ与Kafka架构上的巨大差异之2 – CommitLog与ConsumeQueue</a> **</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/396726719">分布式开放消息系统(RocketMQ)的原理与实践</a>   CHEN川  ***  消息的顺序问题  消息的重复问题</p>
</li>
<li><p><a href="https://blog.csdn.net/sjzsylkn/article/details/121897405">RocketMQ架构原理解析（三）：消息索引（ConsumeQueue &amp; IndexFile）</a> 未</p>
</li>
<li><p><a href="https://www.cnblogs.com/enoc/p/rocketmq-so-no-yon.html">RocketMQ源码详解 | Broker篇 · 其二：文件系统</a>  消息管理的结构层次  ***</p>
</li>
<li><p><a href="https://www.cnblogs.com/enoc/p/rocketmq-so-no-gou.html">RocketMQ源码详解 | Broker篇 · 其三：CommitLog、索引、消费队列</a>  未  </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV173411H7JR/">【IT老齐134】请简述RocketMQ消息存储与检索原理</a>       ***</p>
</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>数据处理</title>
    <url>/www6vHomeHexo/2023/02/05/gptDataProcess/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">数据处理</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%BF%87%E6%BB%A4">质量过滤</a></li>
<li><a href="#%E5%86%97%E4%BD%99%E5%8E%BB%E9%99%A4">冗余去除</a></li>
<li><a href="#%E9%9A%90%E7%A7%81%E6%B6%88%E9%99%A4">隐私消除</a></li>
<li><a href="#%E8%AF%8D%E5%85%83%E5%88%87%E5%88%86">词元切分</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE">指令数据</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据处理">数据处理</span><a href="#数据处理" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/02/05/gptDataProcess/data_process.png" class>

<h3><span id="质量过滤">质量过滤</span><a href="#质量过滤" class="header-anchor">#</a></h3><ul>
<li>基于分类器的方法</li>
<li>基于启发 式的方法</li>
</ul>
<h3><span id="冗余去除">冗余去除</span><a href="#冗余去除" class="header-anchor">#</a></h3><p>可以在<strong>句子级</strong>、<strong>文档级</strong>和<strong>数据集级</strong>等不同粒度上去重<br>在实践中应该 共同使用这三个级别的去重</p>
<h3><span id="隐私消除">隐私消除</span><a href="#隐私消除" class="header-anchor">#</a></h3><h3><span id="词元切分">词元切分</span><a href="#词元切分" class="header-anchor">#</a></h3><ul>
<li>BPE</li>
<li>WordPiece</li>
<li>Unigram 词元分析</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/639207933">大模型时代下数据的重要性</a> 综述</p>
<p>1xx. <a href="https://finisky.github.io/textbooks-are-all-you-need-summary/">数据为王: Textbooks Are All You Need </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/641013454">数据为王：大模型预训练中的数据处理及思考—The RefinedWeb Dataset for Falcon LLM论文解读</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/c50HrOfKOqgqGPVRHf6EpA">大模型微调究竟需要多少数据：从三个现有代表工作看几组结论及一点思考 </a><br>   &lt;&lt;LIMa：Less Is More for Alignment&gt;&gt;</p>
<h3><span id="指令数据">指令数据</span><a href="#指令数据" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/658128530">如何从数据集中自动识别高质量的指令数据-IFD指标的使用</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671183709">大模型微调技巧 | 高质量指令数据筛选方法-MoDS</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/420295576">哈工大｜15种NLP数据增强方法总结与对比</a></p>
<p>1xx. <a href="https://hub.baai.ac.cn/view/28740">大模型研发核心：数据工程、自动化评估及与知识图谱的结合</a><br>   <a href="https://mp.weixin.qq.com/s/SvDnQD886E3DBtw8k9asgg">大模型研发核心：数据工程、自动化评估及与知识图谱的结合 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 任务</title>
    <url>/www6vHomeHexo/2023/02/05/gptNLPTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="nlp任务">NLP任务</span><a href="#nlp任务" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/02/05/gptNLPTask/NLP_tasks.jpg" class>


<ul>
<li><strong>文本摘要</strong> text summarization</li>
<li>信息提取 information extraction</li>
<li><strong>问答</strong> question answering</li>
<li><strong>文本分类</strong> text classification</li>
<li>对话 conversation</li>
<li>代码生成 code generation</li>
<li><strong>推理</strong> reasoning</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Toolformer</title>
    <url>/www6vHomeHexo/2023/02/03/gptToolformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="toolformer1">Toolformer[1]</span><a href="#toolformer1" class="header-anchor">#</a></h1><ul>
<li>🔗 文章：Toolformer: Language Models Can Teach Themselves to Use Tools <a href="https://arxiv.org/abs/2302.04761">https://arxiv.org/abs/2302.04761</a></li>
<li>🔑关键词和摘要<ul>
<li>Keywords: Large-scale PLMs,  Tool Learning</li>
<li>xxx<ul>
<li>驱动语言模型去使用简单的模型来调用外部的工具</li>
<li>Toolformer通过语言模型的方法去决定去调用哪些API，传入哪些参数</li>
<li>Tooformer是在自监督层面执行的，只需要对每个API的语言描述</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>方法   <ul>
<li>Toolformer调用示例：xxx</li>
<li>关键要素：<ul>
<li>模型对工具的使用应该是自监督的，这样可以省去很大的标注开销</li>
<li>模型应该自行地去决定在何时间，用何方法来调用工具</li>
</ul>
</li>
<li><strong>方法概要：</strong><ul>
<li>受到in-context learning的启发，给定少量的人写的关于API的描述，让模型去自行生成潜在API调用的语言建模数据</li>
<li>构建一个自监督的Loss函数，让模型来决定哪些API的调用有助于它的语言建模的预测</li>
</ul>
</li>
<li><strong>方法细节：</strong><ul>
<li>xxx<ul>
<li>给定一个纯文本数据集，构建出一个带有API调用的数据集，然后在此数据集上做微调</li>
<li>第一步：使用in-context learning来生成大量的潜在可能的API调用</li>
<li>第二步：执行这些API，返回得到结果</li>
<li>第三步：检查返回的结果是否有助于语言模型的预测，过滤掉其他的API</li>
</ul>
</li>
<li>API调用采样<ul>
<li>给每一个API来撰写提示来鼓励模型使用这些API，例如QA的提示是 xxx</li>
<li>对于文本的每一个位置，如果这个位置是<api>（即API调用的开始）的概率大于一个阈值，则将此位置保留到一个集合I中</api></li>
<li>对于集合I中的每一个位置，通过模型生成最多m个API调用，并且以结尾（如果生成的调用没有以结尾，直接舍去）</li>
</ul>
</li>
<li>API执行<ul>
<li>去执行所有的API调用，返回文本序列</li>
</ul>
</li>
<li>API过滤<ul>
<li>构建自监督的语言模型的loss函数</li>
<li>第一个的含义：进行API的调用，并且使用API结果的Loss</li>
<li>第二个的含义：空字符串的Loss和调用API但不返回结果Loss的最小值</li>
<li>这时我们希望模型使用API并且返回结果对语言建模有帮助，且帮助很明显-&gt;前者的loss显著比后者小</li>
</ul>
</li>
<li>微调和推理<ul>
<li>在经过如上操作后，就可以得到带有API调用的数据集，然后将模型在上面进行微调</li>
<li>当模型在解码阶段输出”-&gt;”符号时，意味着需要调用API了，调用得到返回结果然后拼接上去</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实验<ul>
<li>模型：GPT-J （67亿参数）</li>
<li>原始数据：CCNet</li>
<li>知识探测任务LAMA<ul>
<li>Toolformer可以大幅超过之前的方法，甚至是GPT-3等大模型</li>
</ul>
</li>
<li>数学数据集</li>
<li>问答</li>
<li>这里即使是Toolformer也无法超越GPT-3，可见预训练规模可以囊括更多知识</li>
<li>模型规模的影响</li>
<li>模型的参数量到一定规模后才拥有使用工具的能力</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>将语言模型使用外部工具的进行很自然的结合</li>
<li><strong>不需要标注大量数据，使用自监督的方法进行学习</strong></li>
</ul>
</li>
<li>缺点<ul>
<li><strong>工具无法交互，也无法链式使用（每个API调用都是独立的）</strong></li>
<li>定义的工具尚且有限，扩展工具则需要用模型标注新的数据</li>
<li>随着基础模型zero-shot能力的增强，这种需要构建数据并且fine-tune的做法可能会比较麻烦</li>
</ul>
</li>
</ul>
</li>
<li>OpenBMB BMTools: <a href="https://github.com/OpenBMB/BMTools">https://github.com/OpenBMB/BMTools</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18s4y1u7nJ/">清华博士带你搞懂大模型自学工具使用（Toolformer)【论文速读】</a> V 有思维导图<br>1xx. <a href="https://finisky.github.io/toolformer-summary/">使LLM善假于物: Toolformer </a><br>1xx. <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis">Prompt Engineering </a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>涌现现象（Emergent）</title>
    <url>/www6vHomeHexo/2023/02/03/gptEmergent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="emergent-abilities">Emergent Abilities</span><a href="#emergent-abilities" class="header-anchor">#</a></h1><ul>
<li>🔗 文章：Emergent Abilities of Large Language Models  (2022.10)  (arxiv.org)</li>
<li>🔑关键词和摘要<ul>
<li>Keywords: LLMs, Emergent Ability, Scaling</li>
<li>abstract<ul>
<li>不可预测</li>
<li>不能从小模型的的性能外推</li>
<li>是否能通过继续扩大模型规模来获得更多涌现能力</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>定义<ul>
<li>通常的涌现现象</li>
<li>大模型的涌现现象<ul>
<li>小模型接近随机</li>
<li><strong>大模型突然出现</strong></li>
<li>相变</li>
</ul>
</li>
<li>实验框架<ul>
<li>performance vs 1. FLOPs, model parameters</li>
<li><input checked disabled type="checkbox"> Training datasets</li>
<li>叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。</li>
</ul>
</li>
<li>实验1<ul>
<li>Few-shot Prompting</li>
<li>测试数据说明:<ul>
<li>A: 三位数加法，两位数乘法</li>
<li>B: [dɪfərənt], 复原 “different,” </li>
<li>C: 从 e l h l o 复原 hello</li>
<li>D: 波斯语问答</li>
<li>E: 针对GPT-3 对抗标的问答</li>
<li>…</li>
</ul>
</li>
<li>结果<ul>
<li>这些 task，以 few-shot 形式展示过以后，都有 emergent</li>
<li>不同模型 emergent scale 不一样</li>
<li>有的 task，只有 540B 的 PaLM  emerge了</li>
</ul>
</li>
</ul>
</li>
<li>实验2<ul>
<li>增强语言模型能力的 emerge 现象</li>
<li>已知的一些大模型技巧在何种规模下发挥作用？<ul>
<li>大模型技巧<ul>
<li>思维链 Chain-of-thought: Let’s think step by step.</li>
<li>指令微调 请写一段XXX的描述</li>
<li>草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6&#x3D;11，进位1”</li>
</ul>
</li>
</ul>
</li>
<li>这些增强语言模型能力的方法都有一定程度的涌现</li>
<li>联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？</li>
</ul>
</li>
</ul>
</li>
<li>讨论<ul>
<li><strong>Emergent 现象的解释</strong><ul>
<li><strong>多步能力说</strong><ul>
<li>每个子能力达到 90%  -&gt; 一无是处</li>
<li>每个子能力达到 95% -&gt; 能完成一些任务了</li>
</ul>
</li>
<li>指标缺陷说</li>
<li>奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降</li>
</ul>
</li>
<li><strong>Emergent 的阈值可能会越来越小</strong><ul>
<li>更干净的数据，更好的训练技巧，更优秀的模型结构都可以是  Emergent阈值变小</li>
</ul>
</li>
<li>未来方向：<ul>
<li>继续扩大 model scale，远未达到上限</li>
<li>一些新结构的 scaling</li>
<li>数据的 scaling</li>
<li>理解 prompt 机制</li>
<li>更前沿的 task，用来指导 emergent</li>
<li>理解 emergence</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>第一次正式提出 emergent 实验</li>
<li><strong>做了充分的实验表明该现象在各种数据集上广泛存在</strong></li>
<li>甚至验证了一些“方法”的涌现</li>
<li>提出了一些解释该现象的观点，并提出质疑</li>
</ul>
</li>
<li>改进点<ul>
<li><strong>还是不知道为啥 emerge</strong></li>
<li>实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1qX4y1i78J/">清华博士带你思考大语言模型LLM的涌现现象（Emergent）</a>  有脑图<br> Emergent Abilities of Large Language Models （<a href="https://arxiv.org/abs/2206.07682%EF%BC%89">https://arxiv.org/abs/2206.07682）</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Emergent</category>
      </categories>
      <tags>
        <tag>Emergent</tag>
      </tags>
  </entry>
  <entry>
    <title>继续-预训练</title>
    <url>/www6vHomeHexo/2023/02/03/gptContinualPretraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="继续-预训练-continual-pre-training">继续-预训练 continual pre-training</span><a href="#继续-预训练-continual-pre-training" class="header-anchor">#</a></h1><ul>
<li><p>继续预训练的目的<br>为了得到<strong>适应不同行业&#x2F;任务领域</strong>的预训练模型，<strong>提升下游任务的效果</strong></p>
</li>
<li><p>什么时候需要继续预训练？<br><strong>预训练(pre-train)的语料与下游任务(finetune)语料的【数据分布&#x2F;领域差异】大时</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/545092184">浅谈一下「继续预训练」</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/654463331">如何更好地继续预训练（Continue PreTraining）</a><br>warmup  +  学习率<br>1xx. <a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/120695507">Don’t stop pretraining，继续预训练！</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>推理-框架</title>
    <url>/www6vHomeHexo/2023/02/02/gptInferenceFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E6%A1%86%E6%9E%B611">推理 框架[1.1]</a></li>
<li><a href="#lmdeploy-%E6%8E%A8%E7%90%86%E5%AE%9E%E6%88%98-3">lmdeploy-推理实战 [3]</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">模型转换</a></li>
<li><a href="#turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">TurboMind 推理+命令行本地对话</a></li>
<li><a href="#turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TurboMind推理+API服务</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%86%E6%9E%B6">框架</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="推理-框架11">推理 框架[1.1]</span><a href="#推理-框架11" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/02/02/gptInferenceFramework/inference.jpg" class>

<ul>
<li><p>server 云端<br>vLLM，TensorRT， deepspeed</p>
</li>
<li><p>pc&#x2F;edge 移动端<br> llama.cpp<br>mlc-llm<br>ollama</p>
</li>
<li><p>服务 Server<br>Triton Server</p>
</li>
</ul>
<h1><span id="lmdeploy-推理实战-3">lmdeploy-推理实战 [3]</span><a href="#lmdeploy-推理实战-3" class="header-anchor">#</a></h1><h3><span id="模型转换">模型转换</span><a href="#模型转换" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/02/02/gptInferenceFramework/convert.png" class>
<h3><span id="turbomind-推理命令行本地对话">TurboMind 推理+命令行本地对话</span><a href="#turbomind-推理命令行本地对话" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/02/02/gptInferenceFramework/infer.png" class>
<h3><span id="turbomind推理api服务">TurboMind推理+API服务</span><a href="#turbomind推理api服务" class="header-anchor">#</a></h3><ul>
<li>启动服务<img src="/www6vHomeHexo/2023/02/02/gptInferenceFramework/infer-api.png" class></li>
<li>Client访问服务<img src="/www6vHomeHexo/2023/02/02/gptInferenceFramework/infer-api-client.png" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h3><p>1.1. <a href="https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzA5MTIxNTY4MQ==&scene=1&album_id=2959126655292211206">探秘LLM应用开发</a>   8-19</p>
<ol start="100">
<li><a href="https://github.com/www6v/llm-action/tree/main/inference">https://github.com/www6v/llm-action/tree/main/inference</a></li>
<li><a href="https://www.zhihu.com/question/625415776/answer/3243562246">https://www.zhihu.com/question/625415776/answer/3243562246</a></li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="3">
<li><a href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md">lmdeploy 量化部署</a><br><a href="https://www.bilibili.com/video/BV1iW4y1A77P/">(5)LMDeploy 大模型量化部署实践</a> V</li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/666849728">TensorRT-LLM保姆级教程（一）-快速入门</a><br><a href="https://zhuanlan.zhihu.com/p/667572720">TensorRT-LLM保姆级教程（二）-离线环境搭建、模型量化及推理</a><br><a href="https://zhuanlan.zhihu.com/p/629336492">模型推理服务化框架Triton保姆式教程（一）：快速入门</a><br><a href="https://zhuanlan.zhihu.com/p/634143650">模型推理服务化框架Triton保姆式教程（二）：架构解析</a><br><a href="https://zhuanlan.zhihu.com/p/634444666">模型推理服务化框架Triton保姆式教程（三）：开发实践</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>《SRE 工作手册》</title>
    <url>/www6vHomeHexo/2023/02/01/sreWorkbook/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2023/02/01/sreWorkbook/sre-workbook.jpg" class title="SRE 知识体系脑图">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://martinliu.cn/blog/sre-knowedge-body-mind-map-live-show/">SRE 实践的知识体系梳理</a></p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>sre</category>
      </categories>
      <tags>
        <tag>sre</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Tracing</title>
    <url>/www6vHomeHexo/2023/01/28/observabilityTracing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8E%9F%E7%90%86-0">原理 [0]</a></li>
<li><a href="#apm-%E4%BA%A7%E5%93%81">APM 产品</a><ul>
<li><a href="#apm%E4%BA%A7%E5%93%81%E6%AF%94%E5%AF%B9-5">APM产品比对 [5]</a></li>
<li><a href="#apm%E4%BA%A7%E5%93%81%E6%AF%94%E5%AF%B9-6">APM产品比对 [6]</a></li>
</ul>
</li>
<li><a href="#%E4%BD%BF%E7%94%A8-9">使用 [9]</a><ul>
<li><a href="#apm-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5">APM 故障排查</a></li>
<li><a href="#apm-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%8C%87%E6%A0%87">APM 稳定性指标</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><br>  * <a href="#%E6%A0%87%E5%87%86%E5%8E%9F%E7%90%86">标准&amp;原理</a><br>  * <a href="#%E4%B8%9A%E7%95%8C">业界</a><br>  * <a href="#%E4%BD%BF%E7%94%A8">使用</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="原理-0">原理 [0]</span><a href="#原理-0" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/28/observabilityTracing/tracing.JPG" class>

<h1><span id="apm-产品">APM 产品</span><a href="#apm-产品" class="header-anchor">#</a></h1><h3><span id="apm产品比对-5">APM产品比对 [5]</span><a href="#apm产品比对-5" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/28/observabilityTracing/apm1.JPG" class>

<h3><span id="apm产品比对-6">APM产品比对 [6]</span><a href="#apm产品比对-6" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/28/observabilityTracing/apm.JPG" class>

<h1><span id="使用-9">使用 [9]</span><a href="#使用-9" class="header-anchor">#</a></h1><h3><span id="apm-故障排查">APM 故障排查</span><a href="#apm-故障排查" class="header-anchor">#</a></h3><p>   <img src="https://user-images.githubusercontent.com/5608425/66256533-43942f00-e7c1-11e9-8fe8-80565025c792.png" alt="apm-fault"></p>
<h3><span id="apm-稳定性指标">APM 稳定性指标</span><a href="#apm-稳定性指标" class="header-anchor">#</a></h3><p>   <img src="https://user-images.githubusercontent.com/5608425/66256535-4727b600-e7c1-11e9-82c9-cd2222fce9bb.png" alt="apm-tracing"></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h5><span id="标准amp原理">标准&amp;原理</span><a href="#标准amp原理" class="header-anchor">#</a></h5><ol start="0">
<li>《25 | 分布式Trace：横跨几十个分布式组件的慢请求要如何排查？》</li>
<li><a href="https://github.com/opentracing-contrib/opentracing-specification-zh/blob/master/specification.md">OpenTracing语义标准</a>  archived</li>
<li><a href="https://wu-sheng.gitbooks.io/opentracing-io/content/pages/spec.html">opentracing文档中文版</a> archived</li>
<li><a href="http://bigbully.github.io/Dapper-translation/">Dapper，大规模分布式系统的跟踪系统</a>  论文</li>
</ol>
<h5><span id="业界">业界</span><a href="#业界" class="header-anchor">#</a></h5><ol start="4">
<li><a href="https://github.com/StabilityMan/StabilityGuide/blob/master/docs/processing/monitor/%E8%99%BE%E7%B1%B3SRE%E5%AE%9E%E8%B7%B5_%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF.md">虾米SRE实践_监控体系升级之路</a> ***</li>
<li><a href="https://my.oschina.net/u/3770892/blog/3005395">分布式调用链调研（pinpoint,skywalking,jaeger,zipkin等对比）</a>  对比的表格 ***</li>
<li><a href>微服务架构实战160讲 第四模块 ：微服务调用链监控CAT架构和实践 69.调用链监控产品和比较</a> 杨波</li>
<li><a href="https://www.sofastack.tech/blog/sofa-rpc-link-tracking/">剖析 | SOFARPC 框架之 SOFARPC 链路追踪剖析</a> 未</li>
</ol>
<h5><span id="使用">使用</span><a href="#使用" class="header-anchor">#</a></h5><ol start="8">
<li><a href="https://mp.weixin.qq.com/s/QA_BTF1D3GJJ7_nYQ6oAzQ">如何检测 Web 服务请求丢失问题</a> 问题排查 应用： Nginx tracing + Tomcat tracing</li>
<li><a href="https://yq.aliyun.com/articles/60994?spm=5176.100239.blogcont61320.29.6SwFH6">鹰眼跟踪、限流降级，EDAS的微服务解决之道</a></li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>tracing</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Log</title>
    <url>/www6vHomeHexo/2023/01/28/observabilityLog/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="log-模型1">Log 模型[1]</span><a href="#log-模型1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2023/01/28/observabilityLog/log.JPG" class title="log">

<h2><span id="日志框架">日志框架</span><a href="#日志框架" class="header-anchor">#</a></h2><ul>
<li>ELK，EFK</li>
<li>Loki-基于tag</li>
</ul>
<h2><span id="日志采集">日志采集</span><a href="#日志采集" class="header-anchor">#</a></h2><ul>
<li>Java应用中<ul>
<li>API<ul>
<li>log4j</li>
</ul>
</li>
<li>基于AOP的采集[4]<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="meta">@LogRecord(</span></span><br><span class="line"><span class="meta">     content = &quot;修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”&quot;,</span></span><br><span class="line"><span class="meta">     operator = &quot;#request.userName&quot;, bizNo=&quot;#request.deliveryOrderNo&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">modifyAddress</span><span class="params">(updateDeliveryRequest request, String oldAddress)</span>&#123;</span><br><span class="line">    <span class="comment">// 更新派送信息 电话，收件人、地址</span></span><br><span class="line">    doUpdate(request);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>容器中[2]</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>[微服务架构实战160讲 第七模块 ：微服务监控告警Prometheus架构和实践 119.监控模式分类] 杨波 partial</li>
<li><a href="https://yq.aliyun.com/articles/674327">容器日志采集利器Log-Pilot</a>  阿里开源的Log-Pilot 容器日志采集模式</li>
<li><a href="https://microservices.io/patterns/microservices.html">Pattern: Microservice Architecture</a></li>
<li><a href="https://tech.meituan.com/2021/09/16/operational-logbook.html">如何优雅地记录操作日志？</a>  美团 ***</li>
<li><a href="https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md">日志：每个软件工程师都应该知道的有关实时数据的统一概念</a>  论文翻译 *** 未</li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>log</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent-Tools</title>
    <url>/www6vHomeHexo/2023/01/27/gptAgentTool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="tool-learning-with-foundation-models">Tool Learning with Foundation Models</span><a href="#tool-learning-with-foundation-models" class="header-anchor">#</a></h3><p>1xx. <a href="https://github.com/thunlp/ToolLearningPapers">ToolLearningPapers</a> git<br>1xx. <a href="https://arxiv.org/pdf/2304.08354.pdf">Tool Learning with Foundation Models</a> paper<br>1xx. <a href="https://blog.csdn.net/xixiaoyaoww/article/details/130278978">清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/624459759">大模型工具学习权威综述，BMTools 背后的论文！</a></p>
<h3><span id="augmented-language-models">Augmented Language Models</span><a href="#augmented-language-models" class="header-anchor">#</a></h3><p>1xx. <a href="https://blog.csdn.net/qq_39388410/article/details/130798125">Augmented Language Models（增强语言模型）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/611492200">增强语言模型（ALM）之综述篇</a></p>
<h3><span id="gorilla">Gorilla</span><a href="#gorilla" class="header-anchor">#</a></h3><p>1xx. <a href="https://ar5iv.labs.arxiv.org/html/2305.15334">Gorilla: Large Language Model Connected with Massive APIs</a> paper<br>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b">Gorilla：链接海量API的大型语言模型</a> V<br>1xx. <a href="https://github.com/ShishirPatil/gorilla">gorilla</a> git<br>1xx. <a href="https://gorilla.cs.berkeley.edu/">Gorilla: Large Language Model Connected with Massive APIs</a><br>1xx. <a href="https://gorilla.cs.berkeley.edu/blog.html">Gorilla blog</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/632583909">大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/640697382">Gorilla：与大规模API相连的大型语言模型</a></p>
<h3><span id="others">Others</span><a href="#others" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/633654195">LLM能够自己制作工具了：详解Large Language Models as Tool Makers</a><br>1xx. <a href="https://www.bilibili.com/video/BV1EN4y1q7Zn/">THUNLP成员领读EMNLP大模型工具创造新框架“CREATOR”</a> V 有思维导图</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>PromptTuning 实战</title>
    <url>/www6vHomeHexo/2023/01/25/gptPromptTuningPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/646748939">大模型参数高效微调技术实战（二）-Prompt Tuning</a><br><a href="https://zhuanlan.zhihu.com/p/635686756">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a><br><a href="https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb">peft_prompt_tuning_clm.ipynb</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>不可能三角</title>
    <url>/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%921">不可能三角[1]</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">不可能三角</a></li>
<li><a href="#%E5%BC%A5%E8%A1%A5%E6%96%B9%E6%B3%95">弥补方法</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96-%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">其他 不可能三角</a><ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F">分布式系统</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8">分布式存储</a></li>
</ul>
</li>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a><ul>
<li><a href="#pretrain-finetune-%E8%8C%83%E5%BC%8F3">pretrain, finetune 范式[3]</a></li>
<li><a href="#pretrain-prompt-predict-%E8%8C%83%E5%BC%8F3">pretrain, prompt, predict 范式[3]</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#scaling-law10">Scaling Law[10]</a><ul>
<li><a href="#scaling-law">Scaling Law</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F">参数量 vs 数据量</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F-1">参数量 vs 数据量</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92-1">不可能三角</a></li>
<li><a href="#scaling-law-1">Scaling Law</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="不可能三角1">不可能三角[1]</span><a href="#不可能三角1" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/impossibleTriangle.JPG" class>

<ul>
<li>预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果</li>
<li>而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的<strong>少样本效果依旧比不过中等模型的精调</strong></li>
</ul>
<h3><span id="弥补方法">弥补方法</span><a href="#弥补方法" class="header-anchor">#</a></h3><ul>
<li><strong>优化size</strong><ul>
<li>对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低</li>
</ul>
</li>
<li><strong>优化few-shot</strong><ul>
<li>对于提升少样本表现，<strong>数据增强</strong>是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限</li>
</ul>
</li>
<li><strong>fine-tuning</strong><ul>
<li>对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA</li>
</ul>
</li>
</ul>
<h1><span id="其他-不可能三角">其他 不可能三角</span><a href="#其他-不可能三角" class="header-anchor">#</a></h1><h3><span id="分布式系统">分布式系统</span><a href="#分布式系统" class="header-anchor">#</a></h3><ul>
<li>CAP理论<ul>
<li>C 一致性</li>
<li>A 可用性</li>
<li>P 分区</li>
</ul>
</li>
</ul>
<h3><span id="分布式存储">分布式存储</span><a href="#分布式存储" class="header-anchor">#</a></h3><ul>
<li>RUM猜想<ul>
<li>Read-overhead </li>
<li>Update-overhead </li>
<li>Memory-overhead</li>
</ul>
</li>
</ul>
<h1><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h1><h3><span id="pretrain-finetune-范式3">pretrain, finetune 范式[3]</span><a href="#pretrain-finetune-范式3" class="header-anchor">#</a></h3><p>第三阶段范式</p>
<h3><span id="pretrain-prompt-predict-范式3">pretrain, prompt, predict 范式[3]</span><a href="#pretrain-prompt-predict-范式3" class="header-anchor">#</a></h3><p>第四阶段范式</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响</p>
<h1><span id="scaling-law10">Scaling Law[10]</span><a href="#scaling-law10" class="header-anchor">#</a></h1><h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/scalingLaw.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/paramVSdataSize.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2023/01/25/gptImpossibleTriangle/computeVSDatasize.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/501381510">预训练模型的下一步？突破Impossible Triangle</a></li>
<li><a href="https://arxiv.org/pdf/2204.06130.pdf">Impossible Triangle: What’s Next for Pre-trained Language Models?</a></li>
<li><a href="https://blog.csdn.net/zandaoguang/article/details/124395479">微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」</a></li>
<li><a href="/www6vHomeHexo/2023/01/06/gptPromptTuning/" title="Prompt Tuning">Prompt Tuning</a> self</li>
</ol>
<h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/667489780">解析大模型中的Scaling Law</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663296750">论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models</a><br>2xx. <a href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/">Training Compute-Optimal Large Language Models 简读 </a></li>
</ol>
<p>2xx. <a href="https://zhuanlan.zhihu.com/p/536053110">【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！</a><br>《Scaling Laws for Neural Language Models》<br>《Training Compute-Optimal Large Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-Agents</title>
    <url>/www6vHomeHexo/2023/01/21/gptMultiAgents/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8D%8F%E4%BD%9C%E5%9E%8B%E7%9A%84-multi-agent-%E7%B3%BB%E7%BB%9F12">协作型的 multi-agent 系统[1][2]</a><ul>
<li><a href="#%E6%97%A0%E5%BA%8F%E5%90%88%E4%BD%9C">无序合作</a></li>
<li><a href="#%E6%9C%89%E5%BA%8F%E5%90%88%E4%BD%9C">有序合作</a></li>
</ul>
</li>
<li><a href="#%E7%AB%9E%E4%BA%89%E5%9E%8B%E7%9A%84-multi-agent-%E7%B3%BB%E7%BB%9F12">竞争型的 multi-agent 系统[1][2]</a></li>
<li><a href="#%E7%AB%9E%E4%BA%89%E5%9E%8B-vs-%E5%8D%8F%E4%BD%9C%E5%9E%8B">竞争型 vs 协作型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="协作型的-multi-agent-系统12">协作型的 multi-agent 系统[1][2]</span><a href="#协作型的-multi-agent-系统12" class="header-anchor">#</a></h1><h3><span id="无序合作">无序合作</span><a href="#无序合作" class="header-anchor">#</a></h3><p>当系统中有三个或三个以上的Agent时，每个Agent都可以自由地公开表达自己的观点和意见。他们可以提供反馈和建议，以修改与当前任务相关的反应。<strong>整个讨论过程不受控制，没有特定的顺序，也没有引入标准化的协作工作流程</strong>。我们把这种多Agent合作称为<strong>无序合作</strong>。</p>
<p>multi-Agent系统中引入一个专门的<strong>协调Agent</strong>，负责整合和组织所有Agent的响应，从而更新最终答案。</p>
<blockquote>
<p><strong>ChatLLM 网络</strong>是这一概念的典范代表</p>
</blockquote>
<h3><span id="有序合作">有序合作</span><a href="#有序合作" class="header-anchor">#</a></h3><p>当系统中的Agent遵守特定规则时，例如按顺序逐一发表意见，下游Agent只需关注上游的产出。这样，任务完成效率就会大大提高，整个讨论过程也会变得井然有序。</p>
<blockquote>
<p><strong>CAMEL</strong> 是<strong>双Agent</strong>合作系统的成功实施案例。<br><strong>MetaGPT</strong> 从软件开发中的<strong>经典瀑布模型</strong>中汲取灵感，<strong>将Agent的输入&#x2F;输出标准化为工程文档</strong>。通过将先进的人类流程管理经验编码到Agent提示中，多个Agent之间的合作变得更有条理。然而，在 MetaGPT 的实践探索中，我们发现了Multi-Agent合作的潜在威胁。<strong>如果不制定相应的规则，多个Agent之间的频繁互动会无限放大轻微的幻觉</strong>。</p>
</blockquote>
<h1><span id="竞争型的-multi-agent-系统12">竞争型的 multi-agent 系统[1][2]</span><a href="#竞争型的-multi-agent-系统12" class="header-anchor">#</a></h1><p>当多个Agent在 “针锋相对”的状态下表达自己的论点时，一个<strong>Agent可以从其他Agent那里获得大量外部反馈，从而纠正自己扭曲的想法</strong>。</p>
<blockquote>
<p><strong>ChatEval</strong>建立了一个基于角色扮演的多Agent裁判团队。</p>
</blockquote>
<h1><span id="竞争型-vs-协作型">竞争型 vs 协作型</span><a href="#竞争型-vs-协作型" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th>协作型</th>
<th>竞争型</th>
</tr>
</thead>
<tbody><tr>
<td>系统目标</td>
<td>整体</td>
<td>个体</td>
</tr>
<tr>
<td>主流结构</td>
<td>中心化</td>
<td>去中心化</td>
</tr>
<tr>
<td>agent 功能</td>
<td>相对分散</td>
<td>相对同质</td>
</tr>
<tr>
<td>agent 关系</td>
<td>相互依赖</td>
<td>相互独立</td>
</tr>
<tr>
<td>是否自运行</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>系统资源</td>
<td>通常不共享</td>
<td>共享</td>
</tr>
</tbody></table>
<img src="/www6vHomeHexo/2023/01/21/gptMultiAgents/multi-agents.webp" class>
<p>基于 LLM 的多个代理的交互场景。在合作互动中，代理以无序或有序的方式进行协作，以实现共同目标。在对抗式交互中，代理以针锋相对的方式展开竞争，以提高各自的性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/665644399">NLP（廿二）：LLM 时代的 multi-agent 系统</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/656676717">《综述：全新大语言模型驱动的Agent》</a>  *** 4.2</li>
<li><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">The Rise and Potential of Large Language Model Based Agents: A Survey</a> *** 未</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agents</category>
      </categories>
      <tags>
        <tag>Agents</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 论文</title>
    <url>/www6vHomeHexo/2023/01/20/gptStudyPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#paper">Paper</a></li>
<li><a href="#gpt%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%911">GPT研究方向[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="paper">Paper</span><a href="#paper" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/www6v/paper-reading">paper-reading</a> 李牧大神</p>
<ul>
<li>Transformer  *** <ul>
<li>GPT-4</li>
<li>Instruct GPT *** </li>
<li>GPT, GPT-2, GPT-3 精读  ***</li>
</ul>
</li>
<li>多模态<ul>
<li>CLIP</li>
<li>ViLT</li>
</ul>
</li>
<li>Chain of Thought  ***</li>
</ul>
</li>
<li><p><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a><br>基础篇 - 论文 *** </p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/129508065">LLM&#x2F;ChatGPT与多模态必读论文150篇(已更至第101篇)</a> </p>
</li>
<li><p><a href="https://github.com/zjunlp/LLMAgentPapers">LLMAgentPapers</a> 浙江大学</p>
</li>
<li><p><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a> 浙江大学</p>
</li>
</ul>
<h1><span id="gpt研究方向1">GPT研究方向[1]</span><a href="#gpt研究方向1" class="header-anchor">#</a></h1><ul>
<li>Efficient (PEFT)</li>
<li>Existing stuff(pretrained model)  -应用<br>New directions</li>
<li>Plug-and-play<br> 通用模块组件，能用在各个领域， baseline</li>
<li>Dataset,  evaluation and survey</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1oX4y1d7X6">大模型时代下做科研的四个思路【论文精读·52】</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态</title>
    <url>/www6vHomeHexo/2023/01/18/gptMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h1><h3><span id="clip">CLIP</span><a href="#clip" class="header-anchor">#</a></h3><p>双塔</p>
<h3><span id="blip">BLIP</span><a href="#blip" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/643969218">[Transformer 101系列] 多模态的大一统之路</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/511517344">DeepMind出手！多模态小样本打败精调</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/670821058">[论文阅读] 双子座：一个功能强大的多模态模型系列，Gemini: A Family of Highly Capable Multimodal Models</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/667942680">写在多模态征服一切之前（未来数据和模型应该是什么样的？）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/663655741">166页超长论文阅读，大多模态模型的黎明：GPT-4V的初步探索，The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) [上]</a></p>
<p><a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d282e4b007b201a34052">使用大型语言模型为MiniGPT-4构建视觉语言理解能力</a> V</p>
<p><a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d4fde4b0d1e42e7fc7e6">基于视觉指令调整的多模态聊天机器人 LLaVA</a>  V</p>
<p><a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]</a></p>
<p><a href="https://baoyu.io/translations/lmm/multimodality-and-large-multimodal-models">多模态和多模态大模型 (LMM)[译]</a></p>
<p><a href="https://github.com/Coobiw/MiniGPT4Qwen">MiniGPT4Qwen</a> git<br><a href="https://zhuanlan.zhihu.com/p/664612306">多模态大模型实战-MiniGPT4Qwen系列1：3090+2小时+通义千问&#x3D;个人版双语多模态大模型</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>训练Train-实战</title>
    <url>/www6vHomeHexo/2023/01/15/gptLargeModelTrainingPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/636270877">【LLM】从零开始训练大模型</a> ***  未<br>     <a href="https://www.bilibili.com/video/BV1a14y1o7fr/">从零开始训练大模型</a> V<br>1xx. <a href="https://zhuanlan.zhihu.com/p/637996787">【Falcon Paper】我们是靠洗数据洗败 LLaMA 的！</a> 未<br>1xx. <a href="http://arthurchiao.art/blog/how-to-train-a-gpt-assistant-zh/">[译] 如何训练一个企业级 GPT 助手（OpenAI，2023）</a> 未<br>1xx. <a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb">chatgpt2 训练</a>  10.5   10.6</p>
<h3><span id="小模型训练-poc">小模型训练 PoC</span><a href="#小模型训练-poc" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/660759033">LLM从0开始预训练系列：1、大模型训练踩坑</a><br>1xx. <a href="http://arthurchiao.art/blog/gpt-as-a-finite-state-markov-chain-zh/">[译] GPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）</a>  未</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain  Agent</title>
    <url>/www6vHomeHexo/2023/01/11/gptLangchainAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="langchain-agent">Langchain Agent</span><a href="#langchain-agent" class="header-anchor">#</a></h1><ul>
<li>Conversational</li>
<li>OpenAI assistants</li>
<li>OpenAI functions</li>
<li>OpenAI Multi Functions Agent</li>
<li>OpenAI tools<br>OpenAI parallel function calling (a.k.a. tool calling)</li>
<li>ReAct<br>ZeroShotReactAgent</li>
<li>Self-ask with search</li>
<li>Structured tool chat</li>
</ul>
<h1><span id="langchain-apps">Langchain Apps</span><a href="#langchain-apps" class="header-anchor">#</a></h1><h3><span id="rag-chroma-private-2">rag-chroma-private [2]</span><a href="#rag-chroma-private-2" class="header-anchor">#</a></h3><p><strong>本地 部署</strong><br>This template performs RAG with no reliance on external APIs.<br>It utilizes <strong>Ollama the LLM, GPT4All for embeddings, and Chroma for the vectorstore</strong>.</p>
<h3><span id="research-assistant-34">research-assistant [3][4]</span><a href="#research-assistant-34" class="header-anchor">#</a></h3><p>This template implements a version of<br>“GPT Researcher” that you can use as a starting point for a <strong>research agent</strong>.</p>
<h1><span id="langgraph5">LangGraph[5]</span><a href="#langgraph5" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/langchain-app">Langchain Apps</a> Project Code</li>
<li><a href="https://www.bilibili.com/video/BV1JV411F7Yj/">LangChain Agents 保姆级教程 | 动画演示 讲清 核心模块 Agents | Code 讲解 | Demo 演示</a></li>
<li><a href="https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/">“Research Assistant”: Exploring UXs Besides Chat</a></li>
<li><a href="https://www.youtube.com/watch?v=DjuXACWYkkU">Building a Research Assistant from Scratch</a> </li>
<li><a href="https://blog.langchain.dev/langgraph/">LangGraph</a></li>
<li><a href="https://github.com/www6v/gpt-researcher/">gpt-researcher</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>数据集</title>
    <url>/www6vHomeHexo/2023/01/08/gptDataSet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dataset">DataSet</span><a href="#dataset" class="header-anchor">#</a></h1><h3><span id="pretrain数据集">Pretrain数据集</span><a href="#pretrain数据集" class="header-anchor">#</a></h3><ul>
<li><p>大而全<br><a href="http://opendatalab.com/">OpenDataLab 是引领AI大模型时代的开放数据平台</a>  </p>
<p><a href="https://www.luge.ai/#/">千言数据集</a> </p>
</li>
<li><p>个人收集<br><a href="https://zhuanlan.zhihu.com/p/641187337">LLM大模型数据集之谜</a></p>
<p><a href="https://github.com/brightmart/nlp_chinese_corpus">大规模中文自然语言处理语料</a></p>
<p><a href="https://github.com/Glanvery/LLM-Travel/blob/main/LLM_Pretrain_Datasets.md">开源的可用于LLM Pretrain数据集</a></p>
</li>
</ul>
<h3><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h3><p><a href="https://github.com/chaoswork/sft_datasets">开源SFT数据集整理</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatGLM</title>
    <url>/www6vHomeHexo/2023/01/06/gptChatGLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://www.bilibili.com/video/BV1ju411T74Y/">第十一课：ChatGLM</a> V<br><a href="https://blog.csdn.net/v_JULY_v/article/details/129880836">ChatGLM两代的部署&#x2F;微调&#x2F;实现：从基座GLM、ChatGLM的LoRA&#x2F;P-Tuning微调、6B源码解读到ChatGLM2的微调与实现</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/625468667">【Instruction Tuning】ChatGLM 微调实战（附源码）</a></p>
<p><a href="https://github.com/www6v/transformers_tasks/blob/main/LLM/chatglm_finetune/readme.md">Finetune ChatGLM-6B</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ChatGLM</category>
      </categories>
      <tags>
        <tag>ChatGLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Instruct Tuning</title>
    <url>/www6vHomeHexo/2023/01/06/gptInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#in-context-learning-icl-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0">In Context Learning ( ICL ) 上下文学习</a></li>
<li><a href="#instruction-learning-1">Instruction Learning [1]</a><ul>
<li><a href="#instruct-tuning-">Instruct Tuning-</a></li>
<li><a href="#instructgpt">instructGPT</a></li>
<li><a href="#chatgpt">chatGPT</a></li>
</ul>
</li>
<li><a href="#instruction-tuning">Instruction Tuning</a></li>
<li><a href="#limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="in-context-learning-icl-上下文学习">In Context Learning ( ICL ) 上下文学习</span><a href="#in-context-learning-icl-上下文学习" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/06/gptInstructTuning/ICL.webp" class>

<ul>
<li><strong>in context learning</strong>，大意是在<strong>prompt learning的基础上，将少量有标签样本融入prompt</strong>。</li>
<li>上图的ICL模型可以理解成<strong>有监督、无训练</strong>的<strong>小样本学习</strong>。</li>
<li>但<strong>并非所有ICL都不训练</strong>。比如下图右上角的<strong>FLAN</strong>就是用instruction tuning<strong>训练参数</strong>的。</li>
</ul>
<img src="/www6vHomeHexo/2023/01/06/gptInstructTuning/ICL-tech.webp" class>
<ul>
<li><strong>FLAN</strong>，<strong>既属于 in context learning，也属于 instruction learning</strong></li>
</ul>
<h1><span id="instruction-learning-1">Instruction Learning [1]</span><a href="#instruction-learning-1" class="header-anchor">#</a></h1><h3><span id="instruct-tuning-">Instruct Tuning-</span><a href="#instruct-tuning-" class="header-anchor">#</a></h3><pre><code>FLANv1, FLANv2
</code></pre>
<h3><span id="instructgpt">instructGPT</span><a href="#instructgpt" class="header-anchor">#</a></h3><h3><span id="chatgpt">chatGPT</span><a href="#chatgpt" class="header-anchor">#</a></h3><h1><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/06/gptInstructTuning/instructTuning.webp" class>

<ul>
<li><p>对于已有的预训练模型，继续在多项任务（B、C、D等）上做训练，在其他任务（A）上做预测。<strong>虽然依然没见过任务A，但是根据对B、C、D等的训练，对A的效果有所提升；</strong> [1]</p>
</li>
<li><p><strong>Instruct Tuning 本质上也是Prompt Tuning</strong> [2]</p>
</li>
<li><p>研究了缩放对指令微调的影响 [3]<br>  与微调指令的任务数量有关，<strong>任务数量越多效果越好</strong><br>  与模型的大小有关，<strong>模型越大效果越好</strong></p>
</li>
<li><p>Prompt vs. Instruction Tuning  [4]<br>  Prompt是去激发语言模型的<strong>补全能力</strong>，比如给出上半句生成下半句、或者做完形填空，都还是像在做language model任务.<br>  而Instruction Tuning则是激发语言模型的<strong>理解能力</strong>，通过给出更明显的指令&#x2F;指示，让模型去理解并做出正确的action<br>  <strong>Prompt tuning</strong>都是针对<strong>一个任务</strong>的，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务，而经过<strong>Instruction Tuning多任务</strong>精调后，可以用于其他任务的zero-shot</p>
</li>
<li><p>Instruction Tuning 指令微调  [4]</p>
<ul>
<li>Self Instruction<ul>
<li>Alpaca &#x3D; LLaMA + Intruction Tuning [2]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</span><a href="#limitation-of-instruction-finetuning-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/06/gptInstructTuning/limitation.JPG" class>
<p>问题1.  开放性问题<br>问题2.  看图</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/619406727">各种tuning的简单逻辑解释</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1cm4y1e7Cc/">第九课：Instruct Tuning</a> *** V</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/646136859">FLANv2：大模型指令微调必看论文</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/408166011">Instruction Tuning｜谷歌Quoc V.Le团队提出又一精调范式</a></p>
</li>
<li><p><a href="https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2">June 2023, A Stage Review of Instruction Tuning</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/629461665">【LLM系列之FLAN-T5&#x2F;PaLM】Scaling Instruction-Finetuned Language Models</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/597036814">如何优化大模型的In-Context Learning效果？</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Instruct-Tuning</category>
      </categories>
      <tags>
        <tag>Instruct-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt Tuning</title>
    <url>/www6vHomeHexo/2023/01/06/gptPromptTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="npl范式3">NPL范式[3]</span><a href="#npl范式3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/06/gptPromptTuning/npl4Paragiam.jpg" class title="4种范式">

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li><a href="https://zhuanlan.zhihu.com/p/396098543">[综述]鹏飞大神的Pre-train, Prompt, and Predict [1]</a></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/395115779">近代自然语言处理技术发展的“第四范式”</a>  Prompt Learning</p>
<p><a href="https://zhuanlan.zhihu.com/p/396971490">Prompt范式的缘起｜Pattern-Exploiting Training</a><br><a href="https://zhuanlan.zhihu.com/p/400790006">Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/423306405">清华P-tuning v2、谷歌SPoT｜Prompt可以超过精调了吗？</a></p>
<p><a href="https://www.bilibili.com/video/BV1Wg4y1K77R/">第七课：Prompt Tuning</a> ***  V  有ppt<br><a href="https://www.bilibili.com/video/BV18P411E7VK/">清华博后带你轻松吃透Prompt Tuning顶会大模型论文</a> V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>训练-并行</title>
    <url>/www6vHomeHexo/2023/01/06/gptTrainParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-1">分布式训练 [1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C">数据并行</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C">模型并行</a><ul>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C-3">张量并行 [3]</a></li>
<li><a href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C-3">流水线并行 [3]</a></li>
</ul>
</li>
<li><a href="#3d%E5%B9%B6%E8%A1%8C">3D并行</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="分布式训练-1">分布式训练 [1]</span><a href="#分布式训练-1" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/06/gptTrainParallelism/pararllelTraining.jpg" class>

<h3><span id="数据并行">数据并行</span><a href="#数据并行" class="header-anchor">#</a></h3><h3><span id="模型并行">模型并行</span><a href="#模型并行" class="header-anchor">#</a></h3><p><strong>张量并行</strong>与<strong>流水线并行</strong>都属于<strong>模型并行</strong>，<br>区别在于对模型参数的切分“方向”不同：<br><strong>张量并行</strong>把模型的<strong>每层进行切分 (intra-layer)<strong>，而</strong>流水线并行</strong>则<strong>按层进行切分 (inter-layer) 并在不同设备处理</strong>。[2]</p>
<h5><span id="张量并行-3">张量并行 [3]</span><a href="#张量并行-3" class="header-anchor">#</a></h5> <img src="/www6vHomeHexo/2023/01/06/gptTrainParallelism/tensor.png" class>
<h5><span id="流水线并行-3">流水线并行 [3]</span><a href="#流水线并行-3" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2023/01/06/gptTrainParallelism/pipeline.png" class>

<h3><span id="3d并行">3D并行</span><a href="#3d并行" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">How to Train Really Large Models on Many GPUs? </a></p>
</li>
<li><p><a href="https://finisky.github.io/how-to-train-large-language-model/">大模型分布式训练的并行策略</a></p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/132462452">大模型并行训练指南：通俗理解Megatron-DeepSpeed之模型并行与数据并行</a></p>
</li>
</ol>
<p>1xx. <a href="https://techdiylife.github.io/big-model-training/deepspeed/deepspeed-chat.html">第1章：DeepSpeed-Chat 模型训练实战</a>  Bili<br>      <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat">DeepSpeed-Chat</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/465967735">分布式训练硬核技术——通信原语</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/613196255">图解大模型训练之：流水线并行（Pipeline Parallelism），以Gpipe为例</a>  系列文章 </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/622212228">图解大模型训练之：张量模型并行(TP)，Megatron-LM</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/450854172">全网最全-超大模型+分布式训练架构和经典论文</a> 未</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/664604792">[Transformer 101系列] LLM分布式训练面面观</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/636488690">大模型流水线并行（Pipeline）实战</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT Lora 实战</title>
    <url>/www6vHomeHexo/2023/01/05/gptPEFTLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8Ellama%E7%9A%84sft">基于LLaMA的SFT</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Ebloom%E7%9A%84%E5%BE%AE%E8%B0%83">基于bloom的微调</a></li>
<li><a href="#lora-%E5%8F%82%E6%95%B0">Lora 参数</a></li>
<li><a href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">最佳实践</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#bloom">bloom</a></li>
<li><a href="#llama">LLaMA</a></li>
<li><a href="#chatglm">ChatGLM</a></li>
<li><a href="#others">others</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于llama的sft">基于LLaMA的SFT</span><a href="#基于llama的sft" class="header-anchor">#</a></h1><ul>
<li><p>版本</p>
<ul>
<li>deepspeed的版本  [3.1]</li>
<li>AutoGPTQ的版本  0.6.0 -&gt; git下载到本地安装</li>
</ul>
</li>
<li><p>代码错误</p>
<ul>
<li>use_flash_attention_2 相关的错误 [3.2]</li>
</ul>
</li>
<li><p>脚本 [3.3]</p>
<ul>
<li>modescope 下载 shakechen&#x2F;Llama-2-7b-chat-hf</li>
<li>单卡训练<br>1个epoch 差不多7小时</li>
</ul>
</li>
<li><p>checkpoint 生成文件</p>
</li>
</ul>
<img src="/www6vHomeHexo/2023/01/05/gptPEFTLora/llama-lora.png" class>

<img src="/www6vHomeHexo/2023/01/05/gptPEFTLora/llama-lora1.png" class>

<ul>
<li>模型生成文件</li>
</ul>
<img src="/www6vHomeHexo/2023/01/05/gptPEFTLora/model1.png" class>

<img src="/www6vHomeHexo/2023/01/05/gptPEFTLora/model2.png" class>



<h1><span id="基于bloom的微调">基于bloom的微调</span><a href="#基于bloom的微调" class="header-anchor">#</a></h1><ul>
<li><p>简单基础  [2]</p>
<ul>
<li>基座模型<br>Langboat&#x2F;bloom-1b4-zh </li>
<li>数据集<br>shibing624&#x2F;alpaca-zh</li>
</ul>
</li>
<li><p>稍复杂[1]</p>
<ul>
<li>基座模型<br>bloomz-560m </li>
<li>数据集<br>ought&#x2F;raft</li>
</ul>
</li>
</ul>
<h1><span id="lora-参数">Lora 参数</span><a href="#lora-参数" class="header-anchor">#</a></h1><ul>
<li><p>LoraConfig [2]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LoraConfig( </span><br><span class="line">base_model_name_or_path=<span class="string">&#x27;Langboat/bloom-1b4-zh&#x27;</span>, </span><br><span class="line">task_type=&lt;TaskType.CAUSAL_LM: <span class="string">&#x27;CAUSAL_LM&#x27;</span>&gt;, </span><br><span class="line">inference_mode=<span class="literal">False</span>, </span><br><span class="line">r=<span class="number">8</span>, </span><br><span class="line">target_modules=&#123;<span class="string">&#x27;query_key_value&#x27;</span>&#125;, </span><br><span class="line">lora_alpha=<span class="number">32</span>, </span><br><span class="line">lora_dropout=<span class="number">0.1</span>, </span><br><span class="line">modules_to_save=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数说明 [1]</p>
<ul>
<li>task_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。</li>
<li>inference_mode：是否在推理模式下使用Peft模型。</li>
<li>r： LoRA低秩矩阵的维数。关于秩的选择，通常，使用4，8，16即可。</li>
<li>lora_alpha： LoRA低秩矩阵的缩放系数，为一个常数超参，调整alpha与调整学习率类似。</li>
<li>lora_dropout：LoRA 层的丢弃（dropout）率，取值范围为[0, 1)。</li>
<li>target_modules：要替换为 LoRA 的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样.</li>
</ul>
</li>
<li><p>target_modules [1]<br>在 PEFT 中支持的模型默认的模块名如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = &#123;</span><br><span class="line">    <span class="string">&quot;t5&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>],</span><br><span class="line">    <span class="string">&quot;mt5&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>],</span><br><span class="line">    <span class="string">&quot;bart&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt2&quot;</span>: [<span class="string">&quot;c_attn&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;bloom&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;blip-2&quot;</span>: [<span class="string">&quot;q&quot;</span>, <span class="string">&quot;v&quot;</span>, <span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;opt&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gptj&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt_neox&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;gpt_neo&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;bert&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>], <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;roberta&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;xlm-roberta&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;electra&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;deberta-v2&quot;</span>: [<span class="string">&quot;query_proj&quot;</span>, <span class="string">&quot;value_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;deberta&quot;</span>: [<span class="string">&quot;in_proj&quot;</span>],</span><br><span class="line">    <span class="string">&quot;layoutlm&quot;</span>: [<span class="string">&quot;query&quot;</span>, <span class="string">&quot;value&quot;</span>],</span><br><span class="line">    <span class="string">&quot;llama&quot;</span>: [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],  <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;chatglm&quot;</span>: [<span class="string">&quot;query_key_value&quot;</span>],  <span class="comment">#</span></span><br><span class="line">    <span class="string">&quot;gpt_bigcode&quot;</span>: [<span class="string">&quot;c_attn&quot;</span>],</span><br><span class="line">    <span class="string">&quot;mpt&quot;</span>: [<span class="string">&quot;Wqkv&quot;</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="最佳实践">最佳实践</span><a href="#最佳实践" class="header-anchor">#</a></h1><ul>
<li>秩r的大小[卢老师]<ul>
<li>模型如果是垂直类的大模型<br>eg. 私有数据<br><strong>r设置大点</strong></li>
<li>模型如果是通用类的大模型<br>eg. 运维大模型<br><strong>r设置小点</strong></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="bloom">bloom</span><a href="#bloom" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/649315197">大模型参数高效微调技术实战（五）-LoRA</a><br><a href="https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_lora_clm.ipynb">bloom Lora</a> git</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13w411y7fq/">【手把手带你实战HuggingFace Transformers-高效微调篇】LoRA 原理与实战</a> V<br> <a href="https://github.com/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora.ipynb">bloom Lora-origin</a>  <a href="https://colab.research.google.com/github/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora.ipynb">bloom Lora-origin</a> git   origin运行有问题<br> <a href="https://github.com/www6v/transformers-code/blob/master/03-PEFT/21-lora/chatbot_lora%5Bworkable%5D.ipynb">bloom Lora-modify</a>  <a href="https://colab.research.google.com/drive/1SNy35_CJOobe4AxAecMZJo4LX1TjXvTm">bloom Lora-modify</a> 修改过可以在colab运行的代码</p>
</li>
</ol>
<h3><span id="llama">LLaMA</span><a href="#llama" class="header-anchor">#</a></h3><ol start="3">
<li><a href="https://github.com/www6v/Llama2-Chinese/tree/ww-workable">Llama2-Chinese</a> 模型微调-&gt; lora SFT<br>3.1 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/requirements.txt">requirements.txt</a><br>3.2 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/train/sft/finetune_clm_lora.py#L460C18-L460C19">finetune_clm_lora.py</a>  注释掉第360行<br>3.3 <a href="https://github.com/www6v/Llama2-Chinese/blob/ww-workable/train/sft/finetune_lora.sh">train&#x2F;sft&#x2F;finetune_lora.sh</a></li>
</ol>
<h3><span id="chatglm">ChatGLM</span><a href="#chatglm" class="header-anchor">#</a></h3><p>1xx. <a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_lora.sh">train_lora.sh</a>  基于法律文本的chatglm的lora<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_lora.sh">train_lora.sh</a>  基于法律文本的chatglm-2的lora<br><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb">十一、小参数量微调</a><br>bili有相关的总结的视频</p>
<p>1xx. <a href="https://github.com/mymusise/ChatGLM-Tuning">ChatGLM-Tuning</a> 卢老师推荐</p>
<h3><span id="others">others</span><a href="#others" class="header-anchor">#</a></h3><p>1xx. <a href="https://lightning.ai/pages/community/lora-insights/">Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments</a> ***<br>     <a href="https://www.bilibili.com/video/BV16u4y1a7MH/">几百次大模型LoRA和QLoRA 微调实践的经验分享</a> V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>排行榜</title>
    <url>/www6vHomeHexo/2023/01/04/gptLeaderBoard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%8E%92%E8%A1%8C%E6%A6%9C">排行榜</a></li>
<li><a href="#%E4%B8%AD%E5%9B%BD%E6%8E%92%E8%A1%8C%E6%A6%9C">中国排行榜</a></li>
</ul>
</li>
<li><a href="#%E6%98%BE%E5%8D%A1">显卡</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="排行榜">排行榜</span><a href="#排行榜" class="header-anchor">#</a></h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">HuggingFaceH 大模型排行榜</a></p>
<p><a href="https://www.promptingguide.ai/models/collection">LLM Collection</a></p>
<h3><span id="中国排行榜">中国排行榜</span><a href="#中国排行榜" class="header-anchor">#</a></h3><p><a href="https://github.com/www6v/awesome-LLMs-In-China">中国大模型 </a></p>
<ul>
<li>通用 39</li>
<li>金融 25</li>
<li>司法 8</li>
<li>法律 6</li>
<li>医学 13</li>
<li>医疗 24</li>
<li>教育 13</li>
<li>科研 17</li>
<li>工业 23</li>
<li>政务 12</li>
<li>运维 7</li>
</ul>
<h1><span id="显卡">显卡</span><a href="#显卡" class="header-anchor">#</a></h1><ul>
<li><p>显卡天梯榜<br> <a href="https://topic.expreview.com/GPU">显卡天梯榜</a></p>
</li>
<li><p>显卡<br>显卡 &#x3D; GPU +  显存</p>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>leaderBoard</category>
      </categories>
      <tags>
        <tag>leaderBoard</tag>
      </tags>
  </entry>
  <entry>
    <title>垂类大模型</title>
    <url>/www6vHomeHexo/2023/01/04/gptLargeModelDomain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="垂类大模型">垂类大模型</span><a href="#垂类大模型" class="header-anchor">#</a></h1><h3><span id="金融">金融</span><a href="#金融" class="header-anchor">#</a></h3><ul>
<li>BloombergGPT(未开源)</li>
<li>FinGPT  哥大  </li>
<li>FinBERT</li>
</ul>
<h3><span id="医疗">医疗</span><a href="#医疗" class="header-anchor">#</a></h3><ul>
<li>LLaMA<ul>
<li>ChatDoctor  </li>
<li>华驼&#x2F;本草  哈工大</li>
<li>PMC-LLaMA 上海交大</li>
</ul>
</li>
<li>ChatGLM-6B<ul>
<li>ChatGLM-Med  哈工大</li>
<li>DoctorGLM</li>
<li>明医 (MING)  MedicalGPT-zh  上海交通大学</li>
</ul>
</li>
</ul>
<h3><span id="法律">法律</span><a href="#法律" class="header-anchor">#</a></h3><ul>
<li>ChatLaw </li>
<li>LawGPT_zh</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/131550529?spm=1001.2014.3001.5502">医疗金融法律大模型：从ChatDoctor到BloombergGPT&#x2F;FinGPT&#x2F;FinBERT、ChatLaw&#x2F;LawGPT_zh</a><br>1xx. <a href="/www6vHomeHexo/2023/01/04/gptLeaderBoard/" title="排行榜">排行榜</a> self<br>1xx. <a href="https://finisky.github.io/lawyer-llama-summary/">训练中文垂类大模型：Lawyer LLaMA </a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/642611747">垂直领域大模型的一些思考及开源模型汇总</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>NL2SQL</title>
    <url>/www6vHomeHexo/2023/01/03/gptNL2SQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/NL2SQL">https://github.com/www6v/NL2SQL</a><br><a href="https://github.com/www6v/nl2sql-">https://github.com/www6v/nl2sql-</a><br><a href="https://blog.langchain.dev/llms-and-sql/">LLMs and SQL</a><br><a href="https://zhuanlan.zhihu.com/p/640580808">大模型与数据科学：从Text-to-SQL 开始（一）</a> 多款产品</p>
<p><a href="https://zhuanlan.zhihu.com/p/668557045">C3: Zero-shot Text-to-SQL with ChatGPT笔记</a><br><a href="https://github.com/bigbigwatermalon/C3SQL">C3SQL  </a> git</p>
<p>百度千帆-ppt<br>QCon-ppt</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUxNzk5MTU3OQ==&mid=2247487028&idx=1&sn=7b6767878b7f6b891fc69e408f248ef1">语义解析 (Text-to-SQL) 技术研究及应用 上篇 </a><br><a href="https://mp.weixin.qq.com/s/5lTLW5OOuRMo2zjbzMxr_Q">语义解析 (Text-to-SQL) 技术研究及应用 下篇 </a></p>
<p><a href="https://zhuanlan.zhihu.com/p/670509396">LLM在中文Text2SQL的实践</a><br><a href="https://zhuanlan.zhihu.com/p/673474672">LLM在中文Text2SQL任务上的优化V2.0</a><br><a href="https://zhuanlan.zhihu.com/p/670913902">LLM在中文Text2SQL任务上的优化V1.0</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NL2SQL</category>
      </categories>
      <tags>
        <tag>NL2SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>推理-优化</title>
    <url>/www6vHomeHexo/2023/01/01/gptInference/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E4%BC%98%E5%8C%96">推理 优化</a><ul>
<li><a href="#overview22">overview[2.2]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-21">模型压缩 [2.1]</a></li>
<li><a href="#kv-cache2324">KV Cache[2.3][2.4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%BC%98%E5%8C%96">优化</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>
  
<h1><span id="推理-优化">推理 优化</span><a href="#推理-优化" class="header-anchor">#</a></h1><h3><span id="overview22">overview[2.2]</span><a href="#overview22" class="header-anchor">#</a></h3><p>有几种方法可以在内存中<strong>降低推理成本</strong>或&#x2F;和<strong>加快推理速度</strong>。</p>
<ul>
<li>应用各种<strong>并行处理方式</strong>，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。</li>
<li><strong>内存卸载</strong>，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。</li>
<li><strong>智能批处理策略</strong>；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。</li>
<li><strong>网络压缩技术</strong>，如<strong>修剪、量化、蒸馏</strong>。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。</li>
<li>针对目标模型架构的特定改进。许多<strong>架构变化</strong>，特别是针对注意力层的变化，有助于提高Transformer解码速度。</li>
</ul>
<h3><span id="模型压缩-21">模型压缩 [2.1]</span><a href="#模型压缩-21" class="header-anchor">#</a></h3><p>剪枝（Pruning）<br>知识蒸馏（Knowledge Distillation，KD）<br>量化（Quantization）<br>低秩分解（Low-Rank Factorization）</p>
<h3><span id="kv-cache2324">KV Cache[2.3][2.4]</span><a href="#kv-cache2324" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h3><p>2.1. <a href="https://mp.weixin.qq.com/s/glPPSqHjsnDjC0DZSuuPzA">一文探秘LLM应用开发(13)-模型部署与推理(优化理论) </a><br>2.2 <a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a><br>2.3. <a href="https://zhuanlan.zhihu.com/p/659770503">NLP（二十）：漫谈 KV Cache 优化方法，深度理解 StreamingLLM</a> ***<br>2.4. <a href="https://zhuanlan.zhihu.com/p/662498827">大模型推理加速：看图学KV Cache</a> ***</p>
<ol start="103">
<li><a href="https://zhuanlan.zhihu.com/p/656485997">大语言模型推理性能优化综述</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA</title>
    <url>/www6vHomeHexo/2023/01/01/gptLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="llama-分支1">LLaMA 分支[1]</span><a href="#llama-分支1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>项目</th>
<th>描述</th>
<th>数据集</th>
</tr>
</thead>
<tbody><tr>
<td>LLaMa</td>
<td>基座模型</td>
<td>公开可用的数据集(1T token)</td>
</tr>
<tr>
<td>Stanford Alpaca</td>
<td>结合英文语料通过Self Instruct方式微调LLaMA 7B</td>
<td>Self Instruct from davinci-003 API(52K)</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune)</td>
<td>用户共享对话(70K sample)</td>
</tr>
<tr>
<td>BELLE</td>
<td>结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA</td>
<td></td>
</tr>
<tr>
<td>Chinese-LLaMA&#x2F;Chinese-Alpaca</td>
<td>通过中文数据预训练&#x2F;指令微调LLaMA</td>
<td></td>
</tr>
<tr>
<td>姜子牙系列模型Ziya-LLaMA-13B-v1</td>
<td>基于LLaMA-13B的中英文模型</td>
<td></td>
</tr>
<tr>
<td>ChatLLaMA(英文版)</td>
<td>LLaMA的RLHF版</td>
<td></td>
</tr>
<tr>
<td>ColossalChat</td>
<td>通过self-instruct技术指令微调LLaMA且加上RLHF</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="llama21">LLaMA2[1]</span><a href="#llama21" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/01/gptLlama/llama2.png" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="分支">分支</span><a href="#分支" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/129709105">LLaMA的解读与其微调：Alpaca-LoRA&#x2F;Vicuna&#x2F;BELLE&#x2F;中文LLaMA&#x2F;姜子牙&#x2F;LLaMA 2</a> ***<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485019&idx=1&sn=e3417472c0c1f98aede498fbe905e1a0&">我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 </a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/618695885">NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究</a><br>1xx. <a href="https://github.com/www6v/Llama2-Chinese">https://github.com/www6v/Llama2-Chinese</a><br>1xx. &lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;</li>
</ol>
<h3><span id="基座">基座</span><a href="#基座" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/649756898">Llama 2详解</a>  ***<br>    <a href="https://www.bilibili.com/video/BV12h4y1N7C8/">Llama 2 模型结构解析</a> *** V<br>1xx. <a href="https://www.bilibili.com/video/BV1nN41157a9/">第十五课：LLaMA</a>  *** 华为  V<br>1xx. <a href="https://www.bilibili.com/video/BV1Me411z7ZV/">第十六课：LLaMA2</a> *** 华为  V<br>1xx. <a href="http://arthurchiao.art/blog/llama-paper-zh/">[译][论文] LLaMA：开放和高效的基础语言模型集</a><br>1xx. <a href="http://arthurchiao.art/blog/llama2-paper-zh/">[译][论文] LLaMA 2：开放基础和微调聊天模型</a><br>1xx.  <a href="https://zhuanlan.zhihu.com/p/618321077">从0到1复现斯坦福羊驼（Stanford Alpaca 7B）</a><br>    GPUs: 8 卡 A800 80GB GPUs<br>1xx. <a href="https://zhuanlan.zhihu.com/p/644671690">Llama2技术细节&amp;开源影响</a></p>
<p>1xx. <a href="https://llama.family/">Llama中文社区</a><br>2xx. <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"> Chinese-LLaMA-Alpaca</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent 实践</title>
    <url>/www6vHomeHexo/2023/01/01/gptAgentPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#assistant-api%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D-7">Assistant API功能介绍 [7]</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E5%BE%AE%E8%B0%83%E7%9A%84agent-function-call12">基于微调的Agent-function call[1][2]</a></li>
<li><a href="#multi-agnt">multi-agnt</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="assistant-api功能介绍-7">Assistant API功能介绍 [7]</span><a href="#assistant-api功能介绍-7" class="header-anchor">#</a></h1><p>从功能实现层面来说，Assistant API是截至目前最完整、性能最强大的AI应用开发API，具体功能如下：</p>
<ul>
<li>首先，Assistant API前所未有的能够<strong>调用OpenAI各模型的各项能力</strong>，包括可以调用Chat系列模型（即GPT系列模型）完成文本对话、调用DALL·E 3进行绘图、调用GPT-4-vision进行图像识别、以及调用Text-to-Speech模型进行语音转文字等，并且支持在一轮对话中调用不同模型；</li>
<li>其次，Assistant API还<strong>内置了代码解释器功能（Code interpreter）和海量文本信息提取功能（Knowledge retrieval）</strong>同时也一如既往支持借助<strong>Function calling</strong>进行模型功能层面拓展，此外，非常重要的是，Assistant API还支持在一轮对话中调用多个工具；</li>
<li>其三，此外对于开发者非常友好的一点是，Assistant API最小运行单元为持久化的线程对象（persistent Threads），因此在实际运行Assistant API时，不仅能可以精确控制每一步的执行过程，同时persistent Threads也会保留每轮对话的核心信息，并且当超出模型接收信息最大上下文限制时能够自动删除早期信息，从而实现对模型短期记忆的合理管理；</li>
<li>其四，Assistant API还能够直<strong>接连接OpenAI在线文档库</strong>，即如果用户将外部文档保存在OpenAI云空间内，则可以在调用Assistant API时实时访问文档库中的任意文件，甚至可以在不同线程中调用不同的文档。而在借助Assistant API的Knowledge retrieval功能，则可以让大模型实时获取这些文件信息，并且合理管理短期记忆；</li>
</ul>
<h1><span id="基于微调的agent-function-call12">基于微调的Agent-function call[1][2]</span><a href="#基于微调的agent-function-call12" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2023/01/01/gptAgentPractice/dirs.JPG" class>

<img src="/www6vHomeHexo/2023/01/01/gptAgentPractice/xtuner-agent.png" class>




<h1><span id="multi-agnt">multi-agnt</span><a href="#multi-agnt" class="header-anchor">#</a></h1><ul>
<li>CrewAI - OpenAI</li>
<li>AutoGPT</li>
<li>AutoGen</li>
<li>MetaGPT</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/InternLM/tutorial/blob/main/xtuner/README.md">xtuner</a> 4【补充】用 MS-Agent 数据集 赋予 LLM 以 Agent 能力</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1yK4y1B75J/">(4)XTuner 大模型单卡低成本微调实战</a></p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/tree/master/%E4%B9%9D%E5%A4%A9Hector/Assistant%20API%E8%AF%A6%E8%A7%A3%E4%B8%8EAgent%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-%E4%B9%9D%E5%A4%A9Hector">Assistant API详解与Agent开发实战-九天Hector</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 实践</title>
    <url>/www6vHomeHexo/2022/12/31/gptRAGPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9A%84rag">基于文本的RAG</a><ul>
<li><a href="#langchain-chatchat-%E6%9E%B6%E6%9E%84">Langchain-Chatchat 架构</a></li>
<li><a href="#langchain-chatchat">Langchain-Chatchat</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag-%E5%A4%9A%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%99%A8-1011">多模态RAG-多向量检索器 [10][11]</a><ul>
<li><a href="#semi-structured-tables-text-rag-12">semi-structured (tables + text) RAG [12]</a></li>
<li><a href="#multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG [13]</a></li>
<li><a href="#private-multi-modal-text-tables-images-rag-14">private multi-modal (text + tables + images)  RAG [14]</a></li>
<li><a href="#%E7%BB%84%E4%BB%B6">组件</a></li>
</ul>
</li>
<li><a href="#vectorkg-rag1516">Vector+KG RAG[15][16]</a></li>
<li><a href="#data-processing17">Data processing[17]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%96%87%E6%9C%AC">文本</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81">多模态</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于文本的rag">基于文本的RAG</span><a href="#基于文本的rag" class="header-anchor">#</a></h1><h3><span id="langchain-chatchat-架构">Langchain-Chatchat 架构</span><a href="#langchain-chatchat-架构" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/12/31/gptRAGPractice/langchain+chatglm.jpg" class>

<ul>
<li>组件<ul>
<li>本地知识库</li>
<li>Embedding 模型</li>
<li>向量数据库</li>
<li>Prompt Template</li>
</ul>
</li>
</ul>
<h3><span id="langchain-chatchat">Langchain-Chatchat</span><a href="#langchain-chatchat" class="header-anchor">#</a></h3><ul>
<li>部署 <ul>
<li>windows 10 [5]<br>部署本地， 没显存，卡</li>
<li>Linux [2]<br>部署   32C125G ，没显存， 推理很慢 </li>
<li>Docker</li>
</ul>
</li>
</ul>
<h1><span id="多模态rag-多向量检索器-1011">多模态RAG-多向量检索器 [10][11]</span><a href="#多模态rag-多向量检索器-1011" class="header-anchor">#</a></h1><h3><span id="semi-structured-tables-text-rag-12">semi-structured (tables + text) RAG [12]</span><a href="#semi-structured-tables-text-rag-12" class="header-anchor">#</a></h3><p> 分析pdf中表格 </p>
<h3><span id="multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG [13]</span><a href="#multi-modal-text-tables-images-rag-13" class="header-anchor">#</a></h3><h3><span id="private-multi-modal-text-tables-images-rag-14">private multi-modal (text + tables + images)  RAG [14]</span><a href="#private-multi-modal-text-tables-images-rag-14" class="header-anchor">#</a></h3><p>分析PDF中图片<br><strong>Option 1</strong> </p>
<ul>
<li>Use multimodal embeddings <strong>(such as <a href="https://openai.com/research/clip">CLIP</a>)</strong> to embed images and text</li>
<li>Retrieve both using similarity search</li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis</li>
</ul>
<p><strong>Option 2</strong> </p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce <strong>text summaries from images</strong></li>
<li>Embed and retrieve text </li>
<li>Pass text chunks to an LLM for answer synthesis</li>
</ul>
<p><strong>Option 3</strong> </p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce text summaries from images</li>
<li>Embed and retrieve image summaries with a reference to the raw image </li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis</li>
</ul>
<h3><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h3><ul>
<li>pdf解析<br>unstructured</li>
<li>store<br>MultiVectorRetriever - 元数据+数据</li>
</ul>
<h1><span id="vectorkg-rag1516">Vector+KG RAG[15][16]</span><a href="#vectorkg-rag1516" class="header-anchor">#</a></h1><h1><span id="data-processing17">Data processing[17]</span><a href="#data-processing17" class="header-anchor">#</a></h1><p>长文本   变成   QA pair</p>
<ul>
<li>规则匹配</li>
<li>利用LLM抽取</li>
<li>人工处理</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="文本">文本</span><a href="#文本" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://github.com/chatchat-space/Langchain-Chatchat">Langchain-Chatchat </a> master<br>Langchain 与 ChatGLM 等语言模型的本地知识库问答<br><a href="https://github.com/chatchat-space/Langchain-Chatchat/tree/v0.2.4">Langchain-Chatchat</a>  v0.2.4<br><a href="https://gitee.com/deepeye/langchain-ChatGLM">langchain-ChatGLM</a>  gitee </p>
</li>
<li><p><a href="https://github.com/www6v/Langchain-Chatchat-Colab">Colab for Langchain-Chatchat</a>   linux 可以部署  v0.2.6</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/649055955">langChain-ChatGLM 尝试，踩坑记录</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651189680">Langchain-Chatchat + 阿里通义千问Qwen 保姆级教程 | 次世代知识管理解决方案</a>    Langchain-Chatchat + 通义千问</p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_43094965/article/details/133044128">win10 安装 Langchain-Chatchat 避坑指南（2023年9月18日v0.2.4版本，包含全部下载内容！）</a></p>
</li>
</ol>
<h3><span id="多模态">多模态</span><a href="#多模态" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://www.zhihu.com/question/628651389/answer/3321989558">检索增强生成（RAG）有什么好的优化方案？</a> </p>
</li>
<li><p><a href="https://blog.langchain.dev/semi-structured-multi-modal-rag/">Multi-Vector Retriever for RAG on tables, text, and images</a> *** </p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi_Structured_RAG</a><br><a href="https://github.com/www6v/AIGC/blob/master/Advanced-RAG/01_semi_structured_data.ipynb">Advanced-RAG semi_structured_data</a>   code 半结构化-解析pdf中的表格，  运行没问题，能问表格中的数据</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb">Semi_structured_and_multi_modal_RAG</a>  </p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/blob/master/langchain-cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a>  code 多模态- 解析pdf中的图片  运行有问题<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a></p>
</li>
<li><p><a href="https://neo4j.com/developer-blog/unstructured-knowledge-graph-neo4j-langchain/">Enhanced QA Integrating Unstructured Knowledge Graph Using Neo4j and LangChain</a>  </p>
</li>
<li><p><a href="https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/">Using a Knowledge Graph to implement a DevOps RAG application</a></p>
</li>
<li><p>&lt;&lt;大模型结合 RAG 构建客服场景自动问答系统&gt;&gt;  NVIDIA大模型日系列活动  </p>
</li>
<li><p><a href="https://llamahub.ai/">LlamaHub</a> 未<br> Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Retrievers</title>
    <url>/www6vHomeHexo/2022/12/31/gptRetrievers/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#langchain-retrievers10">Langchain Retrievers[10]</a><ul>
<li><a href="#multiqueryretriever">MultiQueryRetriever</a></li>
<li><a href="#contextual-compression">Contextual compression</a></li>
<li><a href="#ensemble-retriever">Ensemble Retriever</a></li>
<li><a href="#multivector-retriever">MultiVector Retriever</a></li>
<li><a href="#parent-document-retriever">Parent Document Retriever</a></li>
<li><a href="#self-querying">Self-querying</a></li>
</ul>
</li>
<li><a href="#langchian-retriever10">Langchian Retriever[10]</a></li>
<li><a href="#langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="langchain-retrievers10">Langchain Retrievers[10]</span><a href="#langchain-retrievers10" class="header-anchor">#</a></h1><h3><span id="multiqueryretriever">MultiQueryRetriever</span><a href="#multiqueryretriever" class="header-anchor">#</a></h3><p>The MultiQueryRetriever automates the process of prompt tuning by using an LLM to <strong>generate multiple queries from different perspectives for a given user input query</strong>. </p>
<h3><span id="contextual-compression">Contextual compression</span><a href="#contextual-compression" class="header-anchor">#</a></h3><h3><span id="ensemble-retriever">Ensemble Retriever</span><a href="#ensemble-retriever" class="header-anchor">#</a></h3><p>The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and <strong>rerank the results based on the Reciprocal Rank Fusion algorithm</strong>.<br>The most common pattern is to <strong>combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity)</strong>, because their strengths are complementary. It is also known as “hybrid search”.</p>
<h3><span id="multivector-retriever">MultiVector Retriever</span><a href="#multivector-retriever" class="header-anchor">#</a></h3><p>The methods to create multiple vectors per document include:<br>    - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).<br>    - Summary: create a summary for each document, embed that along with (or instead of) the document.<br>    - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.</p>
<h3><span id="parent-document-retriever">Parent Document Retriever</span><a href="#parent-document-retriever" class="header-anchor">#</a></h3><p>chunks of data</p>
<h3><span id="self-querying">Self-querying</span><a href="#self-querying" class="header-anchor">#</a></h3><p>This allows the retriever to not only use the user-input query for <strong>semantic similarity comparison</strong> with the contents of stored documents but to also extract filters from the user query on <strong>the metadata</strong> of stored documents and to execute those filters.</p>
<h1><span id="langchian-retriever10">Langchian Retriever[10]</span><a href="#langchian-retriever10" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Name</th>
<th>Index Type</th>
<th>Uses an LLM</th>
<th>When to Use</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore">Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you are just getting started and looking for something quick and easy.</td>
<td>This is the <strong>simplest method</strong> and the one that is easiest to get started with. It involves creating embeddings for each piece of text.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever">ParentDocument</a></td>
<td>Vectorstore + Document Store</td>
<td>No</td>
<td>If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.</td>
<td>This involves indexing <strong>multiple chunks</strong> for each document. Then you find the  chunks that are most similar in embedding space, but you retrieve the  <strong>whole parent</strong> document and <strong>return</strong> that (rather than individual chunks).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector">Multi Vector</a></td>
<td>Vectorstore + Document Store</td>
<td>Sometimes during indexing</td>
<td>If you are able to extract information from documents that you think is more relevant to index than the text itself.</td>
<td>This involves creating multiple vectors for each document. Each vector could be created in a <strong>myriad of ways</strong> - examples include <strong>summaries of the text</strong> and <strong>hypothetical questions</strong>.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query">Self Query</a></td>
<td>Vectorstore</td>
<td>Yes</td>
<td>If users are asking questions that are better answered by fetching  documents based on metadata rather than similarity with the text.</td>
<td>This uses an LLM to transform user input into two things: (1) a string to  look up semantically, (2) a <strong>metadata filer</strong> to go along with it. This is  useful because oftentimes questions are about the METADATA of documents  (not the content itself).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression">Contextual Compression</a></td>
<td>Any</td>
<td>Sometimes</td>
<td>If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.</td>
<td>This puts a <strong>post-processing step</strong> on top of another retriever and extracts  only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/time_weighted_vectorstore">Time-Weighted Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you have timestamps associated with your documents, and you want to retrieve the most recent ones</td>
<td>This fetches documents based on a combination of semantic similarity (as in  normal vector retrieval) and recency (looking at timestamps of indexed  documents)</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever">Multi-Query Retriever</a></td>
<td>Any</td>
<td>Yes</td>
<td>If users are asking questions that are complex and require multiple pieces of distinct information to respond</td>
<td>This uses an LLM to <strong>generate multiple queries</strong> from the original one. This is useful when the original query needs pieces of information about  multiple topics to be properly answered. By generating multiple queries, we can then fetch documents for each of them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble">Ensemble</a></td>
<td>Any</td>
<td>No</td>
<td>If you have multiple retrieval methods and want to try combining them.</td>
<td>This fetches documents from <strong>multiple retrievers</strong> and then <strong>combines</strong> them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder">Long-Context Reorder</a></td>
<td>Any</td>
<td>No</td>
<td>If you are working with a long-context model and noticing that it’s not  paying attention to information in the middle of retrieved documents.</td>
<td>This fetches documents from an underlying retriever, and then reorders them  so that the most similar are near the beginning and end. This is useful  because it’s been shown that for longer context models they sometimes  don’t pay attention to information in the middle of the context window.</td>
</tr>
</tbody></table>
<h1><span id="langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</span><a href="#langchain-vs-llamaindex-1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>langchain</th>
<th>llamaindex</th>
</tr>
</thead>
<tbody><tr>
<td>Ensemble</td>
<td>Hybrid Fusion</td>
</tr>
<tr>
<td>Rewrite-Retrieve-Read</td>
<td>Query Rewriting</td>
</tr>
<tr>
<td></td>
<td>AutoMerging</td>
</tr>
<tr>
<td>ParentDocumentRetrieval</td>
<td>Small-to-Big Retrieval</td>
</tr>
<tr>
<td></td>
<td>Sentence Window Retrieval</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1qe411r78b/">【高级RAG || 原理介绍】Llamaindex 5种高级RAG方法</a> V </li>
<li><a href="https://python.langchain.com/docs/modules/data_connection/retrievers">retrievers</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Retrievers</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>LLMOps</title>
    <url>/www6vHomeHexo/2022/12/28/gptLLMOps/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li><a href="https://drive.google.com/file/d/1LZXTrRdrloIqAJT6xaNTl4WQd6y95o7K/view">LLMOps: Deployment and Learning in Production</a><br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/">LLMOps: Deployment and Learning in Production</a><br><a href="https://zhuanlan.zhihu.com/p/629589593">[必读] LLM 应用开发全栈指南</a> LLMOps</li>
<li><a href="https://zhuanlan.zhihu.com/p/632026876">了解一下新领域 LLMOps: 大模型运维</a><br><a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations">Understanding LLMOps: Large Language Model Operations</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>Fine-Tuning 时机</title>
    <url>/www6vHomeHexo/2022/12/28/gptFineTuningWhen/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BD%95%E6%97%B6%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%831">何时进行微调[1]</a></li>
<li><a href="#what-4">what [4]</a></li>
<li><a href="#common-use-cases2">Common use cases[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="何时进行微调1">何时进行微调[1]</span><a href="#何时进行微调1" class="header-anchor">#</a></h1><p>语言模型（LLM）可以通过至少两种方式学习新知识：权重更新（例如预训练或微调）或提示（例如检索增强生成，RAG）。模型的权重就像长期记忆，而提示就像短期记忆。这个OpenAI Cookbook给出了一个有用的比喻：当你对模型进行微调时，就像是在离考试还有一周的时候准备复习。当你通过提示（例如检索）向提示中插入知识时，就像是在有开放笔记的考试中。</p>
<p>基于这一点，<strong>不建议使用微调来教授LLM新的知识或事实回忆</strong>；OpenAI的John Schulman在一次讲话中指出，微调可能会<strong>增加虚构</strong>。微调<strong>更适合教授专门的任务</strong>，但应与提示或RAG相对比。正如这里所讨论的，对于具有丰富示例和&#x2F;或缺乏上下文学习能力的LLM来说，微调对于定义明确的任务可能是有帮助的。这篇Anyscale博客很好地总结了这些观点：<strong>微调是为形式而非事实</strong>[3]。</p>
<h1><span id="what-4">what [4]</span><a href="#what-4" class="header-anchor">#</a></h1><p>这是一个很好的问题。我大致将微调类比为人的专业知识：</p>
<ul>
<li><strong>用文字描述一个任务 ~&#x3D; 零样本提示</strong></li>
<li><strong>给出解决任务的示例 ~&#x3D; 少样本提示</strong></li>
<li><strong>允许人们练习任务 ~&#x3D; 微调</strong></li>
</ul>
<p>考虑到这个比喻，令人惊奇的是我们有了可以仅通过提示就能在许多任务上达到高水平准确性的模型，但我也预计达到顶级性能可能需要微调，特别是在具有明确定义的具体任务的应用中，在这些任务中我们可以收集大量数据并在其上进行“练习”。</p>
<p>这可能是一个需要牢记的<strong>粗略图景</strong>。<strong>小型模型</strong>无法进行上下文学习，并且从提示工程中受益甚少，但根据任务的难度，<strong>仍然有可能将它们微调为表现良好的专家</strong>。</p>
<p>需要注意的是，所有这些都还是非常新颖的。</p>


<h1><span id="common-use-cases2">Common use cases[2]</span><a href="#common-use-cases2" class="header-anchor">#</a></h1><p>微调可以改善结果的一些常见<strong>用例</strong>包括：</p>
<ul>
<li><strong>设定风格、语气、格式或其他定性因素</strong></li>
<li><strong>提高生成所需输出的可靠性</strong></li>
<li><strong>纠正无法按照复杂提示要求执行的问题</strong></li>
<li>以特定方式处理许多边缘情况</li>
<li><strong>执行难以用提示清晰表达的新技能或任务</strong></li>
</ul>
<p>从较高层面来看，这些情况下微调更容易实现“<strong>展示而非告诉</strong>”的效果。在接下来的部分中，我们将探讨如何为微调设置数据以及各种示例，这些示例中微调改善了基线模型的性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/">Using LangSmith to Support Fine-tuning</a><br>  <a href="https://colab.research.google.com/drive/1tpywvzwOS74YndNXhI8NUaEfPeqOc7ub?usp=sharing&ref=blog.langchain.dev">colab</a>   LANGCHAIN_API_KEY</p>
</li>
<li><p><a href="https://platform.openai.com/docs/guides/fine-tuning">Fine-tuning</a>  openai *** </p>
</li>
<li><p><a href="https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts">Fine tuning is for form, not facts</a> ***</p>
</li>
<li><p><a href="https://twitter.com/karpathy/status/1655994367033884672">Andrej Karpathy twitter</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 性能-OpenAI案例</title>
    <url>/www6vHomeHexo/2022/12/27/gptRAGPerformanceOpenAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="openai-rag-案例3">OpenAI RAG 案例[3]</span><a href="#openai-rag-案例3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/12/27/gptRAGPerformanceOpenAI/openai-rag.jpg" class>

<ol>
<li>retrieval with consine similarity</li>
<li><strong>HyDE retrieval</strong> [5]<br>Fine-tune Embeddings<br><strong>Chunk&#x2F;embedding experiments</strong></li>
<li><strong>Reranking</strong> [6][8]<br>Classification step</li>
<li>Prompt engineering<br><strong>Tool use</strong><br><strong>Query expansion</strong>[5]</li>
</ol>
<h3><span id="query-transformations5">Query Transformations[5]</span><a href="#query-transformations5" class="header-anchor">#</a></h3><ul>
<li><strong>Query expansion</strong><br>Multi-query retriever </li>
<li><strong>HyDE</strong></li>
<li>Step back prompting<br> [抽象prompting]</li>
<li>Rewrite-Retrieve-Read</li>
</ul>
<h3><span id="query-construction-4">Query Construction [4]</span><a href="#query-construction-4" class="header-anchor">#</a></h3>

<table>
<thead>
<tr>
<th><strong>Examples</strong></th>
<th><strong>Data source</strong></th>
<th><strong>References</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Text-to-metadata-filter</strong></td>
<td>Vectorstores</td>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/?ref=blog.langchain.dev#constructing-from-scratch-with-lcel"><strong>Docs</strong></a></td>
</tr>
<tr>
<td><strong>Text-to-SQL</strong></td>
<td>SQL DB</td>
<td><a href="https://python.langchain.com/docs/use_cases/qa_structured/sql?ref=blog.langchain.dev"><strong>Docs</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/llms-and-sql/"><strong>blog</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/"><strong>blog</strong></a></td>
</tr>
</tbody></table>
<ul>
<li>Text-to-metadata-filter [7]</li>
</ul>
<p>A <strong>self-querying</strong> retriever is one that, as the name suggests, has the  ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a <strong>structured query</strong> and then applies that structured query to its underlying  VectorStore. This allows the retriever to not only use the user-input  query for semantic similarity comparison with the contents of stored  documents but to also <strong>extract filters from the user query on the  metadata of stored documents and to execute those filters</strong>.</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li><p><a href="https://blog.langchain.dev/applying-openai-rag/">Applying OpenAI’s RAG Strategies</a>   *** </p>
</li>
<li><p><a href="https://blog.langchain.dev/query-construction/">Query Construction</a> ***</p>
</li>
<li><p><a href="https://blog.langchain.dev/query-transformations/">Query Transformations</a></p>
</li>
<li><p><a href="https://txt.cohere.com/rerank/">Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here</a><br><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-pinecone-rerank">Rerank</a><br><a href="https://python.langchain.com/docs/integrations/retrievers/cohere-reranker">Cohere Reranker</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/docs/docs/modules/data_connection/retrievers/self_query.ipynb">self_query</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb">RAG Fusion</a><br><a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1">Forget RAG, the Future is RAG-Fusion</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT 实战</title>
    <url>/www6vHomeHexo/2022/12/20/gptFineTuningPEFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="huggingface-peft中的任务1">Huggingface  PEFT中的任务[1]</span><a href="#huggingface-peft中的任务1" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class TaskType(str, enum.Enum):</span><br><span class="line">    SEQ_CLS = &quot;SEQ_CLS&quot;  # 3. 序列分类任务</span><br><span class="line">    SEQ_2_SEQ_LM = &quot;SEQ_2_SEQ_LM&quot;  # 2. 条件生成任务</span><br><span class="line">    CAUSAL_LM = &quot;CAUSAL_LM&quot;  #  1. 因果语言建模任务</span><br><span class="line">    TOKEN_CLS = &quot;TOKEN_CLS&quot;  #  4. Token 分类任务</span><br><span class="line">    QUESTION_ANS = &quot;QUESTION_ANS&quot;</span><br><span class="line">    FEATURE_EXTRACTION = &quot;FEATURE_EXTRACTION&quot;</span><br></pre></td></tr></table></figure>

<h3><span id="1-因果语言建模任务causal-language-modeling">1. 因果语言建模任务（Causal Language Modeling）</span><a href="#1-因果语言建模任务causal-language-modeling" class="header-anchor">#</a></h3><p>  因果语言建模任务（CLM），在这种建模方法中，模型试图预测给定上下文中的下一个单词，该上下文通常包括在当前单词之前的所有单词。</p>
<h3><span id="2-条件生成任务conditional-generation">2. 条件生成任务（Conditional Generation）</span><a href="#2-条件生成任务conditional-generation" class="header-anchor">#</a></h3><p>  条件生成任务（Conditional Generation），根据给定的输入（可能是文本、图片等）生成符合条件的输出。<br>  条件生成的应用包括但不限于机器翻译、文本摘要、图像描述等。这些任务通常需要模型在输入和输出之间建立复杂的映射关系。</p>
<blockquote>
<p>因果语言建模任务  vs.  条件生成任务<br>  因果语言建模主要关注于生成连贯、自然的文本，而条件生成关注于生成满足特定条件或任务要求的文本。这两种建模方法在某些场景下可能会互相使用和结合，以实现更复杂的自然语言处理任务。</p>
</blockquote>
<h3><span id="3-序列分类任务sequence-classification">3. 序列分类任务（Sequence Classification）</span><a href="#3-序列分类任务sequence-classification" class="header-anchor">#</a></h3><p>  序列分类（Sequence Classification），对整个句子进行分类。如: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关等</p>
<h3><span id="4-token-分类任务token-classification">4. Token 分类任务（Token Classification）</span><a href="#4-token-分类任务token-classification" class="header-anchor">#</a></h3><p>  Token 分类任务（Token Classification），对句子中的每个词进行分类。如: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/651744834">大模型参数高效微调技术实战（一）-PEFT概述</a></li>
<li><a href="https://github.com/www6v/llm-action#llm%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98">LLM微调实战</a> 李国东</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 系列</title>
    <url>/www6vHomeHexo/2022/12/11/gptFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%9B%E5%8C%96%E6%97%B6%E9%97%B4%E7%BA%BF">进化时间线</a></li>
<li><a href="#gpt1-1">GPT1 [1]</a></li>
<li><a href="#gpt2-1">GPT2 [1]</a><ul>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">核心思想</a></li>
<li><a href="#gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</a></li>
</ul>
</li>
<li><a href="#gpt3-1">GPT3 [1]</a><ul>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">下游任务评估方法</a></li>
<li><a href="#few-shot-vs-fine-tuning">Few-shot vs fine-tuning</a></li>
<li><a href="#gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</a></li>
</ul>
</li>
<li><a href="#instructgpt-1">InstructGPT [1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88">技术方案</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="进化时间线">进化时间线</span><a href="#进化时间线" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/12/11/gptFamily/family.jpg" class>

<h1><span id="gpt1-1">GPT1 [1]</span><a href="#gpt1-1" class="header-anchor">#</a></h1><ol>
<li>它是最早一批提出在 NLP 任务上使用 <strong>pre-train + fine-tuning 范式</strong>的工作。</li>
<li>GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</li>
<li><strong>预训练模型具有 zero-shot 的能力</strong>，并且能随着预训练的进行不断增强</li>
</ol>
<h1><span id="gpt2-1">GPT2 [1]</span><a href="#gpt2-1" class="header-anchor">#</a></h1><h3><span id="核心思想">核心思想</span><a href="#核心思想" class="header-anchor">#</a></h3><p>当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，<strong>不需要在下游任务微调</strong>。</p>
<h3><span id="gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</span><a href="#gpt-2-vs-gpt-1" class="header-anchor">#</a></h3><ol>
<li><strong>主推 zero-shot</strong>，而 GPT-1 为 pre-train + fine-tuning；</li>
<li>训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；</li>
<li>模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；</li>
<li>模型结构调整，层归一化和参数初始化方式；</li>
<li>训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；</li>
</ol>
<h1><span id="gpt3-1">GPT3 [1]</span><a href="#gpt3-1" class="header-anchor">#</a></h1><h3><span id="下游任务评估方法">下游任务评估方法</span><a href="#下游任务评估方法" class="header-anchor">#</a></h3><p>GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：<br><strong>Zero-shot</strong>：仅使用当前任务的自然语言描述，不进行任何梯度更新；<br><strong>One-shot</strong>：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；<br><strong>Few-shot</strong>：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；</p>
<ul>
<li>Shot[2]<ul>
<li>One-shot</li>
<li>Few-Shot</li>
<li>Zero-Shot</li>
</ul>
</li>
</ul>
<h3><span id="few-shot-vs-fine-tuning">Few-shot vs fine-tuning</span><a href="#few-shot-vs-fine-tuning" class="header-anchor">#</a></h3><p>其中 <strong>Few-shot</strong> 也被称为 <strong>in-context learning</strong>，虽然它与 fine-tuning 一样都需要一些<strong>有监督标注数据</strong>，但是两者的区别是：<br>【本质区别】<br><strong>fine-tuning</strong> 基于标注数据<strong>对模型参数进行更新</strong><br>而<strong>in-context learning</strong>使用标注数据时不做任何的梯度回传, <strong>模型参数不更新</strong></p>
<h3><span id="gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</span><a href="#gpt-3-vs-gpt-2" class="header-anchor">#</a></h3><ol>
<li>效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章；</li>
<li><strong>主推 few-shot</strong>，相比于 GPT-2 的 zero-shot，具有很强的创新性；</li>
<li>模型结构略微变化，采用 <strong>sparse attention</strong> 模块；</li>
<li>海量训练语料 <strong>45TB</strong>（清洗后 570GB），相比于 GPT-2 的 40GB；</li>
<li>海量模型参数，最大模型为 <strong>1750 亿</strong>，GPT-2 最大为 15 亿参数；</li>
</ol>
<h1><span id="instructgpt-1">InstructGPT [1]</span><a href="#instructgpt-1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>有监督微调，</li>
<li>奖励模型训练，</li>
<li>强化学习训练</li>
</ul>
<h3><span id="技术方案">技术方案</span><a href="#技术方案" class="header-anchor">#</a></h3><ul>
<li><p>有监督微调（SFT）<br>本质上来说，<strong>SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3</strong>。但是值得一提的是，这里<strong>标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别</strong>。<br>InstructGPT 在 SFT 中标注的数据，正是为了<strong>消除这种模型预测与用户表达习惯之间的 gap</strong>。在标注过程中，他们<strong>从 GPT-3 的用户真实请求中采样</strong>大量下游任务的描述，然后让<strong>标注人员对任务描述进行续写</strong>，从而得到该问题的高质量回答。</p>
</li>
<li><p>基于人类反馈的强化学习（RLHF）</p>
<img src="/www6vHomeHexo/2022/12/11/gptFamily/instructGPT.jpg" class></li>
</ul>
<h3><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h3><ol>
<li>解决 GPT-3 的<strong>输出与人类意图</strong>之间的<strong>Align问题</strong>；</li>
<li>让具备丰富世界知识的大模型，<strong>学习“人类偏好”</strong>；</li>
<li>标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠；</li>
<li>InstructGPT 在<strong>真实性</strong>，<strong>丰富度</strong>上表现更好；</li>
<li>InstructGPT 对有害结果的生成控制的更好，但是对于<strong>“偏见”没有明显改善</strong>；</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/609716668">GPT &#x2F; GPT-2 &#x2F; GPT-3 &#x2F; InstructGPT 进化之路</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/624793654">Few-Shot, Zero-Shot &amp; One-shot 的通俗理解</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/642282717">[Transformer 101系列] ChatGPT是怎么炼成的?</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 性能</title>
    <url>/www6vHomeHexo/2022/12/07/gptRAGPerformance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F-12">索引方式 [1][2]</a><ul>
<li><a href="#smaller-chunks">Smaller chunks</a></li>
<li><a href="#hypothetical-questions">Hypothetical questions</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</li>
<li><a href="#%E5%88%86%E5%9D%973">分块[3]</a><ul>
<li><a href="#%E5%88%86%E5%9D%97%E5%8F%82%E6%95%B0">分块参数</a></li>
</ul>
</li>
<li><a href="#%E6%A3%80%E7%B4%A2%E5%99%A8-retriever">检索器 Retriever</a></li>
<li><a href="#embedding">Embedding</a></li>
<li><a href="#reranker">Reranker</a><ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AFreranker-6">什么是Reranker [6]</a></li>
<li><a href="#bge-ranker-4">BGE Ranker [4]</a></li>
<li><a href="#%E4%BC%98%E7%A7%80%E7%9A%84%E7%BB%84%E5%90%88-5">优秀的组合 [5]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="索引方式-12">索引方式  [1][2]</span><a href="#索引方式-12" class="header-anchor">#</a></h1><h3><span id="smaller-chunks">Smaller chunks</span><a href="#smaller-chunks" class="header-anchor">#</a></h3><p>Indexing by <strong>small data chunks</strong><br>按子部分索引数据块：将文本块拆分为较小的部分，如句子，进行多次索引。这有助于<br>处理复杂文本块，减少噪音输出，确保更准确匹配用户查询。</p>
<h3><span id="hypothetical-questions">Hypothetical questions</span><a href="#hypothetical-questions" class="header-anchor">#</a></h3><p>Indexing by <strong>the questions the document answers</strong><br>按文本块回答的问题索引数据块：让LLM生成与拆分的文本块相关的假设性问题，并用<br>于索引。这种方法保持用户查询与数据核心内容一致，降低模糊性。</p>
<h3><span id="summary">Summary</span><a href="#summary" class="header-anchor">#</a></h3><p>Indexing by <strong>the summary of the document</strong></p>
<p>按文本块摘要索引数据块：类似于第二种方法，使用块摘要而不是回答的假设问题来创<br>建索引。特别适用于文本块中包含多余信息或与用户查询无关的情况。</p>
<h1><span id="分块3">分块[3]</span><a href="#分块3" class="header-anchor">#</a></h1><h3><span id="分块参数">分块参数</span><a href="#分块参数" class="header-anchor">#</a></h3><p>chuck_size, ,chunk overlap<br>top_k</p>
<blockquote>
<p>最佳实践<br>  按<strong>逻辑分块</strong>可以明显提升<strong>检索器的准确率</strong></p>
</blockquote>
<h1><span id="检索器-retriever">检索器 Retriever</span><a href="#检索器-retriever" class="header-anchor">#</a></h1><ul>
<li>Ensemble Retriever<br>最常见的模式是将<strong>稀疏检索器（如BM25）</strong>与<strong>密集检索器（如嵌入相似度）</strong>结合起来，因为它们的优势是互补的。这也被称为“混合搜索”。<strong>稀疏检索器</strong>擅长基于<strong>关键词查找</strong>相关文档，而<strong>密集检索器</strong>擅长基于<strong>语义相似性查找</strong>相关文档。</li>
</ul>
<blockquote>
<p>最佳实践<br><strong>BM25+FAAIS   好于 FAAIS相似度搜索</strong><br><strong>FAAIS相似度搜索 好于 HyDE和上下文压缩</strong></p>
</blockquote>
<h1><span id="embedding">Embedding</span><a href="#embedding" class="header-anchor">#</a></h1><ul>
<li>HyDE<br>At a high level, HyDE is an embedding technique that takes queries, <strong>generates a hypothetical answer</strong>, and then embeds that generated document and uses that as the final example.</li>
</ul>
<blockquote>
<p>最佳实践<br><strong>BGE</strong> 优于 OpenAI ADA02</p>
</blockquote>
<h1><span id="reranker">Reranker</span><a href="#reranker" class="header-anchor">#</a></h1><h3><span id="什么是reranker-6">什么是Reranker [6]</span><a href="#什么是reranker-6" class="header-anchor">#</a></h3><p>A reranking model — also known as a <strong>cross-encoder</strong> — is a type of model that,** given a query and document pair, will output a similarity score.** </p>
<h3><span id="bge-ranker-4">BGE Ranker [4]</span><a href="#bge-ranker-4" class="header-anchor">#</a></h3><p><strong>交叉编码器</strong>将对查询和答案实时计算相关性分数，这比**向量模型(即双编码器)**更准确，但比向量模型更耗时。 因此，它可以用来对嵌入模型返回的前k个文档重新排序。 我们在多语言数据上训练了交叉编码器，数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。 </p>
<h3><span id="优秀的组合-5">优秀的组合 [5]</span><a href="#优秀的组合-5" class="header-anchor">#</a></h3><p>OpenAI + CohereRerank<br>Voyage + big-reranker-large</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1dH4y1C7Ck/">3种高级索引方法，有效提升RAG性能</a> V<br><a href="https://thetechbuffet.substack.com/p/rag-indexing-methods">The Tech Buffet #12: Improve RAG Pipelines With These 3 Indexing Methods</a><br><a href="https://newsletter.theaiedge.io/p/how-to-optimize-your-rag-pipelines">How To Optimize Your RAG Pipelines</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Vu4y1H72s/">【RAG实战】 Multi-Vector-Retrieval实现三种高级索引方法</a> V<br><a href="https://github.com/www6v/AIGC/blob/master/retriever%2Bindex/MultiVectorRetriever">MultiVectorRetriever</a><br>   <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector">MultiVector Retriever</a></p>
</li>
<li><p><a href="https://hustai.gitee.io/zh/posts/rag/Chunking-Strategies.html">大语言模型应用中的文本分块策略</a><br><a href="https://yangfei.me/tutorials/chunking-strategies">LLM 应用中的分块策略 </a></p>
</li>
<li><p><a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md">BGE Reranker</a><br><a href="https://www.bilibili.com/video/BV1sQ4y137Ft/">transformers二次开发——bge-reranker模型微调流程</a> V</p>
</li>
<li><p><a href="https://luxiangdong.com/2023/11/06/rerank-ev/#">提升RAG——选择最佳Embedding和重新排名模型 </a><br><a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">Boosting RAG: Picking the Best Embedding &amp; Reranker models</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/series/rag/rerankers/">Rerankers and Two-Stage Retrieval</a><br>文中的第二阶段就是指Reranker</p>
</li>
</ol>
<p>1xx. <a href="https://www.youtube.com/watch?v=ahnGLM-RC1Y">A Survey of Techniques for Maximizing LLM Performance</a>  *** V</p>
<pre><code>[A Survey of Techniques for Maximizing LLM Performance梳理](https://zhuanlan.zhihu.com/p/670880685) 
</code></pre>
<p>1xx. <a href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">A Cheat Sheet and Some Recipes For Building Advanced RAG</a><br>     <a href="https://mp.weixin.qq.com/s/KM8c3PUww1SOK1dbLjn1Tw">LlamaIndex官方年度巨献：高清大图纵览高级 RAG技术，强烈推荐收藏 </a> *** 看图<br>     <a href="https://baoyu.io/translations/rag/a-cheat-sheet-and-some-recipes-for-building-advanced-rag">构建高级 RAG 的指南和技巧 [译]</a></p>
<p>1xx. <a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced RAG Techniques: an Illustrated Overview</a><br>     <a href="https://mp.weixin.qq.com/s/CO7hMv4RW7OE6zwUmVfp5A">最全的RAG技术概览 </a></p>
<p>1xx. <a href="https://baoyu.io/translations/rag/5-levels-of-text-splitting">文本分割的五个层次 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/www6vHomeHexo/2022/11/30/gptTransformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a></li>
<li><a href="#attention-3">Attention [3]</a><ul>
<li><a href="#%E4%BC%98%E5%8C%964">优化[4]</a></li>
</ul>
</li>
<li><a href="#transformer-2">Transformer [2]</a><ul>
<li><a href="#encoder-decoder%E6%9E%B6%E6%9E%84-1">Encoder-Decoder架构 [1]</a></li>
<li><a href="#self-attention">Self-attention</a></li>
<li><a href="#multi-head-attentionmha">Multi-Head Attention(MHA)</a></li>
<li><a href="#positional-encoding">Positional Encoding</a></li>
<li><a href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84-67">架构 [6][7]</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E7%82%B9">优化点</a></li>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B95">关注点[5]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#attention">Attention</a></li>
<li><a href="#transformer-%E5%AE%9E%E7%8E%B0">Transformer 实现</a></li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">位置编码</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="神经网络">神经网络</span><a href="#神经网络" class="header-anchor">#</a></h1><ul>
<li><p>正向传播<br>损失函数  </p>
</li>
<li><p>反相传播<br>梯度</p>
</li>
</ul>
<h1><span id="attention-3">Attention [3]</span><a href="#attention-3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/30/gptTransformer/self-attention.jpg" class>

<h3><span id="优化4">优化[4]</span><a href="#优化4" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/11/30/gptTransformer/attentions.jpg" class>

<h1><span id="transformer-2">Transformer [2]</span><a href="#transformer-2" class="header-anchor">#</a></h1><h3><span id="encoder-decoder架构-1">Encoder-Decoder架构 [1]</span><a href="#encoder-decoder架构-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/11/30/gptTransformer/Transformer_decoder.jpg" class>
<img src="/www6vHomeHexo/2022/11/30/gptTransformer/transformer_resideual_layer_norm_3.jpg" class>

<p>transfomer 架构在GPU上的并行</p>
<h3><span id="self-attention">Self-attention</span><a href="#self-attention" class="header-anchor">#</a></h3><p>Q&#x3D;K&#x3D;V<br>aligment</p>
<h3><span id="multi-head-attentionmha">Multi-Head Attention(MHA)</span><a href="#multi-head-attentionmha" class="header-anchor">#</a></h3><h3><span id="positional-encoding">Positional Encoding</span><a href="#positional-encoding" class="header-anchor">#</a></h3><h3><span id="layer-normalization">Layer Normalization</span><a href="#layer-normalization" class="header-anchor">#</a></h3><ul>
<li>Post-LN</li>
<li>Pre-LN</li>
<li>Sandwich-LN<br>layerNorm是针对序列数据提出的一种归一化方法，主要在layer维度进行归一化，即对整个序列进行归一化。</li>
</ul>
<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="架构-67">架构 [6][7]</span><a href="#架构-67" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/11/30/gptTransformer/bigModelArch.jpg" class>

<img src="/www6vHomeHexo/2022/11/30/gptTransformer/bigModelArch1.jpg" class>

<h3><span id="优化点">优化点</span><a href="#优化点" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/11/30/gptTransformer/transformers.jpg" class>

<h3><span id="关注点5">关注点[5]</span><a href="#关注点5" class="header-anchor">#</a></h3><ul>
<li><p><strong>Mask attention 的策略不同</strong></p>
<ul>
<li>bert  [双向都能看到]</li>
<li>chatgpt  [只能看到单项的]</li>
<li>chatglm  [左边像bert, 右边像gpt]</li>
</ul>
</li>
<li><p><strong>训练任务目标不同</strong></p>
<ul>
<li>bert [mask掉一个次, 在原位置把它预测出来]</li>
<li>gpt [预测下一个词]</li>
<li>chatglm [用gpt的方式来做bert的任务]</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a> ***<br><a href="https://baoyu.io/translations/llm/illustrated-transformer">图解 Transformer [译]</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/311156298">Transformer - Attention is all you need</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/410776234">超详细图解Self-Attention</a> ***</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a>  *** [架构]+训练+微调</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1gY4y1d7nk/">基于ChatGLM对话系统实战</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
</ol>
<p>2xx. <a href="https://www.bilibili.com/video/BV16h4y1W7us/">第一课：Transformer</a> ***  华为<br>2xx. <a href="https://bbycroft.net/llm">LLM Visualization</a> ***  可视化<br>2xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/127411638">Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT</a> *** </p>
<h3><span id="attention">Attention</span><a href="#attention" class="header-anchor">#</a></h3><p>1xx. <a href="https://blog.csdn.net/kkm09/article/details/120855658">李宏毅《深度学习》- Self-attention 自注意力机制</a><br>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/134228287">一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA</a></p>
<h3><span id="transformer-实现">Transformer 实现</span><a href="#transformer-实现" class="header-anchor">#</a></h3><p>1xx. <a href="http://arthurchiao.art/blog/transformers-from-scratch-zh/">Transformers from scratch</a> V, github 未<br>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/130090649">从零实现Transformer的简易版与强大版：从300多行到3000多行</a></p>
<h3><span id="位置编码">位置编码</span><a href="#位置编码" class="header-anchor">#</a></h3><p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/134085503">一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long</a></p>
<p>1xx.<a href="https://baoyu.io/translations/llm/the-random-transformer">深入解析随机 Transformer [译]</a> ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>向量数据库</title>
    <url>/www6vHomeHexo/2022/11/27/gptVectorStore/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#embedding">Embedding</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93">向量数据库</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F-7">向量数据库-索引方式 [7]</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%953">向量的相似度算法[3]</a><ul>
<li><a href="#%E6%AF%94%E8%BE%834">比较[4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="embedding">Embedding</span><a href="#embedding" class="header-anchor">#</a></h1><ul>
<li><p>example [5]</p>
<ul>
<li><strong>降维</strong>:   t-SNE  </li>
<li>K-Means 聚类</li>
<li>文本搜索  相似度搜索</li>
</ul>
</li>
<li><p>Embedding 价值 [6]</p>
<ul>
<li><strong>降维</strong><br>将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</li>
<li>捕捉语义信息<br>Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。</li>
<li>泛化能力<br>由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示</li>
</ul>
</li>
<li><p>应用 [6]</p>
<ul>
<li>语义表示和语义相似度</li>
<li>词语关系和类比推理</li>
<li>上下文理解</li>
<li>文本分类和情感分析</li>
<li>机器翻译和生成模型</li>
</ul>
</li>
<li><p>天梯榜<br><a href="https://huggingface.co/spaces/mteb/leaderboard">mteb&#x2F;leaderboard</a></p>
</li>
</ul>
<h1><span id="向量数据库">向量数据库</span><a href="#向量数据库" class="header-anchor">#</a></h1><ul>
<li><p>国产</p>
<ul>
<li>Milvus</li>
<li>Tencent </li>
<li>zilliz cloud</li>
</ul>
</li>
<li><p>国外</p>
<ul>
<li>Pinecone</li>
<li>FAISS<br>[ANN]</li>
<li>Chroma</li>
<li>Weaviate</li>
</ul>
</li>
</ul>
<h1><span id="向量数据库-索引方式-7">向量数据库-索引方式 [7]</span><a href="#向量数据库-索引方式-7" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/27/gptVectorStore/index.jpg" class>

<h1><span id="向量的相似度算法3">向量的相似度算法[3]</span><a href="#向量的相似度算法3" class="header-anchor">#</a></h1><ul>
<li>Cosine Similarity *<br>余弦</li>
<li>Dot Product *</li>
<li>Squared Euclidean (L2-Squared) *<br>欧式距离</li>
<li>Manhattan (L1 Norm or Taxicab Distance) *</li>
<li>Hamming *</li>
<li>ANN</li>
</ul>
<h3><span id="比较4">比较[4]</span><a href="#比较4" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>Similarity Metric</th>
<th>Vector properties considered</th>
</tr>
</thead>
<tbody><tr>
<td>Euclidean distance</td>
<td>Magnitudes and direction</td>
</tr>
<tr>
<td>Cosine similarity</td>
<td>Only direction</td>
</tr>
<tr>
<td>Dot product similarity</td>
<td>Magnitudes and direction</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/476025527">云原生向量数据库Milvus扫盲，看完这篇就够了</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/477231485">云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema</a></p>
</li>
<li><p><a href="https://weaviate.io/blog/distance-metrics-in-vector-search?ref=blog.langchain.dev">Distance Metrics in Vector Search</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/vector-similarity/">Vector Similarity Explained</a></p>
</li>
<li><p><a href="https://github.com/www6v/openai-quickstart/blob/main/openai_api/embedding.ipynb">embedding</a> git</p>
</li>
<li><p>《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding  </p>
</li>
<li><p><a href="https://www.modb.pro/db/1694527960317513728">向量数据库（第 1 部分）：每个数据库有何不同？</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2352088">微信向量检索分析一体化数仓探索：OLAP For Embedding</a> *** 未</p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/135311471">一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge</a> 未</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/646832642">Meta向量数据库Faiss介绍</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>向量数据库</category>
      </categories>
      <tags>
        <tag>向量数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>FinGTP</title>
    <url>/www6vHomeHexo/2022/11/24/gptFinGPT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<ul>
<li><p>Model:<br><a href="https://huggingface.co/FinGPT">https://huggingface.co/FinGPT</a></p>
</li>
<li><p>FinGPT-Forecaster:<br><a href="https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster">https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster</a></p>
</li>
<li><p>Github Repo:<br><a href="https://github.com/www6v/FinGPT">https://github.com/www6v/FinGPT</a></p>
</li>
<li><p>medium<br><a href="https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99">https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99</a></p>
</li>
<li><p>Paper<br>五篇paper</p>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>FinGTP</category>
      </categories>
      <tags>
        <tag>FinGTP</tag>
      </tags>
  </entry>
  <entry>
    <title>训练</title>
    <url>/www6vHomeHexo/2022/11/19/gptLargeModelTraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#training-pipeline0">Training Pipeline[0]</a><ul>
<li><a href="#%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0-2">设置训练参数 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%87%8F-2">参数量 vs 训练数据量 [2]</a></li>
</ul>
</li>
<li><a href="#pre-training">Pre-training</a><ul>
<li><a href="#pre-training-4">Pre-training [4]</a></li>
<li><a href="#tokenizer-%E5%88%86%E8%AF%8D">tokenizer 分词</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="training-pipeline0">Training Pipeline[0]</span><a href="#training-pipeline0" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/19/gptLargeModelTraining/bigModelTrainingPipeline.jpg" class>

<p><strong>模型训练分为四个阶段</strong> [2]</p>
<ul>
<li>预训练（Pretraining） –&gt;Base model  <ul>
<li>预训练技术<br>预训练本质上是⼀个⽆监督学习过程</li>
</ul>
</li>
<li>监督微调（Supervised Finetuning） –&gt; SFT model<br>核⼼原因还是在于需要“赋予”⼤模型更加定制化的功能</li>
<li>奖励建模（Reward Modeling）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<p><strong>三个角度解析</strong> [2]</p>
<ul>
<li>数据量：<strong>预训练</strong>阶段所需的<strong>数据量很大</strong>，但<strong>质量要求不高</strong>；而<strong>后面的三个阶段</strong>恰恰相反，需要的<strong>数据质量较高</strong>。</li>
<li>训练方法：<strong>预训练和监督微调</strong>的训练方法相同，都是<strong>预测下一个单词</strong>。奖励模型和强化学习的训练方法则不同。<strong>奖励模型</strong>是<strong>二元分类学习</strong>，而<strong>强化学习</strong>则鼓励模型生成奖励模型评分较高的回答。</li>
<li>训练所需资源：预训练阶段的资源消耗巨大，使用数千颗GPU，花费<strong>数月</strong>时间，占总训练时间的99%。后面的三个阶段只需使用数十颗GPU，训练时间约<strong>数天</strong>。</li>
</ul>
<h3><span id="设置训练参数-2">设置训练参数 [2]</span><a href="#设置训练参数-2" class="header-anchor">#</a></h3><p>设置训练参数，如batch-size、learning rate等</p>
<ul>
<li>预训练阶段的<strong>Batch Size非常大</strong>，范围在0.5M到4M之间。</li>
<li><strong>Learning rate设定较小</strong>，且随着网络规模的增大，Learning rate越来越小。</li>
</ul>
<h3><span id="参数量-vs-训练数据量-2">参数量 vs 训练数据量 [2]</span><a href="#参数量-vs-训练数据量-2" class="header-anchor">#</a></h3><p><strong>参数量并不是衡量模型能力的唯一标准，训练数据量也是一个非常重要的因素。</strong><br>LLaMA模型，尽管它的参数量只有650亿，但其性能与参数量为1750亿的GPT-3模型相比也非常优秀。主要原因在于，LLaMA模型的训练数据量达到了1.4万亿，而GPT-3只有3000亿。</p>
<h1><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor">#</a></h1><h3><span id="pre-training-4">Pre-training [4]</span><a href="#pre-training-4" class="header-anchor">#</a></h3><ul>
<li><p>⾃回归与⽣成式</p>
<ul>
<li><strong>⾃回归模型</strong>是⼀种序列模型，它在预测下⼀个输出时，会将之前的所有输出作为输⼊，然后<strong>根据统计规律、结合已经输⼊的样本</strong>，预测下个位置各单词出现的概率，然后输出概率最⼤的单词，类似于完形填空；</li>
<li><strong>⽣成式模型</strong>的预测过程和⾃回归模型类似，都是根据统<br>计规律预测下个单词的概率，所不同的是，<strong>⽣成式模型可以根据之前的样本的<br>概率分布⽣成下⼀个词，⽣成式模型预测时会存在⼀定的随机性；</strong></li>
</ul>
</li>
<li><p>GPT来说，就是⼀个⾃回归⽣成式模型 [4]<br>⼀个⾃回归⽣成式模型在进⾏预测的时候，<strong>会⾸先根据⾃回归模型，在参考到⽬前为⽌<br>已经⽣成的词的情况下确定下⼀个词的概率分布，然后再根据⽣成式的⽅式来根据这个<br>分布⽣成下⼀个词</strong></p>
</li>
</ul>
<h3><span id="tokenizer-分词">tokenizer 分词</span><a href="#tokenizer-分词" class="header-anchor">#</a></h3><ul>
<li>单词分词法</li>
<li>单字分词法</li>
<li>子词分词法<br>BPE [GPT系列], WordPiece</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a>  from ppt</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://techdiylife.github.io/big-model-training/deepspeed/LLM-state-of-GPT.html">大模型训练入门实战</a>  ***<br><a href="https://karpathy.ai/stateofgpt.pdf">State of GPT</a><br><a href="https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg">State of GPT：大神Andrej揭秘OpenAI大模型原理和训练过程 </a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
<li><p>大模型入门必看教程  九天Hector</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/458452872">NLP（二）：浅谈分词</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>Fine-Tuning 原理</title>
    <url>/www6vHomeHexo/2022/11/18/gptFineTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E7%B1%BB">分类</a></li>
<li><a href="#peft-%E5%88%86%E7%B1%BB">PEFT 分类</a></li>
<li><a href="#chatgpt-%E8%AE%AD%E7%BB%83-1">ChatGPT 训练  [1]</a></li>
<li><a href="#%E5%BE%AE%E8%B0%83%E6%8C%87%E4%BB%A4%E7%9A%84%E7%94%9F%E6%88%90-56">微调指令的生成 [5][6]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="分类">分类</span><a href="#分类" class="header-anchor">#</a></h1><ul>
<li><p>全量微调</p>
</li>
<li><p>局部微调</p>
<ul>
<li>PEFT(Parameter-Efficient Fine-Tuning)  PEFT</li>
</ul>
</li>
</ul>
<h1><span id="peft-分类">PEFT 分类</span><a href="#peft-分类" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/18/gptFineTuning/category.png" class>

<p>高效微调技术可以粗略分为以下三大类：增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R）。而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。</p>
<ul>
<li><p>PEFT</p>
<ul>
<li>[本质   基于有监督学习]</li>
</ul>
</li>
<li><p>PEFT(Parameter-Efficient Fine-Tuning)  PEFT</p>
<ul>
<li><p><strong>引入重参数化（R）</strong>    </p>
<ul>
<li><strong>LoRA</strong>: Low-Rank Adaptation of LLMs<br>LoRA   【 并联方式的外挂】 [效果比较好]</li>
<li>QLoRA: Efficient Finetuning of Quantized LLMs</li>
<li>AdaLoRA: Adaptive Budget Allocation for PEFT</li>
</ul>
</li>
<li><p>增加额外参数（A）</p>
<ul>
<li><strong>软提示（Soft prompts）</strong> <ul>
<li><strong>Prefix Tuning</strong><br>增加一个可被训练的Embedding层</li>
<li>Prompt Tuning</li>
<li><strong>P-Turning</strong></li>
</ul>
</li>
<li>Adapter-Tuning   【 串联方式的外挂】</li>
</ul>
</li>
<li><p>选取一部分参数更新（S）</p>
<ul>
<li>BitFit</li>
</ul>
</li>
<li><p>additive</p>
<ul>
<li>IA3</li>
</ul>
</li>
</ul>
</li>
<li><p>统一微调框架<br>  UniPELT</p>
</li>
</ul>
<img src="/www6vHomeHexo/2022/11/18/gptFineTuning/overview.jpg" class>

<h1><span id="chatgpt-训练-1">ChatGPT 训练  [1]</span><a href="#chatgpt-训练-1" class="header-anchor">#</a></h1><ul>
<li>基于人类反馈的强化学习微调技术 RLHF<ul>
<li>使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型<ul>
<li>Supervised fine-tuning (SFT)<br>&#x3D; Instruction Tuning</li>
</ul>
</li>
<li>训练奖励模型 Reward Model（RM）</li>
<li>使用强化学习算法微调语言模型<ul>
<li>RLHF<br>[本质  基于强化学习, 强化学习算法]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="微调指令的生成-56">微调指令的生成 [5][6]</span><a href="#微调指令的生成-56" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://shimo.im/docs/KlkKv4XQDouwWRqd/read">AI 大模型微调训练营大纲</a> </p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1t8411D7v4?p=8">大模型干货教程看这一个就够了~2023年全网最硬核最全面的大模型公开课|大模型微调 | ChatGLM | LangChain</a> ***</p>
</li>
<li><p><a href="https://github.com/www6v/llm-action#llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86">llm微调技术原理</a>  李国东<br>4.1 <a href="https://zhuanlan.zhihu.com/p/635152813">大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介</a></p>
<p>4.2  <a href="https://zhuanlan.zhihu.com/p/649755252">大模型参数高效微调技术原理综述（七）-最佳实践、总结</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/650596719">大模型SFT微调指令数据的生成</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/618334308">让ChatGPT生成训练ChatGPT的训练数据</a></p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/132116949">LLM高效参数微调方法：从Prefix Tuning、Prompt Tuning、P-Tuning V1&#x2F;V2到LoRA、QLoRA(含对模型量化的解释)</a> *** 未<br>1xx. <a href="https://aicarrier.feishu.cn/file/H1YvbRyacopEs6xzgZ8c9DDcnIh">大模型参数高效微调技术原理及实践</a> pdf<br>   <a href="https://www.bilibili.com/video/BV1qw411c7Hd/">如何高效微调大模型？技术原理与最佳实践揭秘！</a> V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Function Call</title>
    <url>/www6vHomeHexo/2022/11/16/gptFunctionCall/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><h3><span id="调用顺序-0-12">调用顺序  [0] [1][2]</span><a href="#调用顺序-0-12" class="header-anchor">#</a></h3><ul>
<li>Function Calling 整个功能的调用顺序大致如下<ul>
<li>声明函数：定义当前函数的名称，描述，以及对应的参数信息，并请求对应的接口；</li>
<li>解析函数参数：接受对应的接口返回，并解析对应的函数参数信息；</li>
<li>执行函数：根据对应的参数信息调用本地函数；</li>
<li>上报结果：将本地函数执行的结果上报给 Chat 接口；</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2022/11/16/gptFunctionCall/functioncall1.png" class>

<h3><span id="代码-2">代码 [2]</span><a href="#代码-2" class="header-anchor">#</a></h3><h3><span id="goal">goal</span><a href="#goal" class="header-anchor">#</a></h3><p> The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="http://lihuaxi.xjx100.cn/news/1382737.html">大模型开发(十一)：Chat Completions模型的Function calling功能详解</a> </p>
</li>
<li><p><a href="https://www.duidaima.com/Group/Topic/OtherTools/13709">如何使用Chat Completions接口的函数调用功能</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131892482">OpenAI开发系列（十一）：Function calling功能的实际应用流程与案例解析</a>   代码  流程图<br><a href="https://github.com/www6v/AIGC/tree/master/Function%20calling%E5%8A%9F%E8%83%BD%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%B5%81%E7%A8%8B%E4%B8%8E%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90">代码</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131933871">OpenAI开发系列（十三）：利用Function calling功能开发基于大模型的实时天气查询助手</a> 未</p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131912170">OpenAI开发系列（十二）：Function calling功能的流程优化与多轮对话实现</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Function Call</category>
      </categories>
      <tags>
        <tag>Function Call</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt Engineering</title>
    <url>/www6vHomeHexo/2022/11/10/gptPromptEngineering/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#basic-prompting-2">Basic Prompting [2]</a><ul>
<li><a href="#zero-shot-prompting-3">Zero-Shot Prompting [3]</a></li>
<li><a href="#few-shot-prompting-3">Few-Shot Prompting [3]</a></li>
</ul>
</li>
<li><a href="#cot-2">CoT [2]</a><ul>
<li><a href="#chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</a></li>
<li><a href="#self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</a></li>
<li><a href="#tree-of-thoughts-tot">Tree of Thoughts (ToT)</a></li>
<li><a href="#cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</a></li>
<li><a href="#tips-and-extensions-2">Tips and Extensions   [2]</a></li>
</ul>
</li>
<li><a href="#automatic-prompt-design-2">Automatic Prompt Design [2]</a></li>
<li><a href="#cot4">CoT[4]</a></li>
<li><a href="#six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</a><ul>
<li><a href="#write-clear-instructions">Write clear instructions</a></li>
<li><a href="#provide-reference-text">Provide reference text</a></li>
<li><a href="#split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</a></li>
<li><a href="#give-the-model-time-to-think">Give the model time to “think”</a></li>
<li><a href="#use-external-tools">Use external tools</a></li>
<li><a href="#test-changes-systematically">Test changes systematically</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E7%82%B9vs-%E7%BC%BA%E7%82%B9">优点vs 缺点</a><ul>
<li><a href="#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="basic-prompting-2">Basic Prompting [2]</span><a href="#basic-prompting-2" class="header-anchor">#</a></h1><h3><span id="zero-shot-prompting-3">Zero-Shot Prompting [3]</span><a href="#zero-shot-prompting-3" class="header-anchor">#</a></h3><h3><span id="few-shot-prompting-3">Few-Shot Prompting [3]</span><a href="#few-shot-prompting-3" class="header-anchor">#</a></h3><h1><span id="cot-2">CoT [2]</span><a href="#cot-2" class="header-anchor">#</a></h1><h3><span id="chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</span><a href="#chain-of-thought-promptingcot-3" class="header-anchor">#</a></h3><ul>
<li>Few-shot CoT</li>
<li>Zero-shot COT<br><strong>“Let’s think step by step”</strong></li>
</ul>
<h3><span id="self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</span><a href="#self-consistencycot-sc-3" class="header-anchor">#</a></h3><p>The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to <strong>select</strong> the most consistent answer.  </p>
<h3><span id="tree-of-thoughts-tot">Tree of Thoughts (ToT)</span><a href="#tree-of-thoughts-tot" class="header-anchor">#</a></h3><h3><span id="cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</span><a href="#cot-vs-cot-sc-vs-tot-3" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/11/10/gptPromptEngineering/TOT.jpg" class>

<h3><span id="tips-and-extensions-2">Tips and Extensions   [2]</span><a href="#tips-and-extensions-2" class="header-anchor">#</a></h3><p>Self-Ask </p>
<h1><span id="automatic-prompt-design-2">Automatic Prompt Design [2]</span><a href="#automatic-prompt-design-2" class="header-anchor">#</a></h1><ul>
<li>Automatic Chain-of-Thought (Auto-CoT) [3]</li>
</ul>
<h1><span id="cot4">CoT[4]</span><a href="#cot4" class="header-anchor">#</a></h1><ul>
<li><p>CoT(Chain of Thought)</p>
<ul>
<li>CoT-SC(Self Consistency)</li>
</ul>
</li>
<li><p>ToT(Tree of Thoughts)<br>分为了Thought Decomposition，Thought Generator，State Evaluator，Search algorithms</p>
</li>
<li><p>GoT(Graph of Thoughts)</p>
</li>
<li><p>AoT(Algorithm of Thoughts)</p>
</li>
</ul>
<h1><span id="six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</span><a href="#six-strategies-for-getting-better-results1" class="header-anchor">#</a></h1><h3><span id="write-clear-instructions">Write clear instructions</span><a href="#write-clear-instructions" class="header-anchor">#</a></h3><p>   清晰的指令</p>
<h3><span id="provide-reference-text">Provide reference text</span><a href="#provide-reference-text" class="header-anchor">#</a></h3><h3><span id="split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</span><a href="#split-complex-tasks-into-simpler-subtasks" class="header-anchor">#</a></h3><pre><code>复杂任务简单化
</code></pre>
<h3><span id="give-the-model-time-to-think">Give the model time to “think”</span><a href="#give-the-model-time-to-think" class="header-anchor">#</a></h3><p>   给模型时间去思考</p>
<h3><span id="use-external-tools">Use external tools</span><a href="#use-external-tools" class="header-anchor">#</a></h3><p>   使用外部工具</p>
<h3><span id="test-changes-systematically">Test changes systematically</span><a href="#test-changes-systematically" class="header-anchor">#</a></h3><h1><span id="优点vs-缺点">优点vs 缺点</span><a href="#优点vs-缺点" class="header-anchor">#</a></h1><h3><span id="优点">优点</span><a href="#优点" class="header-anchor">#</a></h3><p>简单  容易上手</p>
<h3><span id="缺点">缺点</span><a href="#缺点" class="header-anchor">#</a></h3><ul>
<li>上限有限  </li>
<li>模型适配<br>prompt要适配每个模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">Prompt engineering</a>  openai</li>
<li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">Prompt Engineering </a> paper</li>
<li><a href="https://www.promptingguide.ai/techniques">Prompt Engineering Guide</a> guide<br><a href="https://github.com/www6v/Prompt-Engineering-Guide">Prompt-Engineering-Guide </a> *** git</li>
<li><a href="https://zhuanlan.zhihu.com/p/654034193">2023年能够解决复杂问题的思维链技术：Cot，ToT，GoT，AoT</a><br>1xx. <a href="https://blog.langchain.dev/the-prompt-landscape/">The Prompt Landscape</a>  langchain<br>1xx. <a href="https://colab.research.google.com/github/comet-ml/comet-llm/blob/main/examples/CometLLM_Prompts.ipynb">CometLLM - suite of LLMOps tools - track and visualize LLM prompts and chains</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/671915693">大模型 PUA 指南：来自 Google Meta Microsoft 等大厂</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/632369186">NLP（十三）：Prompt Engineering 面面观</a><br>1xx. <a href="https://github.com/brexhq/prompt-engineering?tab=readme-ov-file"> prompt-engineering</a> git<br>1xx. <a href="https://finisky.github.io/chain-of-thought-prompting-summary/">Chain-of-Thought Prompting 简读 </a></li>
</ol>
<h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="200">
<li><a href="https://mp.weixin.qq.com/s/nXoZJ4xfgihA2mnBQ8EdIQ">运维大模型探索之 Text2PromQL 问答机器人 </a>     架构图， 最后两个重点总结   未</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent 原理</title>
    <url>/www6vHomeHexo/2022/11/02/gptAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9E%B6%E6%9E%84%E5%9B%BE">架构图</a><ul>
<li><a href="#%E7%BB%84%E4%BB%B6-6">组件 [6]</a></li>
<li><a href="#planning-6">Planning [6]</a></li>
<li><a href="#memory-6">Memory [6]</a></li>
<li><a href="#tool-use-6">Tool Use [6]</a></li>
</ul>
</li>
<li><a href="#patterns-3">Patterns [3]</a></li>
<li><a href="#agent%E5%88%86%E7%B1%BB-123">Agent分类 [1][2][3]</a><ul>
<li><a href="#example">Example</a><ul>
<li><a href="#hugginggpt">HuggingGPT</a></li>
<li><a href="#babyagi-aigc">BabyAGI [AIGC]</a></li>
<li><a href="#autogpt35">AutoGPT[3][5]</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7-4">问题和局限性 [4]</a></li>
<li><a href="#%E6%8C%91%E6%88%98-8">挑战 [8]</a><ul>
<li><a href="#%E5%A6%82%E4%BD%95%E8%AE%A9-agent-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E5%B7%A5%E5%85%B7">如何让 agent 选择合适的工具</a></li>
<li><a href="#%E4%B8%8D%E5%BF%85%E8%A6%81%E7%9A%84%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8">不必要的工具使用</a></li>
<li><a href="#agent-%E8%BF%94%E5%9B%9E%E7%9A%84%E6%A0%BC%E5%BC%8F%E4%B8%8D%E7%A8%B3%E5%AE%9A">Agent 返回的格式不稳定</a></li>
<li><a href="#%E8%AE%B0%E4%BD%8F%E4%B9%8B%E5%89%8D%E7%9A%84%E6%93%8D%E4%BD%9C%E9%81%BF%E5%85%8D%E9%87%8D%E5%A4%8D">记住之前的操作，避免重复</a></li>
<li><a href="#%E5%A4%84%E7%90%86%E8%B6%85%E9%95%BF%E7%9A%84-observation">处理超长的 observation</a></li>
<li><a href="#%E4%B8%93%E6%B3%A8%E4%BA%8E%E7%9B%AE%E6%A0%87">专注于目标</a></li>
<li><a href="#%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BC%B0">结果评估</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#planning">Planning</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="架构图">架构图</span><a href="#架构图" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/02/gptAgent/agent-overview.jpg" class>

<h3><span id="组件-6">组件  [6]</span><a href="#组件-6" class="header-anchor">#</a></h3><p>Agent &#x3D; LLM + plan[规划能力] + memory[记忆能力] +Tools[工具使用能力] </p>
<h3><span id="planning-6">Planning [6]</span><a href="#planning-6" class="header-anchor">#</a></h3><ul>
<li><p>Task Decomposition</p>
<ul>
<li>CoT </li>
<li>ToT</li>
</ul>
</li>
<li><p>Self-Reflection</p>
<ul>
<li>ReAct [20]</li>
<li>Reflexion [21][22]</li>
<li>Chain of Hindsight</li>
</ul>
</li>
</ul>
<h3><span id="memory-6">Memory [6]</span><a href="#memory-6" class="header-anchor">#</a></h3><ul>
<li>Types of Memory<ul>
<li><strong>Sensory memory</strong> as learning <strong>embedding representations for raw inputs, including text, image or other modalities</strong>;</li>
<li><strong>Short-term memory</strong> as <strong>in-context learning</strong>. It is short and finite, as it is restricted by the finite context window length of Transformer.</li>
<li><strong>Long-term memory</strong> as the external <strong>vector store</strong> that the agent can attend to at query time, accessible via fast retrieval.</li>
</ul>
</li>
</ul>
<h3><span id="tool-use-6">Tool Use [6]</span><a href="#tool-use-6" class="header-anchor">#</a></h3><ul>
<li><p>让 agent 选择合适的工具 [8]</p>
<ul>
<li>可以 retrieve 相关示例来做 <strong>few-shot prompt</strong>。</li>
<li>也可以进一步 <strong>fine tune 特定模型</strong>，例如之前的 Toolformer。</li>
</ul>
</li>
<li><p>Research</p>
<ul>
<li><strong>TALM</strong> (Tool Augmented Language Models; Parisi et al. 2022) [6]</li>
<li><strong>Toolformer</strong> (Schick et al. 2023)   [6]</li>
<li><strong>Gorilla</strong> [8]</li>
</ul>
</li>
<li><p>Production  [6]</p>
<ul>
<li>ChatGPT <strong>Plugins</strong> </li>
<li>OpenAI API <strong>function calling</strong></li>
</ul>
</li>
</ul>
<h1><span id="patterns-3">Patterns  [3]</span><a href="#patterns-3" class="header-anchor">#</a></h1><ul>
<li><p>ReACT 范式 [20]<br>把<strong>融合了Reasoning和Acting</strong>的一种范式，推理过程是浅显易懂，仅仅<strong>包含thought-action-observation步骤</strong>，很容易判断推理的过程的正确性，使用ReAct做决策甚至超过了强化学习.  </p>
<ul>
<li>chain-of-thought推理-问题<br> 事实幻想（fact hallucination）和错误传递（error propagation）</li>
</ul>
</li>
<li><p>Self-ask<br>Self-ask是一种follow-up的使用范式，仅仅包含follow-up, immediate answer步骤，至于follow-up多少个step，完全由它自己决定，估计这就是Self-ask的名字的由来。</p>
</li>
<li><p>Plan-and-execute agents<br>本质上是先计划再执行，即先把用户的问题分解成一个个的子任务，然后再执行各个子任务，最后合并输出得到结果</p>
</li>
</ul>
<h1><span id="agent分类-123">Agent分类 [1][2][3]</span><a href="#agent分类-123" class="header-anchor">#</a></h1><ul>
<li><p>Action agents  </p>
<ul>
<li>Function Call</li>
<li>ReACT<br>Thought: xxx<br>Action: xxx<br>Observation: xxx</li>
</ul>
</li>
<li><p>Simulation agents<br>  生成式智能体， CAMEL，  Generative Agents</p>
</li>
<li><p>Automomous Agent<br>  <strong>AutoGPT</strong>， <strong>BabyAGI</strong>,  <strong>AutoGen</strong><br>  <strong>MetaGPT</strong></p>
</li>
<li><p>跨模态Agents<br>  HuggingGPT</p>
</li>
<li><p>ChatDev， AutoGen</p>
</li>
</ul>
<h2><span id="example">Example</span><a href="#example" class="header-anchor">#</a></h2><h3><span id="hugginggpt">HuggingGPT</span><a href="#hugginggpt" class="header-anchor">#</a></h3><h3><span id="babyagi-aigc">BabyAGI  [AIGC]</span><a href="#babyagi-aigc" class="header-anchor">#</a></h3><p>Plan-and-execute agents<br>The <strong>planning</strong> is almost always done <strong>by an LLM</strong>.<br>The <strong>execution</strong> is usually done by a <strong>separate agent (equipped with tools)</strong>.</p>
<h3><span id="autogpt35">AutoGPT[3][5]</span><a href="#autogpt35" class="header-anchor">#</a></h3><p>AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下</p>
<ol>
<li>AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals </li>
<li>Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如<code>browser_website(url = &quot;www.baidu.com&quot;)</code> )</li>
<li>这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，</li>
<li>回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）</li>
</ol>
<h1><span id="问题和局限性-4">问题和局限性 [4]</span><a href="#问题和局限性-4" class="header-anchor">#</a></h1><ul>
<li><p>记忆召回问题<br>只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好</p>
</li>
<li><p>错误累积问题</p>
</li>
<li><p>探索效率问题<br>中途引入人工的判断干预和反馈输入</p>
</li>
<li><p>任务终止与结果验证<br>模型 agent 的工作如何终止也是一个挑战</p>
</li>
</ul>
<h1><span id="挑战-8">挑战 [8]</span><a href="#挑战-8" class="header-anchor">#</a></h1><h3><span id="如何让-agent-选择合适的工具">如何让 agent 选择合适的工具</span><a href="#如何让-agent-选择合适的工具" class="header-anchor">#</a></h3><ul>
<li>Toolformer - fine tune</li>
<li>Gorilla - retrieval，fine tune</li>
</ul>
<h3><span id="不必要的工具使用">不必要的工具使用</span><a href="#不必要的工具使用" class="header-anchor">#</a></h3><p>“Human Input”也写成一种工具，让模型来主动发起对人类的提问<br><a href="https://python.langchain.com/docs/integrations/tools/human_tools">Human as a tool</a></p>
<h3><span id="agent-返回的格式不稳定">Agent 返回的格式不稳定</span><a href="#agent-返回的格式不稳定" class="header-anchor">#</a></h3><p>这里常见的做法是让 LLM <strong>按照 json 这类常见的 schema 来返回</strong>，一般稳定性会高一些（相比“Action:”这种）。<br>此外自动修复重试也很实用，可以利用 LangChain 里的 <strong>output parsers</strong> 来帮助完成。</p>
<h3><span id="记住之前的操作避免重复">记住之前的操作，避免重复</span><a href="#记住之前的操作避免重复" class="header-anchor">#</a></h3><p>AutoGPT - retrieval 结合近期操作记录</p>
<h3><span id="处理超长的-observation">处理超长的 observation</span><a href="#处理超长的-observation" class="header-anchor">#</a></h3><p>需要用一些工具从中<strong>提取有用信息</strong>，或者<strong>放到外部存储中再借助 retrieval 来使用</strong>。</p>
<h3><span id="专注于目标">专注于目标</span><a href="#专注于目标" class="header-anchor">#</a></h3><p>简单的做法是<strong>在 prompt 结尾处再把目标加上</strong>，引起 agent 的注意。<br>另外像 BabyAGI，HuggingGPT 这种把 <strong>planning 和 execution 分开</strong>的做法也是很有用。<strong>拆分的比较细</strong>的任务往往步骤比较短，也不容易丢失目标。</p>
<h3><span id="结果评估">结果评估</span><a href="#结果评估" class="header-anchor">#</a></h3><ul>
<li><strong>评估最终结果</strong>是否正确</li>
<li><strong>过程的细化评估</strong><ul>
<li>选择的中间步骤是否正确。</li>
<li>生成 action 的 input 是否正确。</li>
<li>生成的步骤序列是否合理高效。</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>公开课</li>
<li>公开课</li>
<li><a href="https://zhuanlan.zhihu.com/p/642357544">2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用</a> ***  论文+代码</li>
<li><a href="https://zhuanlan.zhihu.com/p/622947810">AutoGPT与LLM Agent解析</a> *** </li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> git<br><a href="https://link.zhihu.com/?target=https://godmode.space/">带界面的 AutoGPT 产品</a></li>
<li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents </a> paper </li>
<li>xxx</li>
<li><a href="https://zhuanlan.zhihu.com/p/633033220">LLM 全栈开发指南补遗</a>  Agents  ***<br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/">Harrison Chase: Agents</a>  ***</li>
</ol>
<h3><span id="planning">Planning</span><a href="#planning" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://react-lm.github.io/">ReAct: Synergizing Reasoning and Acting in Language Models</a> paper</li>
<li><a href="https://zhuanlan.zhihu.com/p/639254455">【论文阅读】Reflexion: 大模型如何从错误经验中学习？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/671508578">Reflexion: 带言语强化学习的语言智体</a><br>2xx. <a href="https://zhuanlan.zhihu.com/p/671491031">ReWOO: 高效增强语言模型中解偶观测和推理</a></li>
</ol>
<p>3xx. <a href="https://zhuanlan.zhihu.com/p/678203245">智体AI在多模态交互领域的综述（上）</a><br>3xx. <a href="https://zhuanlan.zhihu.com/p/678222381">智体AI在多模态交互领域的综述（下）</a><br>3xx. <a href="https://zhuanlan.zhihu.com/p/678238642">个人LLM智体的综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain</title>
    <url>/www6vHomeHexo/2022/11/02/gptLangchain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#modules">Modules</a><ul>
<li><a href="#main-modules">main modules</a><ul>
<li><a href="#model-io">Model I&#x2F;O</a></li>
<li><a href="#retrieval">Retrieval</a></li>
<li><a href="#agent">Agent</a></li>
</ul>
</li>
<li><a href="#additional-modules">Additional modules</a><ul>
<li><a href="#chains">Chains</a></li>
<li><a href="#memory-10">Memory [10]</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#function-call">Function Call</a></li>
<li><a href="#%E5%BA%94%E7%94%A84">应用[4]</a></li>
<li><a href="#chains-1-89">Chains [1] [8][9]</a></li>
<li><a href="#templates7">Templates[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="modules">Modules</span><a href="#modules" class="header-anchor">#</a></h1><h2><span id="main-modules">main modules</span><a href="#main-modules" class="header-anchor">#</a></h2><h3><span id="model-ix2fo">Model I&#x2F;O</span><a href="#model-ix2fo" class="header-anchor">#</a></h3><ul>
<li>Language models  [10]        <ul>
<li>LLM</li>
<li>Chat Model</li>
<li><strong>Embedding</strong></li>
</ul>
</li>
<li>Prompts <ul>
<li>Prompt Template</li>
<li>Few-shot example</li>
<li>Example Selectors [类比选择]<br>关键字  相似度  长度</li>
</ul>
</li>
<li>Output parsers</li>
<li><strong>function call</strong>[2]</li>
</ul>
<h3><span id="retrieval">Retrieval</span><a href="#retrieval" class="header-anchor">#</a></h3><ul>
<li>Document Loaders</li>
<li>Text Splitters</li>
<li><strong>Retrievers</strong>[10]</li>
<li>VectorStores</li>
<li>index</li>
</ul>
<h3><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h3><ul>
<li>Plan-and-execute agents</li>
</ul>
<h2><span id="additional-modules">Additional modules</span><a href="#additional-modules" class="header-anchor">#</a></h2><h3><span id="chains">Chains</span><a href="#chains" class="header-anchor">#</a></h3><ul>
<li>2大类<ul>
<li>Chain interface[Legacy]</li>
<li>LangChain Expression Language (LCEL)<br>LCEL is a declarative way to compose chains.</li>
</ul>
</li>
<li>Foundational<ul>
<li>LLM</li>
<li>Sequential- SequentialChain</li>
<li><strong>Router</strong></li>
<li>Transformation</li>
</ul>
</li>
</ul>
<h3><span id="memory-10">Memory [10]</span><a href="#memory-10" class="header-anchor">#</a></h3><ul>
<li>帮语言模型补充上下文</li>
<li>ConversationBufferMemory</li>
<li>ConversationBufferWindowMemory<br>窗口</li>
<li>ConversationSummaryMemory</li>
<li>VectorStoreRetrieverMemory</li>
</ul>
<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.base <span class="keyword">import</span> (</span><br><span class="line">    create_openai_fn_chain,</span><br><span class="line">    create_structured_output_chain,[<span class="number">2</span>]</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.citation_fuzzy_match <span class="keyword">import</span> (</span><br><span class="line">    create_citation_fuzzy_match_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.extraction <span class="keyword">import</span> (</span><br><span class="line">    create_extraction_chain,</span><br><span class="line">    create_extraction_chain_pydantic,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.qa_with_structure <span class="keyword">import</span> (</span><br><span class="line">    create_qa_with_sources_chain,</span><br><span class="line">    create_qa_with_structure_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.tagging <span class="keyword">import</span> (</span><br><span class="line">    create_tagging_chain,</span><br><span class="line">    create_tagging_chain_pydantic,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h1><span id="应用4">应用[4]</span><a href="#应用4" class="header-anchor">#</a></h1><ul>
<li>Question &amp; Answering Using Documents As Context[3]</li>
<li>Extraction[Kor]</li>
<li>Evaluation</li>
<li>Querying Tabular Data[sqlite]</li>
<li>Code Understanding</li>
<li>Interacting with APIs</li>
<li>Chatbots</li>
</ul>
<h1><span id="chains-1-89">Chains [1] [8][9]</span><a href="#chains-1-89" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;stuff&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;map_reduce&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;refine&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_qa_chain(llm, chain_type=<span class="string">&quot;map_rerank&quot;</span>, verbose=<span class="literal">True</span>, return_intermediate_steps=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>链类型</th>
<th>整合方法</th>
<th>优缺点</th>
</tr>
</thead>
<tbody><tr>
<td>stuff</td>
<td>将所有内容放入一个提示中，输入LLM</td>
<td>简单、廉价、效果好&#x2F; 对输入文本有一定token限制</td>
</tr>
<tr>
<td>Map_reduce</td>
<td>每个问题和文本块单独给语言模型，并将答案汇总生成最终结果</td>
<td>输入任意数量文本，且并行处理&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Refine</td>
<td>迭代处理多个文本，基于前一个文档答案构建下一个答案</td>
<td>用于组合信息，依次构建答案&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Map_rerank</td>
<td>每个文档单独调用LLM,并要求返回一个得分，然后选择最高的得分</td>
<td>需要告诉模型评分的规则&#x2F; 费token</td>
</tr>
</tbody></table>
<img src="/www6vHomeHexo/2022/11/02/gptLangchain/chains-type.jpg" class>


<h1><span id="templates7">Templates[7]</span><a href="#templates7" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/gkamradt/langchain-tutorials">https://github.com/gkamradt/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-functioncall.py">functioncall</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-qaOnDoc.py">qaOnDoc</a></p>
</li>
<li><p><a href="https://github.com/www6v/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb">LangChain Cookbook Part 2: Use Cases</a><br> 10.公开课</p>
</li>
<li><p><a href="https://github.com/kyrolabs/awesome-langchain">https://github.com/kyrolabs/awesome-langchain</a></p>
</li>
<li><p><a href="https://github.com/Crossme0809/langchain-tutorials">https://github.com/Crossme0809/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/templates/docs/INDEX.md">Templates</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/666656208">吴恩达短课_LangChain</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651216604">精华笔记：吴恩达 x LangChain 《使用LangChain构建与数据对话的聊天机器人》（下）</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2313918">一文入门最热的LLM应用开发框架LangChain</a> 未</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2331337">大模型LangChain框架基础与使用示例</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 原理</title>
    <url>/www6vHomeHexo/2022/11/02/gptRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rag-overview2">RAG Overview[2]</a></li>
<li><a href="#advanced-rag">Advanced RAG</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
<li><a href="#rag-fusion">RAG Fusion</a></li>
</ul>
</li>
<li><a href="#rag-vs-ft-2">RAG vs FT [2]</a></li>
<li><a href="#self-rag-3">Self-RAG [3]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag35">多模态+RAG[3][5]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
<li><a href="#self-rag">Self-RAG</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81">多模态</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="rag-overview2">RAG Overview[2]</span><a href="#rag-overview2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/02/gptRAG/rag-overview.jpg" class>

<h1><span id="advanced-rag">Advanced RAG</span><a href="#advanced-rag" class="header-anchor">#</a></h1><h3><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h3><ul>
<li>离线 index</li>
<li>在线 查询</li>
</ul>
<img src="/www6vHomeHexo/2022/11/02/gptRAG/rag.jpg" class>

<h3><span id="rag-fusion">RAG Fusion</span><a href="#rag-fusion" class="header-anchor">#</a></h3><h1><span id="rag-vs-ft-2">RAG vs FT [2]</span><a href="#rag-vs-ft-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/11/02/gptRAG/rag-vs-ft.jpg" class>

<h1><span id="self-rag-3">Self-RAG [3]</span><a href="#self-rag-3" class="header-anchor">#</a></h1><p>Self-RAG 则是更加主动和智能的实现方式，主要步骤概括如下：</p>
<ol>
<li>判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回</li>
<li>平行处理每个片段：生产prompt+一个片段的生成结果</li>
<li>使用**反思字段(Reflection tokens)**，检查输出是否相关，选择最符合需要的片段；</li>
<li>再重复检索</li>
<li>生成结果会引用相关片段，以及输出结果是否符合该片段，便于查证事实。</li>
</ol>
<h1><span id="多模态rag35">多模态+RAG[3][5]</span><a href="#多模态rag35" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.langchain.dev/deconstructing-rag/">Deconstructing RAG</a> ***</li>
</ol>
<h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol start="2">
<li><a href="https://zhuanlan.zhihu.com/p/673910600">LLM之RAG理论（二）| RAG综述论文详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> ***</li>
</ol>
<h3><span id="self-rag">Self-RAG</span><a href="#self-rag" class="header-anchor">#</a></h3><ol start="4">
<li><a href="https://github.com/www6v/self-rag">original implementation of SELF-RAG</a></li>
</ol>
<h3><span id="多模态">多模态</span><a href="#多模态" class="header-anchor">#</a></h3><ol start="5">
<li><a href="https://zhuanlan.zhihu.com/p/665078079">万字综述：2023年多模态检索增强生成技术(mRAG)最新进展与趋势-图片、代码、图谱、视频、声音、文本</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673922981">高级检索增强生成技术(RAG)全面指南：原理、分块、编码、索引、微调、Agent、展望</a> 未<br>1xx. <a href="https://baoyu.io/translations/ai-paper/2005.11401-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">知识密集型自然语言处理任务的检索增强生成技术研究 [译]</a><br>1xx. <a href="https://baoyu.io/translations/rag/mastering-rag-how-to-architect-an-enterprise-rag-system">构建企业级 RAG 系统的高级指南 [译]</a><br>1xx. <a href="https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey">面向大语言模型的检索增强生成技术：综述 [译]</a><br>1xx. <a href="https://baoyu.io/translations/ai-paper/2401.05856v1-seven-failure-points-when-engineering-a-retrieval-augmented-generation-system">在构建检索增强型生成系统时的七大挑战 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型</title>
    <url>/www6vHomeHexo/2022/10/30/gptLargeModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><p>1xx. <a href="http://arthurchiao.art/blog/llm-practical-guide-zh/">[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）</a>   实战  未<br>1xx. <a href="https://zhuanlan.zhihu.com/p/597586623">通向AGI之路：大型语言模型（LLM）技术精要</a> *** 未<br>1xx. <a href="http://aibox.ruc.edu.cn/docs/2023-08/cb9badcb213f4c8b89d00d579eed4a4c.pdf">大语言模型综述</a> 中文  v10<br>     <a href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf">大语言模型综述</a> 中文<br>     <a href="https://arxiv.org/pdf/2303.18223.pdf">A Survey of Large Language Models</a> 英文<br>     <a href="https://github.com/www6v/LLMSurvey">LLMSurvey</a>  github<br>     <a href="https://zhuanlan.zhihu.com/p/630203554">[论文]大语言模型综述</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/640784855">[Transformer 101系列] 初探LLM基座模型</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/664046612">LLM从0开始预训练系列：2、大模型技术报告总结（GPT&#x2F;PaLM&#x2F;GLM&#x2F;LLaMA&#x2F;Skywork）</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671710012">高效大语言模型：综述</a>  *** 大模型各个维度的优化</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-Direct Connect</title>
    <url>/www6vHomeHexo/2022/10/30/awsNetworkDX/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="direct-connect12">Direct Connect[1][2]</span><a href="#direct-connect12" class="header-anchor">#</a></h2><ul>
<li><p>Virtual Interfaces </p>
<ul>
<li>Public VIF<br>公共 VIF 使您的网络能够访问所有区域（中国除外）的 AWS 全球骨干网络上的所有 AWS 公共 IP 地址。</li>
<li>Private VIF<br>私有 VIF 使您的网络能够通过其私有 IP 地址访问已在您的虚拟私有云 (VPC) 中配置的资源。</li>
</ul>
</li>
<li><p>高可用<br>[常规做法: DX + VPN]</p>
</li>
<li><p>双向转发检测 (BFD)<br>DR</p>
</li>
<li><p>Billing</p>
<ul>
<li>两个主要成本组成部分<ul>
<li>所有 AWS Direct Connect 位置的每端口小时定价和 [使用时长]</li>
<li>AWS Direct Connect 位置和 AWS 区域的数据传出费用  [数据传输的量]</li>
</ul>
</li>
</ul>
</li>
<li><p>Direct Connect gateway[3][4]<br>Virtual private gateway associations<br>private VIF that references the Gateway and the Connection</p>
<img src="/www6vHomeHexo/2022/10/30/awsNetworkDX/directConnectionGateway.JPG" class title="Direct Connect gateway"></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/531166462">Chapter5 AWS Direct Connect</a> </li>
<li>[SAP-1] Direct Connect Section</li>
<li><a href="https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/">New – AWS Direct Connect Gateway – Inter-Region VPC Access</a></li>
<li><a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html">Working with Direct Connect gateways</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-VPC</title>
    <url>/www6vHomeHexo/2022/10/30/awsNetworkVPC/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="vpc-概念和组件123">VPC 概念和组件[1][2][3]</span><a href="#vpc-概念和组件123" class="header-anchor">#</a></h2><ul>
<li><p>Subnet &amp; CIDR<br><a href="https://www.bilibili.com/video/BV1Ff4y1S7Lf/">010-计算机网络-无分类编址CIDR</a><br><a href="https://network00.com/NetworkTools/IPv4SubnetCreator/">Tool</a></p>
</li>
<li><p>Routing Table<br>[实际中没有看到路由器,只有路由表]<br><a href="https://help.aliyun.com/document_detail/106224.html">路由表概述</a> 阿里云 有例子</p>
</li>
<li><p>Security Groups &amp; Network ACLs [1]</p>
<ul>
<li>Security Groups<br>EC2 level-工作在EC2 instance level<br>stateful firewall</li>
<li>Network ACLs<br>工作在subnet level<br>stateless firewall</li>
</ul>
</li>
<li><p>NAT Gateway &amp;&amp; NAT Instance [1]</p>
<ul>
<li>NAT Gateway<br>[通过internet gateway,访问外网, 外网访问不到内部, 多对一]</li>
<li>NAT Instance</li>
</ul>
</li>
<li><p>Internet Gateway<br>[一对一的, 静态ip绑定到internet gateway, 外网访问内网的EC2]</p>
</li>
<li><p>弹性网络接口ENI[2]</p>
<ul>
<li>弹性网络接口必须有一个主要的私有 IPv4 地址，并且始终与至少一个安全组相关联。</li>
<li>弹性网络接口可以在运行时（热连接）、停止时（热连接）或启动时（冷连接）连接到实例。</li>
<li>不能分离主网络接口。</li>
<li>弹性网络接口仅限于单个可用区。</li>
</ul>
</li>
<li><p>IP 寻址</p>
</li>
<li><p>仅出口 Internet 网关 (EIGW)</p>
</li>
<li><p>虚拟专用网关 (VGW)、客户网关和VPN</p>
</li>
<li><p>VPC peering</p>
</li>
<li><p>归置组</p>
</li>
<li><p>DNS 服务器</p>
</li>
</ul>
<h2><span id="bring-your-own-ip-addresses-byoip">Bring your own IP addresses (BYOIP)</span><a href="#bring-your-own-ip-addresses-byoip" class="header-anchor">#</a></h2><p><strong>Bring Your Own IP (BYOIP) enables customers to move all or part of their existing publicly routable IPv4 or IPv6 address space to AWS for use with their AWS resources.</strong> Customers will continue to own the IP range. Customers can create Elastic IPs from the IPv4 space they bring to AWS and use them with EC2 instances, NAT Gateways, and Network Load Balancers. Customers can also associate up to 5 CIDRs to a VPC from the IPv6 space they bring to AWS. Customers will continue to have access to Amazon-supplied IPs and can choose to use BYOIP Elastic IPs, Amazon-supplied IPs, or both.</p>
<p>参考<br><a href="https://aws.amazon.com/cn/vpc/faqs/">Amazon VPC 常见问题</a>  *** BYOIP<br><a href="https://aws.amazon.com/cn/blogs/networking-and-content-delivery/introducing-bring-your-own-ip-byoip-for-amazon-vpc/">Introducing Bring Your Own IP (BYOIP) for Amazon VPC</a> ***  50%<br><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html">Bring your own IP addresses (BYOIP) in Amazon EC2</a><br>[Practice Set 1] 32题</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>[SAP-1] VPC Section *** </li>
<li><a href="https://zhuanlan.zhihu.com/p/529181222">Chapter 2-Amazon Virtual Private Cloud (Amazon VPC) and Networking Fundamentals</a> *** </li>
<li><a href="https://www.bilibili.com/video/BV1CG41137bx/">【云计算】AWS高级网络.LAB1.1.vpc_peering</a></li>
<li><a href="https://jayendrapatil.com/aws-virtual-private-cloud-vpc/">VPC</a> ***  未<br><a href>UCloud 陈煌栋-UCloud VPC的技术演进之路</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>网络空间安全-Cyber Security</title>
    <url>/www6vHomeHexo/2022/10/23/cyberSecurity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h2><p><a href="https://www.doc88.com/p-69916034297662.html?r=1">基于CiteSpace的国内外网络空间安全研究综述</a> ***</p>
<h2><span id="web安全">Web安全</span><a href="#web安全" class="header-anchor">#</a></h2><ul>
<li><p>Web安全[6]</p>
<img src="/www6vHomeHexo/2022/10/23/cyberSecurity/web-security.JPG" class title="Web安全">
</li>
<li><p>OWASP TOP 10[2]</p>
<ul>
<li>失效的访问控制<ul>
<li>提权-root</li>
</ul>
</li>
<li>加密失败<ul>
<li>弱随机数生成器</li>
<li>忘记加“盐”</li>
</ul>
</li>
<li>注入  <ul>
<li>SQL注入</li>
<li>命令注入</li>
<li>XSS</li>
</ul>
</li>
<li>安全配置错误</li>
</ul>
</li>
</ul>
<h2><span id="linux安全3">Linux安全[3]</span><a href="#linux安全3" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/10/23/cyberSecurity/linux-security.JPG" class title="linux安全">


<h2><span id="攻击模型">攻击模型</span><a href="#攻击模型" class="header-anchor">#</a></h2><p><a href="https://www.doc88.com/p-38973089899040.html">网络攻击模型研究综述</a><br>ATT&amp;CK模型</p>
<h2><span id="攻击手段-8">攻击手段 [8]</span><a href="#攻击手段-8" class="header-anchor">#</a></h2><ul>
<li>漏洞利用 <ul>
<li>SQL 注入漏洞</li>
<li>跨站漏洞 </li>
<li>授权验证绕过漏洞 </li>
<li>权限提升漏洞</li>
</ul>
</li>
<li>口令爆破 </li>
<li>钓鱼攻击 <ul>
<li>内网钓鱼</li>
<li>外网钓鱼</li>
</ul>
</li>
<li>供应链攻击 </li>
<li>VPN仿冒接入  </li>
<li>近源攻击     </li>
<li>DDoS[1] +<ul>
<li>CC攻击</li>
<li>HTTP慢速攻击</li>
</ul>
</li>
<li>MITM 中间人</li>
<li>DNS欺骗</li>
<li>勒索软件-木马</li>
</ul>
<h2><span id="漏洞防御与渗透测试">漏洞防御与渗透测试</span><a href="#漏洞防御与渗透测试" class="header-anchor">#</a></h2><h2><span id="安全防御工具">安全防御工具</span><a href="#安全防御工具" class="header-anchor">#</a></h2><a href="/www6vHomeHexo/2022/03/12/cyberSecurityTool/" title="安全产品">安全产品</a>



<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://wenku.baidu.com/view/7f2c9810c8aedd3383c4bb4cf7ec4afe05a1b14c?fr=xueshu">基于Web应用层的DDoS攻击模型研究</a> *</p>
</li>
<li><p>《Web 漏洞挖掘实战》  王昊天</p>
</li>
<li><p>《13 | Linux系统安全：多人共用服务器，如何防止别人干“坏事”？》  何为舟</p>
</li>
<li><p>xxx</p>
</li>
<li><p>xxx</p>
</li>
<li><p>《模块串讲（一）丨Web安全：如何评估用户数据和资产数据面临的威胁？》 何为舟</p>
</li>
<li><p>xxx</p>
</li>
<li><p>《红蓝攻防》</p>
</li>
<li><p><a href="https://tech.meituan.com/2021/04/08/threat-modeling-security.html">实践之后，我们来谈谈如何做好威胁建模</a>  美团  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>安全</category>
        <category>网络空间安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S高可用-零停机[探针]</title>
    <url>/www6vHomeHexo/2022/10/22/k8sAvailableHealth/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5">健康检查</a><ul>
<li><a href="#liveness-probe-2">Liveness Probe [2]</a></li>
<li><a href="#readiness-probe-2">Readiness Probe  [2]</a></li>
<li><a href="#startupprobe-4">startupProbe [4]</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E9%9B%85%E7%BB%88%E6%AD%A2-5">优雅终止 [5]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-6">代码 [6]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="健康检查">健康检查</span><a href="#健康检查" class="header-anchor">#</a></h1><h3><span id="liveness-probe-2">Liveness Probe [2]</span><a href="#liveness-probe-2" class="header-anchor">#</a></h3><p><strong>确定何时重启容器</strong>. 例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去。<br><strong>liveness的初始值为成功。</strong></p>
<h3><span id="readiness-probe-2">Readiness Probe  [2]</span><a href="#readiness-probe-2" class="header-anchor">#</a></h3><p><strong>确定容器是否已经就绪可以接受流量.</strong> 该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。<br><strong>readiness的初始值为失败。</strong></p>
<h3><span id="startupprobe-4">startupProbe [4]</span><a href="#startupprobe-4" class="header-anchor">#</a></h3><p>启动检查, 使用启动探针检测容器应用程序是否已经启动<br>对于较新的（≥v1.16）Kubernetes 集群，如果是具有<strong>不可预测或可变启动时间</strong>的应用程序应使用 startup 探针。</p>
<p><strong>只运行一次。</strong></p>
<ul>
<li>探针类型<br>httpGet: 指定端口和路径执行 HTTP GET 请求<br>tcpSocket: 对容器的 IP 地址上的指定端口执行 TCP 检查<br>命令,exec: 在容器内执行指定命令</li>
</ul>
<h1><span id="优雅终止-5">优雅终止 [5]</span><a href="#优雅终止-5" class="header-anchor">#</a></h1><p>  系统底层默认会向主进程发送 SIGTERM 信号，而对剩余子进程发送 SIGKILL 信号。系统这样做的大概原因是因为大家在设计主进程脚本的时候都不会进行信号的捕获和传递，这会导致容器关闭时，多个子进程无法被正常终止，所以系统使用 SIGKILL 这个不可屏蔽信号，而是为了能够在没有任何前提条件的情况下，能够把容器中所有的进程关掉。</p>
<p>  也就是说如果主进程自身不是服务本身，可能会导致是被强制Kill的，解决的方法也很简单，也就是在主进程中对收到的信号做个转发，发送到容器中的其他子进程，这样容器中的所有进程在停止时，都会收到 SIGTERM，而不是 SIGKILL 信号了。</p>
<h1><span id="代码-6">代码 [6]</span><a href="#代码-6" class="header-anchor">#</a></h1><ul>
<li><p>Probe</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="comment"># 存活检测</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">5084</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 就绪检测</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">5084</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 优雅退出</span></span><br><span class="line">    <span class="attr">lifecycle:</span> </span><br><span class="line">      <span class="attr">preStop:</span> </span><br><span class="line">        <span class="attr">exec:</span> </span><br><span class="line">          <span class="attr">command:</span> </span><br><span class="line">          <span class="bullet">-</span> <span class="string">sleep</span></span><br><span class="line">          <span class="bullet">-</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">60</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Service<br>Cluster 模式（externalTrafficPolicy: Cluster）</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">externalTrafficPolicy:</span> <span class="string">Cluster</span>  <span class="comment">###</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<p>Local 模式（externalTrafficPolicy: Local）</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">externalTrafficPolicy:</span> <span class="string">Local</span>  <span class="comment">###</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br></pre></td></tr></table></figure>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>健康检查</p>
<ol>
<li><a href="https://blog.51cto.com/3842834/2317986">Liveness和Readiness两种Health Check手段在Kubernetes中的使用</a>  耕耘实录</li>
<li><a href="https://github.com/rootsongjc/kubernetes-handbook/blob/master/guide/configure-liveness-readiness-probes.md">配置Pod的liveness和readiness探针</a>  宋净超</li>
<li><a href="https://www.cnblogs.com/xuxinkun/p/11785521.html">liveness与readiness的探针工作方式源码解析</a>  xinkun的博客</li>
<li><a href="https://mp.weixin.qq.com/s/wT_NQF9xYfKD3wVm6yUUMw">Kubernetes 探针详解！</a> </li>
<li>04 | 理解进程(3):为什么我在容器中的进程被强制杀死了? -  李程远 </li>
<li><a href="https://blog.csdn.net/alisystemsoftware/article/details/106520606">更新应用时，如何实现 K8s 零中断滚动更新</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>数据中台</title>
    <url>/www6vHomeHexo/2022/10/16/dataMiddlePlatform/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="数据中台">数据中台</span><a href="#数据中台" class="header-anchor">#</a></h1><h3><span id="数据中台全景">数据中台全景</span><a href="#数据中台全景" class="header-anchor">#</a></h3><ul>
<li><p>数据中台全景[6]</p>
<img src="/www6vHomeHexo/2022/10/16/dataMiddlePlatform/middleStage-data.jpg" class>
</li>
<li><p>数据中台全景-阿里[7]</p>
<img src="/www6vHomeHexo/2022/10/16/dataMiddlePlatform/middleStage-data-ali.jpg" class title="数据中台全景-阿里">

</li>
<li><p>zhyt</p>
<img src="/www6vHomeHexo/2022/10/16/dataMiddlePlatform/zhyt.png" class title="中和应泰"></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="6">
<li><a href="https://www.esensoft.com/industry-news/dx-24039.html">一文读懂数据中台架构体系</a> *** </li>
<li><a href="https://xie.infoq.cn/article/8147ffdb15528ce08008d8100">数据中台各种架构图</a></li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>数据中台</category>
      </categories>
      <tags>
        <tag>数据中台</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 学习资源</title>
    <url>/www6vHomeHexo/2022/10/01/awsStudyResource/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="commons">commons</span><a href="#commons" class="header-anchor">#</a></h2><ul>
<li><p>AWS 认证云从业者 (CLF-C01)</p>
</li>
<li><p>Devops</p>
<ul>
<li>AWS 认证开发人员 – 助理 (DVA-C01) </li>
<li>AWS 认证 SysOps 管理员 – 助理 (SOA-C02)<br> <a href="https://www.bilibili.com/video/BV15U4y1S73s/">2022 终极 AWS 认证 - SysOps 管理员助理-上</a> 大量实操 中文<br><a href="https://www.bilibili.com/video/BV1F3411N7TJ/">2022 终极 AWS 认证 - SysOps 管理员助理-下</a> 大量实操 中文</li>
<li>AWS 认证 DevOps 工程师 – 专业 (DOP-C01)</li>
</ul>
</li>
<li><p>Solution</p>
<ul>
<li>AWS 认证解决方案架构师 – 助理 (SAA-C02)<br><a href="https://www.bilibili.com/video/BV1wR4y1F7YM/">Ultimate AWS Certified Solutions Architect Associate 2022（P1）-上</a> hand-on 中文 200<br><a href="https://www.bilibili.com/video/BV16L4y177kj/">Ultimate AWS Certified Solutions Architect Associate 2022（P2）- 下</a> hand-on 中文 156<br><a href="https://www.bilibili.com/video/BV12K411p7uy/">[AWS Certified Solutions Architect - Associate][2020][机翻字幕]</a> 269个<br><a href="https://www.bilibili.com/video/BV1K7411H7xm/">YOUTUBE上播放量超过16万人次的AWS SAA认证视频</a> 不全</li>
<li>AWS 认证解决方案架构师 – 专业 (SAP-C01)<br><a href="https://www.bilibili.com/video/BV1nR4y1N72u/">Amazon ECS 和 Fargate 大师班 - AWS 上的 Docker</a> 中文<br><a href="https://www.bilibili.com/video/BV1S541187uv/">AWS Solution Architect Professional, Stephane Maarek-2020</a> 无字幕</li>
</ul>
</li>
</ul>
<h2><span id="special">special</span><a href="#special" class="header-anchor">#</a></h2><ul>
<li>AWS 认证高级网络 – 专业 (ANS-C00)<br><a href="https://space.bilibili.com/412127397/search/video?keyword=aws">乾颐堂 aws网络</a> *** bili<br><a href="https://www.bilibili.com/video/BV1CG41137bx/">【云计算】AWS高级网络.LAB1.1.vpc_peering</a> </li>
<li>AWS 认证安全 – 专业 (SCS-C01)</li>
<li>AWS 认证机器学习 – 专业 (MLS-C01)</li>
<li>AWS 认证数据库 – 专业 (DBS-C01)</li>
<li>AWS 认证数据分析 – 专业 (DAS-C01)</li>
</ul>
<h2><span id="备考">备考</span><a href="#备考" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1ph411y7TQ/">AWS认证备考细则</a> *** 要重新看<br>AWS考试指导书<br>AWS产品白皮书</li>
<li>考点<br><a href="https://www.pearsonvue.com.cn/aws">Pearson VUE</a><br>PSI 抵制</li>
<li>改期和取消 <ul>
<li>改期<br>24小时之前改期，只能改期2次 </li>
<li>AWS考试券-半价</li>
</ul>
</li>
</ul>
<p>参考:<br><a href="https://www.xiaoheiwoo.com/choosing-the-right-aws-certification/">11 项 AWS 认证：哪一项适合你和你的团队？</a> *<br><a href="https://www.bilibili.com/video/BV1gU4y177TE/">AWS认证之路 - 备战 AWS Certification 考试</a> bilibili  *</p>
<h2><span id="官方">官方</span><a href="#官方" class="header-anchor">#</a></h2><p><a href="https://aws.amazon.bokecc.com/">亚马逊云科技 视频中心</a> ***<br>在线研讨会, re:Invent, Innovate,  Summit,  Transformation Day<br><a href="https://aws.amazon.com/cn/about-aws/events/">亚马逊云科技中国市场及培训活动</a> ***<br><a href="https://aws.amazon.com/cn/about-aws/events/webinar/2019/">aws 在线研讨会</a> ***  2015-2019<br><a href="https://space.bilibili.com/418158141">亚马逊云科技官方账号</a> *** bili  - INNOVATE 2020<br><a href="https://blog.csdn.net/awschina?type=blog">亚马逊云开发者</a>  亚马逊CSDN blog-会议,咨询<br><a href="https://amazonaws-china.com/cn/blogs/architecture/">AWS Architecture Blog</a> ***<br><a href="https://www.allthingsdistributed.com/">All Things Distributed</a>  aws cto</p>
<h2><span id="非官方">非官方</span><a href="#非官方" class="header-anchor">#</a></h2><p><a href="https://www.koudaizy.com/">口袋资源</a> ***<br><a href="https://www.zhihu.com/column/c_1347591909771182080">全是aws干货</a> 分享aws云经验，提供实操干货</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 所有的Services</title>
    <url>/www6vHomeHexo/2022/10/01/awsAllServices/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="analytics">Analytics:</span><a href="#analytics" class="header-anchor">#</a></h2><p>• Amazon Athena<br> Amazon Athena is a serverless, interactive analytics service built on open-source frameworks, supporting open-table and file formats.<br>• AWS Data Exchange<br>• AWS Data Pipeline<br>• Amazon EMR<br>• AWS Glue<br>AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.<br>[ETL]<br>• Amazon Kinesis Data Analytics<br>• Amazon Kinesis Data Firehose<br>• Amazon Kinesis Data Streams<br>• AWS Lake Formation<br>• Amazon Managed Streaming for Apache Kafka (Amazon MSK)<br>• Amazon OpenSearch Service<br>• Amazon QuickSight<br> BI tool</p>
<h2><span id="application-integration">Application Integration:</span><a href="#application-integration" class="header-anchor">#</a></h2><p>• Amazon AppFlow<br>• AWS AppSync<br>• Amazon EventBridge (Amazon CloudWatch Events)  @<br>• Amazon MQ  @<br>• Amazon Simple Notification Service (Amazon SNS)  @<br>• Amazon Simple Queue Service (Amazon SQS)  @<br>• AWS Step Functions  @</p>
<h2><span id="business-applications">Business Applications:</span><a href="#business-applications" class="header-anchor">#</a></h2><p>• Alexa for Business<br>• Amazon Simple Email Service (Amazon SES)</p>
<h2><span id="blockchain">Blockchain:</span><a href="#blockchain" class="header-anchor">#</a></h2><p>• Amazon Managed Blockchain</p>
<h2><span id="cloud-financial-management">Cloud Financial Management:</span><a href="#cloud-financial-management" class="header-anchor">#</a></h2><p>• AWS Budgets<br>• AWS Cost and Usage Report<br>• AWS Cost Explorer<br>• Savings Plans</p>
<h2><span id="compute">Compute:</span><a href="#compute" class="header-anchor">#</a></h2><p>• AWS App Runner<br>• AWS Auto Scaling<br>• AWS Batch  @<br>  AWS Batch enables you to easily and efficiently run batch computing workloads of any scale on AWS using Amazon EC2 and Amazon EC2 Spot.<br>• Amazon EC2  @<br>• Amazon EC2 Auto Scaling  @<br>• AWS Elastic Beanstalk<br>  Amazon Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.<br>• Amazon Elastic Kubernetes Service (Amazon EKS)<br>• Elastic Load Balancing  @<br>• AWS Fargate  @<br>  AWS Fargate 是一种无服务器、随用随付的计算引擎，可让您专注于构建应用程序，而无需管理服务器。AWS Fargate 与 Amazon Elastic Container Service (ECS) 和 Amazon Elastic Kubernetes Service (EKS) 兼容。<br>• AWS Lambda  @<br>• Amazon Lightsail<br>  Amazon Lightsail 以经济实惠的月度价格提供易于使用的虚拟专用服务器 (VPS) 实例、容器、存储、数据库等。<br>  Lightsail 是由 AWS 推出的面向开发人员、小型企业、学生等人员的轻量级 VPS 云计算服务。(非官方)<br>• AWS Outposts  @<br>  AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility.<br>• AWS Wavelength</p>
<h2><span id="containers">Containers:</span><a href="#containers" class="header-anchor">#</a></h2><p>• Amazon Elastic Container Registry (Amazon ECR)<br>• Amazon Elastic Container Service (Amazon ECS)  @<br>  Amazon ECS is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications. It deeply integrates with the rest of the AWS platform to provide a secure and easy-to-use solution for running container workloads in the cloud and now on your infrastructure with Amazon ECS Anywhere.<br>• Amazon ECS Anywhere<br>• Amazon Elastic Kubernetes Service (Amazon EKS)  @<br>  Amazon EKS is a managed service that makes it easy for you to use Kubernetes on AWS without needing to install and operate your own Kubernetes control plane.<br>• Amazon EKS Anywhere<br>• Amazon EKS Distro</p>
<h2><span id="database">Database:</span><a href="#database" class="header-anchor">#</a></h2><p>• Amazon Aurora  @<br>  Designed for unparalleled high performance and availability at global scale with full MySQL and PostgreSQL compatibility<br>  Amazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.<br>• Amazon Aurora Serverless  @<br>• Amazon DocumentDB (with MongoDB compatibility)<br>  Scale JSON workloads with ease using a fully managed document database service<br>  (with MongoDB compatibility)<br>  [Free]<br>• Amazon DynamoDB  @<br>  Fast, flexible NoSQL database service for single-digit millisecond performance at any scale<br>  [Free]<br>• Amazon ElastiCache  @<br>  Unlock microsecond latency and scale with in-memory caching<br>  [Free]<br>• Amazon Keyspaces (for Apache Cassandra)<br>• Amazon Neptune<br>  Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets.<br>• Amazon RDS  @<br>• Amazon Redshift<br>• Amazon Timestream</p>
<h2><span id="developer-tools">Developer Tools:</span><a href="#developer-tools" class="header-anchor">#</a></h2><p>• AWS Cloud9<br>• AWS CodeArtifact<br>• AWS CodeBuild<br>• AWS CodeCommit<br>• AWS CodeDeploy<br>• Amazon CodeGuru<br>• AWS CodePipeline<br>• AWS CodeStar<br>• AWS X-Ray</p>
<h2><span id="end-user-computing">End User Computing:</span><a href="#end-user-computing" class="header-anchor">#</a></h2><p>• Amazon AppStream 2.0<br>• Amazon WorkSpaces</p>
<h2><span id="frontend-web-and-mobile">Frontend Web and Mobile:</span><a href="#frontend-web-and-mobile" class="header-anchor">#</a></h2><p>• AWS Amplify<br>• Amazon API Gateway<br>• AWS Device Farm<br>• Amazon Pinpoint</p>
<h2><span id="internet-of-things">Internet of Things:</span><a href="#internet-of-things" class="header-anchor">#</a></h2><p>• AWS IoT Analytics<br>• AWS IoT Core<br>• AWS IoT Device Defender<br>• AWS IoT Device Management<br>• AWS IoT Events<br>• AWS IoT Greengrass<br>• AWS IoT SiteWise<br>• AWS IoT Things Graph<br>• AWS IoT 1-Click</p>
<h2><span id="machine-learning">Machine Learning:</span><a href="#machine-learning" class="header-anchor">#</a></h2><p>• Amazon Comprehend<br>• Amazon Forecast<br>• Amazon Fraud Detector<br>• Amazon Kendra<br>• Amazon Lex<br>  Build chatbots with conversational AI<br>• Amazon Personalize<br>• Amazon Polly<br>  Amazon Polly uses deep learning technologies to synthesize natural-sounding human speech, so you can convert articles to speech.<br>• Amazon Rekognition<br>  Automate your image and video analysis with machine learning<br>• Amazon SageMaker<br>  Build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows<br>• Amazon Textract<br>• Amazon Transcribe<br>• Amazon Translate</p>
<h2><span id="management-and-governance">Management and Governance:</span><a href="#management-and-governance" class="header-anchor">#</a></h2><p>• AWS CloudFormation  @<br>• AWS CloudTrail<br>  AWS CloudTrail monitors and records account activity across your AWS infrastructure, giving you control over storage, analysis, and remediation actions.<br>• Amazon CloudWatch  @<br>• Amazon CloudWatch Logs  @<br>• AWS Command Line Interface (AWS CLI)<br>• AWS Compute Optimizer<br>• AWS Config<br>• AWS Control Tower<br>• AWS License Manager<br>• Amazon Managed Grafana<br>• Amazon Managed Service for Prometheus<br>• AWS Management Console<br>• AWS Organizations  @<br>• AWS Personal Health Dashboard<br>• AWS Proton<br>• AWS Service Catalog<br>• Service Quotas<br>• AWS Systems Manager<br>• AWS Trusted Advisor<br>• AWS Well-Architected Tool</p>
<h2><span id="media-services">Media Services:</span><a href="#media-services" class="header-anchor">#</a></h2><p>• Amazon Elastic Transcoder<br>• Amazon Kinesis Video Streams</p>
<h2><span id="migration-and-transfer">Migration and Transfer:</span><a href="#migration-and-transfer" class="header-anchor">#</a></h2><p>• AWS Application Discovery Service<br>• AWS Application Migration Service (CloudEndure Migration)<br>• AWS Database Migration Service (AWS DMS)<br>  Homogeneous Database Migrations<br>  Heterogeneous Database Migrations<br>• AWS DataSync<br>  Simplify and accelerate secure data migrations<br>• AWS Migration Hub<br>  Discover the tools that you need to simplify your migration and modernization<br>  AWS Migration Hub provides a central location to collect server and application inventory data for the assessment, planning, and tracking of migrations to AWS. Migration Hub can also help accelerate application modernization following migration.<br>• AWS Schema Conversion Tool (AWS SCT)<br>• AWS Snow Family  @<br>• AWS Transfer Family<br>  AWS Transfer Family securely scales your recurring business-to-business file transfers to AWS Storage services using SFTP, FTPS, FTP, and AS2 protocols. </p>
<h2><span id="networking-and-content-delivery">Networking and Content Delivery:</span><a href="#networking-and-content-delivery" class="header-anchor">#</a></h2><p>• Amazon CloudFront  @<br>• AWS Direct Connect  @<br>• Elastic Load Balancing (ELB)  @<br>• AWS Global Accelerator  @<br>  <a href="https://kebingzao.com/2020/08/13/aws-ga/">使用 AWS Global Accelerator 加速你的服务</a><br>• AWS PrivateLink  @<br>• Amazon Route 53  @<br>• AWS Transit Gateway  @<br>• Amazon VPC  @<br>• AWS VPN  @</p>
<h2><span id="security-identity-and-compliance">Security, Identity, and Compliance:</span><a href="#security-identity-and-compliance" class="header-anchor">#</a></h2><p>• AWS Artifact<br>• AWS Audit Manager<br>• AWS Certificate Manager (ACM)<br> Provision and manage SSL&#x2F;TLS certificates with AWS services and connected resources<br>• AWS CloudHSM<br> Manage single-tenant hardware security modules (HSMs) on AWS<br>• Amazon Cognito<br>• Amazon Detective<br>• AWS Directory Service<br>• AWS Firewall Manager @<br> Centrally configure and manage firewall rules across your accounts<br>• Amazon GuardDuty<br>• AWS Identity and Access Management (IAM)  @<br>• Amazon Inspector<br>• AWS Key Management Service (AWS KMS)  @<br>• Amazon Macie<br>• AWS Network Firewall  @<br>• AWS Resource Access Manager (AWS RAM)<br>• AWS Secrets Manager<br>• AWS Security Hub<br>• AWS Security Token Service (AWS STS)<br>• AWS Shield<br>• AWS Single Sign-On  @<br>• AWS WAF  @</p>
<h2><span id="storage">Storage:</span><a href="#storage" class="header-anchor">#</a></h2><p>• AWS Backup<br>  Centrally manage and automate data protection<br>• Amazon Elastic Block Store (Amazon EBS)  @<br>  Easy to use, high performance block storage at any scale<br>• AWS Elastic Disaster Recovery (CloudEndure Disaster Recovery)<br>• Amazon Elastic File System (Amazon EFS)  @<br>  Simple, serverless, set-and-forget, elastic file system<br>• Amazon FSx (for all types)<br>  Launch, run, and scale feature-rich and highly-performant file systems with just a few clicks<br>• Amazon S3  @<br>• Amazon S3 Glacier  @<br>  Long-term, secure, durable storage classes for data archiving at the lowest cost and milliseconds access<br>• AWS Storage Gateway  @<br>  Provide on-premises applications with access to virtually unlimited cloud storage</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://us-east-1.console.aws.amazon.com/console/services?region=us-east-1">AWS所有服务</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Storage-S3</title>
    <url>/www6vHomeHexo/2022/10/01/awsStorageS3/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="s3">S3</span><a href="#s3" class="header-anchor">#</a></h2><ul>
<li><p>基本特性[5]</p>
<ul>
<li>S3的文件存储在存储桶（Buckets）内，可以理解为文件夹</li>
<li>存储桶创建之后会生成一个URL<br>S3是以HTTPS的形式展现的，而非HTTP</li>
<li>S3的存储桶创建的时候可以选择所在区域（Region），但不能选择可用区（AZ），AWS会负责S3的高可用、容灾问题 *<ul>
<li>S3创建的时候可以选择某个AWS区域，一旦选择了就不能更改</li>
<li>如果要在其他区域使用该S3的内容，可以使用跨区域复制  #2</li>
</ul>
</li>
<li>S3拥有不同的等级（Standard, Stantard-IA, Onezone-IA, RRS, Glacier） #1</li>
<li>启用了版本控制（Version Control）你可以恢复S3内的文件到之前的版本  #4</li>
<li>S3可以开启生命周期管理 #3<ul>
<li>要启用生命周期管理需要先启用版本控制功能</li>
</ul>
</li>
<li>支持加密功能  </li>
<li>使用访问控制列表（Access Control Lists）和桶策略（Bucket Policy）可以控制S3的访问安全</li>
</ul>
</li>
<li><p>S3[2]</p>
<ul>
<li>Static content</li>
<li>serverless</li>
<li>pay-as-you-go</li>
</ul>
</li>
<li><p>Type #1</p>
<img src="/www6vHomeHexo/2022/10/01/awsStorageS3/s3-type.JPG" class title="S3 Type">
</li>
<li><p>S3 – Replication [2][4] #2</p>
<ul>
<li>Cross Region Replication (CRR)</li>
<li>Same Region Replication (SRR)</li>
<li>Combine with Lifecycle Policies</li>
</ul>
</li>
<li><p>S3 Select &amp; Glacier Select[3]</p>
<ul>
<li>S3 Select<br>server side filtering</li>
<li>Glacier Select</li>
</ul>
</li>
<li><p>Lifecycle Management #3</p>
<ul>
<li>Transition actions</li>
<li>Expiration actions</li>
</ul>
</li>
<li><p>访问S3</p>
<ul>
<li>S3访问策略[1]  <img src="/www6vHomeHexo/2022/10/01/awsStorageS3/s3-accessPolicy.JPG" class title="s3访问策略"></li>
<li>访问方式[4]<ul>
<li>private Address</li>
<li>public Address</li>
</ul>
</li>
</ul>
</li>
<li><p>Versioning #4</p>
<ul>
<li>preserve, retrieve, and restore</li>
</ul>
</li>
<li><p>Access Points [2]</p>
<ul>
<li>Access Point gets its own DNS and policy to limit who can access it<br>One policy per Access Point<br><a href="https://aws.amazon.com/s3/features/access-points/">Amazon S3 Access Points</a><br><a href="https://aws.amazon.com/s3/features/multi-region-access-points/">Amazon S3 Multi-Region Access Points</a><br><a href="../../../../2022/06/17/awsNetworkVPCendpoint/">AWS Network-VPC Endpoint</a> self</li>
</ul>
</li>
</ul>
<h2><span id="aws-storage-gateway46">AWS Storage Gateway[4][6]</span><a href="#aws-storage-gateway46" class="header-anchor">#</a></h2><ul>
<li>File Gateway<br>SMB or NFS-based access</li>
<li>Volume Gateway<br>Block storage – iSCSI protocol<ul>
<li>Cached volumes<br>provides low-latency access to your frequently accessed data but not to the entire data.</li>
<li>Stored volumes<br>store your primary data locally, while asynchronously back up that data to AWS.</li>
</ul>
</li>
<li>Tape Gateway<br><a href="https://docs.aws.amazon.com/storagegateway/index.html">AWS Storage Gateway Documentation</a></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1hJ411U7vd">AWS解决方案架构师认证 Professional(SAP)中文视频培训课程2022</a>  P10</li>
<li>[SAP-2] Storage Section *** </li>
<li><a href="https://aws.amazon.com/blogs/aws/s3-glacier-select/">S3 Select and Glacier Select – Retrieving Subsets of Objects</a></li>
<li>[SAP-1] *** </li>
<li><a href="http://www.cloudbin.cn/?p=1968">AWS学习笔记（十） Amazon Simple Storage Service (S3)</a> </li>
<li><a href>Practice Set 1</a> Question 13</li>
</ol>
<p>Series<br>10. <a href="https://www.iloveaws.cn/1238.html">08-S3存储桶策略（S3 Bucket Policies）</a><br>11. <a href="https://www.iloveaws.cn/1361.html">09-配置跨账户S3存储桶的访问（Cross Account S3 Bucket Configuration）</a><br>12. <a href="https://www.iloveaws.cn/1426.html">10-S3标准 ACL（Canned ACL）</a><br>13. <a href="https://www.iloveaws.cn/2428.html">39-S3存储桶跨区域复制 (CRR)</a></p>
<p><a href="https://www.bilibili.com/video/BV14a4y1W77S/">海量数据云归档最佳实践</a> bili ucloud</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Database</title>
    <url>/www6vHomeHexo/2022/10/01/awsDatabase/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="rds2">RDS[2]</span><a href="#rds2" class="header-anchor">#</a></h2><ul>
<li>database engines<ul>
<li>Amazon Aurora</li>
<li>MySQL</li>
<li>MariaDB</li>
<li>Oracle</li>
<li>Microsoft SQL Server</li>
<li>PostgreSQ</li>
</ul>
</li>
<li>Backup and Recovery<ul>
<li>Automated Backups</li>
<li>Manual Backups (Snapshot)</li>
</ul>
</li>
<li>RDS Replication<ul>
<li>RPO &#x3D; 10 minutes, RTO &#x3D; 5 minutes</li>
</ul>
</li>
<li><strong>Amazon RDS Multi-AZ and Read Replicas</strong></li>
</ul>
<table>
<thead>
<tr>
<th><strong>Multi-AZ Deployments</strong></th>
<th><strong>Read Replicas</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Synchronous replication – highly durable</td>
<td>Asynchronous replication – highly scalable</td>
</tr>
<tr>
<td>Only database engine on primary instance is active</td>
<td>All read replicas are accessible and can be used for read scaling</td>
</tr>
<tr>
<td>Automated backups are taken from standby</td>
<td>No backups configured by default</td>
</tr>
<tr>
<td>Always span two Availability Zones within a single Region</td>
<td>Can be within an Availability Zone, Cross-AZ, or Cross-Region</td>
</tr>
<tr>
<td>Database engine version upgrades happen on primary</td>
<td>Database engine version upgrade is independent from source instance</td>
</tr>
<tr>
<td>Automatic failover to standby when a problem is detected</td>
<td>Can be manually promoted to a standalone database instance</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注：</span><br><span class="line">RPO = Recovery Point Objective</span><br><span class="line">RT0 - Recovery Time Objective</span><br></pre></td></tr></table></figure>



<h2><span id="aurora-2">Aurora [2]</span><a href="#aurora-2" class="header-anchor">#</a></h2><ul>
<li><p>Replicas  </p>
<ul>
<li>Aurora Replicas are within a region</li>
<li>Replicas scale-out read requests</li>
<li>Can promote Aurora Replica to be a new primary or create new primary</li>
<li>Can use Auto Scaling to add replicas</li>
</ul>
</li>
<li><p>Cross-Region Replica with Aurora MySQL</p>
<ul>
<li>Asynchronous replication<br><strong>Replication uses the MySQL database engine</strong></li>
</ul>
</li>
<li><p>Global Database</p>
<ul>
<li><strong>Replication uses the Aurora storage layer</strong></li>
<li>Applications can connect to the cluster Reader Endpoint [3]</li>
</ul>
</li>
<li><p>Fault Tolerance</p>
<ul>
<li>Fault tolerance across 3 AZs</li>
</ul>
</li>
<li><p>Multi-Master</p>
<ul>
<li>All nodes allow reads&#x2F;writes</li>
<li>Available for MySQL only</li>
<li>Up to four read&#x2F;write nodes</li>
<li>Single Region only</li>
</ul>
</li>
</ul>
<p>参考:<br><a href="https://zhuanlan.zhihu.com/p/159304158">在 Amazon Aurora Global Database 中使用全球分布式 MySQL 程序</a>  未<br><a href="https://aws.amazon.com/cn/getting-started/hands-on/aurora-global-database/">使用 Amazon Aurora Global Database 进行快速的跨区域灾难恢复和低延迟全球读取</a> 未<br><a href="https://www.bilibili.com/video/BV1P64y1M7fu/">力从地起 - 揭秘 Aurora 底层存储 (Level 300)</a>  *** global database，snapshot</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://aws.amazon.com/rds/features/multi-az/">Amazon RDS Multi-AZ</a></li>
<li>SAP-1  Database</li>
<li><a href="https://aws.amazon.com/cn/blogs/aws/new-reader-endpoint-for-amazon-aurora-load-balancing-higher-availability/">New Reader Endpoint for Amazon Aurora – Load Balancing &amp; Higher Availability</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-Overview</title>
    <url>/www6vHomeHexo/2022/10/01/awsNetwork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="overview13">Overview[1][3]</span><a href="#overview13" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/10/01/awsNetwork/awsNetwork.jpg" class title="AWS网路产品">


<h5><span id="云上网络场景">云上网络[场景]</span><a href="#云上网络场景" class="header-anchor">#</a></h5><ul>
<li>vpc</li>
<li>vpc peering ***<ul>
<li>[劣势：没有transitive 特性]</li>
</ul>
</li>
<li>private link<ul>
<li>vpc上打个洞，用的比较少</li>
</ul>
</li>
<li>ELB</li>
</ul>
<h5><span id="跨地域网络场景">跨地域网络[场景]</span><a href="#跨地域网络场景" class="header-anchor">#</a></h5><ul>
<li>sd-wan    <ul>
<li>cloudwan</li>
</ul>
</li>
</ul>
<h5><span id="混合云网络场景">混合云网络[场景]</span><a href="#混合云网络场景" class="header-anchor">#</a></h5><ul>
<li>vpn<ul>
<li>client vpn ***</li>
<li>vpn gateway  </li>
<li>vpn site-to-site ***<br><a href="https://zhuanlan.zhihu.com/p/395805857">AWS Site-to-Site VPN</a><br><a href="https://docs.aws.amazon.com/zh_cn/vpn/latest/s2svpn/how_it_works.html">AWS Site-to-Site VPN 的工作原理</a></li>
</ul>
</li>
<li>专线<ul>
<li>direct connect ***</li>
</ul>
</li>
<li>transit gateway ***<ul>
<li>[劣势： 不能跨region. 如果要跨region, 需要TGW之间做peering]</li>
</ul>
</li>
</ul>
<h5><span id="解决方案">(解决方案)</span><a href="#解决方案" class="header-anchor">#</a></h5><ul>
<li>transit vpc</li>
</ul>
<h5><span id="vpc-peering-vs-transit-vpc-vs-transit-gateway">VPC Peering vs. Transit VPC vs. Transit Gateway</span><a href="#vpc-peering-vs-transit-vpc-vs-transit-gateway" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/10/01/awsNetwork/aws-network-compare.JPG" class title="VPC Peering vs. Transit VPC vs. Transit Gateway">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>1.<a href="https://www.bilibili.com/video/BV1gQ4y1k7LH/">亚马逊云科技企业组网解决方案 | 一期一会</a><br>3. <a href="https://www.bilibili.com/video/BV1CG41137bx/">【云计算】AWS高级网络.LAB1.1.vpc_peering</a><br>7. <a href="https://www.zhihu.com/column/c_1520366118765621248">AWS networking</a> *** 笔记  未</p>
<h3><span id="white-paper">white paper</span><a href="#white-paper" class="header-anchor">#</a></h3><p><a href="https://d1.awsstatic.com/whitepapers/building-a-scalable-and-secure-multi-vpc-aws-network-infrastructure.pdf">Building a Scalable and Secure Multi-VPC AWS Network Infrastructure-AWS Whitepaper</a> ***  未<br><a href="https://www.bilibili.com/video/BV1Cd4y1377m/">教主技术进化论2022第24期 AWS网络白皮书.1.vpc_peering</a> </p>
<p><a href="https://d1.awsstatic.com/whitepapers/building-a-scalable-and-secure-multi-vpc-aws-network-infrastructure.pdf">https://d1.awsstatic.com/whitepapers/building-a-scalable-and-secure-multi-vpc-aws-network-infrastructure.pdf</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Security</title>
    <url>/www6vHomeHexo/2022/10/01/awssecurity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="landing-zone1">Landing Zone[1]</span><a href="#landing-zone1" class="header-anchor">#</a></h2><ul>
<li>Control Tower</li>
</ul>
<h2><span id="wafweb-application-firewall-13">WAF(Web Application Firewall) [1][3]</span><a href="#wafweb-application-firewall-13" class="header-anchor">#</a></h2><ul>
<li>计费方式</li>
<li>部署方式<ul>
<li>传统WAF<br>部署在ELB之后，部署在EC2之上</li>
<li>AWS WAF<br>部署在CloudFront之上</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2022/10/01/awssecurity/waf-security-automations-architecture.png" class title="Security Automations for AWS WAF">    
<h2><span id="firewall-manager1">Firewall Manager[1]</span><a href="#firewall-manager1" class="header-anchor">#</a></h2><p>以规模化方式管理AWS WAF规则</p>
<h2><span id="kms-1todo">KMS [1][todo]</span><a href="#kms-1todo" class="header-anchor">#</a></h2><h2><span id="kms-4">KMS [4]</span><a href="#kms-4" class="header-anchor">#</a></h2><ul>
<li><p>KMS Key Types</p>
<ul>
<li>Symmetric (AES-256 keys)</li>
<li>Asymmetric (RSA &amp; ECC key pairs)</li>
</ul>
</li>
<li><p>Types of KMS Keys</p>
<img src="/www6vHomeHexo/2022/10/01/awssecurity/kms-keyType.JPG" class title="Types of KMS Keys">
</li>
<li><p>KMS Key Material Origin</p>
<ul>
<li>KMS (AWS_KMS) – default</li>
<li>External (EXTERNAL)<br>BYOK， 外部的key导入KMS</li>
<li>Custom Key Store (AWS_CLOUDHSM)<br>用户自定义的Key Store  [感觉类似加密机，硬件加密]</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>bilibili</p>
<ol>
<li><a href="https://www.bilibili.com/video/BV1ka4y1v7ZN/">AWS 常见安全参考架构 (Level 200)</a></li>
<li>亚马逊云科技 安全架构连连看</li>
<li><a href="https://aws.amazon.com/cn/solutions/implementations/security-automations-for-aws-waf/">Security Automations for AWS WAF</a></li>
<li>[SAP-2]  Security Section</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang内置类型-Map</title>
    <url>/www6vHomeHexo/2022/09/22/golangMap/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#map-%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0">Map 内部实现</a><br>- <a href="#map%E7%9A%84%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84">map的内部结构</a><br>- <a href="#map%E7%9A%84%E5%86%85%E9%83%A8%E5%87%BD%E6%95%B0-4">map的内部函数 [4]</a><br>- <a href="#%E6%89%A9%E5%AE%B9-2">扩容 [2]</a><br>- <a href="#%E6%89%A9%E5%AE%B9-4">扩容 [4]</a><br>- <a href="#map-%E8%A7%A3%E5%86%B3-hash-%E5%86%B2%E7%AA%81-3">map 解决 hash 冲突 [3]</a><br>- <a href="#%E7%BC%BA%E9%99%B7-4">缺陷 [4]</a></li>
<li><a href="#map%E7%9A%84%E4%BD%BF%E7%94%A8">map的使用</a><br>- <a href="#map%E7%9A%84%E4%BD%BF%E7%94%A8-1">map的使用</a><br>- <a href="#%E5%B9%B6%E5%8F%91">并发</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="map-内部实现">Map 内部实现</span><a href="#map-内部实现" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/09/22/golangMap/golang-map.jpg" class title="Golang Map">

<h5><span id="map的内部结构">map的内部结构</span><a href="#map的内部结构" class="header-anchor">#</a></h5><ul>
<li>hmap<ul>
<li>bucket<ul>
<li>topHash<br><strong>快速定位key,以空间换时间</strong><br>每个 bucket 的 tophash 区域其实是用来快速定位 key 位置的. 这是一种以空间换时间的思路。</li>
<li>key</li>
<li>value<br>Go 运行时采用了<strong>把 key 和 value 分开存储的方式，而不是采用一个 kv 接着一个 kv 的 kv 紧邻方式存储</strong>，这带来的其实是算法上的复杂性，但却减少了因内存对齐带来的内存浪费。</li>
<li>overflow</li>
</ul>
</li>
</ul>
</li>
<li>bmap</li>
</ul>
<h5><span id="map的内部函数-4">map的内部函数 [4]</span><a href="#map的内部函数-4" class="header-anchor">#</a></h5><ul>
<li>mapassign 写</li>
<li>mapdelete 删</li>
<li>mapaccess 读</li>
</ul>
<h5><span id="扩容-2">扩容 [2]</span><a href="#扩容-2" class="header-anchor">#</a></h5><ul>
<li>buckets &amp;&amp; oldbuckets </li>
<li><strong>两种扩容方式 [渐进式扩容, 类似redis rehash]</strong><ul>
<li>因为 overflow bucket 过多导致的“扩容”，实际上运行时会新建一个和现有规模一样的 bucket 数组，然后在 assign 和 delete 时做排空和迁移。</li>
<li>因为当前数据数量超出 LoadFactor 指定水位而进行的扩容，那么运行时会建立一个两倍于现有规模的 bucket 数组，但真正的排空和迁移工作也是在 assign 和 delete 时逐步进行的。</li>
</ul>
</li>
</ul>
<h5><span id="扩容-4">扩容 [4]</span><a href="#扩容-4" class="header-anchor">#</a></h5><p>触发:  mapassign<br>时机: load factor 过大  OR overflow bucket 过多<br>搬运过程:  渐进式</p>
<ul>
<li>mapassign<ul>
<li>elem cout &gt; bucket*6.5  -&gt; bigger size grow</li>
<li>overflow too many  –&gt; same size grow <ul>
<li>noverflow &gt;&#x3D;2^15</li>
<li>nvoerflow &lt; 2^15 &amp;&amp; nvoerflow &gt; bucket count</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="map-解决-hash-冲突-3">map 解决 hash 冲突 [3]</span><a href="#map-解决-hash-冲突-3" class="header-anchor">#</a></h5><p>在 map 解决 hash &#x2F;分桶 冲突问题时，实际上结合了拉链法和开放寻址法两种思路. 以 map 的插入写流程为例，进行思路阐述：<br>（1）桶数组中的每个桶，严格意义上是一个单向桶链表，以桶为节点进行串联；<br>（2）每个桶固定可以存放 8 个 key-value 对；<br>（3）当 key 命中一个桶时，首先根据开放寻址法，在桶的 8 个位置中寻找空位进行插入；<br>（4）倘若桶的 8 个位置都已被占满，则基于桶的溢出桶指针，找到下一个桶，重复第（3）步；<br>（5）倘若遍历到链表尾部，仍未找到空位，则基于拉链法，在桶链表尾部续接新桶，并插入 key-value 对.</p>
<h5><span id="缺陷-4">缺陷 [4]</span><a href="#缺陷-4" class="header-anchor">#</a></h5><ul>
<li>已经扩容的map, 无法 缩容</li>
<li>保证并发安全时, 要手动读写锁，易出错</li>
<li>多核心下表现差</li>
</ul>
<h2><span id="map的使用">map的使用</span><a href="#map的使用" class="header-anchor">#</a></h2><h5><span id="map的使用">map的使用</span><a href="#map的使用" class="header-anchor">#</a></h5><p>  value没有任何的限制, key有严格的限制</p>
<h5><span id="并发">并发</span><a href="#并发" class="header-anchor">#</a></h5><ul>
<li>不可以并发读写<br>可以并发读</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1194y1o77s/?spm_id_from=pageDriver&vd_source=f6e8c1128f9f264c5ab8d9411a644036">Go面试题系列：Go map的底层实现原理</a></li>
<li>《16|复合数据类型：原始map类型的实现机制是这样的？》 TonyBai</li>
<li><a href="https://zhuanlan.zhihu.com/p/597483155">Golang map 实现原理</a></li>
<li>《09 神奇的内置数据结构》 V</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>用户画像</title>
    <url>/www6vHomeHexo/2022/09/21/personProflie/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F-3">用户画像 [3]</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
<li><a href="#%E7%94%A8%E6%88%B7%E6%A0%87%E7%AD%BE-2">用户标签 [2]</a><ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A0%87%E7%AD%BE-3">基础标签 [3]</a></li>
<li><a href="#%E8%A1%8C%E4%B8%BA%E6%A0%87%E7%AD%BE">行为标签</a></li>
<li><a href="#%E5%81%8F%E5%A5%BD%E6%A0%87%E7%AD%BE-3">偏好标签 [3]</a></li>
<li><a href="#%E9%A2%84%E6%B5%8B%E6%A0%87%E7%AD%BE">预测标签</a></li>
</ul>
</li>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B9">关注点</a><ul>
<li><a href="#%E6%A0%87%E7%AD%BE%E7%9A%84%E7%B2%92%E5%BA%A6-3">标签的粒度 [3]</a></li>
<li><a href="#%E9%9D%99%E6%80%81%E5%8A%A8%E6%80%81%E6%A0%87%E7%AD%BE-3">静态&#x2F;动态标签 [3]</a></li>
</ul>
</li>
<li><a href="#%E5%9C%88%E4%BA%BA-%E5%9F%BA%E4%BA%8E%E6%A0%87%E7%AD%BE%E8%81%9A%E7%B1%BB%E7%9A%84%E4%BA%BA%E7%BE%A4%E7%94%9F%E6%88%90-3">圈人-基于标签聚类的人群生成 [3]</a><ul>
<li><a href="#%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F">实现方式</a></li>
</ul>
</li>
<li><a href="#%E5%85%B3%E7%B3%BB%E5%BA%93-id-mapping-2">关系库 ID-Mapping [2]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF3">应用场景[3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="用户画像-3">用户画像 [3]</span><a href="#用户画像-3" class="header-anchor">#</a></h1><p>用户画像就是与该用户相关联的数据的可视化的展现，一句话来总结就是<strong>用户信息标签化</strong>.</p>
<p>这些标签的来源就是一些<strong>用户的行为</strong>.</p>
<h1><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/09/21/personProflie/process.jpg" class>

<img src="/www6vHomeHexo/2022/09/21/personProflie/arch.jpg" class>


<h1><span id="用户标签-2">用户标签 [2]</span><a href="#用户标签-2" class="header-anchor">#</a></h1><h3><span id="基础标签-3">基础标签 [3]</span><a href="#基础标签-3" class="header-anchor">#</a></h3><ul>
<li>人口属性<br>性别 年龄 职业 等</li>
<li>购买能力<br>收入及购买能力、购买频次和渠道</li>
<li>用户等级<br>高级会员<br>VIP会员</li>
<li>上课情况<ul>
<li>下粉批次</li>
<li>直播间类型</li>
<li>完课率</li>
<li>听课阶段</li>
</ul>
</li>
</ul>
<h3><span id="行为标签">行为标签</span><a href="#行为标签" class="header-anchor">#</a></h3><ul>
<li><p>绑定手机号</p>
</li>
<li><p>添加微信</p>
</li>
<li><p>删除微信</p>
</li>
<li><p>回复问候</p>
</li>
<li><p>下载软件</p>
</li>
<li><p>购买课程</p>
<ul>
<li>购买9.9直播</li>
<li>接听dayN直播报名电话</li>
<li>报名dayN直播</li>
<li>进入dayN直播间</li>
<li>观看dayN预习视频</li>
<li>回复dayN预习作业</li>
<li>回复dayN作业</li>
<li>购买高级会员</li>
<li>购买VIP</li>
</ul>
</li>
</ul>
<h3><span id="偏好标签-3">偏好标签 [3]</span><a href="#偏好标签-3" class="header-anchor">#</a></h3><ul>
<li><p>时间偏好<br>PC端活跃时间<br>App端活跃时间<br>Web端活跃时间</p>
</li>
<li><p>渠道偏好<br>常用PC端<br>常用App端<br>常用Web端</p>
</li>
<li><p>板块关注偏好<br>科技类<br>汽车类<br>电力能源</p>
</li>
<li><p>消费需求<br>消费习惯和消费偏好</p>
</li>
<li><p>风险偏好</p>
</li>
</ul>
<h3><span id="预测标签">预测标签</span><a href="#预测标签" class="header-anchor">#</a></h3><h1><span id="关注点">关注点</span><a href="#关注点" class="header-anchor">#</a></h1><h3><span id="标签的粒度-3">标签的粒度 [3]</span><a href="#标签的粒度-3" class="header-anchor">#</a></h3><p>比如年龄标签是20-30岁和21岁，就是明显不同粒度的标签<br>比如活跃时间是白天或者晚上和下午3点，就是明显不同粒度的标签</p>
<h3><span id="静态x2f动态标签-3">静态&#x2F;动态标签 [3]</span><a href="#静态x2f动态标签-3" class="header-anchor">#</a></h3><ul>
<li><p>静态标签<br>性别、年龄、地域、收入 相对是静态标签</p>
</li>
<li><p>动态标签<br>用户访问设备、用户的48小时是否活跃、内容访问偏好、消费偏好等属于时常在发生变动的，这些动态特征可以变成动态标签</p>
</li>
</ul>
<h1><span id="圈人-基于标签聚类的人群生成-3">圈人-基于标签聚类的人群生成 [3]</span><a href="#圈人-基于标签聚类的人群生成-3" class="header-anchor">#</a></h1><h3><span id="实现方式">实现方式</span><a href="#实现方式" class="header-anchor">#</a></h3><ul>
<li>按维度(n天内)加权汇总某类主题和实体(买家)在某种对象(课程)上的相关行为(点击 买课)，然后归一化到[0,1]之间，取TopN或全部输出。</li>
<li>调整的可以是维度（天数）, 实体（买家），对象（课程）， 行为(点击 买课)，权重等。</li>
</ul>
<h1><span id="关系库-id-mapping-2">关系库 ID-Mapping [2]</span><a href="#关系库-id-mapping-2" class="header-anchor">#</a></h1><ul>
<li><p>关系库主要是 IDMapping<br>IDMapping主要指用户设备的打通，用于识别用户的唯一性</p>
</li>
<li><p>ID-Mapping的存储<br>JanusGraph</p>
</li>
</ul>
<h1><span id="应用场景3">应用场景[3]</span><a href="#应用场景3" class="header-anchor">#</a></h1><ul>
<li>精细化运营</li>
<li>商业分析</li>
<li>搜索</li>
<li>精准广告营销<br>人群圈选，做定向投放</li>
<li>个性化推荐<br>千人千面</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247499260&idx=2&sn=5c6f1fb40cd90edd63ea7974284af09b">日处理数据量超10亿：友信金服基于Flink构建实时用户画像系统的实践</a></li>
<li><a href="https://mp.weixin.qq.com/s/jyiDWiK0zczEaZKY5Hy5xg">网易大数据用户画像实践 </a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247500642&idx=1&sn=15b22586962cee5c58bb58d898c9a465">用户画像技术及方法论</a> ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/466822319">实战案例：手把手教你构建电商用户画像(附代码）</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>用户画像</category>
      </categories>
      <tags>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title>用户行为分析</title>
    <url>/www6vHomeHexo/2022/09/15/userBehaviorAnalysis/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9E%B6%E6%9E%84-5">架构 [5]</a></li>
<li><a href="#%E5%AE%8C%E6%95%B4%E9%93%BE%E8%B7%AF-4">完整链路 [4]</a></li>
<li><a href="#%E6%8C%87%E6%A0%87-2">指标 [2]</a><ul>
<li><a href="#%E7%B2%98%E6%80%A7%E6%8C%87%E6%A0%87">粘性指标</a></li>
<li><a href="#%E6%B4%BB%E8%B7%83%E6%8C%87%E6%A0%87">活跃指标</a></li>
<li><a href="#%E4%BA%A7%E5%87%BA%E6%8C%87%E6%A0%87">产出指标</a></li>
</ul>
</li>
<li><a href="#%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90">用户行为分析</a><ul>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95-1-2-4-6">常用方法 [1] [2] [4] [6]</a></li>
</ul>
</li>
<li><a href="#%E6%BC%8F%E6%96%97%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90">漏斗模型分析</a><ul>
<li><a href="#%E9%9C%80%E6%B1%82%E5%9C%BA%E6%99%AF-9">需求场景 [9]</a></li>
<li><a href="#%E6%BC%8F%E6%96%97%E7%B1%BB%E5%9E%8B">漏斗类型</a></li>
<li><a href="#%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B">整体流程</a></li>
<li><a href="#aarrr%E6%BC%8F%E6%96%97%E6%A8%A1%E5%9E%8B-1-3">AARRR漏斗模型 [1] [3]</a></li>
</ul>
</li>
<li><a href="#%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90">路径分析</a><ul>
<li><a href="#%E8%BD%AC%E5%8C%96%E7%8E%87%E8%AE%A1%E7%AE%97">转化率计算</a></li>
</ul>
</li>
<li><a href="#%E9%87%87%E9%9B%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE-2">采集用户行为数据 [2]</a><ul>
<li><a href="#%E5%B9%B3%E5%8F%B0%E8%AE%BE%E7%BD%AE%E5%9F%8B%E7%82%B9-6-10">平台设置埋点 [6] [10]</a></li>
<li><a href="#%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BB%9F%E8%AE%A1%E5%B7%A5%E5%85%B7">第三方统计工具</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="架构-5">架构 [5]</span><a href="#架构-5" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/09/15/userBehaviorAnalysis/arch.jpg" class>

<p>图中底层蓝色的部分是数据处理计算部分，依托于存储与计算分离的特性以及预计算能力， 很好地满足在海量数据场景下的数据高效计算能力。同时提供了完善的企业级平台运维处理能力，支持多种架构满足各类客户的场景需求。</p>
<p>往上，通过多维数据模型框架，对用户基础数据进行语义分类和转化，提供多种维度分类的数据主题模型。基于这些数据主题模型，就可以直接对接应用分析端进行常用的事件分析，漏斗分析，留存分析。</p>
<h1><span id="完整链路-4">完整链路 [4]</span><a href="#完整链路-4" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/09/15/userBehaviorAnalysis/flow.jpg" class>

<p>埋点 -&gt; 使用app -&gt; 数据上报 -&gt; 数据模型 -&gt; 行为分析 </p>
<h1><span id="指标-2">指标 [2]</span><a href="#指标-2" class="header-anchor">#</a></h1><h3><span id="粘性指标">粘性指标</span><a href="#粘性指标" class="header-anchor">#</a></h3><ul>
<li>关注用户周期内持续访问的情况<br>新用户数与比例、活跃用户数与比例、用户转化率、用户留存率、用户流失率、用户访问率</li>
</ul>
<h3><span id="活跃指标">活跃指标</span><a href="#活跃指标" class="header-anchor">#</a></h3><ul>
<li>用户访问的参与度<br>活跃用户、新增用户、回访用户、流失用户、平均停留时长、使用频率</li>
</ul>
<h3><span id="产出指标">产出指标</span><a href="#产出指标" class="header-anchor">#</a></h3><ul>
<li>用户创造的直接价值输出<br>页面浏览数PV、独立访客数UV、点击次数、消费频次、消费金额</li>
</ul>
<h1><span id="用户行为分析">用户行为分析</span><a href="#用户行为分析" class="header-anchor">#</a></h1><h3><span id="常用方法-1-2-4-6">常用方法 [1] [2] [4] [6]</span><a href="#常用方法-1-2-4-6" class="header-anchor">#</a></h3><ul>
<li><p>动作分析</p>
<ul>
<li>行为事件分析</li>
<li>页面点击分析</li>
</ul>
</li>
<li><p>转化分析</p>
<ul>
<li><strong>漏斗模型分析</strong> [7] [9]</li>
<li><strong>用户行为路径分析</strong> [7][11]</li>
</ul>
</li>
<li><p>用户分析</p>
<ul>
<li><strong>用户留存分析</strong> [7] [8]</li>
<li><strong>用户分群分析</strong><br>用户画像（基本属性、用户偏好、生活习惯、用户行为等）的标签信息将用户分群</li>
</ul>
</li>
<li><p>福格模型分析</p>
</li>
</ul>
<img src="/www6vHomeHexo/2022/09/15/userBehaviorAnalysis/application.jpg" class>


<h1><span id="漏斗模型分析">漏斗模型分析</span><a href="#漏斗模型分析" class="header-anchor">#</a></h1><h3><span id="需求场景-9">需求场景 [9]</span><a href="#需求场景-9" class="header-anchor">#</a></h3><p>定位用户流失具体原因</p>
<p>针对不同版本，转化率情况对比</p>
<p>检测某个专题活动效果</p>
<h3><span id="漏斗类型">漏斗类型</span><a href="#漏斗类型" class="header-anchor">#</a></h3><p>无序漏斗：在漏斗的周期内，不限定漏斗多个步骤之间事件发生的顺序。<br>有序漏斗：在漏斗的周期内，严格限定漏斗每个步骤之间的发生顺序。</p>
<h3><span id="整体流程">整体流程</span><a href="#整体流程" class="header-anchor">#</a></h3><ul>
<li>确定转化路径</li>
<li>分析流失原因</li>
<li>优化关键因子</li>
</ul>
<h3><span id="aarrr漏斗模型-1-3">AARRR漏斗模型 [1] [3]</span><a href="#aarrr漏斗模型-1-3" class="header-anchor">#</a></h3><ul>
<li>实现用户增长的5个指标<ul>
<li>Acquisition（获取）：指的是用户从各种渠道进入产品的过程。</li>
<li>Activation（激活）：指的是用户开始使用产品，并完成核心操作的过程。</li>
<li>Retention（留存）：指的是用户在完成激活后，继续使用产品，并形成一定程度的用户粘性。</li>
<li>Revenue（收入）：指的是用户在使用产品后，产生的实际收益。</li>
<li>Referral（自传播）：指的是用户在使用产品后，愿意将产品推荐给他人，形成口碑和自然增长。</li>
</ul>
</li>
</ul>
<h1><span id="路径分析">路径分析</span><a href="#路径分析" class="header-anchor">#</a></h1><h3><span id="转化率计算">转化率计算</span><a href="#转化率计算" class="header-anchor">#</a></h3><p>页面转化率<br>路径转化率</p>
<h1><span id="采集用户行为数据-2">采集用户行为数据 [2]</span><a href="#采集用户行为数据-2" class="header-anchor">#</a></h1><h3><span id="平台设置埋点-6-10">平台设置埋点 [6] [10]</span><a href="#平台设置埋点-6-10" class="header-anchor">#</a></h3><h3><span id="第三方统计工具">第三方统计工具</span><a href="#第三方统计工具" class="header-anchor">#</a></h3><p>国内： 百度统计,  CNZZ统计,  GrowingIO,  诸葛IO,  神策IO,  友盟<br>国外：Google Analytics,   Thinking Analytics, Mixpanel, Heap </p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/Sake360/article/details/120350080">用户行为分析</a></li>
<li><a href="https://baijiahao.baidu.com/s?id=1653670195355016641&wfr=spider&for=pc">用户研究：如何做用户行为分析？</a></li>
<li><a href="https://blog.csdn.net/WindyQCF/article/details/123911538">万字详解用户行为分析</a></li>
<li><a href="https://baijiahao.baidu.com/s?id=1663323869315685791&wfr=spider&for=pc">用户行为分析是什么？怎么做？</a></li>
<li><a href="https://www.infoq.cn/article/xZYe1DUopNA9CzLwau3O">数十亿用户数据，上千个用户标签维度，用户分析怎么做？</a> ***<br><a href="https://mp.weixin.qq.com/s?__biz=MzIyNTIyNTYwOA==&mid=2651010996&idx=1&sn=f7ba207a991d595036a11fc3b6797bac">活动回顾 | 数十亿用户数据，上千个用户标签维度，用户分析怎么做？</a>  kylin</li>
<li><a href="https://www.infoq.cn/article/yGOh38XjpYdTKMJjzjoH">如何实现用户行为的动态采集与分析</a></li>
<li><a href="https://www.infoq.cn/article/ecmRgdfrjFl1U3hAd59b">如何基于 Apache Doris 构建简易高效的用户行为分析平台？</a><br><a href="https://www.infoq.cn/article/SoCIclCLD8f4vSzLB4dX">如何基于 Apache Doris 构建简易高效的用户行为分析平台？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI4NjY4MTU5Nw==&mid=2247490504&idx=1&sn=9827b136fa5cfc81467cb1b795f7bc41">用户行为分析模型实践（一）—— 路径分析模型</a>  vivo</li>
<li><a href="https://xie.infoq.cn/article/f305ea8be1935540432aca0d0">用户行为分析模型实践（二）—— 漏斗分析模型</a>  vivo</li>
<li><a href="https://xie.infoq.cn/article/1163e5781f37b4e55a2c43c70">用户行为分析模型实践（三）——H5 通用分析模型</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247486360&idx=1&sn=85504543498dfc82e5e720b77faa602d">基于Spark的用户行为路径分析的产品化实践</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/146639831">【分析框架】用户行为分析</a> *** 未</li>
<li><a href="https://zhuanlan.zhihu.com/p/133962465">淘宝用户行为分析（附Python源码）</a> *** 未</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>用户行为分析</category>
      </categories>
      <tags>
        <tag>用户行为分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 学习资源</title>
    <url>/www6vHomeHexo/2022/09/09/golangStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="golang">Golang</span><a href="#golang" class="header-anchor">#</a></h2><ul>
<li><p>基础</p>
<ul>
<li>极客时间 《Go 并发编程实战课》  鸟窝  ***</li>
<li>极客时间 《Go语言从入门到实战》 好像看过 ***</li>
<li>极客时间 《Go 语言核心 36 讲》 郝林 **</li>
<li>《Effective Go》<br><a href="https://golang.google.cn/doc/effective_go">Effective Go</a>  英文<br><a href="https://learnku.com/docs/effective-go/2020">高效的 Go 编程 Effective Go</a> 中文<br><a href="https://makeoptim.com/golang/effective-go">golang 编程规范 - Effective Go 中文</a> 中文</li>
<li>50 Shades of Go<br><a href="http://devs.cloudimmunity.com/gotchas-and-common-mistakes-in-go-golang/">50 Shades of Go: Traps, Gotchas, and Common Mistakes for New Golang Devs</a><br><a href="https://github.com/wuYin/blog/blob/master/golang/50-shades-of-golang-traps-gotchas-mistakes.md">Golang 新手可能会踩的 50 个坑</a></li>
</ul>
</li>
<li><p>项目&amp;进阶       </p>
<ul>
<li>极客时间 《Go 语言项目开发实战》  孔令飞@腾讯  ***</li>
<li>极客训练营 《go进阶训练营 第4期》 bili V 毛剑  ***</li>
</ul>
</li>
<li><p><a href="https://talkgo.org/">gotalk</a></p>
<ul>
<li><a href="https://github.com/talkgo/night">talkgo @github</a></li>
</ul>
</li>
<li><p>Mix<br> <a href="https://github.com/0voice/Introduction-to-Golang/tree/main/Golang%20PPT">0voice&#x2F;Introduction-to-Golang</a>  *** </p>
</li>
<li><p>Go 源码分析<br><a href="https://github.com/cch123/golang-notes">Go source code analysis(zh-cn) </a>  曹大 滴滴 *** </p>
</li>
<li><p>book<br><a href="https://github.com/gopl-zh/gopl-zh.github.com">Go语言圣经中文版</a><br><a href="https://gopl-zh.github.io/">Go语言圣经（中文版）</a><br><a href="https://chai2010.cn/advanced-go-programming-book/">Go语言高级编程(Advanced Go Programming)</a></p>
</li>
</ul>
<h2><span id="golang-个人blog">Golang 个人blog</span><a href="#golang-个人blog" class="header-anchor">#</a></h2><ul>
<li><a href="https://tonybai.com/">Tony Bai</a> golang大神 ***</li>
<li><a href>鸟窝</a> Java， golang  微博架构师 ***</li>
<li><a href="http://luodw.cc/">罗道文</a>  golang python NSQ 有深度  * 2017 停更</li>
<li><a href="https://www.jianshu.com/u/1381dc29fed9">张晓龙</a>  golang *</li>
</ul>
<h2><span id="学习路线">学习路线</span><a href="#学习路线" class="header-anchor">#</a></h2><ul>
<li>学习路线<ul>
<li><a href="https://www.bilibili.com/video/BV1YY4y1g7RU?vd_source=f6e8c1128f9f264c5ab8d9411a644036">【上集】2022 年 Go 语言最全学习路线：十分钟带你过思维导图！</a></li>
<li><a href="https://www.bilibili.com/video/BV1DZ4y1q78E/?vd_source=f6e8c1128f9f264c5ab8d9411a644036">【下集】2022 年 Go 语言最全学习路线：十分钟带你过思维导图！爆肝几天几夜整理的超详细 Go 学习</a></li>
<li><a href="https://maiyang.me/">作者blog</a></li>
</ul>
</li>
<li><a href="https://www.golangroadmap.com/class/gointerview/">GOLANG ROADMAP</a> ***<br><a href="https://www.golangroadmap.com/">GOLANG ROADMAP</a><br>邀请码：caspar</li>
</ul>
]]></content>
      <categories>
        <category>Golang</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据 调度</title>
    <url>/www6vHomeHexo/2022/09/08/bigDataSchedule/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<ul>
<li><p>Airflow</p>
</li>
<li><p>DolphinScheduler</p>
</li>
<li><p>Azkaban</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>调度</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据 元数据管理</title>
    <url>/www6vHomeHexo/2022/09/08/bigDataMetaMgt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<ul>
<li><p>Apache  Atlas<br>HortonWorks<br>管理Hadoop项目里面的元数据</p>
</li>
<li><p>datahub<br>Linkedin开源<br>The Metadata Platform for the Modern Data Stack</p>
</li>
<li><p>Amundsen<br>Lyft 开源<br>致力于成为现代数据栈中的数据目录产品</p>
</li>
<li><p>Metacat<br>Netflix开源</p>
</li>
<li><p>OpenMetadata</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>元数据</category>
      </categories>
      <tags>
        <tag>元数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Clickhouse</title>
    <url>/www6vHomeHexo/2022/09/07/clickhouse/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%89%B9%E6%80%A7">特性</a></li>
<li><a href="#%E9%99%90%E5%88%B6">限制</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%95%E6%93%8E">数据库引擎</a></li>
<li><a href="#%E8%A1%A8%E5%BC%95%E6%93%8E">表引擎</a><ul>
<li><a href="#mergetree">MergeTree</a></li>
<li><a href="#%E6%97%A5%E5%BF%97">日志</a></li>
<li><a href="#%E9%9B%86%E6%88%90%E5%BC%95%E6%93%8E">集成引擎</a></li>
<li><a href="#%E7%94%A8%E4%BA%8E%E5%85%B6%E4%BB%96%E7%89%B9%E5%AE%9A%E5%8A%9F%E8%83%BD%E7%9A%84%E5%BC%95%E6%93%8E">用于其他特定功能的引擎</a></li>
</ul>
</li>
<li><a href="#%E7%B4%A2%E5%BC%95">索引</a><ul>
<li><a href="#%E4%B8%BB%E9%94%AE%E7%B4%A2%E5%BC%95-3">主键索引 [3]</a></li>
<li><a href="#%E7%A8%80%E7%96%8F%E7%B4%A2%E5%BC%95-4">稀疏索引 [4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h1><ul>
<li>列式数据库</li>
<li>数据压缩</li>
<li>支持SQL</li>
<li>向量引擎</li>
<li>实时的数据更新<br>使查询能够快速在主键中进行范围查找, 以增量的方式有序的存储在MergeTree中</li>
<li>索引<br>按照主键对数据进行排序，对数据特定值或范围的查找。</li>
<li>支持近似计算</li>
<li>支持数据复制和数据完整性<br>副本， 故障后自动恢复</li>
</ul>
<h1><span id="限制">限制</span><a href="#限制" class="header-anchor">#</a></h1><ul>
<li>没有完整的事务支持。</li>
<li>稀疏索引使得ClickHouse不适合通过其键检索单行的点查询</li>
</ul>
<h1><span id="数据库引擎">数据库引擎</span><a href="#数据库引擎" class="header-anchor">#</a></h1><p>默认情况下，ClickHouse使用<strong>Atomic</strong>数据库引擎</p>
<ul>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/mysql">MySQL</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/materialized-mysql">MaterializeMySQL</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/lazy">Lazy</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/atomic">Atomic</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/postgresql">PostgreSQL</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/materialized-postgresql">MaterializedPostgreSQL</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/replicated">Replicated</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/database-engines/sqlite">SQLite</a></li>
</ul>
<h1><span id="表引擎">表引擎</span><a href="#表引擎" class="header-anchor">#</a></h1><h2><span id="mergetree">MergeTree</span><a href="#mergetree" class="header-anchor">#</a></h2><ul>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/mergetree#mergetree">MergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/replacingmergetree#replacingmergetree">ReplacingMergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/summingmergetree#summingmergetree">SummingMergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/aggregatingmergetree#aggregatingmergetree">AggregatingMergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/collapsingmergetree#table_engine-collapsingmergetree">CollapsingMergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/versionedcollapsingmergetree#versionedcollapsingmergetree">VersionedCollapsingMergeTree</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/graphitemergetree#graphitemergetree">GraphiteMergeTree</a></li>
</ul>
<h2><span id="日志">日志</span><a href="#日志" class="header-anchor">#</a></h2><ul>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/log-family/tinylog#tinylog">TinyLog</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/log-family/stripelog#stripelog">StripeLog</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/log-family/log#log">Log</a></li>
</ul>
<h2><span id="集成引擎">集成引擎</span><a href="#集成引擎" class="header-anchor">#</a></h2><ul>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/integrations/kafka#kafka">Kafka</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/integrations/mysql#mysql">MySQL</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/integrations/odbc#table-engine-odbc">ODBC</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/integrations/jdbc#table-engine-jdbc">JDBC</a></li>
<li><a href="https://clickhouse.com/docs/zh/engines/table-engines/integrations/hdfs#hdfs">HDFS</a></li>
</ul>
<h2><span id="用于其他特定功能的引擎">用于其他特定功能的引擎</span><a href="#用于其他特定功能的引擎" class="header-anchor">#</a></h2><ul>
<li><p><a href="https://clickhouse.com/docs/zh/engines/table-engines/special/distributed#distributed">Distributed</a></p>
</li>
<li><p><a href="https://clickhouse.com/docs/zh/engines/table-engines/special/materializedview#materializedview">MaterializedView</a></p>
</li>
</ul>
<h1><span id="索引">索引</span><a href="#索引" class="header-anchor">#</a></h1><h2><span id="主键索引-3">主键索引 [3]</span><a href="#主键索引-3" class="header-anchor">#</a></h2><h2><span id="稀疏索引-4">稀疏索引 [4]</span><a href="#稀疏索引-4" class="header-anchor">#</a></h2><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://clickhouse.com/docs/zh/engines/database-engines">数据库引擎</a></p>
</li>
<li><p><a href="https://clickhouse.com/docs/zh/engines/table-engines">表引擎</a></p>
</li>
<li><p><a href="https://clickhouse.com/docs/zh/guides/best-practices">ClickHouse主键索引最佳实践</a></p>
</li>
<li><p><a href="https://clickhouse.com/docs/zh/guides/improving-query-performance/skipping-indexes">深入理解ClickHouse跳数索引</a></p>
</li>
<li><p><a href="https://blog.csdn.net/wmq880204/article/details/124224992">ClickHouse 深度解析第二篇</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/98135840">ClickHouse深度揭秘</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>Clickhouse</category>
      </categories>
      <tags>
        <tag>Clickhouse</tag>
      </tags>
  </entry>
  <entry>
    <title>Iceberg</title>
    <url>/www6vHomeHexo/2022/09/01/iceberg/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="特性-1-2">特性 [1] [2]</span><a href="#特性-1-2" class="header-anchor">#</a></h2><ul>
<li><p>基于快照的读写分离和回溯</p>
<ul>
<li><strong>快照控制</strong>：可实现使用完全相同的表快照的可重复查询，或者使用户轻松检查更改</li>
<li><strong>Time Travel</strong> </li>
<li><strong>行级更新</strong>  [4]<br>V1版本-Copy On Write（COW）模式<br>V2版本-Copy On Write，还增加了Merge On Read（MOR）</li>
</ul>
</li>
<li><p>流批统一的写入和读取</p>
<ul>
<li>兼容性好：可以存储在任意的云存储系统和HDFS中</li>
<li>快速扫描数据：无需使用分布式SQL引擎即可读取表或查找文件</li>
<li>数据修剪优化：使用表元数据使用分区和列级统计信息修剪数据文件</li>
</ul>
</li>
<li><p>ACID 语义及数据多版本</p>
<ul>
<li>支持<strong>事务</strong>：序列化隔离,表更改是原子性的，读者永远不会看到部分更改或未提交的更改</li>
<li>高并发：<strong>高并发写入</strong>器使用乐观并发，即使写入冲突，也会重试以确保兼容更新成功</li>
<li><strong>版本回滚 Version rollback</strong>：使用户可以通过将表重置为良好状态来快速纠正问题</li>
</ul>
</li>
<li><p>表, 模式及分区的变更</p>
<ul>
<li><strong>模式演化 Schema evolution</strong>：支持添加，删除，更新或重命名，并且没有副作用</li>
<li><strong>隐藏分区 Hidden Partition</strong>：可以防止导致错误提示或非常慢查询的用户错误</li>
<li><strong>分区布局演变 Partition layout evolution</strong>：可以随着数据量或查询模式的变化而更新表的布局</li>
</ul>
</li>
<li><p>不强绑定计算存储引擎</p>
</li>
</ul>
<h2><span id="整体架构-3">整体架构 [3]</span><a href="#整体架构-3" class="header-anchor">#</a></h2><ul>
<li>数据<br>普通的 Parquet 文件</li>
<li>元数据<ul>
<li>catalog<br>version-hint.txt 文件 	</li>
<li>metadata file<br>json 文件</li>
<li>manifestlist file  [snapshot]<br>以 snap- 开头的 avro 文件	</li>
<li>manifest file<br>16db143c,18ce4c4a 开头的 avro 文件</li>
</ul>
</li>
</ul>
<h2><span id="读写过程">读写过程</span><a href="#读写过程" class="header-anchor">#</a></h2><ul>
<li>读写</li>
<li>增量读</li>
<li>实时小文件合并</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/347660549">Flink + Iceberg 全场景实时数仓的建设实践</a>  腾讯数据平台</li>
<li><a href="https://cloud.tencent.com/developer/article/2290397">5分钟入门数据湖IceBerg</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/488467438">Iceberg 原理分析</a></li>
<li><a href="https://z.itpub.net/article/detail/7B5B8C89CC5244F94A0C5FDF7DC83DFB">数据湖Iceberg技术在小米的落地与场景应用</a></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/636273850">Iceberg实时湖仓数据分析性能优化</a><br><a href="https://blog.csdn.net/weixin_46399686/article/details/131308217">火山引擎 Iceberg 数据湖的应用与实践</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247562593&idx=2&sn=a41a5202c21118b1f17619a80eff651f">陈梁：腾讯数据湖查询优化实践 </a></p>
<p><a href="https://zhuanlan.zhihu.com/p/110748218">深度对比delta、iceberg和hudi三大开源数据湖方案</a>  ***<br><a href="https://baijiahao.baidu.com/s?id=1776240000826938540&wfr=spider&for=pc">Apache Iceberg 在严选批流一体的实践</a></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>Iceberg</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>语言 学习资源</title>
    <url>/www6vHomeHexo/2022/08/25/languageStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="rust">Rust</span><a href="#rust" class="header-anchor">#</a></h2><ul>
<li>极客时间 《Rust 编程第一课》  陈天</li>
<li>极客时间 《张汉东的Rust实战课》 视频课</li>
</ul>
<h2><span id="python">Python</span><a href="#python" class="header-anchor">#</a></h2><ul>
<li>极客时间 《零基础学Python》  视频课</li>
<li>极客时间 《Python 自动化办公实战课》</li>
<li>极客时间 《Python核心技术与实战》</li>
<li>极客训练营  《Python进阶训练营 第5期》</li>
</ul>
]]></content>
      <categories>
        <category>语言</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 主从延迟</title>
    <url>/www6vHomeHexo/2022/08/16/mysqlMasterSlaveDelay/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%88%E4%BE%8B-1">案例 [1]</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B%E4%B8%80%E4%B8%BB%E5%BA%93dml%E8%AF%B7%E6%B1%82%E9%A2%91%E7%B9%81">案例一：主库DML请求频繁</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B%E4%BA%8C%E4%B8%BB%E5%BA%93%E6%89%A7%E8%A1%8C%E5%A4%A7%E4%BA%8B%E5%8A%A1">案例二：主库执行大事务</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B%E4%B8%89%E4%B8%BB%E5%BA%93%E5%AF%B9%E5%A4%A7%E8%A1%A8%E6%89%A7%E8%A1%8Cddl%E8%AF%AD%E5%8F%A5">案例三：主库对大表执行DDL语句</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="案例-1">案例 [1]</span><a href="#案例-1" class="header-anchor">#</a></h1><h3><span id="案例一主库dml请求频繁">案例一：主库DML请求频繁</span><a href="#案例一主库dml请求频繁" class="header-anchor">#</a></h3><ul>
<li>解决思路<ul>
<li>如果是MySQL 5.7以下的版本，可以做**分片(sharding)**，通过水平扩展(scale out)的方法打散写请求，提升写请求写入binlog的并行度。</li>
<li>MySQL 5.7以上的版本，<ul>
<li>在MySQL 5.7，使用了<strong>基于逻辑时钟(Group Commit)的并行复制</strong>。</li>
<li>而在MySQL 8.0，使用了<strong>基于Write Set的并行复制</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="案例二主库执行大事务">案例二：主库执行大事务</span><a href="#案例二主库执行大事务" class="header-anchor">#</a></h3><ul>
<li>解决思路<br><strong>拆分大事务语句到若干小事务中</strong>，这样能够进行及时提交，减小主从复制延时。</li>
</ul>
<h3><span id="案例三主库对大表执行ddl语句">案例三：主库对大表执行DDL语句</span><a href="#案例三主库对大表执行ddl语句" class="header-anchor">#</a></h3><ul>
<li>解决思路<ul>
<li><strong>避免业务高峰</strong>，尽量安排在业务低峰期执行 ；</li>
<li>set sql_log_bin&#x3D;0后，分别在主从库上手动执行DDL（此操作对于某些DDL操作会造成数据不一致，请务必严格测试）</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/mingongge/article/details/90310672">高可用数据库主从复制延时的解决方案</a></li>
<li>《26 | 备库为什么会延迟好几个小时？》 未</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>服务发现</title>
    <url>/www6vHomeHexo/2022/08/14/soaDiscovery/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="机制">机制</span><a href="#机制" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/66263484-fa29fb00-e825-11e9-83aa-47bcb97d7580.png" alt="service-find"><br>服务注册和发现</p>
</div>

<h3><span id="模式">模式</span><a href="#模式" class="header-anchor">#</a></h3><ul>
<li>Client-side Discovery  </li>
<li>Server-side Discovery patterns</li>
</ul>
<h1><span id="实现">实现</span><a href="#实现" class="header-anchor">#</a></h1><h3><span id="需求-1">需求 [1]</span><a href="#需求-1" class="header-anchor">#</a></h3><p>RPC 框架依赖的注册中心的服务数据的一致性其实<strong>并不需要满足 CP，只要满足 AP 即可</strong>。</p>
<h3><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h3><ul>
<li>etcd -  CP</li>
<li>nacos </li>
<li>zk  -  CP</li>
<li>eureka -  AP</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《08 | 服务发现：到底是要CP还是AP？》</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>服务发现</category>
      </categories>
      <tags>
        <tag>服务治理</tag>
      </tags>
  </entry>
  <entry>
    <title>优雅关闭</title>
    <url>/www6vHomeHexo/2022/08/14/soaGracefulClose/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="关闭流程">关闭流程</span><a href="#关闭流程" class="header-anchor">#</a></h2><p>关闭流程的优雅处理可以通过以下步骤实现 [gpt 总结]</p>
<ol>
<li>服务提供方在关闭时设置一个<strong>请求挡板</strong>，告知调用方正在关闭并不能处理新的请求。</li>
<li>当服务提供方收到新的业务请求时，直接<strong>返回一个特定的异常（如ShutdownException）给调用方</strong>。</li>
<li><strong>调用方收到异常响应后，将该节点从健康列表中挪出，并自动将请求重试到其他节点，保证业务无损。</strong></li>
<li>除了等待被动调用外，可以加上主动通知流程，提高实时性并避免通知失败的情况。</li>
<li>通过捕获操作系统的进程信号，如使用Java语言中的Runtime.addShutdownHook方法，在关闭钩子中进行关闭标识的设置和服务对象的安全关闭。</li>
<li>在调用链中加入挡板处理器，当新的请求到来时，判断关闭标识，如果正在关闭，则抛出特定异常。</li>
<li>为了完成正在处理的请求，可以在服务对象上添加引用计数器，在开始处理请求前加一，完成处理后减一，根据引用计数器判断是否有正在处理的请求。</li>
<li>服务对象在关闭过程中拒绝新的请求，并根据引用计数器等待正在处理的请求全部结束后真正关闭。</li>
<li>为避免无法正常退出应用，可以在ShutdownHook中添加超时时间控制，当超过指定时间仍未结束，则强制退出应用。</li>
</ol>
<p>通过以上步骤，实现了服务提供方的优雅关闭，保证业务正常处理并最大限度地完成正在处理的请求。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>《13 | 优雅关闭：如何避免服务停机带来的业务损失？》</p>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>优雅关闭</category>
      </categories>
      <tags>
        <tag>优雅关闭</tag>
      </tags>
  </entry>
  <entry>
    <title>优雅启动</title>
    <url>/www6vHomeHexo/2022/08/14/soaGracefulStart/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="优雅启动-实现-1">优雅启动 实现 [1]</span><a href="#优雅启动-实现-1" class="header-anchor">#</a></h2><blockquote>
<p>调用方发起的RPC 调用流程是怎样的，调用方应用通过服务发现能够获取到服务提供方的 IP 地址，然后每次发送请求前，都需要通过负载均衡算法从连接池中选择一个可用连接。<strong>那这样的话，我们是不是就可以让负载均衡在选择连接的时候，区分一下是否是刚启动不久的应用？对于刚启动的应用，我们可以让它被选择到的概率特别低，但这个概率会随着时间的推移慢慢变大，从而实现一个动态增加流量的过程。</strong></p>
</blockquote>
<blockquote>
<p>首先对于调用方来说，我们要<strong>知道服务提供方启动的时间</strong>，这个怎么获取呢？我这里给出两<br>种方法，一种是服务提供方在启动的时候，把自己启动的时间告诉注册中心；另外一种就是<br>注册中心收到的服务提供方的请求注册时间。</p>
</blockquote>
<p>调用方通过服务发现获取服务提供方的IP地址，并通过负载均衡算法选择一个可用连接进行RPC调用。为了实现动态增加流量的过程，可以<strong>让负载均衡在选择连接时区分是否是刚启动不久的应用</strong>。可以通过以下两种方法<strong>获取服务提供方的启动时间</strong>：一种是服务提供方在启动时告知注册中心自己的启动时间，另一种是注册中心记录服务提供方的注册时间。[gpt 总结]</p>
<h2><span id="延迟加载-1">延迟加载 [1]</span><a href="#延迟加载-1" class="header-anchor">#</a></h2><p>上述问题的解决方法是<strong>将应用启动过程中注册服务的步骤延迟到应用启动完成后</strong>，以避免在应用启动未完成时接受请求。此外，<strong>可以在应用启动完成后，预先加载和初始化相关资源，如缓存数据，以降低请求处理错误的概率。</strong> [gpt总结]</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《14 | 优雅启动：如何避免流量打到没有启动完成的节点？》</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>优雅启动</category>
      </categories>
      <tags>
        <tag>db</tag>
      </tags>
  </entry>
  <entry>
    <title>Calico</title>
    <url>/www6vHomeHexo/2022/08/12/k8sCalico1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="best-practice">Best Practice</span><a href="#best-practice" class="header-anchor">#</a></h2><h5><span id="at-a-high-level-the-key-recommendations-are">At a high-level, the key recommendations are:</span><a href="#at-a-high-level-the-key-recommendations-are" class="header-anchor">#</a></h5><ul>
<li>Use the Kubernetes datastore.<br>使用K8S datastore</li>
<li>Install Typha to ensure datastore scalability.<br>安装Typha</li>
<li>Use no encapsulation for single subnet clusters.<br>在单一子网的集群中，不要封装   </li>
<li>Use IP-in-IP in CrossSubnet mode for multi-subnet clusters.<br>在多子网集群中，使用IP-in-IP的跨子网模式</li>
<li>Configure Calico MTU based on the network MTU and the chosen routing mode.<br>MTU</li>
<li>Add global route reflectors for clusters capable of growing above 50 nodes.<br>50个nodes以上使用RR(route reflectors)</li>
<li>Use GlobalNetworkPolicy for cluster-wide ingress and egress rules. Modify the policy by adding namespace-scoped NetworkPolicy.</li>
</ul>
<h2><span id="calico-component-overview">Calico Component Overview</span><a href="#calico-component-overview" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/08/12/k8sCalico1/calico-components.png" class title="Calico Component Overview">

<h5><span id="calico-node">calico-node</span><a href="#calico-node" class="header-anchor">#</a></h5><ul>
<li>Route programming: Based on known routes to pods in the Kubernetes cluster, configure the Linux host to facilitate routing accordingly.   [路由配置  Flex]</li>
<li>Route sharing: Based on pods running on this host, provide a mechanism to share known routes with other hosts. Typically accomplished with (BGP) Border Gateway Protocol.  [路由共享  BIRD]</li>
</ul>
<h5><span id="calico-kube-controller">calico-kube-controller</span><a href="#calico-kube-controller" class="header-anchor">#</a></h5><ul>
<li>The calico-kube-controller is responsible for recognizing changes in Kubernetes objects that impact routing.<br>识别影响路由变化。且包含多个控制器</li>
<li>multiple controllers<ul>
<li>policy-controller</li>
<li>ns-controller</li>
<li>sa-controller</li>
<li>pod-controller</li>
<li>node-controller</li>
</ul>
</li>
</ul>
<h5><span id="typha">Typha</span><a href="#typha" class="header-anchor">#</a></h5><h2><span id="calico-datastore">Calico Datastore</span><a href="#calico-datastore" class="header-anchor">#</a></h2><ul>
<li>Calico supports 2 datastore modes, Kubernetes and etcd.<ul>
<li>Kubernetes Datastore Mode (Recommended)</li>
<li>etcd Datastore Mode</li>
</ul>
</li>
</ul>
<h2><span id="routing-configuration">Routing Configuration</span><a href="#routing-configuration" class="header-anchor">#</a></h2><h5><span id="routing-methods">Routing Methods</span><a href="#routing-methods" class="header-anchor">#</a></h5><ul>
<li><p>3 routing modes </p>
<ul>
<li>Native: Packets routed as-is, no encapsulation.</li>
<li>IP-in-IP: Minimal encapsulation; outer header includes host source&#x2F;destination IPs and inner header includes pod source&#x2F;destination.<br>[tunl]</li>
<li>VXLAN: Robust encapsulation using UDP over IP; outer header includes host source&#x2F;destination IP addresses and inner header includes pod source&#x2F;destination IP addresses as well as Ethernet headers.<br>[on udp, ]</li>
</ul>
</li>
<li><p>1.注意VxLAN模式不需要BGP协议参与！！！但是IPIP模式是需要的。<br>Calico supports two types of encapsulation: VXLAN and IP in IP. VXLAN is supported in some environments where IP in IP is not (for example, Azure). VXLAN has a slightly higher per-packet overhead because the header is larger, but unless you are running very network intensive workloads the difference is not something you would typically notice. The other small difference between the two types of encapsulation is that Calico’s VXLAN implementation does not use BGP, whereas Calico’s IP in IP implementation uses BGP between Calico nodes.   # From Calico Docs</p>
</li>
</ul>
<h5><span id="single-subnet-configuration">Single Subnet Configuration</span><a href="#single-subnet-configuration" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/08/12/k8sCalico1/calico-single-subnet.png" class title="calico-single-subnet">

<h5><span id="multi-subnet-configuration">Multi-Subnet Configuration</span><a href="#multi-subnet-configuration" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/08/12/k8sCalico1/calico-multi-subnet.png" class title="calico-multi-subnet">


<h2><span id="route-distribution">Route Distribution</span><a href="#route-distribution" class="header-anchor">#</a></h2><ul>
<li>BGP Full Mesh<br>node-to-node mesh</li>
<li>Global Route Reflection(RR)</li>
<li>Node-Specific Route Reflection<br>Peering can be configured for <strong>BGP-capable hardware</strong> in a datacenter’s network. Most commonly setup with <strong>top of rack (ToR)</strong> switches.</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://tanzu.vmware.com/developer/guides/container-networking-calico-refarch/">Calico Reference Architecture</a></li>
<li><a href="https://www.yuque.com/wei.luo/cni/agyl5i">20210808-Calico IPIP Mode</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>服务治理-鉴权</title>
    <url>/www6vHomeHexo/2022/08/10/soaAuth/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="备选方案-1">备选方案 [1]</span><a href="#备选方案-1" class="header-anchor">#</a></h1><ul>
<li>分布式 Session</li>
<li>OAuth2.0</li>
<li>JWT</li>
<li>CAS</li>
</ul>
<h3><span id="oauth2-和-jwt的关系gpt4">OAuth2 和 JWT的关系[gpt4]</span><a href="#oauth2-和-jwt的关系gpt4" class="header-anchor">#</a></h3><p>OAuth2和JWT都是用于实现网络应用中的授权和身份验证的技术。但是，它们在实现方式和使用场景上有所不同。</p>
<p><strong>OAuth2</strong>是一个<strong>授权框架</strong>，它允许第三方应用在用户的许可下访问其私有资源。例如，一个应用可以使用OAuth2获取用户的Facebook或Google账户信息，而无需用户提供他们的用户名和密码。</p>
<p><strong>JWT（JSON Web Token）</strong>则是一种<strong>开放标准</strong>（RFC 7519），它定义了一种紧凑且<strong>自包含</strong>的方式，用于在各方之间安全地传输信息作为JSON对象。这些信息可以被验证和信任，因为它们是数字签名的。</p>
<p><strong>OAuth2和JWT可以一起使用</strong>。例如，当一个应用使用OAuth2获取用户的授权时，它可能会接收到一个包含JWT的访问令牌。应用可以解码这个JWT，以获取关于用户的信息，如他们的用户名或电子邮件地址。同时，因为JWT是签名的，应用可以信任这些信息的准确性。</p>
<p>总的来说，OAuth2和JWT都是实现网络应用授权和身份验证的重要工具，但它们在实现细节和使用方式上有所不同。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/107814066">微服务之用户鉴权中心</a></li>
<li><a href="/www6vHomeHexo/2020/03/20/securityOAuth2/" title="安全-OAuth2">安全-OAuth2</a> self</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>鉴权</category>
      </categories>
      <tags>
        <tag>服务治理</tag>
      </tags>
  </entry>
  <entry>
    <title>对象存储</title>
    <url>/www6vHomeHexo/2022/08/08/storageObject/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8">对象存储</a></li>
<li><a href="#%E5%BC%80%E6%BA%90%E7%9A%84%E4%BA%A7%E5%93%81">开源的产品</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81">其他开源产品</a></li>
<li><a href="#%E5%AD%98%E5%82%A8%E5%88%86%E5%B1%82">存储分层</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="对象存储">对象存储</span><a href="#对象存储" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>概念</th>
<th>非开源产品</th>
<th>文件大小</th>
<th>接口</th>
<th>场景</th>
</tr>
</thead>
<tbody><tr>
<td>将数据和元数据当做一个对象</td>
<td>AWS S3，阿里云OSS,Facebook Haystack,</td>
<td>适合各种大小</td>
<td>Restful API</td>
<td>音频，视频，图片。网站资源动静分离; 网盘</td>
</tr>
</tbody></table>
<h1><span id="开源的产品">开源的产品</span><a href="#开源的产品" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>开源的产品</th>
<th>特性</th>
<th>K8S</th>
<th>协议</th>
<th>语言</th>
<th>是否分布式</th>
<th>问题</th>
<th>客户端</th>
<th>Git Star</th>
</tr>
</thead>
<tbody><tr>
<td>ceph</td>
<td>一整套存储解决方案，支持Fuse，大规模可扩展，有自愈能力，成熟</td>
<td>Rook（K8S云存储）</td>
<td>RESTful API ，兼容S3协议 和 OpenStack Swift</td>
<td>C++</td>
<td>是</td>
<td>架构复杂，安装运维复杂，学习成本高</td>
<td>有java客户端</td>
<td>8.5k</td>
</tr>
<tr>
<td>fastdfs</td>
<td>小巧，简单</td>
<td></td>
<td>REST</td>
<td>C</td>
<td></td>
<td><a href="https://blog.csdn.net/zollty/article/details/108331055">fastdfs问题</a></td>
<td>有java客户端</td>
<td>6.9k</td>
</tr>
<tr>
<td>glusterfs</td>
<td>稳定，适合大型应用，支持Fuse</td>
<td>支持</td>
<td>REST(archived)</td>
<td>C</td>
<td>是</td>
<td>扩容麻烦小文件性能较差</td>
<td></td>
<td>2.8k</td>
</tr>
<tr>
<td>MinIO</td>
<td>非常轻量的服务，所有读写操作都严格遵守read-after-write一致性模型可扩容：不同MinIO集群可以组成联邦，并跨越多个数据中心，但不支持动态扩容；可支持扩容</td>
<td>Cloud Native</td>
<td>兼容S3协议</td>
<td>go</td>
<td>是</td>
<td>不支持动态增加节点，后续会采用其它方案来支持扩容可支持扩容</td>
<td>有java客户端</td>
<td>24.7k</td>
</tr>
</tbody></table>
<h1><span id="其他开源产品">其他开源产品</span><a href="#其他开源产品" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>其他开源产品</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>Apache OZone</td>
<td>基于HDFS</td>
</tr>
<tr>
<td>ContainerFS</td>
<td>京东开源</td>
</tr>
<tr>
<td>Lustre</td>
<td>老牌</td>
</tr>
<tr>
<td>OpenStack Swift</td>
<td>依赖OpenStack</td>
</tr>
<tr>
<td>阿里TFS</td>
<td>有大厂在用，但已不更新</td>
</tr>
<tr>
<td>moosefs</td>
<td>性能不错，支持FUSE，C实现，有单点问题</td>
</tr>
</tbody></table>
<h1><span id="存储分层">存储分层</span><a href="#存储分层" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/08/08/storageObject/object-storage.JPG" class title="存储分层">


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/109777654">S3FS 简介及部署</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>存储</category>
        <category>对象存储</category>
      </categories>
      <tags>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>批流一体</title>
    <url>/www6vHomeHexo/2022/08/04/streamingBatchIntegration/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="数据湖-vs-数据仓库1">数据湖  vs 数据仓库[1]</span><a href="#数据湖-vs-数据仓库1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>特性</th>
<th>数据仓库</th>
<th>数据湖</th>
</tr>
</thead>
<tbody><tr>
<td><strong>数据</strong></td>
<td>来自事务系统、运营数据库和业务线应用程序的关系数据</td>
<td>来自 IoT 设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据</td>
</tr>
<tr>
<td><strong>Schema</strong></td>
<td>设计在数据仓库实施之前（写入型 Schema）</td>
<td>写入在分析时（读取型 Schema）</td>
</tr>
<tr>
<td><strong>性价比</strong></td>
<td>更快查询结果会带来较高存储成本</td>
<td>更快查询结果只需较低存储成本</td>
</tr>
<tr>
<td><strong>数据质量</strong></td>
<td>可作为重要事实依据的高度监管数据</td>
<td>任何可以或无法进行监管的数据（例如原始数据）</td>
</tr>
<tr>
<td><strong>用户</strong></td>
<td>业务分析师</td>
<td>数据科学家、数据开发人员和业务分析师（使用监管数据）</td>
</tr>
<tr>
<td><strong>分析</strong></td>
<td>批处理报告、BI 和可视化</td>
<td>机器学习、预测分析、数据发现和分析</td>
</tr>
</tbody></table>
<h1><span id="湖仓一体">湖仓一体</span><a href="#湖仓一体" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://aws.amazon.com/cn/big-data/datalakes-and-analytics/what-is-a-data-lake/">什么是数据湖？</a> AWS</p>
</li>
<li><p><a href="https://developer.aliyun.com/article/706954">大数据架构如何做到流批一体？</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>批流一体</category>
      </categories>
      <tags>
        <tag>批流一体</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-系统构建</title>
    <url>/www6vHomeHexo/2022/08/04/observabilityBuilding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="观察什么1">观察什么[1]</span><a href="#观察什么1" class="header-anchor">#</a></h1><ul>
<li><p>监控能力矩阵<br><img src="https://pic1.zhimg.com/80/v2-ba3823ec17a362eb2cacf406def9ccf8_720w.webp" alt="监控能力矩阵"></p>
</li>
<li><p>监控对象范围<br><img src="https://pic3.zhimg.com/80/v2-fa62d533ac4c097db57dc8562f28cf5a_720w.webp" alt="监控对象范围"></p>
</li>
</ul>
<h1><span id="怎么观察1">怎么观察[1]</span><a href="#怎么观察1" class="header-anchor">#</a></h1><ul>
<li><p>产品架构<br><img src="https://pic1.zhimg.com/80/v2-d74e6fdbee5d2a32c8c035f118adbd54_720w.webp" alt="产品架构"> </p>
</li>
<li><p>技术架构<br><img src="https://pic3.zhimg.com/80/v2-75c71d8c126018976e74014ab3258636_720w.webp" alt="技术架构"></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/594928812">vivo 服务端监控体系建设实践</a> ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/529671344">vivo 容器集群监控系统架构与实践</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/596697068">vivo 故障定位平台的探索与实践</a></li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>系统构建</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT  学习资源</title>
    <url>/www6vHomeHexo/2022/08/01/gptStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E7%A8%8B">工程</a></li>
<li><a href="#%E8%AF%BE%E7%A8%8B">课程</a><ul>
<li><a href="#%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4">极客时间</a></li>
<li><a href="#%E7%9F%A5%E4%B9%8E">知乎</a></li>
<li><a href="#%E6%B8%85%E5%8D%8E">清华</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6">百度</a></li>
<li><a href="#%E4%B9%9D%E5%A4%A9">九天</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E4%B8%9A%E7%95%8C">工业界</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="工程">工程</span><a href="#工程" class="header-anchor">#</a></h1><ul>
<li><a href="https://github.com/www6v/openai-cookbook">openai-cookbook</a><br><a href="https://cookbook.openai.com/">cookbook.openai</a></li>
</ul>
<h1><span id="课程">课程</span><a href="#课程" class="header-anchor">#</a></h1><h3><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h3><ul>
<li><a href="https://shimo.im/docs/47kgM6NewnSO613V">尚硅谷×极客时间《AI 大模型实战训练营》大纲</a> </li>
<li><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a> </li>
<li><a href="https://w.1yb.co/KqBR58E">《AI 大模型微调训练营》大纲</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541101">ChatGPT 从 0 到 1</a>  基础</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541201">ChatGPT 和预训练模型实战课</a></li>
</ul>
<h3><span id="知乎">知乎</span><a href="#知乎" class="header-anchor">#</a></h3><ul>
<li><a href="https://agiclass.feishu.cn/docx/DDzxdQZBooXw9Jx4DdWcLZjLnHd">《AI 大模型全栈工程师》课程表（第 02 期） </a>  </li>
<li><a href="https://www.zhihu.com/people/dou-hong-jian-44/posts">AI Box专栏</a>  中国人大  AI ***<br>大模型survey</li>
</ul>
<h3><span id="清华">清华</span><a href="#清华" class="header-anchor">#</a></h3><ul>
<li><a href="https://www.zhihu.com/education/video-course/1545850719483392000">【清华 NLP X OpenBMB】大模型公开课｜带你从入门到实战</a>  V ***</li>
</ul>
<h3><span id="百度">百度</span><a href="#百度" class="header-anchor">#</a></h3><p><a href="https://cloud.baidu.com/qianfandev/topic/267956">《大模型应用实践》实训营</a></p>
<h3><span id="九天">九天</span><a href="#九天" class="header-anchor">#</a></h3><p><a href="https://appze9inzwc2314.pc.xiaoe-tech.com/p/t_pc/goods_pc_detail/goods_detail/p_64467371e4b0cf39e6c0c026?fromH5=true&entry_type=2002&share_type=5&type=3&entry=2">大模型技术实战课 </a></p>
<h1><span id="工业界">工业界</span><a href="#工业界" class="header-anchor">#</a></h1><p><a href="https://rocketmq-learning.com/">rocketmq-learning 社区</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>分库分表-分页</title>
    <url>/www6vHomeHexo/2022/08/01/dbShardingPaging/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="方案">方案</span><a href="#方案" class="header-anchor">#</a></h1><h3><span id="全局查询法">全局查询法</span><a href="#全局查询法" class="header-anchor">#</a></h3><p>简单</p>
<h3><span id="业务折衷法">业务折衷法</span><a href="#业务折衷法" class="header-anchor">#</a></h3><h5><span id="禁止跳页查询">禁止跳页查询</span><a href="#禁止跳页查询" class="header-anchor">#</a></h5><h5><span id="允许数据精度损失">允许数据精度损失</span><a href="#允许数据精度损失" class="header-anchor">#</a></h5><h3><span id="二次查询法推荐">二次查询法（推荐）</span><a href="#二次查询法推荐" class="header-anchor">#</a></h3><p>复杂</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://juejin.cn/post/7141194628472487972">分库分表必会-跨库分页查询看此一篇就够了</a></li>
<li><a href="https://juejin.cn/post/6981996965353488415">分表分页&#x2F;跨库分页 难玩却不代表没有玩法 </a></li>
<li><a href="https://www.cnblogs.com/yizhiamumu/p/16803364.html">千万级别mysql 分库分表后表分页查询优化方案初探</a></li>
<li><a href="https://mp.weixin.qq.com/s/h99sXP4mvVFsJw6Oh3aU5A">业界难题-“跨库分页”的四种方案 </a> ***  58沈剑</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>DAL</category>
        <category>分库分表</category>
      </categories>
      <tags>
        <tag>DAL</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis  命中率</title>
    <url>/www6vHomeHexo/2022/07/31/redisHitRate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="命中率不高">命中率不高</span><a href="#命中率不高" class="header-anchor">#</a></h1><p>会增加接口响应时间</p>
<h1><span id="提高redis命中率-34">提高redis命中率 [3][4]</span><a href="#提高redis命中率-34" class="header-anchor">#</a></h1><ul>
<li>缓存粒度<br>缓存粒度越小，命中率越高</li>
<li>合理调整缓存有效期的时间<br>  避免缓存同时失效</li>
<li>预加载</li>
<li>防止缓存击穿和穿透[5]</li>
<li>增加存储容量<br>容量不足时会触发Redis内存淘汰机制<ul>
<li>清空策略 [6]<br> FIFO(first in first out)<br>LFU(less frequently used)<br>LRU(least recently used)</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li><a href="https://segmentfault.com/a/1190000023730820">如何提高redis缓存命中率</a></li>
<li><a href="https://www.cnblogs.com/chenhaoyu/p/11308753.html">关于如何提高缓存命中率（redis）</a></li>
<li><a href="/www6vHomeHexo/2022/03/28/redisReliability-1/" title="Redis雪崩、击穿、穿透">Redis雪崩、击穿、穿透</a> self</li>
<li><a href="https://tech.meituan.com/2017/03/17/cache-about.html">缓存那些事</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 集群  容灾（同城多活）</title>
    <url>/www6vHomeHexo/2022/07/31/redisHA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#1-%E8%83%8C%E6%99%AF">1. 背景</a></li>
<li><a href="#2-%E7%9B%AE%E6%A0%87">2. 目标</a></li>
<li><a href="#3-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">3. 解决方案</a><ul>
<li><a href="#31-%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B">3.1 核心能力</a></li>
<li><a href="#32-%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F">3.2 工作模式</a><ul>
<li><a href="#321-%E6%96%B9%E6%A1%88%E4%B8%80%E8%B7%A8%E6%9C%BA%E6%88%BF%E6%B7%B7%E5%90%88%E9%83%A8%E7%BD%B2"><strong>3.2.1 方案一：跨机房混合部署</strong></a><ul>
<li><a href="#3211-%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F"><strong>3.2.1.1 部署方式</strong></a></li>
<li><a href="#3212-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><strong>3.2.1.2 工作机制</strong></a></li>
</ul>
</li>
<li><a href="#322-%E6%96%B9%E6%A1%88%E4%BA%8C%E4%BC%AA%E4%BB%8E%E8%8A%82%E7%82%B9%E5%8D%95%E5%90%91%E5%90%8C%E6%AD%A5"><strong>3.2.2 方案二：伪从节点+单向同步</strong></a><ul>
<li><a href="#3221-%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F"><strong>3.2.2.1 部署方式</strong></a></li>
<li><a href="#3222-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><strong>3.2.2.2 工作机制</strong></a></li>
</ul>
</li>
<li><a href="#323-%E6%96%B9%E6%A1%88%E4%B8%89%E4%BC%AA%E4%BB%8E%E8%8A%82%E7%82%B9%E5%8F%8C%E5%90%91%E5%90%8C%E6%AD%A5"><strong>3.2.3 方案三：伪从节点+双向同步</strong></a><ul>
<li><a href="#3231-%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F"><strong>3.2.3.1 部署方式</strong></a></li>
<li><a href="#3232-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><strong>3.2.3.2 工作机制</strong></a></li>
</ul>
</li>
<li><a href="#324-%E6%96%B9%E6%A1%88%E5%9B%9B%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%8C%E5%86%99%E5%86%99%E7%9B%91%E5%90%AC%E6%9C%8D%E5%8A%A1mq%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><strong>3.2.4 方案四：客户端双写+写监听服务+MQ消息队列</strong></a><ul>
<li><a href="#3241-%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F"><strong>3.2.4.1 部署方式</strong></a></li>
<li><a href="#3242-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><strong>3.2.4.2 工作机制</strong></a></li>
</ul>
</li>
<li><a href="#33-%E6%88%90%E6%9C%AC%E6%AF%94%E8%BE%83">3.3 成本比较</a></li>
<li><a href="#34-%E5%9C%BA%E6%99%AF%E5%BB%BA%E8%AE%AE">3.4 场景建议</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-%E9%87%8C%E7%A8%8B%E7%A2%91">4. 里程碑</a></li>
<li><a href="#5-%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3">5. 相关文档</a></li>
<li><a href="#6-%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">6. 参考资料</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="1-背景">1. 背景</span><a href="#1-背景" class="header-anchor">#</a></h1><ul>
<li>Redis 集群自身已经具备了高可用的特性，即使几个Redis节点异常或者挂掉，Redis 集群也会实现故障自动转移，对应用方来说也可以在很短时间内恢复故障。</li>
<li>但是，如果发生了机房故障(断电、断网等极端情况)，Redis集群节点全部挂掉（过半主节点挂掉），会造成集群服务不可用，对于核心业务来说是不可接受的。</li>
<li>为了应对机房故障情况，保障在这种极端情况下，核心业务仍然可以正常访问 Redis 服务，本文将给出适合我司的 Redis 跨机房高可用解决方案。</li>
</ul>
<h1><span id="2-目标">2. 目标</span><a href="#2-目标" class="header-anchor">#</a></h1><ul>
<li>保障单机房整体故障时 Redis 缓存服务正常运行。</li>
<li>单机房故障 RTO：30s，RPO：1s</li>
</ul>
<h1><span id="3-解决方案">3. 解决方案</span><a href="#3-解决方案" class="header-anchor">#</a></h1><h2><span id="31-核心能力">3.1 核心能力</span><a href="#31-核心能力" class="header-anchor">#</a></h2><ul>
<li>客户端流量路由：支持按一定的策略，把流量分流到不同机房；机房故障后，流量自动流向其他机房。</li>
<li>服务端故障转移：支持机房故障后，当前机房原来主节点的从节点，通过选举自动倒换成新的主节点。</li>
<li>管理平台正常服务：任一机房故障，Renault 管理平台使用不受影响。</li>
</ul>
<h2><span id="32-工作模式">3.2 工作模式</span><a href="#32-工作模式" class="header-anchor">#</a></h2><p>组件在机房高可用场景下一般有多种工作模式，典型的有单集群模式及多集群模式。本节描述组件在各种工作模式下的部署方式及工作机制。</p>
<table>
<thead>
<tr>
<th>部署方案</th>
<th>方案说明</th>
</tr>
</thead>
<tbody><tr>
<td>跨机房混合部署</td>
<td>将Redis集群主节点平均分配到各个机房，主从节点在不同机房</td>
</tr>
<tr>
<td>各机房独立部署集群 + 数据单向同步</td>
<td>各机房均独立部署集群，集群为热备模式，写请求均写入同一个集群，然后同步到其他集群</td>
</tr>
<tr>
<td>各机房独立部署集群 + 数据双向同步</td>
<td>每个机房部署一套Redis集群，同步核心业务写请求</td>
</tr>
</tbody></table>
<p>数据同步方法：</p>
<table>
<thead>
<tr>
<th>数据同步方案</th>
<th>方案说明</th>
</tr>
</thead>
<tbody><tr>
<td>客户端双写</td>
<td>客户端同时写入到各个集群</td>
</tr>
<tr>
<td>客户端代理</td>
<td><a href="https://github.com/Netflix/dynomite">Netflix开源实现的Dynomite</a>，通过代理层接受数据后写入各个需要同步的节点<br><a href="https://github.com/Netflix/dynomite">改造难度大</a><br><a href="https://github.com/Netflix/dynomite">客户端需要优化添加Dyno Client</a><br><a href="https://github.com/Netflix/dynomite">服务端每个节点需要部署Dyno Node</a><br><a href="https://github.com/Netflix/dynomite">read&#x2F;write性能较原生差异较大，</a>主要体现在write上，之间的差异随着node节点数越多越严重</td>
</tr>
<tr>
<td>伪从节点</td>
<td>基于Redis的Master-Slave复制协议，实现低延时、高可用的Redis多数据中心、跨公网数据复制<br>携程开源系统：<a href="https://github.com/ctripcorp/x-pipe">https://github.com/ctripcorp/x-pipe</a><br>阿里RedisShake同步工具</td>
</tr>
<tr>
<td>写事件监听+MQ跨集群消息同步</td>
<td>读写在本机房，监听写事件 + MQ消息同步到其他机房<br>需要开启事件通知（PUB），修改Redis配置文件中的 notify-keyspace-events 配置（默认的redis并没有开启这个功能）<br>需要独立服务订阅写事件（SUB），并同步到其他集群<br>依赖MQ组件</td>
</tr>
</tbody></table>
<p>不考虑客户端代理、发布订阅写事件</p>
<p>下面详细比较如下四种方案</p>
<ul>
<li>方案一：跨机房混合部署</li>
<li>方案二：伪从节点+单向同步</li>
<li>方案三：伪从节点+双向同步</li>
<li>方案四：客户端双写+写监听服务+MQ消息队列</li>
</ul>
<h3><span id="321-方案一跨机房混合部署"><strong>3.2.1 方案一：跨机房混合部署</strong></span><a href="#321-方案一跨机房混合部署" class="header-anchor">#</a></h3><h5><span id="3211-部署方式"><strong>3.2.1.1 部署方式</strong></span><a href="#3211-部署方式" class="header-anchor">#</a></h5><p>Redis集群主节点平均分配到各个机房，每个机房都有一个分片的副本；单个机房主节点数据占比不能过半。</p>
<p>[pic]</p>
<p>跨机房部署注意事项</p>
<ul>
<li>从节点选举需要过半主节点投票，因此不适合双机房部署，至少需要3机房</li>
<li>业务请求访问响应时间会不稳定，同机房请求延迟在0.1ms，跨机房请求在1-3ms</li>
</ul>
<h5><span id="3212-工作机制"><strong>3.2.1.2 工作机制</strong></span><a href="#3212-工作机制" class="header-anchor">#</a></h5><ul>
<li><p>流量路由</p>
</li>
<li><ul>
<li>默认读主节点（ReadFrom：Master）</li>
<li>读从节点（ReadFrom: Replication）</li>
<li>随机读主从（ReadFrom: Any）</li>
<li>优先读本地机房（ReadFrom：LocalDC）— TODO：新增路由策略</li>
</ul>
</li>
<li><p>故障切换</p>
</li>
<li><ul>
<li>机房故障后，Redis 集群高可用机制，会将集群在30s内自动倒换并恢复正常访问</li>
<li>同城机房间网络传输响应延迟2ms内，几乎不影响集群故障判定</li>
<li>Redis 集群故障转移后，客户端30s内自动刷新集群拓扑关系</li>
</ul>
</li>
<li><p>故障恢复</p>
</li>
<li><ul>
<li>机房恢复后，故障节点 Redis 会以从节点角色自动启动，并全量同步主节点数据（数据同步流量风暴 – 会在机房间路由器端口上限速控制）</li>
</ul>
</li>
<li><p>迁移方案</p>
</li>
<li><ul>
<li>服务核心业务 Redis 集群将会改造成跨三机房部署模式，不影响客户端正常使用</li>
<li>业务根据实际情况，可将读请求路由策略修改为优先从本地机房读</li>
</ul>
</li>
</ul>
<p>Redis 故障恢复机制</p>
<ul>
<li><p>投票选主：只有持有槽的主节点才会处理故障选举消息，获得N&#x2F;2+1以上选票的从节点将为新主。</p>
</li>
<li><p>替换主节点：取消复制 → clusterDelSlot&#x2F;clusterAddSlot把槽委派给自己 → 向集群广播通知变为主节点并接管了故障主节点的槽信息。</p>
</li>
<li><ul>
<li>全量同步过程</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>从服务器连接主服务器，发送SYNC命令；</li>
<li>主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件，并使用缓冲区记录此后执行的所有写命令；</li>
<li>主服务器BGSAVE执行完后，向所有从服务器发送RDB文件；</li>
<li>从服务器收到快照文件后丢弃所有旧数据，载入收到的RDB快照；</li>
<li>主服务器快照发送完毕后，开始向从服务器发送缓冲区中的写命令；</li>
<li>从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>全量同步数据评估，假如100GRedis集群，三机房，一主二从</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>根据Renault公共集群统计，8G使用容量的Redis，生成RDB文件大小约3G左右，生成时间75s</li>
<li>RDB传输，单一机房故障恢复时，将有10台Redis执行主从同步，约有30G流量从其他两个机房流入。</li>
<li>RDB加载，3G RDB数据加载时间约90s</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="322-方案二伪从节点单向同步"><strong>3.2.2 方案二：伪从节点+单向同步</strong></span><a href="#322-方案二伪从节点单向同步" class="header-anchor">#</a></h3><h5><span id="3221-部署方式"><strong>3.2.2.1 部署方式</strong></span><a href="#3221-部署方式" class="header-anchor">#</a></h5><p>双机房部署，业务读写默认集群，开发部署 Renault 复制服务（<strong>RRS</strong>, Renault Replicate Service），伪装从节点实时将默认集群数据同步到备用集群。</p>
<p>[pic]</p>
<h5><span id="3222-工作机制"><strong>3.2.2.2 工作机制</strong></span><a href="#3222-工作机制" class="header-anchor">#</a></h5><ul>
<li><p>流量路由</p>
</li>
<li><ul>
<li>双机房独立部署，均访问默认集群（热），故障后访问备用集群（冷）</li>
</ul>
</li>
<li><p>故障切换</p>
</li>
<li><ul>
<li>默认集群故障时，SDK熔断默认集群请求，并将流量切换到备用集群</li>
</ul>
</li>
<li><p>故障恢复</p>
</li>
<li><ul>
<li>机房故障恢复后，原默认集群将被reset，RRS 服务将反向同步</li>
</ul>
</li>
<li><p>迁移方案</p>
</li>
<li><ul>
<li>服务核心业务 Redis 集群将会搭建备用集群，并实时同步默认集群数据</li>
<li>业务客户端需要升级 SDK，支持自动故障切换功能</li>
</ul>
</li>
</ul>
<p>DC2机房业务客户端自动故障切换：</p>
<p>[pic]</p>
<p>故障恢复后（反向同步）：</p>
<p>[pic]</p>
<h3><span id="323-方案三伪从节点双向同步"><strong>3.2.3 方案三：伪从节点+双向同步</strong></span><a href="#323-方案三伪从节点双向同步" class="header-anchor">#</a></h3><h5><span id="3231-部署方式"><strong>3.2.3.1 部署方式</strong></span><a href="#3231-部署方式" class="header-anchor">#</a></h5><p>双机房独立部署，均访问本地集群；开发部署 Renault 复制服务（<strong>RRS</strong>, Renault Replicate Service），伪装从节点实时双向同步。</p>
<p>[pic]</p>
<h5><span id="3232-工作机制"><strong>3.2.3.2 工作机制</strong></span><a href="#3232-工作机制" class="header-anchor">#</a></h5><ul>
<li><p>流量路由</p>
</li>
<li><ul>
<li>双机房独立部署，业务优先访问本地机房集群（TODO：需要开发一种集群选择策略）</li>
<li>RRS 服务双向同步数据（需要解决双向同步成环问题）</li>
</ul>
</li>
<li><p>故障切换</p>
</li>
<li><ul>
<li>DC1机房集群故障时候，上层流量自动切换到DC2机房，Redis层不需要处理</li>
</ul>
</li>
<li><p>故障恢复</p>
</li>
<li><ul>
<li>DC1机房集群故障恢复，首先通过RRS服务从DC2全量同步数据，之后增量同步</li>
</ul>
</li>
<li><p>迁移方案</p>
</li>
<li><ul>
<li>服务核心业务 Redis 集群将会搭建双活集群，并通过 RRS 服务实时双向同步</li>
<li>业务客户端需要升级 SDK，支持启动时选择哪个集群</li>
</ul>
</li>
</ul>
<p>数据库层面的多活，双向同步存在以下困难：</p>
<ul>
<li><p><strong>两边都改了如何解决冲突？</strong></p>
</li>
<li><ul>
<li>跨机房双写无法保证缓存的一致性，需要应用侧可以容忍对应的缓存不一致场景</li>
</ul>
</li>
<li><p><strong>RRS复制中断或者故障如何处理？</strong></p>
</li>
<li><p><strong>数据同步如何防环？</strong></p>
</li>
<li><ul>
<li>方法1：通常需要应用方配置使用，按业务类型分流（不能为通用解决方案）</li>
<li>方法2：数据层面添加字段标识数据源（带来一定开销，并且对于数值计算类数据不能添加标识）</li>
<li>方法3：x-pipe - 定制Redis，在内容分发上做处理，服务端能够识别不同的链接类型，在同步数据之初便加以控制在内容分发上做处理，服务端能够识别不同的链接类型，从而做到有的放矢，在同步数据之初便加以控制</li>
</ul>
</li>
</ul>
<h3><span id="324-方案四客户端双写写监听服务mq消息队列"><strong>3.2.4 方案四：客户端双写+写监听服务+MQ消息队列</strong></span><a href="#324-方案四客户端双写写监听服务mq消息队列" class="header-anchor">#</a></h3><h5><span id="3241-部署方式"><strong>3.2.4.1 部署方式</strong></span><a href="#3241-部署方式" class="header-anchor">#</a></h5><p>双机房&#x2F;三机房每个独立部署，应用客户端均访问本地集群；借助MQ通过异步双写机制双写同步到其他集群。</p>
<p>[pic]</p>
<h5><span id="3242-工作机制"><strong>3.2.4.2 工作机制</strong></span><a href="#3242-工作机制" class="header-anchor">#</a></h5><ul>
<li><p>流量路由</p>
</li>
<li><ul>
<li>多机房独立部署，业务优先访问本地机房集群（同方案三）</li>
<li>MQ中间件+写事件监听服务做双写同步</li>
</ul>
</li>
<li><p>故障切换</p>
</li>
<li><ul>
<li>DC1机房集群故障时候，上层流量自动切换到DC2机房，Redis层不需要处理（同方案三）</li>
</ul>
</li>
<li><p>故障恢复</p>
</li>
<li><ul>
<li>DC1机房集群故障恢复，首先通过RRS服务从DC2全量同步数据，之后增量同步（同方案三）</li>
</ul>
</li>
<li><p>迁移方案</p>
</li>
<li><ul>
<li>服务核心业务 Redis 集群将会搭建多活集群，并通过 MQ 发布订阅方式实现多集群间双向同步</li>
<li>业务客户端需要升级 SDK，支持启动时选择哪个集群</li>
</ul>
</li>
</ul>
<p>双机房&#x2F;三机房每个独立部署，多活部署主要问题：</p>
<ul>
<li><p><strong>两边都改了如何解决冲突？</strong></p>
</li>
<li><p>跨机房双写<strong>无法保证缓存的一致性</strong>，需要<strong>应用侧</strong>可以<strong>容忍对应的缓存不一致场景</strong>，应用如果依赖缓存强一致性，则不合适该方案。</p>
</li>
<li><ul>
<li>DC1和DC2两边都写了同一个Key，最终互相覆盖</li>
</ul>
</li>
<li><p>如何保证消息的顺序？如何保证消息成功发送及消费？</p>
</li>
</ul>
<h3><span id="33-成本比较">3.3 成本比较</span><a href="#33-成本比较" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th></th>
<th>部署成本</th>
<th>改造成本</th>
<th>使用成本</th>
</tr>
</thead>
<tbody><tr>
<td><strong>方案一：跨机房混合部署</strong></td>
<td>成本-低<br>适用于三机房及以上<br>2副本，增加50%容量</td>
<td>改造成本-低<br>只用按照要求部署集群<br>SDK路由-优先读本地机房</td>
<td>性能有下降<br>本机房访问不受影响，跨机房访问延迟1-3ms</td>
</tr>
<tr>
<td><strong>方案二：伪从节点+单向同步</strong></td>
<td>成本-中<br>新增备用集群，增加100%容量<br>部署同步服务</td>
<td>改造成本-中<br>需要开发集群间数据同步服务（已有待完善）<br>读写分离</td>
<td>性能有下降<br>本机房访问不受影响，跨机房访问延迟1-3ms<br>读可优化成读本地机房，性能不受影响</td>
</tr>
<tr>
<td><strong>方案三：伪从节点+双向同步</strong></td>
<td>成本-中<br>各机房均新增集群，双机房增加100%容量，三机房增加200%<br>容量部署同步服务</td>
<td>改造成本-高<br>双向复制成环很难解决，需要定制Redis<br>两边都写，冲突问题难解决<br>需要开发集群间数据同步服务（已有待完善）</td>
<td>性能不受影响<br>读写本地机房</td>
</tr>
<tr>
<td><strong>方案四：客户端双写+写监听+MQ消息同步</strong></td>
<td>成本-高<br>各机房均新增集群，双机房增加100%容量，三机房增加200%容量<br>部署写监听服务<br>部署MQ消息集群</td>
<td>改造成本-中<br>依赖MQ中间件</td>
<td>性能稍有下降<br>同步写跨机房访问，性能会严重下降<br>异步写性能稍有下降<br>应用能容忍两边不一致场景</td>
</tr>
</tbody></table>
<p>方案选定：</p>
<ul>
<li>目前同城多活选用方案一（<strong>跨机房混合部署</strong>）</li>
<li>未来异地多活再考虑方案三和四</li>
</ul>
<h3><span id="34-场景建议">3.4 场景建议</span><a href="#34-场景建议" class="header-anchor">#</a></h3><p>方案一适应场景</p>
<ul>
<li>适用于三机房及以上【两机房故障时候无法成功选举】</li>
<li>要能接受写缓存时间在1-2ms内【不可避免跨集群写数据，目前sh1读写tx1时延约1.3ms，sh1读写sh1时延约0.2ms】</li>
<li>配置优先读本地机房的话，能接收读数据时延【主从数据同步毫秒级时延】</li>
</ul>
<p>其他场景建议</p>
<ul>
<li><p>不接受写缓存慢场景、不接受读从节点数据延迟场景</p>
</li>
<li><ul>
<li>服务端：每个机房都要独立部署Redis集群，并且双向同步数，推荐方案三</li>
<li>SDK改造：需要支持优先读写本地Redis集群路由策略</li>
<li>业务端：升级SDK版本</li>
</ul>
</li>
</ul>
<h1><span id="4-里程碑">4. 里程碑</span><a href="#4-里程碑" class="header-anchor">#</a></h1><p><em>描述组件为了达成机房高可用需要做的事情，包括事项、优先级、预期完成时间、负责人等信息。</em></p>
<p><em>对于中间件，一般有如下重要时间点：</em></p>
<ul>
<li><em>服务端机房高可用方案设计完成</em></li>
<li><em>服务端机房高可用方案在测试集群验证通过</em></li>
<li><em>客户端机房高可用方案设计开发完成</em></li>
<li><em>业务集群机房高可用方案验证：明确 RPO 和 RTO</em></li>
</ul>
<p>方案一跨机房混合部署改造事项：</p>
<ul>
<li><p>管理平台：</p>
</li>
<li><ul>
<li>搭建核心集群</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>机器添加机房标识 (Done)，</li>
<li>机架标识，是否属于同一机架，</li>
<li>机房高可用校验 （Done）</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><ul>
<li>主从节点数至少3个，且分布在不同机房</li>
<li>单机房主节点数不能过半</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>集群调整，确保核心集群Redis实例机房按大集群要求部署，</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>核心应用识别及迁移</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>核心缓存标识 - 源于用户？核心应用标识？</li>
<li>新增核心缓存将绑定到核心集群，非核心缓存绑定到公共集群</li>
<li>缓存客户端监控（CAT QPS）集成到管理平台 - 便于缓存迁移时，从技术角度判定是否需要迁移 </li>
<li>核心缓存迁移</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><ul>
<li>已迁移（arch-100%，cl-100%）</li>
<li>待迁移（md、mkt、yw）（需要推动升级客户端并迁移）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>客户端：</p>
</li>
<li><ul>
<li>读写分离，优先读本机房实例 ，</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>熟悉Lettuce 读写分离相关源码，确定可行方案（方案可行 - 选择NEAREST或者自定义ReadFrom）</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><ul>
<li><a href="https://lettuce.io/core/release/reference/">NEAREST：Read from any node of the cluster with the lowest latency. https://lettuce.io/core/release/reference/</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>自定义ReadFrom，优先从本机房读，本机房无实例则从Master节点读 （done）</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><ul>
<li>用户在 Apollo 配置读取数据方式（Master&#x2F;Slaver&#x2F;Any&#x2F;Nearest&#x2F;LocalZone）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Redis集群：</p>
</li>
<li><ul>
<li>故障演练，</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>repl-timeout 60s适当大小；</li>
<li>完整操作机房Redis实例下线、Redis实例恢复操作</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>缓存恢复时的数据同步抑制方法， （Done，需要时在交换机端口上限速）</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>机房故障后，强制下线故障机房Redis实例</li>
<li>机房恢复后，重新部署Redis实例，逐一添加到集群，并调整合理主从关系</li>
</ul>
</li>
</ul>
</li>
<li><p>ZK &amp; Apollo ， </p>
</li>
<li><ul>
<li>切换演练 </li>
<li>在线客户端列表对比（老zk的会清零，新zk临时节点数目和原来的一致）</li>
<li>将线上ZK从zk1切换到zk3</li>
</ul>
</li>
</ul>
<p>2021-11-30 Redis 集群机房高可用方案整体设计完成</p>
<p>2021-12-31 客户端路由开发、跨集群测试验证通过</p>
<p>2022-2-28 推广接入试点，及真实数据故障演练</p>
<p>2022-3-31 业务集群机房高可用方案验证</p>
<h1><span id="5-相关文档">5. 相关文档</span><a href="#5-相关文档" class="header-anchor">#</a></h1><ul>
<li><a href="https://wiki.tuhu.cn/pages/createpage.action?spaceKey=~wangwei18&title=2.+%E6%9C%BA%E6%88%BF%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88&linkCreation=true&fromPageId=250252290">途虎机房高可用方案</a></li>
<li><a href="https://wiki.tuhu.cn/pages/createpage.action?spaceKey=~wangwei18&title=%E5%90%8C%E5%9F%8E%E5%A4%9A%E6%B4%BB%E8%B5%84%E6%BA%90%E6%88%90%E6%9C%AC%E9%9C%80%E6%B1%82&linkCreation=true&fromPageId=250252290">同城多活资源成本需求调研</a></li>
</ul>
<h1><span id="6-参考资料">6. 参考资料</span><a href="#6-参考资料" class="header-anchor">#</a></h1><ul>
<li><a href="https://tehub.com/a/3Rr1UcvLty">美团KV存储架构及实践</a> *** </li>
<li><a href="https://support.huaweicloud.com/productdesc-dcs/GlobalDRPolicy.html">华为容灾多活策略</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/96917394">Redis异地多活行业方案</a></li>
<li><a href="https://github.com/ctripcorp/x-pipe#%E6%9C%BA%E6%88%BF%E5%88%87%E6%8D%A2">携程Redis多数据中心复制管理系统</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/144527180">携程异地多活-MySQL实时双向（多向）复制实践</a></li>
<li><a href="https://cachecloud.github.io/2016/11/03/Redis%20Cluster%E5%A4%9A%E6%9C%BA%E6%88%BF%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%9E%E7%8E%B0/">Redis Cluster多机房高可用实现–基于客户端</a>  </li>
<li><a href="https://www.modb.pro/db/37775">同城双活-Redis篇</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34958596">饿了么实时双向复制工具</a></li>
</ul>
<hr>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/449398741">CKV+异地容灾探索和实践</a> *** 未</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jb_NnI6pnvJ2eWO6HUrWHg">干货 | 携程Redis跨IDC多向同步实践</a> 未</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/54RX6nSGLBZJxQoaADf6Jw">干货 | 五大实例详解，携程 Redis 跨机房双向同步实践</a> 未</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/trFXXlrel0RmTOCCZjmsWQ">阿里云数据库全新功能Redis读写分离，全维度技术解析</a> 未</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/LA7EaOnaxKjBSTOoCKJSmQ">企业打开Redis的正确方式，来自阿里云云数据库团队的解读</a> 未<br>Figure 2：Redis异地多活架构方案示意图</p>
</li>
<li><p><a href="https://github.com/ctripcorp/x-pipe">https://github.com/ctripcorp/x-pipe</a>  未</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>电商 总结</title>
    <url>/www6vHomeHexo/2022/07/24/eCommerce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="模式">模式</span><a href="#模式" class="header-anchor">#</a></h1><ul>
<li>B2B(经济组织对经济组织)<ul>
<li>平台<br>阿里巴巴， 慧聪网, 中国制造网</li>
<li>工业类<br>震坤行， 1688工业品牌</li>
</ul>
</li>
<li>B2C(经济组织对消费者)<br>当当、京东</li>
<li>B2B2C(企业对企业对消费者)<ul>
<li>定义<br>  第一个B指广义的卖方（即成品、半成品、材料提供商等），第二个B指交易平台，即提供卖方与买方的联系平台，同时提供优质的附加服务，C即指买方。</li>
<li>平台<br>  天猫商城   京东商城   亚马逊中国</li>
</ul>
</li>
<li>C2C(消费者对消费者)<br>淘宝的小店铺</li>
<li>O2O(网上与网下相结合)<br>大众点评 O2O</li>
<li>M2C(生产厂商对消费者)</li>
</ul>
<h1><span id="用户">用户</span><a href="#用户" class="header-anchor">#</a></h1><ul>
<li>每天新增注册用户数＝UV*1%(参考数据)</li>
<li>活跃用户＝注册用户&#x2F;10(参考数据)</li>
<li>最高同时在线＝活跃用户*20%(参考数据)</li>
<li>收费交易客户数＝活跃用户*5%(参考数据)</li>
<li>销售额：收费交易客户数*商品平均价格</li>
<li>客单价: per customer transaction零售术语又称ATV，即每位顾客平均购买商品金额</li>
</ul>
<h1><span id="商品物流及客户管理">商品物流及客户管理</span><a href="#商品物流及客户管理" class="header-anchor">#</a></h1><ul>
<li><p>QC：即英文Quality Control的简称，中文意义是品质控制，又称质检</p>
</li>
<li><p>SKU：即英文Stock Keeping Unit的简称，即库存进出计量的单位</p>
</li>
<li><p>3PL：即第三方物流(Third Party Logistics)</p>
</li>
<li><p>实际库存：实际仓库中的实际库存量。</p>
</li>
<li><p>虚拟库存：即网站前台展示的库存数量，是电商行业特定属性的产物</p>
</li>
<li><p>库存预警：库存预警是指设置一个库存警戒线</p>
</li>
<li><p>ITO库存周转率：Inventory turn over，一般缩写为ITO，一种衡量材料在工厂里或是整条价值流中，流动快慢的标准。</p>
</li>
<li><p>SRM是Supplier Relationship Management的缩写，即供应商关系管理。</p>
</li>
<li><p>ERP是Enterprise Resource Planning的缩写，即企业资源计划。</p>
</li>
<li><p>OMS是Order Management System的缩写，即订单管理系统。</p>
</li>
<li><p>CRM：是Customer Relationship Management的缩写，即客户关系管理。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://note.youdao.com/s/1FbvLSjw">【扫盲】史上最全的互联网专业词语汇总，小白必备，人手一套！</a></p>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>电商</category>
      </categories>
      <tags>
        <tag>应用架构</tag>
      </tags>
  </entry>
  <entry>
    <title>整洁架构</title>
    <url>/www6vHomeHexo/2022/07/22/cleanCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="整洁架构-clean-architecture">整洁架构 Clean Architecture</span><a href="#整洁架构-clean-architecture" class="header-anchor">#</a></h1><ul>
<li><p>核心观点 [7][8]</p>
<ul>
<li>不与框架绑定<br>java-spring, Quarkus</li>
<li>可测试<br>mock- gomock, Testify</li>
<li>不与UI绑定</li>
<li>不与数据库绑定<br>DDD 中的Repo</li>
<li>不依赖任何外部代理</li>
</ul>
</li>
<li><p>Go的实现 [9][10]</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="7">
<li><p>《24 直播：框架之上的业务分层》  体系课_Go高级工程师实战营(完结)</p>
</li>
<li><p><a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">The Clean Architecture</a></p>
</li>
<li><p><a href="https://github.com/eminetto/clean-architecture-go-v2">clean-architecture-go-v2</a> git</p>
</li>
<li><p><a href="https://github.com/bxcodec/go-clean-arch">go-clean-arch</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/454054072">Golang 简洁架构实战</a>  未</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/608097903">Go整洁架构实践</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>整洁架构</category>
      </categories>
      <tags>
        <tag>应用架构</tag>
      </tags>
  </entry>
  <entry>
    <title>异步化 Reactive</title>
    <url>/www6vHomeHexo/2022/07/21/asyncReactive/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#reactive">Reactive</a></li>
<li><a href="#java8-rxjava-reactor%E6%AF%94%E8%BE%8311">Java8、RxJava、Reactor比较[11]</a></li>
<li><a href="#%E5%85%A8%E5%BC%82%E6%AD%A5%E5%8C%96">全异步化</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><br>  * <a href="#reactivereactivex">Reactive，ReactiveX</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="reactive">Reactive</span><a href="#reactive" class="header-anchor">#</a></h1><ul>
<li>ReactiveX<br>An API for asynchronous programming  with observable streams</li>
<li>响应式流（Reactive Stream）<br>具备“异步非阻塞”特性和“流量控制”能力的数据流.</li>
<li>RSocket<br> 是一个支持 reactive-stream 语义的开源网络通信协议，它将 reactive 语义的复杂逻辑封装了起来，使得上层可以方便实现网络程序。</li>
</ul>
<h1><span id="java8-rxjava-reactor比较11">Java8、RxJava、Reactor比较[11]</span><a href="#java8-rxjava-reactor比较11" class="header-anchor">#</a></h1>

<ul>
<li>核心特性 async，back-pressure(Stream)</li>
</ul>
<h1><span id="全异步化">全异步化</span><a href="#全异步化" class="header-anchor">#</a></h1><ul>
<li>全异步化(基于消息和事件)【7,8】<ul>
<li>框架  Akka （Actor+mailbox）</li>
<li>库  RxJava (Observable+event) </li>
<li>协议 RSocket</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h5><span id="reactivereactivex">Reactive，ReactiveX</span><a href="#reactivereactivex" class="header-anchor">#</a></h5><ol start="7">
<li><a href="https://github.com/benjycui/introrx-chinese-edition?utm_source=tuicool&utm_medium=referral">The introduction to Reactive Programming</a></li>
<li><a href="https://mp.weixin.qq.com/s/Cfg-7MzabvPOLWrrlTVXzA">全面异步化：淘宝反应式架构升级探索</a></li>
<li><a href="https://www.baeldung.com/rsocket">Introduction to RSocket</a></li>
<li><a href="https://juejin.im/post/5cd04b6e51882540e53fdfa2">我为什么不再推荐RxJava</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1356284">八个层面比较 Java 8, RxJava, Reactor</a> good</li>
<li><a href="http://reactivex.io/">reactivex 官网</a></li>
<li><a href="https://github.com/www6v/reactive-streams-jvm">reactive-streams-jvm git</a></li>
<li><a href="http://www.reactive-streams.org/">Reactive Streams</a></li>
<li><a href="https://blog.csdn.net/get_set/article/details/79455258">（1）什么是响应式编程——响应式Spring的道法术器</a> 未</li>
<li><a href="https://blog.csdn.net/get_set/article/details/79466402">（2）响应式流——响应式Spring的道法术器</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>异步化</category>
      </categories>
      <tags>
        <tag>异步化</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring  Transaction  失效</title>
    <url>/www6vHomeHexo/2022/07/20/springTransactionInvalid/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="spring事务失效问题">Spring事务失效问题</span><a href="#spring事务失效问题" class="header-anchor">#</a></h1><h3><span id="spring事务失效-1">spring事务失效 [1]</span><a href="#spring事务失效-1" class="header-anchor">#</a></h3><pre><code>- 场景：普通方法调用事务方法时，事务会失效
- 解决：要在普通方法(一般是最外层)上加上@Transactional
</code></pre>
<h3><span id="代理不生效-2">代理不生效 [2]</span><a href="#代理不生效-2" class="header-anchor">#</a></h3><ul>
<li><p><strong>非public修饰的方法</strong><br>在AbstractFallbackTransactionAttributeSource类的computeTransactionAttribute方法中有个判断，如果目标方法不是public，则TransactionAttribute返回null，即不支持事务。</p>
</li>
<li><p>被final、static关键字修饰的类或方法<br>spring事务底层使用了aop，也就是通过jdk动态代理或者cglib，帮我们生成了代理类，在代理类中实现的事务功能。但如果某个方法用final修饰了，那么在它的代理类中，就无法重写该方法，而添加事务功能。</p>
</li>
<li><p><strong>类方法内部调用</strong><br>updateStatus方法拥有事务的能力是因为spring aop生成代理了对象，但是这种方法直接调用了this对象的方法，所以updateStatus方法不会生成事务</p>
<ul>
<li>解决方案</li>
</ul>
<ul>
<li>新加一个Service方法</li>
<li>在该Service类中注入自己</li>
<li><strong>通过AopContent类</strong></li>
</ul>
</li>
<li><p>当前类没有被Spring管理</p>
</li>
<li><p>多线程调用<br>spring的事务是通过数据库连接来实现的。当前线程中保存了一个map，key是数据源，value是数据库连接。<br>同一个事务，其实是指同一个数据库连接，只有拥有同一个数据库连接才能同时提交和回滚。如果在不同的线程，拿到的数据库连接肯定是不一样的，所以是不同的事务。</p>
</li>
<li><p>(存储引擎)表不支持事务</p>
</li>
<li><p>未开启事务<br>springboot通过DataSourceTransactionManagerAutoConfiguration类，已经默默的帮你开启了事务。<br>使用的还是传统的spring项目，则需要在applicationContext.xml文件中，手动配置事务相关参数。如果忘了配置，事务肯定是不会生效的。</p>
</li>
<li><p>将注解标注在接口方法上</p>
</li>
</ul>
<h3><span id="错误使用transactional-2">错误使用@Transactional [2]</span><a href="#错误使用transactional-2" class="header-anchor">#</a></h3><ul>
<li><p>错误的传播机制<br>目前只有这三种传播特性才会创建新事务：REQUIRED，REQUIRES_NEW，NESTED。</p>
</li>
<li><p><strong>异常被内部catch</strong><br>如果想要spring事务能够正常回滚，必须抛出它能够处理的异常。如果没有抛异常，则spring认为程序是正常的。</p>
</li>
<li><p>rollbackFor属性设置错误</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Transactional(rollbackFor = BusinessException.class)</span><br><span class="line">public void add(UserModel userModel) throws Exception &#123;</span><br><span class="line">   saveData(userModel);</span><br><span class="line">   updateData(userModel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>嵌套事务<br>可以将内部嵌套事务放在try&#x2F;catch中，并且不继续往上抛异常。这样就能保证，如果内部嵌套事务中出现异常，只回滚内部事务，而不影响外部事务。</p>
</li>
<li><p>手动抛了别的异常</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://segmentfault.com/a/1190000014617571">Spring 踩坑之@Transactional 神奇失效  小鱼儿</a></li>
<li><a href="https://blog.csdn.net/mccand1234/article/details/124571619">spring事务（注解 @Transactional ）失效的12种场景</a> </li>
<li><a href="https://www.45fan.com/article.php?aid=1CO8aGBW5f63eGYH">spring中12种@Transactional的失效场景(小结)</a></li>
<li><a href="https://www.jianshu.com/p/9a0de6577ed7">Spring @Async&#x2F;@Transactional 失效的原因及解决方案</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>spring</category>
        <category>事务</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink SQL</title>
    <url>/www6vHomeHexo/2022/07/18/flinkSQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://xie.infoq.cn/article/b3adcb53fb87e66a613326f19">最佳实践｜如何写出简单高效的 Flink SQL？</a></li>
<li><a href="https://baijiahao.baidu.com/s?id=1709543568363038743">深入分析 Flink SQL 工作机制</a><br><a href="https://blog.csdn.net/weixin_44904816/article/details/106678639">深入分析 Flink SQL 工作机制</a></li>
<li><a href="https://flink.apache.org/2020/07/28/flink-sql-demo-building-an-end-to-end-streaming-application/">Flink SQL Demo: Building an End-to-End Streaming Application</a>   做过</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>流式计算</tag>
      </tags>
  </entry>
  <entry>
    <title>实时数仓</title>
    <url>/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a><ul>
<li><a href="#%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84">参考架构</a><ul>
<li><a href="#%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84-%E5%BF%AB%E6%89%8B-%E5%9F%BA%E4%BA%8Ekafkahive-lambda%E6%9E%B6%E6%9E%84-6">参考架构-快手 基于Kafka+Hive-Lambda架构   [6]</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84-vivo-%E5%9F%BA%E4%BA%8Ehudi-%E6%89%B9%E6%B5%81%E4%B8%80%E4%BD%93%E6%9E%B6%E6%9E%84-4">参考架构-vivo 基于Hudi-批流一体架构   [4]</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84-%E8%85%BE%E8%AE%AF-%E5%9F%BA%E4%BA%8Ehudi-%E6%89%B9%E6%B5%81%E4%B8%80%E4%BD%93%E6%9E%B6%E6%9E%84-3">参考架构-腾讯 基于Hudi-批流一体架构 [3]</a></li>
<li><a href="#%E7%BE%8E%E5%9B%A2-%E5%9F%BA%E4%BA%8Edoris-1">美团 基于Doris [1]</a></li>
</ul>
</li>
<li><a href="#%E7%97%9B%E7%82%B9-5">痛点 [5]</a><ul>
<li><a href="#%E4%BC%A0%E7%BB%9F-t1-%E4%BB%BB%E5%8A%A1">传统 T+1 任务</a></li>
<li><a href="#lambda-%E6%9E%B6%E6%9E%84%E7%97%9B%E7%82%B9">Lambda 架构痛点</a></li>
<li><a href="#kappa-%E6%9E%B6%E6%9E%84%E7%97%9B%E7%82%B9">Kappa 架构痛点</a></li>
</ul>
</li>
<li><a href="#tradeoff-%E6%80%BB%E7%BB%93-5">Tradeoff 总结 [5]</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E5%88%86%E5%B1%82-1">实时数仓-分层 [1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%BA%90-ods">数据源 ODS</a></li>
<li><a href="#%E6%98%8E%E7%BB%86%E5%B1%82-dmd">明细层 DMD</a></li>
<li><a href="#%E6%B1%87%E6%80%BB%E5%B1%82-dms">汇总层 DMS</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B-1">实时数仓-构建流程 [1]</a></li>
<li><a href="#%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1">数仓建模</a><ul>
<li><a href="#%E4%BA%8B%E5%AE%9E%E8%A1%A8-2">事实表 [2]</a></li>
<li><a href="#%E7%BB%B4%E5%BA%A6%E8%A1%A8-dim-2">维度表 DIM [2]</a></li>
<li><a href="#%E7%BB%B4%E5%BA%A6%E5%85%B3%E8%81%94-6">维度关联 [6]</a></li>
<li><a href="#ads-%E5%B1%82-%E4%BB%A5%E7%AA%97%E5%8F%A3%E4%B8%BA%E6%A0%B8%E5%BF%83%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-6">ADS 层 - 以窗口为核心的解决方案 [6]</a></li>
</ul>
</li>
<li><a href="#zhyt">ZHYT</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h1><h3><span id="参考架构">参考架构</span><a href="#参考架构" class="header-anchor">#</a></h3><h5><span id="参考架构-快手-基于kafkahive-lambda架构-6">参考架构-快手 基于Kafka+Hive-Lambda架构   [6]</span><a href="#参考架构-快手-基于kafkahive-lambda架构-6" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/kuaishou-arch.jpg" class>


<h5><span id="参考架构-vivo-基于hudi-批流一体架构-4">参考架构-vivo 基于Hudi-批流一体架构   [4]</span><a href="#参考架构-vivo-基于hudi-批流一体架构-4" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/vivo-arch.jpg" class>


<h5><span id="参考架构-腾讯-基于hudi-批流一体架构-3">参考架构-腾讯 基于Hudi-批流一体架构 [3]</span><a href="#参考架构-腾讯-基于hudi-批流一体架构-3" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/tencent-arch.jpg" class>

<h5><span id="美团-基于doris-1">美团 基于Doris [1]</span><a href="#美团-基于doris-1" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/arch.png" class>

<img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/arch1.png" class>

<h3><span id="痛点-5">痛点 [5]</span><a href="#痛点-5" class="header-anchor">#</a></h3><h5><span id="传统-t1-任务">传统 T+1 任务</span><a href="#传统-t1-任务" class="header-anchor">#</a></h5><ul>
<li>海量的TB级 T+ 1 任务延迟导致下游数据产出时间不稳定。</li>
<li>任务遇到故障重试恢复代价昂贵</li>
<li>数据架构在处理去重和 exactly-once语义能力方面比较吃力</li>
<li>架构复杂，涉及多个系统协调，靠调度系统来构建任务依赖关系</li>
</ul>
<h5><span id="lambda-架构痛点">Lambda 架构痛点</span><a href="#lambda-架构痛点" class="header-anchor">#</a></h5><ul>
<li>同时维护<strong>实时平台和离线平台两套引擎</strong>，运维成本高</li>
<li>实时离线两个平台需要维护两套框架不同但业务逻辑相同代码，开发成本高</li>
<li>数据有两条不同链路，<strong>容易造成数据的不一致性</strong></li>
<li><strong>数据更新成本大</strong>，需要重跑链路</li>
</ul>
<h5><span id="kappa-架构痛点">Kappa 架构痛点</span><a href="#kappa-架构痛点" class="header-anchor">#</a></h5><ul>
<li>对消息队列存储要求高，<strong>消息队列的回溯能力不及离线存储</strong></li>
<li><strong>消息队列本身对数据存储有时效性</strong>，<strong>且当前无法使用 OLAP 引擎直接分析消息队列中的数据</strong></li>
<li>全链路依赖消息队列的实时计算可能因为<strong>数据的时序性</strong>导致结果不正确</li>
</ul>
<h3><span id="tradeoff-总结-5">Tradeoff   总结 [5]</span><a href="#tradeoff-总结-5" class="header-anchor">#</a></h3><p>总的来说，数据湖 替换 Kafka 的<strong>优势</strong>主要包括：</p>
<ul>
<li>实现存储层的<strong>流批统一</strong></li>
<li><strong>中间层支持 OLAP 分析</strong></li>
<li>完美支持高效<strong>回溯</strong></li>
<li>存储成本降低</li>
</ul>
<p>当然，也存在一定的<strong>缺陷</strong>，如：</p>
<ul>
<li>数据延迟从<strong>实时</strong>变成<strong>近实时</strong></li>
<li>对接其他数据系统需要额外开发工作</li>
</ul>
<h1><span id="实时数仓-分层-1">实时数仓-分层 [1]</span><a href="#实时数仓-分层-1" class="header-anchor">#</a></h1><h3><span id="数据源-ods">数据源  ODS</span><a href="#数据源-ods" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/basic.png" class>



<h3><span id="明细层-dmd">明细层 DMD</span><a href="#明细层-dmd" class="header-anchor">#</a></h3><ul>
<li>目的是给下游提供直接可用的数据</li>
<li>要对基础层进行统一的加工，比如清洗、过滤、扩维等</li>
<li>按照主题进行管理</li>
</ul>
<h3><span id="汇总层-dms">汇总层 DMS</span><a href="#汇总层-dms" class="header-anchor">#</a></h3><ul>
<li>所有的指标都统一在汇总层加工</li>
<li>汇总指标池<br>按照统一的规范管理建设，形成可复用的汇总结果</li>
</ul>
<h1><span id="实时数仓-构建流程-1">实时数仓-构建流程 [1]</span><a href="#实时数仓-构建流程-1" class="header-anchor">#</a></h1><ul>
<li><p>搭框架<br>数据建设的层次化</p>
</li>
<li><p>定规范<br>每一层加工到什么程度，每一层用什么样的方式</p>
</li>
<li><p>时效性<br>设计的时候，层次不能太多</p>
</li>
</ul>
<h1><span id="数仓建模">数仓建模</span><a href="#数仓建模" class="header-anchor">#</a></h1><h3><span id="事实表-2">事实表  [2]</span><a href="#事实表-2" class="header-anchor">#</a></h3><ul>
<li>事务事实表   </li>
<li>周期快照事实表 </li>
<li>累积快照事实表</li>
</ul>
<h3><span id="维度表-dim-2">维度表 DIM [2]</span><a href="#维度表-dim-2" class="header-anchor">#</a></h3><ul>
<li><p>类型</p>
<ul>
<li>星型模型<br>维表只和事实表关联，维表之间没有关联，查询性能好，但冗余度高<br>一般而言，我们都使用星型模型。</li>
<li>雪花模型<br>雪花模型是星型模式中的维度表进行规范化处理，进一步分解到附加表（维表）中冗余度小，但是查询性能差。<br>将一个维表拆成核心表和拓展表</li>
</ul>
</li>
<li><p>存储</p>
<ul>
<li>HBase</li>
<li>Redis</li>
<li>MySQL</li>
</ul>
</li>
</ul>
<h3><span id="维度关联-6">维度关联 [6]</span><a href="#维度关联-6" class="header-anchor">#</a></h3><p>在 DWD 层的实战中，DWD 表需要进行<strong>维度扩展</strong>是非常常见的需求。在我们的实战中，维表扩展会基于维表的具体情况选择不同的关联方式。</p>
<ul>
<li>在大多数情况下<strong>维表变化比较稳定</strong>，我们会选择借助<strong>第三方 KV 存储</strong>，使用 UDF 直接访问 KV 存储来实现维表扩展。但在选择第三方 KV 存储时，当维表内容特别大时选择 kiwi、当 QPS 较高时选择 Kcatch。</li>
<li>当<strong>维表变化频繁且对时效性要求较高时</strong>，<strong>选择 interval join</strong>。借助 interval 时间范围的特性来达到合理控制状态大小的目的。</li>
<li>当<strong>维表关联逻辑比较复杂</strong>，为了任务的稳定性和扩展性，我们会<strong>通过自定义维表进行关联</strong>，手动维护状态管理的过程，实现 DWD 维表的扩展。</li>
</ul>
<p>实时数仓的 <strong>DWS 层只有在数据量特别大且聚合后的数据量有明显减少的场景下才会构建</strong>。<strong>如果 DWD 层的 QPS 比较小，一般会直接省去 DWS 层的建设</strong>。这样的做法不仅可以保证数据的<strong>及时性</strong>，同时也<strong>缩短了指标产出的链路</strong>，进而保证了任务的稳定性。</p>
<h3><span id="ads-层-以窗口为核心的解决方案-6">ADS 层 - 以窗口为核心的解决方案 [6]</span><a href="#ads-层-以窗口为核心的解决方案-6" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>场景</th>
<th>窗口</th>
</tr>
</thead>
<tbody><tr>
<td>在针对当日累计的场景，即要求每分钟实时产出从当天 0 点开始到当前统计时间分钟截止的总指标值的需求，</td>
<td>cumulate window。</td>
</tr>
<tr>
<td>针对活动累计场景，即活动一般会持续 n 天，则需求要求每分钟实时产出从活动开始到当前统计时刻为止的总指标值。</td>
<td>infinity_cumulate window。</td>
</tr>
<tr>
<td>在针对分布类的指标需求时，即需求指标会随着时间的推移出现波动。同一粒度下我们需先拿到最新的数据状态，再进行下一步汇总的统计。</td>
<td>unbounded+infinity_cumulate window。</td>
</tr>
<tr>
<td>在针对单直播间累计的场景下，</td>
<td>dynamic_cumulate。</td>
</tr>
</tbody></table>
<h1><span id="zhyt">ZHYT</span><a href="#zhyt" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/zhyt.png" class>

<img src="/www6vHomeHexo/2022/07/18/realtimeDataWarehouse/zhyt-hangqing.png" class>





<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://tech.meituan.com/2021/08/26/data-warehouse-in-meituan-waimai.html">美团外卖实时数仓建设实践</a>  美团 </li>
<li><a href="https://notomato.blog.csdn.net/article/details/110635856">一篇文章搞懂数据仓库：三种事实表（设计原则，设计方法、对比）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/523028640">[数据湖] 基于flink hudi的批流一体实践</a> 腾讯  </li>
<li><a href="https://zhuanlan.zhihu.com/p/594928870">vivo 实时计算平台建设实践</a>  vivo </li>
<li><a href="https://zhuanlan.zhihu.com/p/347660549">Flink + Iceberg 全场景实时数仓的建设实践</a>  腾讯数据平台</li>
</ol>
<ul>
<li><a href="https://flink-learning.org.cn/activity/detail/9075f73ecfd2b87c6c7fbe7d79ad58ca">FFA 2022 实时湖仓</a>  ***<ul>
<li><a href="https://xie.infoq.cn/article/3c80a350e06d88e85d34f4008">美团买菜基于 Flink 的实时数仓建设</a>  未</li>
<li><ol start="6">
<li><a href="https://flink-learning.org.cn/article/detail/de3aa90d2f02195e65e721c1f2a434e1">快手基于 Apache Flink 的实时数仓建设实践</a>  ***</li>
</ol>
</li>
</ul>
</li>
<li><a href="https://flink-learning.org.cn/activity/detail/d3d092c45467c40fb8526c4ec2141be2">FFA 2022 平台建设</a>  ***<ul>
<li><a href="https://xie.infoq.cn/article/acf64bbe900ec426b8699f094">小米基于 Flink 的实时数仓建设实践</a> 未</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>实时数仓</category>
      </categories>
      <tags>
        <tag>实时数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Prometheus业务监控</title>
    <url>/www6vHomeHexo/2022/07/17/observabilityPrometheusBiz/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8A%93%E5%8F%96%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F">抓取业务指标的三种⽅式</a><ul>
<li><a href="#%E7%99%BD%E7%9B%92%E7%9B%91%E6%8E%A7%E6%9C%80%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8C%87%E6%A0%87%E6%8A%93%E5%8F%96%E6%96%B9%E5%BC%8F">白盒监控（最常用的指标抓取方式）</a></li>
<li><a href="#%E9%BB%91%E7%9B%92%E7%9B%91%E6%8E%A7-%E5%85%B6%E4%BB%96%E6%8C%87%E6%A0%87%E6%8A%93%E5%8F%96%E6%96%B9%E5%BC%8F">黑盒监控 （其他指标抓取方式）</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Eannotations-%E6%8A%93%E5%8F%96-%E4%B8%8D%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8">基于Annotations 抓取 (不推荐使用)</a></li>
</ul>
</li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87%E5%91%8A%E8%AD%A6">业务指标告警</a><ul>
<li><a href="#%E9%81%BF%E5%85%8D%E5%91%8A%E8%AD%A6%E7%96%B2%E5%8A%B3">避免告警疲劳 ***</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="抓取业务指标的三种方式">抓取业务指标的三种⽅式</span><a href="#抓取业务指标的三种方式" class="header-anchor">#</a></h1><h3><span id="白盒监控最常用的指标抓取方式">白盒监控（最常用的指标抓取方式）</span><a href="#白盒监控最常用的指标抓取方式" class="header-anchor">#</a></h3><ul>
<li><p>通过 CRD 配置指标抓取</p>
<ul>
<li>ServiceMonitor<ul>
<li><strong>通过 Label 选择器匹配</strong>：ServiceMonitor-&gt;Service-&gt;Endpoints-&gt;Pod</li>
<li><strong>首选类型</strong>，可抓取多个 Pod 指标</li>
</ul>
</li>
<li>PodMonitor<ul>
<li><strong>通过 Label 选择器直接匹配 Pod</strong></li>
<li>适合无 Service 的场景，例如 CronJobs、DaemonSets</li>
</ul>
</li>
</ul>
</li>
<li><p>优势</p>
<ul>
<li>无需学习复杂的 Prometheus 配置，包含默认的 relabeling</li>
<li>分离工程团队和基础设施团队</li>
</ul>
</li>
<li><p>Query Metrics 简单例子</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/// 过去 1 分钟，第 90 个百分位数请求延迟时间的直方图</span><br><span class="line">histogram_quantile(     ---------&gt; 获取直⽅图</span><br><span class="line">   0.9,     ---------&gt; 获取第 90 个百分位数请求延迟时间</span><br><span class="line">     sum(       ---------&gt; 聚合桶，并按桶的上限(le)进行分组</span><br><span class="line">        rate(       ---------&gt;  获得每个桶的变化速率</span><br><span class="line">           http_response_time_seconds_bucket&#123;job=&quot;week9-app&quot;&#125;[1m]    ---------&gt; 过去 1 分钟内 HTTP 请求延迟并分桶</span><br><span class="line">       )</span><br><span class="line">     )</span><br><span class="line">   by(le)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="黑盒监控-其他指标抓取方式">黑盒监控 （其他指标抓取方式）</span><a href="#黑盒监控-其他指标抓取方式" class="header-anchor">#</a></h3><ul>
<li><strong>通过 CRD 配置指标抓取</strong><ul>
<li>Probe<ul>
<li>需要<strong>部署 blackbox_exporter</strong></li>
<li>可以被用在黑盒监控（如 HTTP 探针、网站可用性监控）</li>
<li>监控静态的目标（如网站）或者 Ingress 对象</li>
<li>通过 Label 选择器匹配 Ingress</li>
</ul>
</li>
</ul>
</li>
<li>优势<ul>
<li><strong>不侵入业务逻辑</strong></li>
<li>快速发现故障</li>
</ul>
</li>
</ul>
<h3><span id="基于annotations-抓取-不推荐使用">基于Annotations 抓取 (不推荐使用)</span><a href="#基于annotations-抓取-不推荐使用" class="header-anchor">#</a></h3><h1><span id="业务指标告警">业务指标告警</span><a href="#业务指标告警" class="header-anchor">#</a></h1><h3><span id="避免告警疲劳">避免告警疲劳 ***</span><a href="#避免告警疲劳" class="header-anchor">#</a></h3><ul>
<li><p>告警疲劳</p>
<ul>
<li>过多告警导致忽视，最终酿成生产事故</li>
</ul>
</li>
<li><p>生产实践</p>
<ul>
<li>Alertmanager <strong>route 参数优化</strong><ul>
<li>group_wait：等待多长时间才为一个组发送告警（通常等待告警抑制或收集同一组更多的告警），默认 30s</li>
<li>group_interval：在发送新的告警之前等待多少时间，默认 5m</li>
<li>repeat_interval：重复告警的发送时间，默认 4h</li>
</ul>
</li>
<li>对<strong>告警规则合理分组</strong>减少告警数量，并设置合理的触发持续时间<ul>
<li>group_by：比如按命名空间、集群、数据中心分组</li>
<li>for：5m、10m</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>高纬度告警对象</strong>，例如针对数据中心而不是实例级</p>
</li>
<li><p>提供解决手册和仪表盘链接，更快解决问题</p>
<ul>
<li>annotations:<ul>
<li>runbook_url</li>
</ul>
</li>
</ul>
</li>
<li><p>正确使用 Alertmanager 的功能特性</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>痛点<br></th>
<th>手段</th>
<th>Alertmanager</th>
</tr>
</thead>
<tbody><tr>
<td>只向特定的团队发送告警</td>
<td>路由</td>
<td>通过标签将告警路由到特定的接收者<br></td>
</tr>
<tr>
<td>同时发出了太多告警</td>
<td>抑制</td>
<td>在其他告警触发时抑制某些告警<br></td>
</tr>
<tr>
<td>告警误报</td>
<td>静音</td>
<td>暂时静音告警，比如定期执行维护时<br></td>
</tr>
<tr>
<td><strong>警报过于频繁</strong></td>
<td>节流</td>
<td>优化 router 中的 group_wait、group_interval、repeat_interval 参数<br></td>
</tr>
<tr>
<td>告警组织混乱</td>
<td>分组<br></td>
<td>例如按 environment 标签对告警进行逻辑分组<br>大规模场景不推荐使用 instance、ip_address、instance_id 等分组<br></td>
</tr>
<tr>
<td>通知信息不足</td>
<td>通知模板</td>
<td>使用模板标准化告警消息，添加更多信息</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>《基于 Prometheus 业务指标弹性伸缩》  pdf   极客时间公开课</p>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>metric</category>
      </categories>
      <tags>
        <tag>metric</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis LRU算法</title>
    <url>/www6vHomeHexo/2022/07/16/redisLRU/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lru-%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0">LRU 算法的实现</a><ul>
<li><a href="#%E5%85%A8%E5%B1%80-lru-%E6%97%B6%E9%92%9F%E5%80%BC%E7%9A%84%E8%AE%A1%E7%AE%97">全局 LRU 时钟值的计算</a></li>
<li><a href="#%E9%94%AE%E5%80%BC%E5%AF%B9-lru-%E6%97%B6%E9%92%9F%E5%80%BC%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E6%9B%B4%E6%96%B0">键值对 LRU 时钟值的初始化与更新</a></li>
<li><a href="#%E8%BF%91%E4%BC%BC-lru-%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E9%99%85%E6%89%A7%E8%A1%8C">近似 LRU 算法的实际执行</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="lru-算法的实现">LRU 算法的实现</span><a href="#lru-算法的实现" class="header-anchor">#</a></h1><p>Redis 对近似 LRU 算法的实现分成了三个部分</p>
<h3><span id="全局-lru-时钟值的计算">全局 LRU 时钟值的计算</span><a href="#全局-lru-时钟值的计算" class="header-anchor">#</a></h3><p>这部分包括，Redis 源码为了实现近似 LRU 算法的效果，是<br>如何计算全局 LRU 时钟值的，以用来判断数据访问的时效性；</p>
<ol>
<li><strong>全局 LRU 时钟值就是通过 getLRUClock 函数计算得到的</strong>。</li>
<li><strong>如果一个数据前后两次访问的时间间隔小于 1 秒，那么这 两次访问的时间戳就是一样的</strong>。</li>
<li>serverCron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值<br>是由 Redis 配置文件 redis.conf 中的 <strong>hz 配置项</strong>决定的。hz 配置项的默认值是 10，这表<br>示 serverCron 函数会每 100 毫秒（1 秒 &#x2F;10 &#x3D; 100 毫秒）运行一次。</li>
</ol>
<p>这样，在 serverCron 函数中，全局 LRU 时钟值就会按照这个函数的执行频率，定期调用<br>getLRUClock 函数进行更新，如下所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">serverCron</span><span class="params">(<span class="keyword">struct</span> aeEventLoop *eventLoop, <span class="type">long</span> <span class="type">long</span> id, <span class="type">void</span> *clientData)</span> </span><br><span class="line">... </span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> lruclock = getLRUClock(); <span class="comment">//默认情况下，每100毫秒调用getLRUClock函数更</span></span><br><span class="line">atomicSet(server.lruclock,lruclock); <span class="comment">//设置lruclock变量</span></span><br><span class="line">... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#hz默认设为10，提高它的值将会占用更多的cpu，当然相应的redis将会更快的处理同时到期的许多key，以及更精确的去处理超时。</span><br><span class="line">#hz的取值范围是1~500，通常不建议超过100，只有在请求延时非常低的情况下可以将值提升到100。</span><br><span class="line">hz 10</span><br></pre></td></tr></table></figure>

<h3><span id="键值对-lru-时钟值的初始化与更新">键值对 LRU 时钟值的初始化与更新</span><a href="#键值对-lru-时钟值的初始化与更新" class="header-anchor">#</a></h3><p>这部分包括，Redis 源码在哪些函数中对每个键值对对应的 LRU 时钟值，进行初始化与更新；</p>
<h3><span id="近似-lru-算法的实际执行">近似 LRU 算法的实际执行</span><a href="#近似-lru-算法的实际执行" class="header-anchor">#</a></h3><p>这部分包括，Redis 源码具体如何执行近似 LRU 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。</p>
<ol>
<li>何时触发算法执行？</li>
<li>算法具体如何执行？</li>
</ol>
<img src="/www6vHomeHexo/2022/07/16/redisLRU/r-lru.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>《15 | 为什么LRU算法原理和代码实现不一样？》</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>性能优化-池化Pool</title>
    <url>/www6vHomeHexo/2022/07/14/performancePool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%9E%E6%8E%A5%E6%B1%A0%E4%BC%98%E5%8C%96">连接池优化</a><ul>
<li><a href="#%E5%88%86%E7%B1%BB">分类</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0">数据库连接池</a></li>
</ul>
</li>
<li><a href="#%E7%BA%BF%E7%A8%8B%E6%B1%A0">线程池</a><ul>
<li><a href="#jdk%E7%BA%BF%E7%A8%8B%E6%B1%A0">JDK线程池</a></li>
<li><a href="#tomcat%E7%BA%BF%E7%A8%8B%E6%B1%A0">Tomcat线程池</a></li>
<li><a href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">最佳实践</a></li>
</ul>
</li>
<li><a href="#%E5%AF%B9%E8%B1%A1%E6%B1%A0">对象池</a><ul>
<li><a href="#netty-%E5%AF%B9%E8%B1%A1%E6%B1%A0-12">Netty 对象池 [1][2]</a></li>
<li><a href="#tomcat%E4%B8%AD%E7%9A%84%E5%AF%B9%E8%B1%A1%E6%B1%A0-3">Tomcat中的对象池 [3]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="连接池优化">连接池优化</span><a href="#连接池优化" class="header-anchor">#</a></h1><h3><span id="分类">分类</span><a href="#分类" class="header-anchor">#</a></h3><ul>
<li>数据库连接池 </li>
<li>redis连接池 </li>
<li>http连接池</li>
</ul>
<h3><span id="数据库连接池">数据库连接池</span><a href="#数据库连接池" class="header-anchor">#</a></h3><ul>
<li>最小连接数， 最大连接数<br> 建议<strong>最小连接数控制在 10 左右</strong>，<strong>最大连接数控制在 20～30 左右</strong></li>
<li>连接的可用性<br> 使用连接发送<strong>“select 1”</strong>的命令给数据库看是否会抛出异常，如果抛出异常则将这个连接从连接池中移除，并且尝试关闭.</li>
</ul>
<h1><span id="线程池">线程池</span><a href="#线程池" class="header-anchor">#</a></h1><h3><span id="jdk线程池">JDK线程池</span><a href="#jdk线程池" class="header-anchor">#</a></h3><p>重要参数： <strong>coreThreadCount</strong> 和 <strong>maxThreadCount</strong>，</p>
<img src="/www6vHomeHexo/2022/07/14/performancePool/jdk-threadpool.PNG" class title="JDK线程池">

<p>[chat]<br>当大量请求进入线程池时，ThreadPoolExecutor 会根据其内部的线程池参数来处理请求。具体来说，当请求进入线程池时，ThreadPoolExecutor 会首先尝试将请求放入工作队列中，如果工作队列已满，则会根据线程池的配置来执行饱和策略，例如抛出异常、直接拒绝请求、阻塞等待或调用调用者运行等待任务。</p>
<ul>
<li>线程池中的线程会根据核心线程数和最大线程数的设置进行扩缩容。<ul>
<li><strong>当有新的任务提交到线程池时，如果当前线程数小于核心线程数，线程池会创建新的线程来处理该任务。</strong></li>
<li><strong>如果当前线程数已经达到核心线程数，线程池会将任务放入工作队列中，如果工作队列已满，则线程池会创建新的线程来处理该任务，直到达到最大线程数为止。</strong></li>
<li>当线程池中的线程数量超过核心线程数时，空闲的线程会根据线程空闲时间的超时时间来自动销毁，以保证线程池中的线程数量不会无限制增长。</li>
</ul>
</li>
</ul>
<p>需要注意的是，在ThreadPoolExecutor中，线程池的线程数应该设置为适当的值，如果<strong>设置过小</strong>，则可能会导致线程饱和，影响任务的处理效率；如果<strong>设置过大</strong>，则会浪费系统资源，同时可能会导致线程竞争和锁的争用，影响系统的性能。因此，在使用ThreadPoolExecutor时，需要根据系统的负载和任务特性来合理地设置线程池的参数，以达到最佳的性能和效率。<br>[&#x2F;chat]</p>
<h3><span id="tomcat线程池">Tomcat线程池</span><a href="#tomcat线程池" class="header-anchor">#</a></h3><p>Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，<strong>当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达maxThreadCount</strong>，这样就比较适合于 Web 系统<strong>大量 IO 操作的场景</strong>了。</p>
<h3><span id="最佳实践">最佳实践</span><a href="#最佳实践" class="header-anchor">#</a></h3><p>大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会<strong>频繁地触发 Full GC</strong>，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个<strong>线程池使用了无界队列</strong>.</p>
<h1><span id="对象池">对象池</span><a href="#对象池" class="header-anchor">#</a></h1><h3><span id="netty-对象池-12">Netty 对象池 [1][2]</span><a href="#netty-对象池-12" class="header-anchor">#</a></h3><p>netty为了避免过多的创建对象和频繁的gc使用了对象池，在需要创建ByteBuf的时候，从对象池中找，如果没有才会去创建一个新的ByteBuf。</p>
<h3><span id="tomcat中的对象池-3">Tomcat中的对象池 [3]</span><a href="#tomcat中的对象池-3" class="header-anchor">#</a></h3><p>Tomcat 和 Jetty 都用到了对象池技术，这是因为处理一次 HTTP 请求的时间比较短，但是<br>这个过程中又需要创建大量复杂对象。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/qq_27785239/article/details/105827771">Netty的对象池</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/379687050?utm_id=0">Netty对象池</a></li>
<li>《20 | 总结：Tomcat和Jetty中的对象池技术》</li>
<li>《 07 | 池化技术：如何减少频繁创建数据库连接的性能损耗？》 唐扬</li>
</ol>
]]></content>
      <categories>
        <category>性能</category>
        <category>Pool</category>
      </categories>
      <tags>
        <tag>性能</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云 数据库</title>
    <url>/www6vHomeHexo/2022/07/13/aliyunDB/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-1">Overview [1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93-%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF">数据库 使用场景</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B%E9%97%B4%E7%9A%84%E5%90%8C%E6%AD%A5">数据库之间的同步</a></li>
</ul>
</li>
<li><a href="#%E9%98%BF%E9%87%8C%E4%BA%91-%E6%95%B0%E6%8D%AE%E5%BA%93-1">阿里云 数据库 [1]</a><ul>
<li><a href="#mysql">MySQL</a></li>
<li><a href="#polardb">PolarDB</a></li>
<li><a href="#redis">Redis</a></li>
<li><a href="#hbase">Hbase</a></li>
<li><a href="#hybriddb">HybridDB</a></li>
<li><a href="#mongodb">MongoDB</a></li>
</ul>
</li>
<li><a href="#mysql%E5%AE%B9%E7%81%BE-1">MySQL容灾 [1]</a><ul>
<li><a href="#%E5%9C%BA%E6%99%AF-%E5%A4%8D%E5%88%B6">场景: 复制</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF-%E4%B8%A4%E5%9C%B0n%E4%B8%AD%E5%BF%83">场景: 两地N中心</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF-%E5%8F%8C%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E5%8F%AF%E7%94%A8%E6%80%A7%E6%9C%BA%E6%88%BF%E7%BA%A7">场景: 双节点服务可用性(机房级)</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF-%E5%8F%8C%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E5%8F%AF%E7%94%A8%E6%80%A7%E5%9C%B0%E5%9F%9F%E7%BA%A7">场景: 双节点服务可用性(地域级)</a></li>
</ul>
</li>
<li><a href="#mysql%E6%89%A9%E5%B1%951">MySQL扩展[1]</a><ul>
<li><a href="#%E6%B7%BB%E5%8A%A0%E5%8F%AA%E8%AF%BB%E5%AE%9E%E4%BE%8B">(添加)只读实例</a></li>
<li><a href="#%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB">读写分离</a></li>
<li><a href="#mysql%E6%A8%AA%E5%90%91%E6%89%A9%E5%B1%95">MySQL横向扩展</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h1><h3><span id="数据库-使用场景">数据库 使用场景</span><a href="#数据库-使用场景" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/13/aliyunDB/changjing.JPG" class>

<h3><span id="数据库之间的同步">数据库之间的同步</span><a href="#数据库之间的同步" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/13/aliyunDB/db-sync.JPG" class>

<h1><span id="阿里云-数据库-1">阿里云 数据库 [1]</span><a href="#阿里云-数据库-1" class="header-anchor">#</a></h1><h3><span id="mysql">MySQL</span><a href="#mysql" class="header-anchor">#</a></h3><ul>
<li>类型<ul>
<li>单机版</li>
<li>高可用版</li>
<li>金融版</li>
</ul>
</li>
</ul>
<h3><span id="polardb">PolarDB</span><a href="#polardb" class="header-anchor">#</a></h3><ul>
<li>场景<ul>
<li>使用mysql的所有业务场景</li>
<li>容量大</li>
<li>业务弹性灵活</li>
</ul>
</li>
</ul>
<h3><span id="redis">Redis</span><a href="#redis" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/13/aliyunDB/redis.JPG" class>

<h3><span id="hbase">Hbase</span><a href="#hbase" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/13/aliyunDB/hbase.JPG" class>

<h3><span id="hybriddb">HybridDB</span><a href="#hybriddb" class="header-anchor">#</a></h3><h3><span id="mongodb">MongoDB</span><a href="#mongodb" class="header-anchor">#</a></h3><h1><span id="mysql容灾-1">MySQL容灾 [1]</span><a href="#mysql容灾-1" class="header-anchor">#</a></h1><h3><span id="场景-复制">场景: 复制</span><a href="#场景-复制" class="header-anchor">#</a></h3><ul>
<li>异步复制</li>
<li>半同步复制</li>
</ul>
<h3><span id="场景-两地n中心">场景: 两地N中心</span><a href="#场景-两地n中心" class="header-anchor">#</a></h3><p>DTS同步数据</p>
<h3><span id="场景-双节点服务可用性机房级">场景: 双节点服务可用性(机房级)</span><a href="#场景-双节点服务可用性机房级" class="header-anchor">#</a></h3><h3><span id="场景-双节点服务可用性地域级">场景: 双节点服务可用性(地域级)</span><a href="#场景-双节点服务可用性地域级" class="header-anchor">#</a></h3><h1><span id="mysql扩展1">MySQL扩展[1]</span><a href="#mysql扩展1" class="header-anchor">#</a></h1><h3><span id="添加只读实例">(添加)只读实例</span><a href="#添加只读实例" class="header-anchor">#</a></h3><h3><span id="读写分离">读写分离</span><a href="#读写分离" class="header-anchor">#</a></h3><p>Proxy 层</p>
<h3><span id="mysql横向扩展">MySQL横向扩展</span><a href="#mysql横向扩展" class="header-anchor">#</a></h3><p>MySQL 用DTS迁移到  HybridDB</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1ky4y1a7Sk?p=3">课时3: 云上数据库架构设计及解决方案</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库-读写分离</title>
    <url>/www6vHomeHexo/2022/07/12/dbReadWrite/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://www.bilibili.com/video/BV1kB4y1z78E/">ShardingSphere 核心功能+实操之「读写分离」</a></p>
]]></content>
      <categories>
        <category>中间件</category>
        <category>DAL</category>
        <category>读写分离</category>
      </categories>
      <tags>
        <tag>DAL</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis Cluster Spec</title>
    <url>/www6vHomeHexo/2022/07/11/redisClusterSpec/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87%E5%92%8C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86bing">设计目标和基本原理[bing]</a><ul>
<li><a href="#%E9%AB%98%E6%80%A7%E8%83%BD%E5%92%8C%E7%BA%BF%E6%80%A7%E6%89%A9%E5%B1%95">高性能和线性扩展</a></li>
<li><a href="#%E5%8F%AF%E6%8E%A5%E5%8F%97%E7%9A%84%E5%86%99%E5%AE%89%E5%85%A8%E6%80%A7">可接受的写安全性</a></li>
<li><a href="#%E5%8F%AF%E7%94%A8%E6%80%A7">可用性</a></li>
<li><a href="#%E5%93%88%E5%B8%8C%E6%A7%BD%E5%88%86%E9%85%8D">哈希槽分配</a></li>
<li><a href="#%E8%8A%82%E7%82%B9%E9%97%B4%E9%80%9A%E4%BF%A1">节点间通信</a></li>
<li><a href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E5%AE%9A%E5%90%91">客户端重定向</a></li>
<li><a href="#%E5%93%88%E5%B8%8C%E6%A7%BD%E8%BF%81%E7%A7%BB">哈希槽迁移</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E5%92%8C%E5%89%AF%E6%9C%AC%E6%8F%90%E5%8D%87">故障检测和副本提升</a></li>
</ul>
</li>
<li><a href="#%E5%89%AF%E6%9C%AC%E9%80%89%E4%B8%BE%E5%92%8C%E9%85%8D%E7%BD%AE%E4%BC%A0%E6%92%AD%E7%9A%84%E6%9C%BA%E5%88%B6bing">副本选举和配置传播的机制[bing]</a><ul>
<li><a href="#%E5%89%AF%E6%9C%AC%E9%80%89%E4%B8%BE">副本选举</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E4%BC%A0%E6%92%AD">配置传播</a></li>
<li><a href="#%E8%8A%82%E7%82%B9%E9%87%8D%E7%BD%AE%E5%92%8C%E9%81%97%E5%BF%98">节点重置和遗忘</a></li>
<li><a href="#%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85">发布订阅</a></li>
<li><a href="#crc16%E7%AE%97%E6%B3%95">CRC16算法</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="设计目标和基本原理bing">设计目标和基本原理[bing]</span><a href="#设计目标和基本原理bing" class="header-anchor">#</a></h1><h3><span id="高性能和线性扩展">高性能和线性扩展</span><a href="#高性能和线性扩展" class="header-anchor">#</a></h3><p>  Redis集群可以支持到<strong>1000个节点</strong>，使用异步复制和无代理的架构，避免了值合并的开销。</p>
<h3><span id="可接受的写安全性">可接受的写安全性</span><a href="#可接受的写安全性" class="header-anchor">#</a></h3><p>  Redis集群尽力保留与大多数主节点连接的客户端的写操作，但在<strong>分区窗口期内</strong>可能会<strong>丢失已确认的写操作</strong>。</p>
<h3><span id="可用性">可用性</span><a href="#可用性" class="header-anchor">#</a></h3><p>  Redis集群可以在大多数主节点可达且每个不可达主节点至少有一个可达副本的情况下存活，并且可以通过副本迁移来提高抗故障能力。</p>
<h3><span id="哈希槽分配">哈希槽分配</span><a href="#哈希槽分配" class="header-anchor">#</a></h3><p>Redis集群将<strong>16384个哈希槽</strong>分配给不同的主节点，每个主节点负责存储和服务一部分哈希槽。客户端可以通过计算键的CRC16值对16384取模来得到对应的哈希槽。也可以使用哈希标签来强制将多个键分配到同一个哈希槽，以支持多键操作。</p>
<h3><span id="节点间通信">节点间通信</span><a href="#节点间通信" class="header-anchor">#</a></h3><p>  Redis集群节点之间使用一个TCP端口和一个二进制协议进行通信，称为集群总线。每个节点都与其他所有节点连接，使用<strong>gossip协议</strong>来传播集群的信息，如新节点、故障节点、配置变化等。</p>
<h3><span id="客户端重定向">客户端重定向</span><a href="#客户端重定向" class="header-anchor">#</a></h3><p>  Redis集群节点不会代理请求，而是根据自己的哈希槽映射表，将客户端重定向到正确的节点。客户端可能收到<strong>MOVED或ASK错误</strong>，表示需要重新发送请求到另一个节点。MOVED表示哈希槽被永久地分配给了另一个节点，ASK表示哈希槽正在被迁移，需要发送ASKING命令后再发送请求。</p>
<h3><span id="哈希槽迁移">哈希槽迁移</span><a href="#哈希槽迁移" class="header-anchor">#</a></h3><p>  Redis集群支持在运行时添加和删除节点，这是通过将哈希槽从一个节点移动到另一个节点来实现的。迁移过程中，源节点会将接收到的关于该哈希槽的请求重定向到目标节点，或者直接处理已存在键的请求。目标节点会接收到源节点通过MIGRATE命令发送过来的键，并在迁移完成后更新自己的配置。</p>
<h3><span id="故障检测和副本提升">故障检测和副本提升</span><a href="#故障检测和副本提升" class="header-anchor">#</a></h3><p>  Redis集群使用心跳包和gossip协议来检测节点是否可达，并使用<strong>PFAIL和FAIL</strong>两种标志来表示故障状态。当一个主节点被大多数主节点标记为FAIL时，它的一个副本会发起选举并请求投票。如果获得了大多数主节点的授权，该副本会提升为主节点，并生成一个新的配置版本号（configEpoch）。</p>
<h1><span id="副本选举和配置传播的机制bing">副本选举和配置传播的机制[bing]</span><a href="#副本选举和配置传播的机制bing" class="header-anchor">#</a></h1><h3><span id="副本选举">副本选举</span><a href="#副本选举" class="header-anchor">#</a></h3><p>  当一个主节点失效时，它的一个副本会尝试发起选举，向其他主节点发送授权请求。如果获得了大多数主节点的回复，它就赢得了选举，并获得了一个新的唯一的配置版本号（configEpoch）。它会开始以主节点的身份广播心跳包，并通知其他节点更新配置。</p>
<h3><span id="配置传播">配置传播</span><a href="#配置传播" class="header-anchor">#</a></h3><p>  当一个节点收到一个心跳包或UPDATE消息时，它会根据一些规则来更新自己的哈希槽映射表。如果一个哈希槽是未分配的，或者有一个新的节点使用更高的configEpoch来声明它，那么接收者会将该哈希槽绑定到新的节点。这样可以保证最后一次故障转移胜出，并且所有节点最终会达成一致。</p>
<h3><span id="节点重置和遗忘">节点重置和遗忘</span><a href="#节点重置和遗忘" class="header-anchor">#</a></h3><p>  当一个节点需要从一个集群中移除或加入到另一个集群时，可以使用<strong>CLUSTER RESET</strong>命令来重置它的状态。这个命令有<strong>两种模式：软重置和硬重置</strong>。软重置会保留当前的configEpoch，而硬重置会将其设置为0。当一个节点被移除后，其他节点需要使用<strong>CLUSTER FORGET</strong>命令来删除它在节点表中的条目，并设置一个60秒的禁止期，防止因为gossip协议而重新添加它。</p>
<h3><span id="发布订阅">发布订阅</span><a href="#发布订阅" class="header-anchor">#</a></h3><p>  Redis集群支持发布订阅功能，客户端可以向任何节点发送SUBSCRIBE或PUBLISH命令。集群会将发布的消息转发到所有其他节点。Redis 7.0及以后版本还支持分片发布订阅功能，其中分片频道根据与键相同的算法分配到哈希槽。分片消息必须发送到拥有该哈希槽的节点，集群会将发布的分片消息转发到该哈希槽所在的分片中的所有节点。</p>
<h3><span id="crc16算法">CRC16算法</span><a href="#crc16算法" class="header-anchor">#</a></h3><p>  这篇文章最后给出了计算键对应哈希槽的CRC16算法的C语言实现代码。这个算法使用了1021作为多项式，并且不反转输入输出字节。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://redis.io/docs/reference/cluster-spec/">Redis cluster specification</a> ***</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-学术研究</title>
    <url>/www6vHomeHexo/2022/07/11/gptPromptResearch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2022/07/11/gptPromptResearch/newBing.jpg" class>

<h1><span id="学术prompt">学术Prompt</span><a href="#学术prompt" class="header-anchor">#</a></h1><ul>
<li>在线文献全文分析 [new Bing] <ul>
<li>帮我总结一下这篇文章的<strong>要点</strong></li>
<li>帮我正对本研究论文写一篇<strong>总结报告</strong>， 600字</li>
<li>帮我总结本研究的讨论部分 采用了哪种<strong>写作框架</strong>， 是否进行了与其它研究的对比，有无表明本研究的<strong>局限性</strong>和<strong>未来研究可能性</strong>?</li>
<li>帮我总结本研究的方法部分用了哪些<strong>研究方法</strong>？</li>
<li>本研究方法部分的Western Blot是如何实施的？</li>
<li>总结下本论文Introduction部分在写作方面，有哪些<strong>词汇和句式</strong>值得在SCI论文写作中积累借鉴</li>
</ul>
</li>
</ul>
<h1><span id="tools">Tools</span><a href="#tools" class="header-anchor">#</a></h1><ul>
<li><a href="https://app.seaml.es/">Seamless for science</a> bibi1<br>abstract</li>
<li><a href="https://typeset.io/">scispace</a>  bibi1<br>润色 判重</li>
<li><a href="https://www.txyz.ai/">txyz</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://www.bilibili.com/video/BV18M4y1C7HY/">整合chatGPT的新必应（NewBing chat）简直就是科研神器！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis Cluster</title>
    <url>/www6vHomeHexo/2022/07/11/redisCluster/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#cluster-%E5%8A%9F%E8%83%BD-11">Cluster 功能 [1.1]</a></li>
<li><a href="#slot">Slot</a><ul>
<li><a href="#%E5%88%86%E7%89%87%E6%96%B9%E5%BC%8F">分片方式</a></li>
<li><a href="#%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB">映射关系</a></li>
<li><a href="#slot%E5%88%86%E9%85%8D">slot分配</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4">指令</a></li>
<li><a href="#%E6%9C%80%E5%A4%A7%E6%A7%BD%E6%95%B0%E6%98%AF16384%E7%9A%84%E5%8E%9F%E5%9B%A0-13">最大槽数是16384的原因 [1.3]</a></li>
</ul>
</li>
<li><a href="#%E4%B8%80%E8%87%B4%E6%80%A7">一致性</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="cluster-功能-11">Cluster 功能 [1.1]</span><a href="#cluster-功能-11" class="header-anchor">#</a></h1><ul>
<li><p>Redis Cluster支持多个master，每个master又可以挂载多个slave</p>
<ul>
<li>读写分离</li>
<li>支持数据的高可用</li>
</ul>
</li>
<li><p>故障转移， 高可用<br>由于Cluster自带Sentinel的故障转移机制，内置了高可用的支持， <strong>无需再去使用哨兵功能</strong>  </p>
</li>
<li><p>由对应的集群来负责维护节点、插槽和数据之间的关系</p>
</li>
</ul>
<h1><span id="slot">Slot</span><a href="#slot" class="header-anchor">#</a></h1><p>Redis集群有16384个哈希槽每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽.</p>
<h3><span id="分片方式">分片方式</span><a href="#分片方式" class="header-anchor">#</a></h3><ul>
<li>crc（key）% 16384</li>
</ul>
<h3><span id="映射关系">映射关系</span><a href="#映射关系" class="header-anchor">#</a></h3><p>   key （→ CRC） 分片<br>   分片 (→ 映射关系)  实例 </p>
<h3><span id="slot分配">slot分配</span><a href="#slot分配" class="header-anchor">#</a></h3><ul>
<li>自动分配slot<br>create() </li>
<li>手动分配slot<br>meet()<br>addSlot()</li>
</ul>
<h3><span id="指令">指令</span><a href="#指令" class="header-anchor">#</a></h3><ul>
<li>MOVED指令， ASK 指令</li>
</ul>
<h3><span id="最大槽数是16384的原因-13">最大槽数是16384的原因 [1.3]</span><a href="#最大槽数是16384的原因-13" class="header-anchor">#</a></h3><p>[ redis 作者解答]<br><a href="https://github.com/redis/redis/issues/2576"> why redis-cluster use 16384 slots? #2576 </a>   antirez </p>
<p>[中文翻译]<br>正常的心跳数据包带有节点的完整配置，可以用幂等方式用旧的节点替换旧节点，以便更新旧的配置。 这意味着它们包含原始节点的插槽配置，该节点使用2k的空间和16k的插槽，但是会使用8k的空间（使用65k的插槽）。同时，由于其他设计折衷，Redis集群不太可能扩展到1000个以上的主节点。 因此16k处于正确的范围内，以确保每个主机具有足够的插槽，最多可容纳1000个矩阵，但数量足够少，可以轻松地将插槽配置作为原始位图传播。请注意，在小型群集中，位图将难以压缩，因为当N较小时，位图将设置的slot &#x2F; N位占设置位的很大百分比。</p>
<p>[解读]</p>
<ul>
<li><strong>消息头太大， 发送的心跳包过于庞大</strong><br><strong>如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。</strong></li>
</ul>
<p>在消息头中最占空间的是myslots[CLUSTER_SLOTS&#x2F;8]。当槽位为65536时，这块的大小是:65536÷8÷1024&#x3D;8kb</p>
<p>在消息头中最占空间的是myslots[CLUSTER_SLOTS&#x2F;8]。当槽位为16384时，这块的大小是:16384∶8∶1024&#x3D;2kb</p>
<p>因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。 </p>
<ul>
<li><strong>主节点数量上限为1000，无需65536个slot</strong><br>redis的集群主节点数量基本不可能超过个1000个。<br>集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者不建议redis cluster节点数量超过1000个。那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。</li>
</ul>
<h1><span id="一致性">一致性</span><a href="#一致性" class="header-anchor">#</a></h1><ul>
<li><strong>Redis集群不保证强一致性</strong><br>redis集群不保证强一致性，这意味着在特定的条件下，Redis集群可能会丢掉一些被系统收到的写入请求命令</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/Learning-in-practice/tree/master/Redis/10.Redis%E9%9B%86%E7%BE%A4(cluster)">10.Redis集群(cluster)</a><br>1.1<a href="https://github.com/www6v/Learning-in-practice/blob/master/Redis/10.Redis%E9%9B%86%E7%BE%A4(cluster)/1.Redis%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D.md">Redis集群介绍</a><br>1.2 <a href="https://www.bilibili.com/video/BV13R4y1v7sP/?p=76">Redis Cluster</a><br>1.3 <a href="https://www.bilibili.com/video/BV13R4y1v7sP/?p=84">Redis Cluster</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 主从复制</title>
    <url>/www6vHomeHexo/2022/07/10/redisReplica/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B">主从复制流程</a><ul>
<li><a href="#%E4%B8%BB%E4%BB%8E%E5%BA%93%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%8C%E6%AD%A5%E7%9A%84%E6%B5%81%E7%A8%8B">主从库第一次同步的流程</a></li>
<li><a href="#%E4%B8%BB%E4%BB%8E%E5%BA%93%E5%A2%9E%E9%87%8F%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B">主从库增量复制流程</a></li>
</ul>
</li>
<li><a href="#%E7%BC%BA%E7%82%B9-3">缺点 [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="主从复制流程">主从复制流程</span><a href="#主从复制流程" class="header-anchor">#</a></h1><h3><span id="主从库第一次同步的流程">主从库第一次同步的流程</span><a href="#主从库第一次同步的流程" class="header-anchor">#</a></h3> <img src="/www6vHomeHexo/2022/07/10/redisReplica/r1.jpg" class>   


<h3><span id="主从库增量复制流程">主从库增量复制流程</span><a href="#主从库增量复制流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/10/redisReplica/r2.jpg" class>                          

<p>repl_backlog_buffer 是一个环形缓冲区，<strong>主库会记录自己写到的位置，从库则会记录自己 已经读到的位置</strong>。</p>
<h1><span id="缺点-3">缺点 [3]</span><a href="#缺点-3" class="header-anchor">#</a></h1><ul>
<li>主从延迟<ul>
<li>复制偏移量：master_repl_offset | slave_repl_offset</li>
</ul>
</li>
<li>master挂了怎么办？<ul>
<li>默认情况下，不会在slave节点中自动选一个master</li>
<li>每次都要人工干预<br><strong>需要Redis哨兵(sentinel)</strong></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《06 | 数据同步：主从库如何实现数据一致？》</li>
<li><a href="https://www.cnblogs.com/niejunlei/p/12904984.html"><strong>redis主从复制、主从延迟知几何</strong></a></li>
<li><a href="https://github.com/www6v/Learning-in-practice/blob/master/Redis/8.Redis%E5%A4%8D%E5%88%B6(replica)/4.%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86%E5%92%8C%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.md">复制原理和工作流程</a><br><a href="https://www.bilibili.com/video/BV13R4y1v7sP?p=61">61_redis主从复制之工作流程总结</a> V<br><a href="https://www.bilibili.com/video/BV13R4y1v7sP?p=62">62_redis主从复制之痛点和改进需求</a> V</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang基础-Slice</title>
    <url>/www6vHomeHexo/2022/07/09/golangSlice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h3><span id="array和slice">array和slice</span><a href="#array和slice" class="header-anchor">#</a></h3><ul>
<li>数组长度是固定的， slice长度是可变的</li>
</ul>
<h3><span id="slice底层实现-2">slice底层实现 [2]</span><a href="#slice底层实现-2" class="header-anchor">#</a></h3><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> slice <span class="keyword">struct</span> &#123;</span><br><span class="line">    array unsafe.Pointer</span><br><span class="line">    <span class="built_in">len</span>   <span class="type">int</span></span><br><span class="line">    <span class="built_in">cap</span>   <span class="type">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="slice-初始化-1">slice 初始化 [1]</span><a href="#slice-初始化-1" class="header-anchor">#</a></h3><p>对于 make 来说，它可以初始化 slice 的 length 和 capacity，<strong>如果我们能确定 slice 里面会存放多少元素，从性能的角度考虑最好使用 make 初始化好</strong>，因为对于一个空的 slice append 元素进去每次达到阈值都需要进行<strong>扩容</strong></p>
<h3><span id="code-example-1">Code Example [1]</span><a href="#code-example-1" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.luozhiyun.com/archives/797">[长文]从《100 Go Mistakes》我总结了什么？</a><br>gExamples slice_test.go</li>
<li><a href="https://www.golangroadmap.com/class/gointerview/2-1.html">1.Go slice的底层实现原理?</a></li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 基础-Errors&amp;Bugs</title>
    <url>/www6vHomeHexo/2022/07/07/golangBugs/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#errors">Errors</a><ul>
<li><a href="#%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86-4">异常处理 [4]</a></li>
</ul>
</li>
<li><a href="#bugs">Bugs</a></li>
<li><a href="#%E6%95%85%E9%9A%9C-20">故障 [20]</a></li>
<li><a href="#%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87-20">监控指标 [20]</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E5%A5%97%E8%B7%AF-20">问题排查套路 [20]</a></li>
<li><a href="#go%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7">go压测工具</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="errors">Errors</span><a href="#errors" class="header-anchor">#</a></h1><h3><span id="异常处理-4">异常处理 [4]</span><a href="#异常处理-4" class="header-anchor">#</a></h3><h1><span id="bugs">Bugs</span><a href="#bugs" class="header-anchor">#</a></h1><ul>
<li>Golang 循环变量引用问题[7]</li>
</ul>
<h1><span id="故障-20">故障 [20]</span><a href="#故障-20" class="header-anchor">#</a></h1><p>CPU 用爆了？ 90%？<br>内存用爆了？OOM？<br>Goroutine 用爆了？ 80w？<br>线程数爆了？<br>延迟太高？</p>
<h1><span id="监控指标-20">监控指标 [20]</span><a href="#监控指标-20" class="header-anchor">#</a></h1><ul>
<li>goroutine数，线程数<ul>
<li>goroutine 多， 通过pprof看goroutine在干啥，等锁</li>
</ul>
</li>
<li>GC频率<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">GODEBUG=<span class="string">&#x27;gctrace=1&#x27;</span> <span class="keyword">go</span> run ./cmd/main.<span class="keyword">go</span> </span><br></pre></td></tr></table></figure></li>
<li>MemStats 结构体<ul>
<li>常规统计信息（General statistics）</li>
<li>分配堆内存统计（Heap memory statistics）</li>
<li>栈内存统计（Stack memory statistics）</li>
<li>堆外内存统计信息（Off-heap memory statistics）</li>
<li>垃圾回收器统计信息（Garbage collector statistics）</li>
<li>按 per-size class 大小分配统计（BySize reports per-size class allocation statistics）</li>
</ul>
</li>
</ul>
<h1><span id="问题排查套路-20">问题排查套路 [20]</span><a href="#问题排查套路-20" class="header-anchor">#</a></h1><ul>
<li><p>阻塞问题</p>
<ul>
<li>排除外部问题<br>例如依赖的上游服务(db, redis, mq)延迟过高，在监控系统中查看</li>
<li>锁阻塞<ul>
<li>减少临界区范围</li>
<li>降低锁粒度<ul>
<li>Global lock -&gt; sharded lock</li>
<li>Global lock -&gt; connection level lock</li>
<li>Connection level lock -&gt; request level lock</li>
</ul>
</li>
<li>同步改异步<ul>
<li>日志场景: 同步日志 -&gt; 异步日志</li>
<li>Metric 上报场景: select -&gt; select+default</li>
</ul>
</li>
<li>个别场景使用双buffer 完全消灭阻塞</li>
</ul>
</li>
</ul>
</li>
<li><p>CPU占用过高<br>看CPU profile -&gt; 优化占用CPU较多的部分逻辑</p>
<ul>
<li>应用逻辑导致<ul>
<li>Json序列化</li>
<li>MD5算法hash成本太高 -&gt; 使用cityhash, murmurhash</li>
<li>其他应用逻辑 -&gt; 只能case by case分析</li>
</ul>
</li>
<li>GC使用CPU过高<ul>
<li>减少堆上对象分配<ul>
<li>sync.Pool 进行堆对象重用</li>
<li>Map -&gt; slice</li>
<li>指针 -&gt; 非指针对象</li>
<li>多个小对象 -&gt; 合并为一个大对象</li>
</ul>
</li>
<li>offheap</li>
<li>降低GC频率<ul>
<li>修改GOGC</li>
<li>Make 全局大slice</li>
</ul>
</li>
</ul>
</li>
<li>调度相关的函数使用CPU过高<ul>
<li>尝试使用goroutine pool 减少goroutine的创建与销毁</li>
<li>控制最大 goroutine数量</li>
</ul>
</li>
</ul>
</li>
<li><p>内存占用过高</p>
<ul>
<li>看prometheus <ul>
<li>内存RSS是多少<br>oomkiller</li>
<li>goroutine数量多少<br>普通任务 - goroutine不多， 重点关注heap profile中的inuse<br>定时任务类 - 需要看alloc</li>
<li>goroutine栈占用多少</li>
</ul>
</li>
</ul>
<ul>
<li>堆内存 占用内存空间过高<ul>
<li>sync.Pool 对象复用</li>
<li>为不同大小的对象提供不同大小level的sync.Pool  (参考fasthttp)</li>
<li>offheap</li>
</ul>
</li>
<li>goroutine栈占用过多内存<ul>
<li>减少goroutine数量<ul>
<li>如每个连接一读一写  -&gt; 合并为一个连接一个goroutine</li>
<li>goroutine pool 限制最大 goroutine数量[21]</li>
<li>使用裸 epoll库(evio, gev等) 修改网络编程方式(只适用于对延迟不敏感的业务)</li>
</ul>
</li>
<li>通过修改代码， 减少函数调用层级(难)</li>
</ul>
</li>
</ul>
</li>
<li><p>goroutine数量过多</p>
<ul>
<li>从profile网页去看 goroutine在干什么<br>查死锁 阻塞问题<br>不在意延迟的选择第三方库优化</li>
</ul>
</li>
</ul>
<h1><span id="go压测工具">go压测工具</span><a href="#go压测工具" class="header-anchor">#</a></h1><ul>
<li>wrk</li>
<li>wrk2</li>
<li>Vegeta</li>
<li>ghz</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ul>
<li><ol start="4">
<li><a href="https://blog.csdn.net/wanglei19891210/article/details/128092331">【go实战系列五】 go1.19.2与pkg中error如何wrap与unwrap Errors | 将error进行wrap向上处理思想 | pkg&#x2F;errors</a></li>
</ol>
</li>
<li><ol start="7">
<li><a href="https://cloud.tencent.com/developer/article/2240620">随笔：Golang 循环变量引用问题以及官方语义修复</a></li>
</ol>
</li>
<li><ol start="20">
<li>《28 直播：服务上线后 - 成为 Go 语言性能调优专家》</li>
</ol>
</li>
<li><ol start="21">
<li><a href="/www6vHomeHexo/2022/06/19/golangConcurrency/" title="Golang Concurrency">Golang Concurrency</a> self</li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>Golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>供应链</title>
    <url>/www6vHomeHexo/2022/07/03/supplyChain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BE%9B%E5%BA%94%E9%93%BE">供应链</a></li>
</ul>
<ul>
<li><a href="#%E4%BE%9B%E5%BA%94%E9%93%BEoms">供应链OMS</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%891">定义[1]</a></li>
<li><a href="#%E5%8A%9F%E8%83%BD1">功能[1]</a></li>
</ul>
</li>
<li><a href="#baisheng-%E4%BE%9B%E5%BA%94%E9%93%BEoms">baisheng-供应链OMS</a><ul>
<li><a href="#%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B-11">业务流程 [11]</a></li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E5%8A%9F%E8%83%BD-11">业务功能 [11]</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84-11">架构 [11]</a></li>
<li><a href="#%E5%8A%9F%E8%83%BD-11">功能 [11]</a></li>
<li><a href="#%E9%9B%86%E6%88%90-10">集成 [10]</a></li>
</ul>
</li>
<li><a href="#%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F">电商系统</a></li>
<li><a href="#%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F">订单系统</a><ul>
<li><a href="#%E4%B8%8A%E4%B8%8B%E6%B8%B8-3">上下游 [3]</a></li>
<li><a href="#%E8%AE%A2%E5%8D%95%E4%B8%AD%E6%89%80%E5%8C%85%E5%90%AB%E7%9A%84%E5%86%85%E5%AE%B9%E4%BF%A1%E6%81%AF-2">订单中所包含的内容信息 [2]</a></li>
<li><a href="#%E6%B5%81%E7%A8%8B%E5%BC%95%E6%93%8E-2">流程引擎 [2]</a></li>
<li><a href="#%E7%8A%B6%E6%80%81%E6%9C%BA-2">状态机 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="供应链">供应链</span><a href="#供应链" class="header-anchor">#</a></h2><ul>
<li>oms</li>
<li>wms</li>
<li>tms</li>
<li>bms</li>
</ul>
<h1><span id="供应链oms">供应链OMS</span><a href="#供应链oms" class="header-anchor">#</a></h1><h3><span id="定义1">定义[1]</span><a href="#定义1" class="header-anchor">#</a></h3><p>供应链OMS（Order Management System）是一种用于管理订单处理、库存管理、物流配送等业务流程的信息系统。它可以帮助企业完整地管理订单处理流程，从订单接收到交货的整个过程中提高效率、降低成本、增强客户服务体验。</p>
<h3><span id="功能1">功能[1]</span><a href="#功能1" class="header-anchor">#</a></h3><p>供应链OMS的功能包括订单管理、库存管理、物流配送、客户服务等。具体来说，它可以实现以下功能：</p>
<ul>
<li><strong>订单管理</strong>：包括订单接收、订单处理、订单跟踪等。</li>
<li><strong>库存管理</strong>：包括库存监控、库存调配、库存预警等。</li>
<li><strong>物流配送</strong>：包括物流规划、运输管理、配送调度等。</li>
<li><strong>客户服务</strong>：包括客户查询、客户反馈、客户投诉等。</li>
</ul>
<h1><span id="baisheng-供应链oms">baisheng-供应链OMS</span><a href="#baisheng-供应链oms" class="header-anchor">#</a></h1><h3><span id="业务流程-11">业务流程 [11]</span><a href="#业务流程-11" class="header-anchor">#</a></h3><ul>
<li>入库流程</li>
<li>出库流程</li>
<li>调拨流程</li>
<li>VMI流程</li>
</ul>
<h3><span id="业务功能-11">业务功能 [11]</span><a href="#业务功能-11" class="header-anchor">#</a></h3><ul>
<li>库存调整管理</li>
<li>主数据管理</li>
<li>空白订单管理</li>
<li>订单解析管理</li>
<li>分配订单管理</li>
<li>控制量管理</li>
<li>库比管理</li>
<li>销售订单管理</li>
<li>调拨订单管理</li>
<li>入库订单管理</li>
<li>物流单管理</li>
<li>物流指令管理</li>
<li>发货班表管理</li>
<li>订单差异处理管理</li>
<li>规则管理</li>
</ul>
<h3><span id="架构-11">架构 [11]</span><a href="#架构-11" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/03/supplyChain/supplyChain-bs-2.png" class>

<h3><span id="功能-11">功能 [11]</span><a href="#功能-11" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/03/supplyChain/supplyChain-bs-1.png" class>

<h3><span id="集成-10">集成 [10]</span><a href="#集成-10" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/03/supplyChain/supplyChain-bs.jpg" class>

<h1><span id="电商系统">电商系统</span><a href="#电商系统" class="header-anchor">#</a></h1><p><a href="https://github.com/macrozheng/mall">mall</a> git<br><a href="https://www.macrozheng.com/admin/index.html">mall</a> admin&#x2F;macro123</p>
<h1><span id="订单系统">订单系统</span><a href="#订单系统" class="header-anchor">#</a></h1><h3><span id="上下游-3">上下游 [3]</span><a href="#上下游-3" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/03/supplyChain/supplyChain.png" class>

<h3><span id="订单中所包含的内容信息-2">订单中所包含的内容信息 [2]</span><a href="#订单中所包含的内容信息-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/07/03/supplyChain/supplyChain.jpg" class>

<h3><span id="流程引擎-2">流程引擎 [2]</span><a href="#流程引擎-2" class="header-anchor">#</a></h3><ul>
<li><p>正向流程<br>订单创建&gt;订单支付&gt;订单生产&gt;订单确认&gt;订单完成</p>
</li>
<li><p>逆向流程<br>修改订单 -&gt; 取消订单 -&gt;  退款&#x2F;退货</p>
</li>
</ul>
<h3><span id="状态机-2">状态机 [2]</span><a href="#状态机-2" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>chat</li>
<li><a href="https://www.woshipm.com/pd/1392102.html">订单系统：从0到1设计思路</a> </li>
<li><a href="https://www.sohu.com/a/166616877_114819">解构电商、O2O：订单系统，平台的“生命中轴线”</a></li>
</ol>
<p>baisheng<br>10. 《oms集成架构图》<br>11. 《OMS需求规格说明书V0.6-20200805》</p>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>供应链</category>
      </categories>
      <tags>
        <tag>应用架构</tag>
      </tags>
  </entry>
  <entry>
    <title>istio 数据平面-ambient 模式</title>
    <url>/www6vHomeHexo/2022/07/02/istioDataplaneAmbient/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="sidecar-模式的限制">Sidecar 模式的限制</span><a href="#sidecar-模式的限制" class="header-anchor">#</a></h3><h3><span id="ambient-模式">ambient 模式</span><a href="#ambient-模式" class="header-anchor">#</a></h3><ul>
<li>Ambient Mode 的本质：<br>它的本质是分离 sidecar proxy（Envoy）中的 L4 和 L7 功能，让一部分仅需要安全功能的用户可以最小阻力（低资源消耗、运维成本）地使用 Istio service mesh。</li>
<li>Ambient Mode 的意义：<br> 因为它 sidecar 模式兼容，用户在采纳 Ambient Mode 获得了 mTLS 和有限的可观察性及 TPC 路由等 L4 功能，之后可以更方便的过度到 sidecar mode 以获得完全的 L7 功能。这给用户采纳 Istio 提供了更多模式选择，优化了 Istio 采纳路径。</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://jimmysong.io/blog/istio-ambient-mode/">关于 Istio 推出 Ambient 数据平面模式的看法</a>  ***</li>
<li><a href="https://lib.jimmysong.io/blog/introducing-ambient-mesh/">Istio 无 sidecar 代理数据平面 ambient 模式简介</a></li>
<li><a href="https://lib.jimmysong.io/blog/what-is-ambient-mesh/">什么是 Ambient Mesh？</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>serviceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP-汇总</title>
    <url>/www6vHomeHexo/2022/06/30/tencentTCPSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p><a href="../../../../2022/03/03/tencentTCP1/">腾讯云TCP1-上云迁移</a><br><a href="../../../../2021/07/22/tencentTCP2/">腾讯云TCP2-云原生应用设计</a><br><a href="../../../../2022/01/25/tencentTCP3/">腾讯云TCP3-构建腾讯云上高可用架构</a><br><a href="../../../../2022/01/11/tencentTCP4/">腾讯云TCP4-业务流量高峰处理架构设计</a><br><a href="../../../../2022/01/11/tencentTCP5/">腾讯云TCP5-云上信息安全</a><br><a href="../../../../2022/01/13/tencentTCP6/">腾讯云TCP6-腾讯云大数据产品及服务</a><br><a href="../../../../2022/01/13/tencentTCP7/">腾讯云TCP7-构建混合云</a><br><a href="../../../../2022/01/13/tencentTCP8/">腾讯云TCP8-AI解决方案</a><br><a href="../../../../2022/01/19/tencentTCP9/">腾讯云TCP9-游戏行业解决方案</a><br><a href="../../../../2022/01/19/tencentTCP10/">腾讯云TCP10-视频行业解决方案</a></p>
]]></content>
      <categories>
        <category>汇总</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Migrate</title>
    <url>/www6vHomeHexo/2022/06/28/awsMigrate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="migrate">Migrate</span><a href="#migrate" class="header-anchor">#</a></h2><ul>
<li><p>Data migration  tools</p>
<ul>
<li>DataSync</li>
<li>Transfer Family</li>
<li>Snow Family</li>
<li>S3 Transfer Acceleration</li>
</ul>
</li>
<li><p>Application migration tools </p>
<ul>
<li>Application Discovery Service</li>
<li>ApplicationMigration Service[MGN], [CloudEndure Migration]  [1][2]</li>
<li>AWS Server Migration Service [AWS SMS])</li>
</ul>
</li>
<li><p>7Rs of Migration [3]</p>
<ul>
<li>Refactor *</li>
<li>Replatform *</li>
<li>Repurchase</li>
<li>Rehost *</li>
<li>Relocate</li>
<li>Retain</li>
<li>Retire</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1Ag411T7kS/">AWS迁移实战（三）使用Application Migration Service将应用程序迁移到云</a> </li>
<li><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/a033522c-f256-40f9-9ecb-5b76a71589bc/en-US/rehost/mgn">Migration Immersion Day-&gt;Re-Host &amp; Re-Platform-&gt;MGN</a></li>
<li>[SAP-01]  Section14 ，205</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>高可用 Available</title>
    <url>/www6vHomeHexo/2022/06/26/available/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a><ul>
<li><a href="#cap">CAP</a></li>
<li><a href="#nprt%E5%85%AC%E5%BC%8F-1">nPRT公式 [1]</a><ul>
<li><a href="#%E5%8F%AF%E4%BB%A5%E6%8E%A8%E5%AF%BC%E5%87%BA%E9%A3%8E%E9%99%A9%E6%9C%9F%E6%9C%9B%E7%9A%84%E5%85%AC%E5%BC%8F">可以推导出风险期望的公式</a></li>
<li><a href="#%E6%8E%A7%E5%88%B6%E9%A3%8E%E9%99%A9%E7%9A%844%E5%A4%A7%E5%9B%A0%E7%B4%A0nprt">控制风险的4大因素（nPRT）</a></li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E7%9A%847%E5%A4%A7%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%88%99">高可用架构设计的7大核心原则</a></li>
</ul>
</li>
<li><a href="#%E5%8F%AF%E7%94%A8%E6%80%A7-7-%E7%BA%A7%E5%9B%BE%E8%A1%A8-%E6%88%90%E7%86%9F%E5%BA%A6-5">可用性 7 级图表  [成熟度] [5]</a></li>
</ul>
</li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8-%E7%A0%94%E5%8F%91">高可用-研发</a><ul>
<li><a href="#%E5%AE%B9%E9%87%8F%E8%A7%84%E5%88%92%E5%92%8C%E8%AF%84%E4%BC%B0-7">容量规划和评估 [7]</a></li>
<li><a href="#qps-%E9%A2%84%E4%BC%B0%E6%BC%8F%E6%96%97%E5%9E%8B-7">QPS 预估（漏斗型） [7]</a></li>
</ul>
</li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8-%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B1%82">高可用-服务分层</a><ul>
<li><a href="#%E5%88%86%E5%B1%82%E8%A7%A3%E6%9E%90-6">分层解析 [6]</a></li>
<li><a href="#%E6%8E%A5%E5%85%A5%E5%B1%82-2-r">接入层 [2] [R]</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%B1%82-%E5%BA%94%E7%94%A8%E5%B1%82-6">服务层 应用层  [6]</a></li>
<li><a href="#%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%B1%82">中间件层</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%B1%82-3-p">数据层 [3] [P]</a></li>
</ul>
</li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8-%E8%BF%90%E8%90%A57">高可用-运营[7]</a><ul>
<li><a href="#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83"><strong>灰度发布</strong></a></li>
<li><a href="#%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6">监控+告警</a></li>
<li><a href="#%E5%AE%89%E5%85%A8%E6%80%A7-%E9%98%B2%E6%94%BB%E5%87%BB%E8%AE%BE%E8%AE%A1"><strong>安全性、防攻击设计</strong></a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83%E6%B7%B7%E6%B2%8C%E5%AE%9E%E9%AA%8C"><strong>故障演练（混沌实验）</strong></a></li>
<li><a href="#%E6%8E%A5%E5%8F%A3%E6%8B%A8%E6%B5%8B%E5%B7%A1%E6%A3%80">接口拨测+巡检</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h1><h3><span id="cap">CAP</span><a href="#cap" class="header-anchor">#</a></h3><p>CP系统:  hbase, zookeeper<br>AP系统:  cassandra, eureka</p>
<h3><span id="nprt公式-1">nPRT公式 [1]</span><a href="#nprt公式-1" class="header-anchor">#</a></h3><h5><span id="可以推导出风险期望的公式">可以推导出风险期望的公式</span><a href="#可以推导出风险期望的公式" class="header-anchor">#</a></h5><h5><span id="控制风险的4大因素nprt">控制风险的4大因素（nPRT）</span><a href="#控制风险的4大因素nprt" class="header-anchor">#</a></h5><ul>
<li>减少风险数量，n</li>
<li>降低风险变故障的概率（即：增加风险变故障的难度），P</li>
<li>减小故障影响范围，R</li>
<li>缩短故障影响时长，T</li>
</ul>
<h5><span id="高可用架构设计的7大核心原则">高可用架构设计的7大核心原则</span><a href="#高可用架构设计的7大核心原则" class="header-anchor">#</a></h5><ul>
<li>少依赖原则：能不依赖的，尽可能不依赖，越少越好（n）</li>
<li>弱依赖原则：一定要依赖的，尽可能弱依赖，越弱越好（P）</li>
<li>分散原则：鸡蛋不要放一个篮子，分散风险（R）</li>
<li>均衡原则：均匀分散风险，避免不均衡（R）</li>
<li>隔离原则：控制风险不扩散，不放大（R）</li>
<li>无单点原则：要有冗余或其他版本,做到有路可退（T）</li>
<li>自我保护原则：少流血，牺牲一部分，保护另外一部分（P&amp;R&amp;T）</li>
</ul>
<img src="/www6vHomeHexo/2022/06/26/available/ha.png" class title="高可用">


<h3><span id="可用性-7-级图表-成熟度-5">可用性 7 级图表  [成熟度] [5]</span><a href="#可用性-7-级图表-成熟度-5" class="header-anchor">#</a></h3><p>当一个服务挂了的时候</p>
<ul>
<li>第一级：Crash with data corruption, destruction. </li>
<li>第二级：Crash with new data loss. </li>
<li>第三级：Crash without data loss.<br>   数据高可用-冗余, destruction 测试</li>
<li>第四级：No crash, but with no or very limited service, low service quality.<br>   流控系统， eg. 秒杀流量漏斗</li>
<li>第五级：Partial or limited service, with good to medium service quality. </li>
<li>第六级：Failover with significant user visible delay, near full quality of service<br>   容灾，恢复慢</li>
<li>第七级：Failover with minimal to none user visible delay, near full quality<br>   异地容灾</li>
</ul>
<h1><span id="高可用-研发">高可用-研发</span><a href="#高可用-研发" class="header-anchor">#</a></h1><h3><span id="容量规划和评估-7">容量规划和评估 [7]</span><a href="#容量规划和评估-7" class="header-anchor">#</a></h3><p>[chat]<br>容量规划和评估的概念和流程。<br>容量评估是评估系统需要应对的业务体量，包括请求量、高峰峰值等，可以根据历史数据或产品预估来进行。容量规划则是在系统设计时就要考虑容量问题，规划好系统能够抗多少的量级，涉及到系统架构设计和资源分配等问题。而性能压测则是为了确保容量规划的准确性，通过压测来测试系统的性能指标，如QPS和响应耗时，以确定系统是否能够承受实际业务流量。</p>
<p><strong>性能压测</strong>要关注的指标很多，但是重点要关注是两个指标，<strong>一个是 QPS、一个是响应耗时，</strong>要确保压测的结果符合预期。</p>
<h3><span id="qps-预估漏斗型-7">QPS 预估（漏斗型） [7]</span><a href="#qps-预估漏斗型-7" class="header-anchor">#</a></h3><p>[chat]<br>QPS预估中的漏斗型预估方法。<br>漏斗型预估是根据请求的层面和模块来构建漏斗模型，预估每个层级的QPS量级，随着请求链路的下行，QPS量级会逐步减少。预估的层级包括服务、接口、分布式缓存等各个层面，最终构成完整的QPS漏斗模型。漏斗型预估方法可以帮助我们更准确地预估系统承载的QPS量级，从而做出更合理的容量规划和评估。</p>
<p>QPS 预估（漏斗型）就是需要我们按照请求的层面和模块来构建我们的<strong>预估漏斗模型</strong>，然后预估好每一个层级的量级，包括但不限于从服务、接口、分布式缓存等各个层面来预估，最后构成我们完整的 <strong>QPS 漏斗模型</strong>。</p>
<h1><span id="高可用-服务分层">高可用-服务分层</span><a href="#高可用-服务分层" class="header-anchor">#</a></h1><h3><span id="分层解析-6">分层解析 [6]</span><a href="#分层解析-6" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/06/26/available/layer-analysis.jpg" class>

<h3><span id="接入层-2-r">接入层   [2] [R]</span><a href="#接入层-2-r" class="header-anchor">#</a></h3><ul>
<li>地域&amp;错误感知自动 failover<br>视 endpoint 健康度自动 failover 一定比例流量至其他可用区&#x2F;地域，直至 endpoint 全部不健康时 100% 流量自动 failover 至其他可用区&#x2F;地域。</li>
<li>地域感知流量分发 distribute<br>eg. 上海一区和上海二区按照 80% 和 20% 的比例分发</li>
</ul>
<h3><span id="服务层-应用层-6">服务层 应用层  [6]</span><a href="#服务层-应用层-6" class="header-anchor">#</a></h3><ul>
<li><p>关注点 [7]</p>
<ul>
<li>无状态和负载均衡设计</li>
<li>弹性扩缩容设计</li>
<li>异步解耦和削峰设计（消息队列）</li>
<li>故障和容错设计</li>
<li>过载保护设计（限流、熔断、降级）</li>
</ul>
</li>
<li><p>传统应用高可用</p>
<ul>
<li>CLB+CVM+AS  <ul>
<li>架构图 [pic]</li>
<li>应用实践</li>
</ul>
</li>
</ul>
</li>
<li><p>云原生应用部署</p>
<ul>
<li>涉及的产品<ul>
<li>微服务平台 TSF</li>
<li>API网关</li>
<li>TKE容器服务</li>
</ul>
</li>
<li>云原生应用部署方案[pic]</li>
</ul>
</li>
<li><p>应用的<strong>容灾</strong>设计 [pic 要重新看]</p>
<ul>
<li>单区域容灾   </li>
<li>跨地域容灾</li>
<li>跨地域多活<br>业务拆分, 单元化部署</li>
<li>混合云部署<br>云上和IDC各部署一套完整的业务系统</li>
<li>异地多活set化部署<br>Unit由多个set组成<br>建议单写多读的架构<br>set不一定限制在一个机房，可跨机房、跨地域部署</li>
</ul>
</li>
<li><p>系统中的高可用</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/Br6J3nC51SnRTXBArfs_ug">Kubernetes 之 master高可用集群搭建</a></li>
<li>Redlock - redis分布式锁的高可用</li>
<li><a href="https://tech.meituan.com/2021/05/20/shepherd-api-gateway.html">百亿规模API网关服务Shepherd的设计与实现</a><ul>
<li>服务隔离<br>集群隔离  请求隔离</li>
<li>稳定性<br>流量管控, 请求缓存, 超时管理, 熔断降级</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="中间件层">中间件层</span><a href="#中间件层" class="header-anchor">#</a></h3><ul>
<li>kafka 高可用</li>
<li>zk高可用</li>
<li>系统中的高可用<br><a href="https://mp.weixin.qq.com/s/GMsYVgPmoCbdWW4FQthrAA">面试|图解 kafka 的高可用机制</a>  isr</li>
</ul>
<h3><span id="数据层-3-p">数据层   [3] [P]</span><a href="#数据层-3-p" class="header-anchor">#</a></h3><ul>
<li><p>数据复制</p>
<ul>
<li>主从复制<ul>
<li>同步复制，异步复制</li>
<li>复制日志的实现：<br> 基于语句到复制，<br> 基于wal的传输，<br> 基于行的逻辑日志复制<br> eg. mysql， redis， hbase </li>
<li>复制滞后问题</li>
</ul>
</li>
<li>多主复制</li>
<li>无主复制</li>
</ul>
</li>
<li><p>一致性和共识<br>raft - etcd<br>zab - zookeeper</p>
</li>
<li><p>系统中的高可用<br><a href="https://mp.weixin.qq.com/s/fh_9Mk-FVFSkX5pmsc8HPA">MySQL 同步复制及高可用方案总结</a>  MHA, MMM<br><a href="https://mp.weixin.qq.com/s/Iz7cwun1y_oLUV9fDDh0UQ">这可能是目前最全的Redis高可用技术解决方案总结</a>  Master-slave, Cluster<br><a href="https://mp.weixin.qq.com/s/yH5JVD422k6FNtiqAGw75Q">干货 | 阿里巴巴HBase高可用8年抗战回忆录</a><br>etcd - raft</p>
</li>
</ul>
<h1><span id="高可用-运营7">高可用-运营[7]</span><a href="#高可用-运营7" class="header-anchor">#</a></h1><h3><span id="灰度发布"><strong>灰度发布</strong></span><a href="#灰度发布" class="header-anchor">#</a></h3><h3><span id="监控告警">监控+告警</span><a href="#监控告警" class="header-anchor">#</a></h3><h3><span id="安全性-防攻击设计"><strong>安全性、防攻击设计</strong></span><a href="#安全性-防攻击设计" class="header-anchor">#</a></h3><h3><span id="故障演练混沌实验"><strong>故障演练（混沌实验）</strong></span><a href="#故障演练混沌实验" class="header-anchor">#</a></h3><h3><span id="接口拨测巡检">接口拨测+巡检</span><a href="#接口拨测巡检" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s/CkFHTuxqoj1WJ7d0HUEbAg">高可用的本质</a></li>
<li><a href="https://mp.weixin.qq.com/s/0f9Z8yIsT7-iJ2AUHfgqiw">云原生应用负载均衡系列 (2): 入口流量分发、容错与高可用调度</a>  istio</li>
<li>&lt;&lt;数据密集型应用系统设计&gt;&gt;  5章, 9章</li>
<li>&lt;&lt;亿级流量 网站架构核心技术&gt;&gt;  1.4</li>
<li><a href="https://www.ktanx.com/blog/p/4273">来自 Google 的高可用架构理念与实践</a></li>
<li><a href="/www6vHomeHexo/2022/01/25/tencentTCP3/" title="腾讯云TCP3-构建腾讯云上高可用架构">腾讯云TCP3-构建腾讯云上高可用架构</a> self</li>
<li><a href="https://zhuanlan.zhihu.com/p/592921654">高可用架构和系统设计经验</a>  腾讯 ***</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统架构</category>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-容灾恢复DR</title>
    <url>/www6vHomeHexo/2022/06/26/aliyunDisasterRecovery/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="公有云容灾架构-2">公有云容灾架构 [2]</span><a href="#公有云容灾架构-2" class="header-anchor">#</a></h2><ul>
<li>通用架构</li>
<li>同城容灾架构<br>同城双活</li>
<li>异地容灾架构<br>异地多活</li>
<li>同城+异地容灾架构<br>两地三中心</li>
</ul>
<h2><span id="混合云">混合云</span><a href="#混合云" class="header-anchor">#</a></h2><h5><span id="混合云容灾方案">混合云容灾方案</span><a href="#混合云容灾方案" class="header-anchor">#</a></h5><ul>
<li>应用级别容灾<br>备份应用服务和配置 + 备份数据 </li>
<li>数据级别容灾<br>仅仅备份数据</li>
</ul>
<h5><span id="混合云容灾服务">混合云容灾服务</span><a href="#混合云容灾服务" class="header-anchor">#</a></h5><ul>
<li>本地+异地备份</li>
<li>业务连续性</li>
<li>应用迁移</li>
</ul>
<h5><span id="混合云容灾备份架构-1">混合云容灾备份架构 [1]</span><a href="#混合云容灾备份架构-1" class="header-anchor">#</a></h5><ul>
<li><p>混合云存储备份架构</p>
<ul>
<li>本地备份架构： 备份到同机房的专有云</li>
<li>异地备份架构： 通过物理专线或VPN连通，通过HBR客户端备份到云备份库</li>
</ul>
</li>
<li><p>混合云数据库备份架构</p>
<ul>
<li>数据库热备架构<br>通过物理专线或VPN连通， DTS全量+增量迁移</li>
<li>数据库冷备架构<br>通过存储备份<br>通过数据库备份<br>数据恢复</li>
</ul>
</li>
<li><p>混合云容灾架构</p>
<ul>
<li><p>等级<br>0<br>1 - 2级<br>3级 在线备份<br>4级 定时备份<br>5级 实时备份<br>6级 0数据丢失</p>
</li>
<li><p>HDR异地容灾架构<br>3 - 4级别<br>IDC + 公有云容灾</p>
</li>
<li><p>Apsara Stack<br>线下IDC + 专有云（主机房+备份机房）</p>
</li>
<li><p>多活架构<br>分布式数据库OceanBase<br>6级别</p>
</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《混合云架构》 6.3</li>
<li><a href="https://www.bilibili.com/video/BV1jy4y1a7mK?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里云云上常见架构设计及优化-课时5：云上容灾架构设计及解决方案</a> bilibili</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang内置类型-Channel</title>
    <url>/www6vHomeHexo/2022/06/26/golangChannel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9E%E7%8E%B0-1">实现 [1]</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8">使用</a><br>- <a href="#buffer-channel-vs-%E9%9D%9Ebuffered-channel">buffer channel vs. 非buffered channel</a><br>- <a href="#channel%E7%9A%84%E8%AF%BB%E5%86%99">channel的读写</a><br>- <a href="#channel-closing-principle">Channel closing principle</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="实现-1">实现 [1]</span><a href="#实现-1" class="header-anchor">#</a></h2><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> hchan <span class="keyword">struct</span> &#123;</span><br><span class="line">   qcount   <span class="type">uint</span>           <span class="comment">// total data in the queue</span></span><br><span class="line">   dataqsiz <span class="type">uint</span>           <span class="comment">// size of the circular queue</span></span><br><span class="line">   buf      unsafe.Pointer <span class="comment">// points to an array of dataqsiz elements</span></span><br><span class="line">   elemsize <span class="type">uint16</span></span><br><span class="line">   closed   <span class="type">uint32</span></span><br><span class="line">   elemtype *_type <span class="comment">// element type</span></span><br><span class="line">   sendx    <span class="type">uint</span>   <span class="comment">// send index</span></span><br><span class="line">   recvx    <span class="type">uint</span>   <span class="comment">// receive index</span></span><br><span class="line">   recvq    waitq  <span class="comment">// list of recv waiters</span></span><br><span class="line">   sendq    waitq  <span class="comment">// list of send waiters</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">// lock protects all fields in hchan, as well as several</span></span><br><span class="line">   <span class="comment">// fields in sudogs blocked on this channel.</span></span><br><span class="line">   <span class="comment">//</span></span><br><span class="line">   <span class="comment">// Do not change another G&#x27;s status while holding this lock</span></span><br><span class="line">   <span class="comment">// (in particular, do not ready a G), as this can deadlock</span></span><br><span class="line">   <span class="comment">// with stack shrinking.</span></span><br><span class="line">   lock mutex</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> waitq <span class="keyword">struct</span> &#123;</span><br><span class="line">    first *sudog</span><br><span class="line">    last  *sudog</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2><span id="使用">使用</span><a href="#使用" class="header-anchor">#</a></h2><h5><span id="buffer-channel-vs-非buffered-channel">buffer channel vs. 非buffered channel</span><a href="#buffer-channel-vs-非buffered-channel" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">无缓冲</th>
<th align="center">有缓冲</th>
</tr>
</thead>
<tbody><tr>
<td align="center">创建方式</td>
<td align="center">make(chan TYPE)</td>
<td align="center">make(chan TYPE, SIZE)</td>
</tr>
<tr>
<td align="center">发送阻塞</td>
<td align="center">数据接收前发送阻塞</td>
<td align="center">缓冲满时发送阻塞</td>
</tr>
<tr>
<td align="center">接收阻塞</td>
<td align="center">数据发送前接收阻塞</td>
<td align="center">缓冲空时接收阻塞</td>
</tr>
</tbody></table>
<p><a href="https://www.golangroadmap.com/class/gointerview/4-3.html">3.Go channel有无缓冲的区别？</a>  </p>
<h5><span id="channel的读写">channel的读写</span><a href="#channel的读写" class="header-anchor">#</a></h5><ul>
<li>向关闭的channel中写入数据会panic</li>
<li>关闭的channel中读数据<br>有数据会读到数据，没数据会是nil</li>
</ul>
<h5><span id="channel-closing-principle">Channel closing principle</span><a href="#channel-closing-principle" class="header-anchor">#</a></h5><p>[gpt]<br>prompt: Golang的 Channel closing principle是什么？</p>
<p>在Golang中，关闭一个Channel是一种重要的操作，如果不正确地关闭Channel，可能会导致程序出现死锁或其他问题。下面是Golang中Channel关闭的原则和细则：</p>
<ol>
<li><p>只有发送方才能关闭Channel。<br>发送方是指向Channel发送数据的goroutine，而接收方是指从Channel接收数据的goroutine。在Golang中，只有发送方才能关闭Channel，因为发送方知道何时不再需要发送数据，从而触发关闭Channel的操作。</p>
</li>
<li><p>关闭Channel后，接收方仍然可以接收Channel中的数据。<br>当一个Channel被关闭后，接收方仍然可以从Channel中接收已经存在的数据，直到Channel中的所有数据都被接收完毕。这使得接收方可以在不知道Channel何时被关闭的情况下，安全地从Channel中接收数据。</p>
</li>
<li><p>关闭一个已经关闭的Channel会导致panic。<br>如果尝试关闭一个已经关闭的Channel，会导致panic，因此在关闭Channel之前，应该先检查Channel是否已经关闭。</p>
</li>
<li><p>关闭一个nil的Channel会导致panic。<br>如果尝试关闭一个nil的Channel，也会导致panic，因此在关闭Channel之前，应该先检查Channel是否为nil。</p>
</li>
<li><p>关闭一个有缓冲的Channel时，仍然可以向其中发送数据。<br>当一个有缓冲的Channel被关闭时，发送方仍然可以向其中发送数据，直到Channel被填满。因此，在关闭一个有缓冲的Channel之后，可能还有一些数据需要被发送。</p>
</li>
<li><p>关闭一个Channel会使Channel上的所有阻塞操作立即解除阻塞并返回。<br>当一个Channel被关闭时，Channel上的所有阻塞操作（包括接收和发送操作）都会立即解除阻塞并返回。对于接收操作，返回的值为Channel中剩余的数据，如果Channel已经为空，则返回类型的零值。对于发送操作，返回的结果通常为false，表示数据没有被发送成功。</p>
</li>
</ol>
<p>总之，关闭Channel是一项重要的操作，需要遵循一些原则和细则来保证程序的正确性和稳定性。在使用Channel的过程中，应该注意遵循这些原则和细则。<br>[&#x2F;gpt]</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/haiyux/p/15161495.html">golang channel原理 </a> *** </li>
<li>《09 神奇的内置数据结构》  channel 看动画 ***</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>内置类型</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Serverless  关注点</title>
    <url>/www6vHomeHexo/2022/06/25/serverlessConcern/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="concern">Concern</span><a href="#concern" class="header-anchor">#</a></h2><div style="text-align: center; width: 70%; height: 70%">

<p><img src="https://user-images.githubusercontent.com/5608425/66986650-de5e0900-f0f1-11e9-819a-911d21d9d0ba.jpg" alt="concerns-serverless3"></p>
</div>


<ul>
<li><p>StartUp Latency[3][9][11]</p>
</li>
<li><p>Best Practice[14][1]         </p>
<ul>
<li>1.<a href="https://cloud.tencent.com/developer/article/1158774">让业务感知不到服务器的存在——基于弹性计算的无服务器化实践</a>  腾讯 未 <br> </li>
<li>14.<a href="https://yq.aliyun.com/articles/656329">Serverless下日志采集、存储、分析实践</a>   最佳实践</li>
</ul>
</li>
<li><p>Tool 开放工具[10] 监控[14]  </p>
<ul>
<li>10.<a href="https://yq.aliyun.com/articles/719694">函数计算 2.0 重磅发布，Serverless Computing 开启新篇章</a><br>开发工具 fun，vscode插件</li>
</ul>
</li>
<li><p>knowledge[4][5][6][15]</p>
<ul>
<li>4.<a href="../../../../2019/02/07/xaas/">云计算中的Xaas</a> self      </li>
<li>5.<a href="https://mp.weixin.qq.com/s/7qJUzf8xrGihPPLsvwPEig">无服务计算的未来和挑战: A Berkeley View on Serverless Computing</a> good</li>
<li>6.<a href="https://yq.aliyun.com/articles/574222">当我们在聊Serverless时你应该知道这些</a>  阿里 竹涧， 场景 产品 架构</li>
<li>15.<a href="https://yq.aliyun.com/articles/60966">阿里云函数计算 - 事件驱动的serverless计算平台</a></li>
</ul>
</li>
<li><p>vender lock-in [12][13][17]<br>采用Serverless架构之后，代码都是各个平台的Lambda代码片段，没法迁移。从客户的角度来看，是不希望自己被某家云厂商所绑架的。所以云计算需要有一个标准，产品需要标准化，方便用户无缝在各种云之间迁移。<br>knative试图解决这个问题。             </p>
<ul>
<li><ol start="12">
<li><a href="https://yq.aliyun.com/articles/683675">0基础快速入门运维-EDAS Serverless(FAAS) 产品评测</a></li>
</ol>
</li>
<li><ol start="13">
<li><a href="https://yq.aliyun.com/articles/59483">大道至简 - 基于Docker的Serverless探索之旅</a> 产品</li>
</ol>
</li>
<li><ol start="17">
<li><a href="https://yq.aliyun.com/articles/160370">对Serverless架构的一点体验和思考</a> aws的lambda; serverless的缺点和挑战</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serverless</category>
      </categories>
      <tags>
        <tag>serverless</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps 学习资源</title>
    <url>/www6vHomeHexo/2022/06/25/devopsStudyResource/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="社区amp图书">社区&amp;图书</span><a href="#社区amp图书" class="header-anchor">#</a></h2><p><a href="https://www.devopschina.org/blog/">中国DevOps社区</a>  ***  有b站<br><a href="https://www.srenow.cn/index.html">中国SRE联盟社区 </a><br><a href="https://www.douban.com/doulist/46341810/">DevOps书架</a></p>
<h2><span id="个人">个人</span><a href="#个人" class="header-anchor">#</a></h2><p><a href="https://martinliu.cn/">Martin Liu’s Blog</a>  *** 布道师</p>
<h2><span id="资料">资料</span><a href="#资料" class="header-anchor">#</a></h2><p>《mksz266 - DevOps理论 实践之路》  *  慕课<br>《92-DevOps实战笔记  》 ***  极客时间<br>《lg2064-DevOps 落地笔记-拉钩专栏  》***  拉钩</p>
]]></content>
      <categories>
        <category>devops</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>KVM 虚拟机</title>
    <url>/www6vHomeHexo/2022/06/25/kvm/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="基础概念">基础概念</span><a href="#基础概念" class="header-anchor">#</a></h2><p>VMM(Virtual Machine Monitor) Hypervisor<br>QEMU: 软件虚拟化</p>
<h2><span id="kvm架构图">KVM架构图</span><a href="#kvm架构图" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/06/25/kvm/kvm-arch.jpg" class title="KVM架构图">

<h2><span id="kvm-虚机的创建过程">KVM 虚机的创建过程</span><a href="#kvm-虚机的创建过程" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/06/25/kvm/kvm-start.jpg" class title="KVM 虚机的创建过程">

<p>（1）qemu-kvm 通过对 &#x2F;dev&#x2F;kvm 的 一系列 ICOTL 命令控制虚机</p>
<p>（2）一个 KVM 虚机即一个 Linux qemu-kvm 进程，与其他 Linux 进程一样被Linux 进程调度器调度。</p>
<p>（3）KVM 虚机包括虚拟内存、虚拟CPU和虚机 I&#x2F;O设备，<br>    其中，内存和 CPU 的虚拟化由 KVM 内核模块负责实现，I&#x2F;O 设备的虚拟化由 QEMU 负责实现。</p>
<p>（4）KVM虚机系统的内存是 qumu-kvm 进程的地址空间的一部分。</p>
<p>（5）KVM 虚机的 vCPU 作为 线程运行在 qemu-kvm 进程的上下文中。</p>
<h2><span id="kvm-功能">KVM 功能</span><a href="#kvm-功能" class="header-anchor">#</a></h2><ul>
<li>Core<ul>
<li>支持 CPU 和 memory 超分（Overcommit）</li>
<li>支持对称多处理（Symmetric Multi-Processing，缩写为 SMP）</li>
<li>支持 PCI 设备直接分配和 单根 I&#x2F;O 虚拟化 （SR-IOV）</li>
<li>支持实时迁移（Live Migration）</li>
</ul>
</li>
<li>Advanced<ul>
<li>支持热插拔 （cpu，块设备、网络设备等）</li>
<li>支持半虚拟化 I&#x2F;O （virtio）</li>
<li>支持 内核同页合并 （KSM）</li>
<li>支持 NUMA （Non-Uniform Memory Access，非一致存储访问结构 ）</li>
</ul>
</li>
</ul>
<h2><span id="kvm-vs-xen">KVM vs.  Xen</span><a href="#kvm-vs-xen" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">虚拟化</th>
<th align="center">架构</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KVM</td>
<td align="center">Type2-Hypervisor运行在宿主机操作系统之上</td>
<td align="center">基于硬件虚拟化支持的全虚拟化实现</td>
</tr>
<tr>
<td align="center">Xen</td>
<td align="center">Type1-Hypervisor运行在硬件之上</td>
<td align="center">半虚拟化</td>
</tr>
</tbody></table>
<h2><span id="kvm-动态迁移-3">KVM 动态迁移 [3]</span><a href="#kvm-动态迁移-3" class="header-anchor">#</a></h2><ol>
<li>启动空的target vm</li>
<li>客户机的内存被传输到目的主机上</li>
<li>qemu&#x2F;kvm监控内存的修改， 并传输更改的内容</li>
<li>qemu&#x2F;kvm估计剩余传输的内存可以在300ms内传输完成</li>
<li>qemu&#x2F;kvm 关闭源主机，将剩余的数据量传输到目的主机<br>[全量 + 增量]</li>
</ol>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/sammyliu/p/4543110.html">KVM 介绍（1）：简介及安装</a></li>
<li><a href="https://www.cnblogs.com/sammyliu/p/4543597.html">KVM 介绍（2）：CPU 和内存虚拟化</a></li>
<li>《KVM 实战》 8.1</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 基础-Linter</title>
    <url>/www6vHomeHexo/2022/06/24/golangBasicLinter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="linter-1">linter [1]</span><a href="#linter-1" class="header-anchor">#</a></h2><ul>
<li><p>单个</p>
<ul>
<li>golint</li>
<li>go vet </li>
<li>gocyclo</li>
<li>代码覆盖率<br>codecov</li>
<li>检查错误是否被处理<br>errcheck</li>
<li>bodyclose </li>
<li>sqlrows</li>
</ul>
</li>
<li><p>集合</p>
<ul>
<li>golangci-lint  标准</li>
<li>sonarqube</li>
<li>deepsource</li>
</ul>
</li>
<li><p>golangci-lint + reviewdog  [3]</p>
<ul>
<li>给项目提pr时，自动执行linter，自动CI？</li>
<li>github支持, gitlab也支持<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">actions</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">pull_request:</span></span><br><span class="line">    <span class="attr">types:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">opened</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">reopened</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">synchronize</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ready_for_review</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">golangci-lint:</span></span><br><span class="line">    <span class="attr">if:</span> <span class="string">$&#123;&#123;</span> <span class="type">!github.event.pull_request.draft</span> <span class="string">&#125;&#125;</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">runner</span> <span class="string">/</span> <span class="string">golangci-lint</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Check</span> <span class="string">out</span> <span class="string">code</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">golangci-lint</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">reviewdog/action-golangci-lint@v2</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">github.event_name</span> <span class="string">==</span> <span class="string">&#x27;pull_request&#x27;</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">golangci_lint_flags:</span> <span class="string">&quot;--skip-dirs=mtls/crypto,module/http2 --enable-all --timeout=10m --exclude-use-default=false --tests=false --disable=gochecknoinits,gochecknoglobals,exhaustive,exhaustruct,exhaustivestruct,nakedret,ireturn,interfacer,tagliatelle,varnamelen&quot;</span></span><br><span class="line">          <span class="attr">workdir:</span> <span class="string">pkg</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《25 直播：如何写出优雅的 Go 代码》</li>
<li><a href="https://github.com/analysis-tools-dev/static-analysis"> static-analysis</a><br><a href="https://analysis-tools.dev/">analysis-tools</a></li>
<li><a href="https://github.com/mosn/mosn/blob/master/.github/workflows/reviewdog.yml">mosn的reviewdog</a></li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>Linter</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Management</title>
    <url>/www6vHomeHexo/2022/06/23/awsManagement/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="management">Management</span><a href="#management" class="header-anchor">#</a></h2><ul>
<li>CloudTrail</li>
<li>CloudWatch</li>
<li>Organizations<br>use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts.</li>
<li>Service Catalog</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.jb51.cc/faq/2849541.html">CloudWatch, CloudTrail, Inspector</a>  看图<br><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/cloudtrail-logging.html">使用 AWS CloudTrail 记录 Amazon S3 API 调用</a><br><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/cloudwatch-monitoring.html">使用 Amazon CloudWatch 监控指标</a><br><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events">CloudTrail concepts-Global service events</a> from Practice Set 1-Q10</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Account和Orgnization</title>
    <url>/www6vHomeHexo/2022/06/23/awsAccountOrgnization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="accounts">Accounts</span><a href="#accounts" class="header-anchor">#</a></h2><ul>
<li>企业使用多AWS账户时，常见的账户体系结构：<ul>
<li>身份账户体系结构（Identity Account Architecture）<br>在单一的中心区域对所有用户进行集中管理，以取代在每个AWS账户单独管理用户的方式，并允许他们访问多个AWS账户下的不同的AWS资源。这种在单一的中心区域对所有用户进行管理可以通过跨账户IAM角色和身份联合（Federation）来达成。</li>
<li>日志账户体系结构 (Logging Account Architecture)</li>
<li>发布账户体系结构 (Publishing Account Structure)</li>
<li>账单结构 (Billing Structure)</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.iloveaws.cn/500.html">01-企业的多账户策略（Multi-Account Strategy for Enterprises）</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Computing-ELB</title>
    <url>/www6vHomeHexo/2022/06/22/awsComputingELB/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="lb2">LB[2]</span><a href="#lb2" class="header-anchor">#</a></h2><ul>
<li>3类LB [hand-on]<ul>
<li>classic LB</li>
<li>Application LB</li>
<li>Network LB</li>
</ul>
</li>
</ul>
<p>参考:<br><a href="https://zhuanlan.zhihu.com/p/152022399">AWS中负载均衡器类型</a><br><a href="https://zhuanlan.zhihu.com/p/157289913">Classic Load Balancer</a><br><a href="https://zhuanlan.zhihu.com/p/159446935">应用程序负载均衡器概述</a><br><a href="https://zhuanlan.zhihu.com/p/161848151">配置ALB基于路径的路由功能</a><br><a href="https://zhuanlan.zhihu.com/p/166345804">网络负载均衡器(NLB)</a><br><a href="https://zhuanlan.zhihu.com/p/179949353">配置网络负载均衡器</a></p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>bilibili video</li>
<li>aws中文教程</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS NoSQL</title>
    <url>/www6vHomeHexo/2022/06/21/awsDatabaseNoSQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="dynamodb">DynamoDB</span><a href="#dynamodb" class="header-anchor">#</a></h2><p>global table[1][2]</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://aws.amazon.com/cn/dynamodb/global-tables/">Amazon DynamoDB 全局表</a></li>
<li><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html">Global tables: How it works</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Framework</title>
    <url>/www6vHomeHexo/2022/06/21/golangFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="web-1">Web [1]</span><a href="#web-1" class="header-anchor">#</a></h1><ul>
<li><p>Httprouter<br>radix tree </p>
</li>
<li><p>chi</p>
<ul>
<li>最简单的框架<br>适合作为web框架入门项目<br>核心代码1200行+</li>
</ul>
</li>
<li><p>Gin</p>
<ul>
<li>主要组件<br>router 支持分组<br>middleware<br>binding-decoder和validator<br>logger<br>context</li>
<li>老牌框架<br>核心代码3000行</li>
</ul>
</li>
<li><p>echo</p>
</li>
<li><p>Fiber</p>
<ul>
<li>基于fasthttp</li>
</ul>
</li>
<li><p>beego</p>
</li>
</ul>
<h1><span id="微服务1">微服务[1]</span><a href="#微服务1" class="header-anchor">#</a></h1><h3><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h3><ul>
<li>config</li>
<li>logger</li>
<li>metrics<br>Prometheus</li>
<li>tracing<br>OpenTelemetry</li>
<li>registry</li>
<li>MQ</li>
<li>依赖注入<br>wire，dig</li>
</ul>
<h3><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h3><ul>
<li>GoMicro [2] + </li>
<li>Go-Zero</li>
<li>YOYOGO</li>
<li>Dubbo GO</li>
<li>Kratos[bili开源]</li>
<li>goframe</li>
</ul>
<h1><span id="本地缓存3">本地缓存[3]</span><a href="#本地缓存3" class="header-anchor">#</a></h1><ul>
<li>freecache</li>
<li>bigcache</li>
<li>fastcache</li>
</ul>
<img src="/www6vHomeHexo/2022/06/21/golangFramework/cache.jpg" class>



<h1><span id="time-ampamp-cron4">Time &amp;&amp; cron[4]</span><a href="#time-ampamp-cron4" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《17 直播：社区优秀开源框架对比》</li>
<li><a href="https://magodo.github.io/micro-go-src-server-client/">go-micro 源码解析 - server &amp; client</a></li>
<li><a href="https://blog.csdn.net/weixin_52183917/article/details/127704265">Golang 本地缓存选型对比及原理总结</a></li>
<li><a href="https://colobu.com/2022/11/26/some-time-and-cron-libs/">一些关于时间和定时任务的库 </a></li>
<li><a href="https://talkgo.org/t/topic/3519">Go 开源本地缓存组件选型对比（freecache、bigcache、fastcache 等）</a>  未</li>
<li><a href="https://zhuanlan.zhihu.com/p/624248354">鹅厂微创新Golang缓存组件TCache介绍</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Rumtime-内存模型</title>
    <url>/www6vHomeHexo/2022/06/21/golangMemoryModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%864">系统内存管理[4]</a></li>
<li><a href="#golang%E5%86%85%E5%AD%98">Golang内存</a><ul>
<li><a href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D-45">内存分配 [4][5]</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8-6">内存逃逸 [6]</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F">内存泄漏</a></li>
<li><a href="#%E5%A0%86%E5%92%8C%E6%A0%88">堆和栈</a></li>
<li><a href="#%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8-9-2">垃圾回收器 [9] #2</a></li>
</ul>
</li>
<li><a href="#%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84-1">系统结构 [1]</a><ul>
<li><a href="#%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%A4%9A%E7%BA%A7%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84">现代计算机的多级存储结构</a></li>
<li><a href="#%E5%A4%9A%E6%A0%B8%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98">多核带来的问题</a></li>
<li><a href="#%E5%8D%95%E5%8F%98%E9%87%8F-2">单变量 [2]</a></li>
<li><a href="#%E5%A4%9A%E5%8F%98%E9%87%8F">多变量</a></li>
<li><a href="#false-sharing-3">false sharing  [3]</a></li>
<li><a href="#happen-before">Happen-before</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="系统内存管理4">系统内存管理[4]</span><a href="#系统内存管理4" class="header-anchor">#</a></h1><ul>
<li><p>三个角色 </p>
<ul>
<li>Mutator [App]</li>
<li>Allocator<ul>
<li>Bump&#x2F;Sequential  Allocator</li>
<li>Free List Allocator<ul>
<li>First-Fit</li>
<li>Next-Fit</li>
<li>Best-Fit</li>
<li>Segregated-Fit<br>工业界用的多, golang是这种类型的变种</li>
</ul>
</li>
</ul>
</li>
<li>Collector  #2</li>
</ul>
</li>
<li><p>栈内存管理</p>
<ul>
<li>malloc<ul>
<li>‘&lt;128KB’<br>program break</li>
<li>‘&gt;&#x3D;128KB’<br>mmap</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="golang内存">Golang内存</span><a href="#golang内存" class="header-anchor">#</a></h1><h3><span id="内存分配-45">内存分配 [4][5]</span><a href="#内存分配-45" class="header-anchor">#</a></h3><ul>
<li>三种类型<ul>
<li>Tiny</li>
<li>Small</li>
<li>Large</li>
</ul>
</li>
<li>google tcmalloc<br>每个线程维护一个独立的内存池</li>
<li>多级别管理<br>4K， 8K， 16K …</li>
<li>回收内存<br>放回预先分配的大块内存中</li>
<li><strong>内存管理组件</strong><ul>
<li>mspan<ul>
<li>68个规格， [8个字节…32K]</li>
<li>双向链表</li>
</ul>
</li>
<li>mcache 线程缓存 <ul>
<li>alloc</li>
<li>tiny</li>
<li>tinyoffset</li>
</ul>
</li>
<li>mcentral 管理全局的mspan供所有线程使用<ul>
<li>noneempty<ul>
<li>mspan</li>
</ul>
</li>
<li>empty<ul>
<li>mspan</li>
</ul>
</li>
</ul>
</li>
<li>mheap 管理动态分配内存, 持有的整个堆空间<ul>
<li>arenas</li>
<li>central</li>
</ul>
</li>
<li>多级结构<br>mcache -&gt; mcentral -&gt; mheap</li>
</ul>
</li>
</ul>
<h3><span id="内存逃逸-6">内存逃逸 [6]</span><a href="#内存逃逸-6" class="header-anchor">#</a></h3><ul>
<li>编译器会根据变量是否被外部引用来决定是否逃逸：<ul>
<li>如果函数外部没有引用，则优先放到栈中；</li>
<li>如果函数外部存在引用，则必定放到堆中;</li>
<li>如果栈上放不下，则必定放到堆上;</li>
</ul>
</li>
</ul>
<h3><span id="内存泄漏">内存泄漏</span><a href="#内存泄漏" class="header-anchor">#</a></h3><ul>
<li><p>根因 [7][8]</p>
<ul>
<li>goroutine泄漏</li>
<li>slice造成内存泄漏<br>原因 - 浅拷贝</li>
<li>time.Ticker造成内存泄漏<br>原因 - 没关闭timer</li>
<li>cgo引起的内存泄漏<br>线程个数</li>
</ul>
</li>
<li><p><strong>常规分析手段</strong> [8]<br>可以利用pprof对程序进行分析从而定位内存泄漏地址</p>
</li>
</ul>
<h3><span id="堆和栈">堆和栈</span><a href="#堆和栈" class="header-anchor">#</a></h3><h3><span id="垃圾回收器-9-2">垃圾回收器 [9] #2</span><a href="#垃圾回收器-9-2" class="header-anchor">#</a></h3><p> Go语言的GC使用了**标记(mark)—清除(sweep)**技术</p>
<h1><span id="系统结构-1">系统结构 [1]</span><a href="#系统结构-1" class="header-anchor">#</a></h1><h3><span id="现代计算机的多级存储结构">现代计算机的多级存储结构</span><a href="#现代计算机的多级存储结构" class="header-anchor">#</a></h3><ul>
<li>cacheline</li>
</ul>
<h3><span id="多核带来的问题">多核带来的问题</span><a href="#多核带来的问题" class="header-anchor">#</a></h3><ul>
<li>单变量的并发操作也必须用同步手段,  比如atomic</li>
<li>全局视角下观察到的多变量读写的顺序 可能会乱序</li>
</ul>
<h3><span id="单变量-2">单变量  [2]</span><a href="#单变量-2" class="header-anchor">#</a></h3><p>单变量的原子读&#x2F;写,   多核使用MESI协议保证正确性</p>
<h3><span id="多变量">多变量</span><a href="#多变量" class="header-anchor">#</a></h3><ul>
<li><p>问题<br>乱序执行  内存重排</p>
</li>
<li><p>解决方案<br>Memory barrier</p>
</li>
</ul>
<h3><span id="false-sharing-3">false sharing  [3]</span><a href="#false-sharing-3" class="header-anchor">#</a></h3><p>因为CPU处理读写是以cache line为单位, 所以在并发修改变量时,  会一次性将其他CPU Core中的cache line invalidate 掉, 导致未修改的内存上相邻的变量也需要同步,  带来额外的性能负担</p>
<h3><span id="happen-before">Happen-before</span><a href="#happen-before" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《15 辅导 + 案例分析 + 答疑-更多课程》  体系课_Go高级工程师实战营(完结)  ***</li>
<li><a href="https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm">MESI</a></li>
<li><a href="/www6vHomeHexo/2014/03/05/falseSharing/" title="伪共享 FalseSharing">伪共享 FalseSharing</a>  self</li>
<li>《13 Go 语言的内存管理与垃圾回收》 体系课_Go高级工程师实战营(完结) </li>
<li><a href="https://www.golangroadmap.com/class/gointerview/8-1.html#%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3">1.Go 内存分配机制？</a> </li>
<li><a href="https://www.golangroadmap.com/class/gointerview/8-2.html#%E6%A6%82%E5%BF%B5">2.Go 内存逃逸机制？</a></li>
<li><a href="https://cloud.tencent.com/developer/article/2134737">浅谈Golang内存泄漏</a>  code in git</li>
<li><a href="https://cloud.tencent.com/developer/article/2279678">golang：快来抓住让我内存泄漏的“真凶”！</a>  cgo引起的内存泄漏+常规分析手段</li>
<li><a href="https://colobu.com/2022/07/16/A-Guide-to-the-Go-Garbage-Collector/">Go 垃圾回收器指南</a> </li>
<li><a href="/www6vHomeHexo/2014/01/03/javaMemoryModel/" title="Java内存模型">Java内存模型</a> self</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>内存模型</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 并发Bug</title>
    <url>/www6vHomeHexo/2022/06/20/golangConcurrencyBug/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%AD%BB%E9%94%81-1">死锁 [1]</a><ul>
<li><a href="#example1-1">example1 [1]</a></li>
<li><a href="#example2">example2</a></li>
</ul>
</li>
<li><a href="#bugs-12">bugs [1][2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="死锁-1">死锁 [1]</span><a href="#死锁-1" class="header-anchor">#</a></h1><ul>
<li>通过pprof进入goroutine页面查看</li>
</ul>
<p>[gpt]<br>在Golang中，互斥锁（Mutex）是不支持重入的，如果在一个goroutine中已经获得了互斥锁的锁并且没有释放，那么在同一个goroutine中再次尝试获取该锁时会导致死锁。</p>
<p>这是因为互斥锁是一个排他锁，当一个goroutine获取了互斥锁的锁之后，其他的goroutine就不能再获取该锁，直到该goroutine释放该锁。如果同一个goroutine再次获取该锁，就会导致死锁。</p>
<p>不过，在Golang中也提供了另外一种锁——<strong>读写锁（RWMutex）</strong>，它是支持<strong>重入</strong>的。读写锁允许多个goroutine同时读取共享资源，但在写入时必须独占访问。读写锁内部有一个计数器，用于记录已经获取读锁的goroutine数量，因此同一个goroutine获取读锁是可以重入的。但是，当一个goroutine获取了写锁时，它不能再获取读锁，否则也会导致死锁。</p>
<p>总之，<strong>互斥锁不支持重入，而读写锁支持重入</strong>。在使用锁的时候应该根据实际情况选择合适的锁来避免死锁的发生。<br>[&#x2F;gpt]</p>
<h3><span id="example1-1">example1 [1]</span><a href="#example1-1" class="header-anchor">#</a></h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">m</span><span class="params">()</span></span> &#123;</span><br><span class="line">	b.Lock()</span><br><span class="line">	c.Lock()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">n</span><span class="params">()</span></span> &#123;</span><br><span class="line">	c.Lock()</span><br><span class="line">	b.Lock()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="example2">example2</span><a href="#example2" class="header-anchor">#</a></h3><p>[gpt]<br>以下是一个可能导致死锁的Golang代码示例：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> mu sync.Mutex</span><br><span class="line">    c1 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        mu.Lock()</span><br><span class="line">        <span class="keyword">defer</span> mu.Unlock()</span><br><span class="line">        c1 &lt;- <span class="number">1</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    &lt;-c1</span><br><span class="line">    mu.Lock() <span class="comment">// 这里会导致死锁</span></span><br><span class="line">    <span class="keyword">defer</span> mu.Unlock()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">&quot;Done&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，我们创建了一个互斥锁（Mutex）<code>mu</code> 和一个整数类型的无缓冲通道<code>c1</code>。接下来，我们在一个goroutine中获取了<code>mu</code>的锁，并在获取锁之后往<code>c1</code>通道中发送了一个整数1。在主goroutine中，我们从<code>c1</code>通道中接收1，然后再次试图获取<code>mu</code>的锁。但是，由于在此之前该锁已经在另一个goroutine中被获取并没有释放，因此会导致主goroutine被阻塞，从而导致死锁。</p>
<p>解决这个问题的方法是在goroutine中释放锁，然后在主goroutine中再获取锁，如下所示：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> mu sync.Mutex</span><br><span class="line">    c1 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        mu.Lock()</span><br><span class="line">        <span class="keyword">defer</span> mu.Unlock()</span><br><span class="line">        c1 &lt;- <span class="number">1</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    &lt;-c1</span><br><span class="line">    mu.Unlock() <span class="comment">// 在goroutine中释放锁</span></span><br><span class="line">    mu.Lock()   <span class="comment">// 在主goroutine中再次获取锁</span></span><br><span class="line">    <span class="keyword">defer</span> mu.Unlock()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">&quot;Done&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个例子中，我们在goroutine中释放了锁，然后在主goroutine中再次获取锁，从而避免了死锁。</p>
<p>[&#x2F;gpt]</p>
<h1><span id="bugs-12">bugs [1][2]</span><a href="#bugs-12" class="header-anchor">#</a></h1><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">finishReq</span><span class="params">(timeout time.Duration)</span></span> r ob &#123;</span><br><span class="line">  - ch := <span class="built_in">make</span>(<span class="keyword">chan</span> ob)</span><br><span class="line">  + ch := <span class="built_in">make</span>(<span class="keyword">chan</span> ob, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        result := fn()</span><br><span class="line">        ch &lt;- result <span class="comment">// block</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> result = &lt;- ch:</span><br><span class="line">       	 <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">case</span> &lt;- time.After(timeout):</span><br><span class="line">        	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> Figure <span class="number">1.</span> A blocking bug caused by channel</span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">var</span> group sync.WaitGroup</span><br><span class="line"> group.Add(<span class="built_in">len</span>(pm.plugins))</span><br><span class="line">     <span class="keyword">for</span> _, p := <span class="keyword">range</span> pm.plugins &#123;</span><br><span class="line">     <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(p *plugin)</span></span> &#123;</span><br><span class="line">     	<span class="keyword">defer</span> group.Done()</span><br><span class="line">     &#125;</span><br><span class="line">-    group.Wait()</span><br><span class="line"> &#125;</span><br><span class="line"> + group.Wait()</span><br><span class="line"></span><br><span class="line">  Figure <span class="number">5.</span> A blocking bug caused by WaitGroup.</span><br></pre></td></tr></table></figure>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《15 辅导 + 案例分析 + 答疑-更多课程》  体系课_Go高级工程师实战营(完结)  ***</li>
<li><a href="https://cseweb.ucsd.edu/~yiying/GoStudy-ASPLOS19.pdf">Understanding Real-World Concurrency Bugs in Go</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/400948709">规避 Go 中的常见并发 bug</a> 未</li>
<li><a href="https://cloud.tencent.com/developer/article/2211893">理解真实项目中的 Go 并发 Bug</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>Concurrency</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Concurrency</title>
    <url>/www6vHomeHexo/2022/06/19/golangConcurrency/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B9%B6%E5%8F%91-21">并发 [2][1]</a><br>- <a href="#%E5%B9%B6%E5%8F%91%E5%8E%9F%E8%AF%AD">并发原语</a><br>- <a href="#context-6">Context [6]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95">应用&amp;扩展</a><br>- <a href="#%E6%8E%A7%E5%88%B6-goroutine-%E7%9A%84%E5%B9%B6%E5%8F%91%E6%95%B0%E9%87%8F-5">控制 goroutine 的并发数量 [5]</a><br>- <a href="#%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F-34">并发编程模式 [3][4]</a><br>- <a href="#%E6%89%A9%E5%B1%95%E5%B9%B6%E5%8F%91%E5%8E%9F%E8%AF%AD">扩展并发原语</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h2><span id="并发-21">并发 [2][1]</span><a href="#并发-21" class="header-anchor">#</a></h2><h5><span id="并发原语">并发原语</span><a href="#并发原语" class="header-anchor">#</a></h5><ul>
<li><p>sync.Once [3]</p>
<ul>
<li>保证在 Go 程序运行期间的某段代码只会执行一次</li>
</ul>
</li>
<li><p>sync.Pool [3]</p>
<ul>
<li>两种场景中可以使用Pool做优化<ul>
<li>进程中的inuse-objects数过多,  gc mark 消耗大量CPU</li>
<li>进程中的inuse-objects数过多, 进程RSS占用过高</li>
</ul>
</li>
<li>最佳实践<br>请求生命周期开始时，pool.Get, 请求结束时, pool.Put</li>
</ul>
</li>
<li><p>semaphore 信号量 [3]</p>
<ul>
<li>是锁的实现基础, 所有同步原语的基础设施</li>
<li>内部结构<ul>
<li>treap tree+heap<ul>
<li>二插搜索树<br>树上的每个节点都是一个链表</li>
<li>小顶堆</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>sync.Mutex  互斥锁 [3]</p>
<ul>
<li>内部结构<ul>
<li>state<br>饥饿模式:  最新进自旋的goroutine优先级最高<br>非饥饿模式:  排队</li>
<li>sema 信号量</li>
</ul>
</li>
</ul>
</li>
<li><p>sync.RWMutex 读写锁 [3]</p>
<ul>
<li>内部结构<ul>
<li>w</li>
<li>writerSem</li>
<li>readerSem</li>
<li>readerCount</li>
<li>readerWait</li>
</ul>
</li>
</ul>
</li>
<li><p>sync.Map  [3]</p>
<ul>
<li>线程安全的Map</li>
<li>内部结构<ul>
<li>mu</li>
<li>read</li>
<li>dirty</li>
<li>misses</li>
</ul>
</li>
<li>设计<ul>
<li>map+lock 多核扩展性差一点</li>
<li>sync.Map 在读多写少的情况下，基本上不需要加锁</li>
</ul>
</li>
</ul>
</li>
<li><p>sync.WaitGroup   [3]</p>
<ul>
<li>等待一组 Goroutine 的返回</li>
<li>内部结构<ul>
<li>state1</li>
</ul>
</li>
</ul>
</li>
<li><p>sync.Cond</p>
<ul>
<li>让一组的 Goroutine 都在满足特定条件时被唤醒</li>
</ul>
</li>
<li><p>sync.Context</p>
<ul>
<li>进行上下文信息传递、提供超时和取消机制、控制子 goroutine 的执行</li>
</ul>
</li>
</ul>
<h5><span id="context-6">Context [6]</span><a href="#context-6" class="header-anchor">#</a></h5><p>进行上下文信息传递、提供超时和取消机制、控制子 goroutine 的执行</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithCancel</span><span class="params">(parent Context)</span></span> (ctx Context, cancel CancelFunc) </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithDeadline</span><span class="params">(parent Context, d time.Time)</span></span> (Context, CancelFunc) </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithTimeout</span><span class="params">(parent Context, timeout time.Duration)</span></span> (Context, CancelFunc) </span><br></pre></td></tr></table></figure>
<p><a href="https://pkg.go.dev/context">context</a></p>
<h2><span id="应用amp扩展">应用&amp;扩展</span><a href="#应用amp扩展" class="header-anchor">#</a></h2><h5><span id="控制-goroutine-的并发数量-5">控制 goroutine 的并发数量 [5]</span><a href="#控制-goroutine-的并发数量-5" class="header-anchor">#</a></h5><h5><span id="并发编程模式-34">并发编程模式 [3][4]</span><a href="#并发编程模式-34" class="header-anchor">#</a></h5><ul>
<li>fan-in<br>合并两个channel</li>
<li>or channel<br>多个channel有任一个channel有返回,  就直接返回这个值。</li>
<li>pipeline<br> 串联在一起的channel</li>
</ul>
<h5><span id="扩展并发原语">扩展并发原语</span><a href="#扩展并发原语" class="header-anchor">#</a></h5><p>errgroup<br>Semaphore<br>SingleFlight</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.golangroadmap.com/class/gointerview/">GOLANG ROADMAP</a><br> <a href="https://www.golangroadmap.com/">GOLANG ROADMAP</a><br> 邀请码：caspar<br> 邀请码：Gopher-10645-1382</li>
<li>《Go 并发编程实战课》 极客时间  鸟窝</li>
<li>《15 辅导 + 案例分析 + 答疑-更多课程》  体系课_Go高级工程师实战营(完结) </li>
<li>&lt;&lt; 14 | Channel：透过代码看典型的应用模式 &gt;&gt;   Go 并发编程实战课  鸟窝</li>
<li><a href="https://eddycjy.gitbook.io/golang/di-1-ke-za-tan/control-goroutine">1.6 来，控制一下 goroutine 的并发数量</a></li>
<li><a href="https://github.com/cch123/golang-notes/blob/master/context.md">context</a>   曹大</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>Concurrency</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Deployment</title>
    <url>/www6vHomeHexo/2022/06/18/awsDeployment/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2022/06/18/awsDeployment/deployment.JPG" class title="部署方式"> 


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.iloveaws.cn/4528.html">64-AWS部署方式对比和总结</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Pulsar-数据同步</title>
    <url>/www6vHomeHexo/2022/06/18/mqPulsarSync/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="跨地域复制2">跨地域复制[2]</span><a href="#跨地域复制2" class="header-anchor">#</a></h2><ul>
<li>Full-mesh(全连通)<ul>
<li>Replicator</li>
</ul>
</li>
<li>单向复制</li>
<li>Failover 模式<br>单向复制的特例</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="2">
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUyMjkzMjA1Ng==&mid=2247487443&idx=2&sn=b9dcb5013a17c8afeb7edaf628bd162b">博文推荐 | Apache Pulsar 三大跨地域复制解决方案</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUyMjkzMjA1Ng==&mid=2247491540&idx=1&sn=cf6292ac7b29d8c3b8bbd95a0f6c71c2">博文推荐｜Apache Pulsar 跨地域复制方案选型实践</a> *** </li>
<li><a href="https://pulsar.apache.org/docs/next/concepts-replication/">Geo Replication</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Pulsar</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据 计算Computing</title>
    <url>/www6vHomeHexo/2022/06/17/bigDataComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<table>
<thead>
<tr>
<th align="center">计算能力</th>
<th align="center">数据来源</th>
<th align="center">数据处理方式</th>
<th align="center">底层框架</th>
<th align="center">延迟性</th>
</tr>
</thead>
<tbody><tr>
<td align="center">批计算</td>
<td align="center">历史数据</td>
<td align="center">批处理</td>
<td align="center">MapReduce <br> Spark</td>
<td align="center">要求不高</td>
</tr>
<tr>
<td align="center">流计算</td>
<td align="center">源源不断的流式数据</td>
<td align="center">微批处理 &amp; 逐条处理</td>
<td align="center">Spark Streaming<br>Flink</td>
<td align="center">毫秒&#x2F;秒级延迟</td>
</tr>
<tr>
<td align="center">在线查询</td>
<td align="center">历史数据</td>
<td align="center">逐条处理&#x2F;检索过滤</td>
<td align="center">ES <br> Redis</td>
<td align="center">毫秒</td>
</tr>
<tr>
<td align="center">ad-hoc 即席分析</td>
<td align="center">历史数据</td>
<td align="center">批处理&#x2F;聚合</td>
<td align="center">Impala <br> Kylin <br> ClickHouse</td>
<td align="center">毫秒&#x2F;秒级延迟</td>
</tr>
</tbody></table>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>《数据中台 - 让数据用起来》</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Storage-EFS</title>
    <url>/www6vHomeHexo/2022/06/17/awsStorageEFS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="efs">EFS</span><a href="#efs" class="header-anchor">#</a></h2><ul>
<li>NFS Protocol is used<br>NFS协议</li>
<li>Can connect instances from other VPCs<br>跨VPC</li>
<li>Can simultaneously connect thousands of instances<br>同时访问</li>
<li>On-premises computers can be connected<br>On-premises可访问</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>[SAP-1]  EBS</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Storage-EBS</title>
    <url>/www6vHomeHexo/2022/06/17/awsStorageEBS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="ebs">EBS</span><a href="#ebs" class="header-anchor">#</a></h2><ul>
<li>Limited support for attaching multiple instances*<br>多个实例attach有限制</li>
<li>EC2 instances must be in the same AZ as the EBS volume<br>不能跨AZ</li>
<li>EBS volumes are replicated within an AZ<br>可在同一个AZ内复制</li>
</ul>
<h2><span id="ebs-types">EBS Types</span><a href="#ebs-types" class="header-anchor">#</a></h2><ul>
<li>SSD-Backed Volumes</li>
<li>HDD-Backed Volumes</li>
</ul>
<h2><span id="ebs-vs-instance-store">EBS vs instance store</span><a href="#ebs-vs-instance-store" class="header-anchor">#</a></h2><ul>
<li>Instance Store volumes are physically attached to the host<br>Instance Store 本地盘</li>
<li>EBS volumes are attached over the network<br>EBS over network</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>[SAP-1]  EBS</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-VPC Endpoint</title>
    <url>/www6vHomeHexo/2022/06/17/awsNetworkVPCendpoint/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ul>
<li>VPC Endpoints[2]<br>VPC Endpoints 使您能够将您的 VPC 私下连接到可支持的 AWS 服务和 VPC 终端节点服务（由 AWS PrivateLink 提供支持），而无需 Internet 网关、NAT 设备、VPN 连接或 AWS Direct Connect 连接。<ul>
<li>网关终端节点 - Gateway Endpoint<br>支持S3 和 DynamoDB 的通信<br>网关终端节点将路由表目标用于路由表中的指定路由以获得支持的服务。<ul>
<li>注意点<br>使用 VPC 终端节点， S3 存储桶策略可以允许基于 VPC 标识符或特定 VPC 终端节点标识符的访问。 但是，在使用 VPC 终端节点时，S3 存储桶策略不支持基于 IP 地址的策略。</li>
</ul>
</li>
<li>接口终端节点 - Interface Endpoint<br>支持Kinesis Streams、ELB API、Amazon EC2 API等  .<br>接口终端节点（由 AWS PrivateLink 提供支持）在您的 VPC 中使用具有私有 IP 地址的弹性网络接口，该地址用作流向受支持服务的流量的入口点。<img src="/www6vHomeHexo/2022/06/17/awsNetworkVPCendpoint/vpc-endpoint.JPG" class title="interface endpoint vs. Gateway endpoint"></li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="2">
<li><a href="https://zhuanlan.zhihu.com/p/529181222">Chapter 2-Amazon Virtual Private Cloud (Amazon VPC) and Networking Fundamentals</a> ***</li>
</ol>
<p><a href="https://www.iloveaws.cn/3257.html">44-VPC 终端节点-网关终端节点</a> seen<br><a href="https://www.iloveaws.cn/3606.html">45-VPC 终端节点-接口终端节点</a><br><a href="https://www.iloveaws.cn/3638.html">46-使用VPC终端节点策略控制对服务的访问</a> seen<br><a href="https://www.iloveaws.cn/3882.html">53-VPC 终端节点服务</a>  seen<br><a href="https://www.iloveaws.cn/3656.html">47-配置VPC终端节点策略</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka SLO</title>
    <url>/www6vHomeHexo/2022/06/16/kafkaSLO/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2022/06/16/kafkaSLO/kafkaSLO.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://time.geekbang.org/column/article/628498">15｜组件监控：Kafka的关键指标及采集方法有哪些？</a></p>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>性能分析</title>
    <url>/www6vHomeHexo/2022/06/16/performanceAnalysis/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%96%B9%E6%B3%95">方法</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-7%E6%AD%A5%E6%B3%95-1">性能分析 7步法 [1]</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%9C%BA%E6%99%AF-2">性能测试场景 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E5%85%B7">工具</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="方法">方法</span><a href="#方法" class="header-anchor">#</a></h1><h3><span id="性能分析-7步法-1">性能分析 7步法 [1]</span><a href="#性能分析-7步法-1" class="header-anchor">#</a></h3><p>第一步：压力场景数据。<br>第二步：分析架构图。<br>第三步：拆分响应时间。<br>第四步：全局监控分析。 [3]<br>第五步：定向监控分析。 [3]<br>第六步：判断性能瓶颈点。<br>第七步：确定解决方案。</p>
<h3><span id="性能测试场景-2">性能测试场景 [2]</span><a href="#性能测试场景-2" class="header-anchor">#</a></h3><p>基准场景<br>容量场景<br>稳定性场景<br>异常场景</p>
<img src="/www6vHomeHexo/2022/06/16/performanceAnalysis/p1.png" class width="100" height="50">

<h1><span id="工具">工具</span><a href="#工具" class="header-anchor">#</a></h1><ul>
<li>Java性能分析<ul>
<li>工具<ul>
<li>btrace， 慢响应，生产用</li>
<li>排查占cpu最多的线程</li>
<li>JVM 工具</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《03 | 核心分析逻辑：所有的性能分析，靠这七步都能搞定》    高楼</li>
<li>《10 | 设计基准场景需要注意哪些关键点？ 》 高楼</li>
<li>《15丨性能测试场景：如何进行监控设计？》  ***  高楼 </li>
<li>06丨倾囊相授：我毕生所学的性能分析思路都在这里了   高楼 未</li>
</ol>
]]></content>
      <categories>
        <category>性能</category>
        <category>性能测试</category>
      </categories>
      <tags>
        <tag>性能</tag>
      </tags>
  </entry>
  <entry>
    <title>性能测试</title>
    <url>/www6vHomeHexo/2022/06/15/performanceTest/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#tps%E5%92%8C%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB1">TPS和响应时间的关系[1]</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F-2">性能指标的计算方式 [2]</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7">性能监控</a></li>
<li><a href="#%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7">压测工具</a><ul>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="tps和响应时间的关系1">TPS和响应时间的关系[1]</span><a href="#tps和响应时间的关系1" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/06/15/performanceTest/performance1.jpg" class>

<p>在这个图中，定义了三条曲线、三个区域、两个点以及三个状态描述。</p>
<ol>
<li>三条曲线：吞吐量的曲线（紫色）、使用率 &#x2F; 用户数曲线（绿色）、响应时间曲线（深<br>蓝色）。</li>
<li>三个区域：轻负载区（Light Load）、重负载区（Heavy Load）、塌陷区（Buckle<br>Zone）。</li>
<li>两个点：最优并发用户数（The Optimum Number of Concurrent Users）、最大并发<br>用户数（The Maximum Number of Concurrent Users）。</li>
<li>三个状态描述：资源饱和（Resource Saturated）、吞吐下降（Throughput<br>Falling）、用户受影响（End Users Effected）。</li>
</ol>
<h1><span id="性能指标的计算方式-2">性能指标的计算方式 [2]</span><a href="#性能指标的计算方式-2" class="header-anchor">#</a></h1><p>公式（1）：<br>并发用户数计算的通用公式：<em>C</em> &#x3D; <em>nL&#x2F;T</em><br>其中 C 是平均的并发用户数；n 是 login session 的数量；L 是 login session 的平均长<br>度；T 指考察的时间段长度。</p>
<p>公式（2）：<br>并发用户数峰值： </p>
<img src="/www6vHomeHexo/2022/06/15/performanceTest/performance5.png" class>
<p>C’指并发用户数的峰值，C 就是公式（1）中得到的平均的并发用户数。该公式是假设用<br>户的 login session 产生符合泊松分布而估算得到的。</p>
<p>仔细搜索之后发现会发现这两个公式的出处是 2004 年一个叫 Eric Man Wong 的人写的一篇名叫《Method for Estimating the Number of Concurrent Users》的文章。中英文我都反复看到很多篇。同时也会网上看到有些文章中把这个文章描述成“业界公认”的计算方法。</p>
<h1><span id="性能测试">性能测试</span><a href="#性能测试" class="header-anchor">#</a></h1><ul>
<li>性能测试<ul>
<li>环境假设<ul>
<li>负载模型【4，5】<ul>
<li><ol>
<li>人为模拟请求</li>
</ol>
</li>
<li><ol start="2">
<li>复制线上流量</li>
</ol>
</li>
<li><ol start="3">
<li>引流</li>
</ol>
</li>
</ul>
</li>
<li>故障模拟【6】<ul>
<li>Eg. tcpkill</li>
</ul>
</li>
</ul>
</li>
<li>结果分析<ul>
<li>瓶颈分析</li>
<li>热点  【5】<ul>
<li>20%代码影响了80%</li>
</ul>
</li>
</ul>
</li>
<li>类型【5】<ul>
<li>benchmark</li>
<li>性能测试，负载测试，压力测试</li>
<li>稳定性测试</li>
</ul>
</li>
<li>流程 [参考3]<ul>
<li><ol>
<li>定义响应时间，TP99</li>
</ol>
</li>
<li><ol start="2">
<li>这个响应时间的限制下，找到最高的吞吐量（负载测试）</li>
</ol>
</li>
<li><ol start="3">
<li>二步测试得到的吞吐量连续7天的不间断的压测系统（稳定性测试）</li>
</ol>
</li>
<li><ol start="4">
<li>找到系统的极限值（压力测试，抗峰值 peek）</li>
</ol>
</li>
<li><ol start="5">
<li>Burst Test</li>
</ol>
</li>
</ul>
</li>
<li>注意点<ul>
<li>平均值不靠谱，TP才靠谱(百分比分布统计)<ul>
<li>平均值</li>
<li>标准方差</li>
<li>百分位数</li>
<li>中位数</li>
</ul>
</li>
<li>关系<ul>
<li>响应时间要和吞吐量挂钩<ul>
<li>不同的吞吐量会有不同的响应时间</li>
</ul>
</li>
<li>响应时间&#x2F;吞吐量要和成功率挂钩</li>
</ul>
</li>
</ul>
</li>
<li>应用程序Profile<ul>
<li>问题：让程序运行的性能变低</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="性能监控">性能监控</span><a href="#性能监控" class="header-anchor">#</a></h1><ul>
<li>性能优化<ul>
<li>监控<ul>
<li>分层<ul>
<li>链路跟踪，APM</li>
<li>中间件监控<br>  Eg.Tomcat 线程池</li>
<li>基础设施监控<br>  Eg. cpu使用率，负载</li>
</ul>
</li>
<li>数据可视化，可观察性<ul>
<li>折线图</li>
<li>散点图</li>
<li>热图</li>
</ul>
</li>
<li>告警通知<ul>
<li>阈值 Eg. 比如大于TP99%</li>
</ul>
</li>
<li>性能指标<ul>
<li>响应时间，吞吐量，成功率【5】</li>
<li>低延迟，会有高吞吐</li>
</ul>
</li>
</ul>
</li>
<li>方法论【7-&gt;2.5节】<ul>
<li>工具法</li>
<li>USE，识别系统瓶颈<ul>
<li>Utilization(使用率)</li>
<li>Saturation(饱和度, 负载)</li>
<li>Errors(错误)</li>
</ul>
</li>
<li>工作负载特征归纳</li>
<li>延迟分析</li>
<li>静态性能调整</li>
<li>缓存调优</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="压测工具">压测工具</span><a href="#压测工具" class="header-anchor">#</a></h1><p>ab<br>Jmeter<br>wrk</p>
<h3><span id="网络">网络</span><a href="#网络" class="header-anchor">#</a></h3><p>httpstat [20]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>02丨性能综述：TPS和响应时间之间是什么关系？  高楼</li>
<li>03丨性能综述：怎么理解TPS、QPS、RT、吞吐量这些性能指标？ 高楼</li>
</ol>
<hr>
<ol start="3">
<li><a href="https://coolshell.cn/articles/17381.html">性能测试应该怎么做？</a> </li>
<li><a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a>  self</li>
<li><a href="http://blog.jobbole.com/88958/">关于容量预估&#x2F;性能压测的思考</a> 失效</li>
<li><a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a>   self  重复的</li>
<li>《性能之巅：洞悉系统、企业与云计算》 </li>
<li><a href="https://github.com/davecheney/httpstat"> httpstat</a>  go<br>  <a href="https://github.com/reorx/httpstat"> httpstat</a> python</li>
</ol>
]]></content>
      <categories>
        <category>性能</category>
        <category>性能测试</category>
      </categories>
      <tags>
        <tag>性能</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps-CI/CD</title>
    <url>/www6vHomeHexo/2022/06/15/devopsCICD/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="cix2fcd-pipeline">CI&#x2F;CD pipeline</span><a href="#cix2fcd-pipeline" class="header-anchor">#</a></h3><ul>
<li>CI&#x2F;CD<ul>
<li>jenkins</li>
<li>github actions</li>
</ul>
</li>
<li>GitOps  <ul>
<li>argoCD</li>
</ul>
</li>
</ul>
<h3><span id="jenkins-构建触发器-1">jenkins 构建触发器 [1]</span><a href="#jenkins-构建触发器-1" class="header-anchor">#</a></h3><ul>
<li>定时构建<br>类似cron，每日&#x2F;每周构建</li>
<li>添加gitlab的 webhook<br>gitlab发生变更后触发构建</li>
<li>轮询SCM</li>
</ul>
<h3><span id="测试-1">测试 [1]</span><a href="#测试-1" class="header-anchor">#</a></h3><ul>
<li>单元测试</li>
<li>集成测试</li>
<li>验收测试</li>
</ul>
<h3><span id="最佳实践-1">最佳实践 [1]</span><a href="#最佳实践-1" class="header-anchor">#</a></h3><ul>
<li><strong>主干开发</strong><br>CI中建议”主干开发”工作模式，这样才能真正做到持续集成</li>
<li>构建失败<br> <strong>构建失败后立即修复</strong></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《11  持续集成：软件持续集成，发布信手拈来》  lg2064-DevOps 落地笔记-拉钩专栏</li>
<li><a href="https://cloud.tencent.com/developer/article/2153864">Argo CD 保姆级入门教程</a>  云原生实验室 未</li>
<li><a href="https://cloud.tencent.com/developer/article/2153852">大妈都能看懂的 GitOps 入门指南</a>  云原生实验室 未</li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps 环境管理</title>
    <url>/www6vHomeHexo/2022/06/14/devopsEnvManage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="环境管理">环境管理</span><a href="#环境管理" class="header-anchor">#</a></h1><h3><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/06/14/devopsEnvManage/envManage.JPG" class width="763" height="296">  

<h3><span id="gitops-2">GitOps [2]</span><a href="#gitops-2" class="header-anchor">#</a></h3><p><strong>GitOps &#x3D; 版本控制系统  +  基础设施即代码</strong><br><strong>GitOps &#x3D; GIT +  IaC</strong></p>
<ul>
<li><p>好处<br>环境配置的共享和统一管理</p>
</li>
<li><p>基于持续集成和持续部署的软件开发和部署流程。</p>
</li>
</ul>
<p>开发人员提交代码改动到Git仓库，触发持续集成流水线，进行构建、测试和检查，生成新版本应用的制品并上传到制品库。</p>
<p>针对测试环境的配置仓库创建一个代码合并请求，将环境变更配置合入主干，并自动化地触发部署流水线，将新版本的应用部署到测试环境中。</p>
<p>测试通过后，将代码合并到主分支，再触发全面的集成流水线环节进行更加全面的测试工作。当流水线执行成功后，可以自动针对预发布环境的配置仓库创建一个合并请求，评审通过后，系统自动完成预发布环境的部署。</p>
<p>如果职责分离要求预发布环境的部署必须由运维人员来操作，将合并代码的权限只开放给运维人员。运维人员可以登录版本控制系统，查看本次变更的范围、评估影响、按照部署节奏完成部署。整个过程中的配置过程和参数信息都是透明共享的，使得开发和运维团队的协作更加高效和透明。</p>
<img src="/www6vHomeHexo/2022/06/14/devopsEnvManage/gitOps.JPG" class title="GitOps"> 

<h3><span id="iac基础设施即代码-2">IaC(基础设施即代码) [2]</span><a href="#iac基础设施即代码-2" class="header-anchor">#</a></h3><ul>
<li><p>基础设施即代码<br>用一种描述性的语言，通过文本管理环境配置，并且自动化完成环境配<br>置的方式。典型的就是以 CAPS 为代表的自动化环境配置管理工具</p>
</li>
<li><p>复用性<br>通过将所有环境的配置过程代码化，每个环境都对应一份配置文件，可以<br>实现公共配置的复用。</p>
</li>
<li><p>自动化<br>环境的配置过程，完全可以使用工具自动化批量完成。</p>
</li>
<li><p>Config Management</p>
<ul>
<li>ansible</li>
<li>salt</li>
</ul>
</li>
<li><p>infra Provisioning</p>
<ul>
<li>Terraform</li>
</ul>
</li>
</ul>
<h3><span id="cmdb-1">CMDB [1]</span><a href="#cmdb-1" class="header-anchor">#</a></h3><p>存储跟硬件和软件相关的配置信息</p>
<ul>
<li><p>建模</p>
</li>
<li><p>数据录入</p>
<ul>
<li>录入方式<ul>
<li>自动扫描</li>
<li>人工录入</li>
</ul>
</li>
<li>数据<ul>
<li>环境中的物理服务器的信息</li>
<li>环境中的部署服务的信息</li>
<li>服务部署在哪些服务器上的信息</li>
</ul>
</li>
</ul>
</li>
<li><p>对外的接口</p>
<ul>
<li>模型变更</li>
<li>配置项变更</li>
<li>查询接口</li>
</ul>
</li>
<li><p>与部署脚本集成</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《10  环境管理：交付测试环境的迅猛方法》 lg2064-DevOps 落地笔记-拉钩专栏  V </li>
<li>《16 | 环境管理：一切皆代码是一种什么样的体验？ 》  石雪峰</li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
        <category>环境管理</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps  配置管理</title>
    <url>/www6vHomeHexo/2022/06/14/devopsConfigManage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h5><span id="配置管理-5">配置管理 [5]</span><a href="#配置管理-5" class="header-anchor">#</a></h5><ul>
<li>四个理念<ul>
<li>版本变更标准化，</li>
<li>将一切纳入版本控制，</li>
<li>全流程可追溯和</li>
<li>单一可信数据源</li>
</ul>
</li>
</ul>
<h2><span id="配置管理的时机1">配置管理的时机[1]</span><a href="#配置管理的时机1" class="header-anchor">#</a></h2><ul>
<li>构建阶段<br>不推荐</li>
<li>部署阶段</li>
<li>启动阶段</li>
<li>运行时阶段</li>
<li>发布阶段</li>
</ul>
<h2><span id="实现方式1">实现方式[1]</span><a href="#实现方式1" class="header-anchor">#</a></h2><ul>
<li><p>Spring Boot 的 Profile 形式</p>
<ul>
<li>配置类<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Profile(&quot;dev&quot;)</span> </span><br></pre></td></tr></table></figure></li>
<li>配置文件<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#开发环境</span></span><br><span class="line"><span class="string">application-dev.properties</span></span><br><span class="line"><span class="comment">#测试环境</span></span><br><span class="line"><span class="string">application-test.properties</span></span><br><span class="line"><span class="comment">#生产环境</span></span><br><span class="line"><span class="string">application-prod.properties</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">spring.profiles.active=prod</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>基于 Git 的配置管理<br>通常情况下，Git 库中的分支不会合并</p>
</li>
<li><p>配置管理系统<br>Apollo， Nacos</p>
</li>
<li><p>配置管理数据库 CMDB。</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《09 -配置管理：实现一包到底的必胜手段》  lg2064-DevOps 落地笔记-拉钩专栏</li>
<li><a href>10 | 配置管理：最容易被忽视的DevOps工程实践基础</a> 石雪峰</li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
        <category>配置管理</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning</title>
    <url>/www6vHomeHexo/2022/06/11/aiDeepLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="deeplearning">DeepLearning</span><a href="#deeplearning" class="header-anchor">#</a></h2><ul>
<li><p>Feedforward 前向传播</p>
<ul>
<li>training<br>Get some “ground truth” labeled data, a set of   (𝒙, 𝒚)    i.e. training data</li>
<li>Feedforward:   𝒚′&#x3D; 𝒇(𝒙),  calculate loss: 𝑳(𝒚′, 𝒚)</li>
<li>Gradient Descent</li>
</ul>
</li>
<li><p>Backward</p>
<ul>
<li>Backpropagation 反向传播<br>算出每个权重的梯度</li>
</ul>
</li>
<li><p>parameters learning<br>当我们要去训练一个神经网络的时候我们要做的事情就是先feedforward的前向传播,<br>然后根据这个前向传播的结果算出所有权重的梯度，然后再把这个梯度呢 转换成一个update的值，去update每个权重。</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1GA41157mJ/">系统论文阅读研讨会week9：机器学习系统（一）</a> ***</li>
<li><a href="https://learn-sys.github.io/cn/reading/">W9：机器学习系统（一）</a> ***  对应的PPT</li>
<li><a href="https://zhuanlan.zhihu.com/p/461925341">吴恩达：28张图全解深度学习知识</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pulsar vs. Kafka</title>
    <url>/www6vHomeHexo/2022/06/10/mqComparePulsarVsKafka/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="功能性">功能性</span><a href="#功能性" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th></th>
<th>kafka</th>
<th>pulsar</th>
</tr>
</thead>
<tbody><tr>
<td>租户</td>
<td>单租户,可底层隔离</td>
<td>多租户, 企业级隔离.方便对接k8s的namespace.</td>
</tr>
<tr>
<td>数据转移</td>
<td>mirror maker独立部署</td>
<td>geo-replication 内置功能</td>
</tr>
<tr>
<td>冷热存储</td>
<td>商业使用</td>
<td>层级存储, 支持的非常好</td>
</tr>
<tr>
<td>组件依赖</td>
<td>2.8.0之后剥离ZK</td>
<td>强依赖ZK, 组件复杂</td>
</tr>
<tr>
<td>云原生支持</td>
<td>较为复杂</td>
<td>天然支持</td>
</tr>
</tbody></table>
<h2><span id="非功能性">非功能性</span><a href="#非功能性" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th></th>
<th>kafka</th>
<th>pulsar</th>
</tr>
</thead>
<tbody><tr>
<td>分区</td>
<td>kafka分区与broker强耦合[一个partion一个broker]</td>
<td>partion条带化，分成粒度更细的segment。segment 分散再多个bookeeper的节点上</td>
</tr>
<tr>
<td>水平扩展[某一节点宕机]</td>
<td>1. broker id不会自动转移，要手动维护。2. 新的broker无法承载旧的partition的流量，要手动均衡。</td>
<td>broker计算节点宕机，zookeeper元数据要变更。不影响producer，comsumer。</td>
</tr>
<tr>
<td>追赶读&amp;延迟抖动敏感场景</td>
<td>1. 重度依赖pagecahce， 追赶读的数据会冲刷掉追尾读的未读取部分 2. zero copy在内存有明显争抢情况下，会出现延迟抖动非常剧烈。</td>
<td>使用JVM管理的堆外内存，更可控。对内存的依赖小于kafka.</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV19e4y1M7Ao/">深入对比 Apache Pulsar 与 Kafka</a>  Pulsar Summit Asia 2022 ***</p>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 问题Troubleshoot</title>
    <url>/www6vHomeHexo/2022/06/08/k8sTroubleshoot/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#kubectl-get-pod-%E8%BF%94%E5%9B%9E%E7%8A%B6%E6%80%81">kubectl get pod 返回状态</a></li>
<li><a href="#%E8%BF%90%E8%90%A5%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98-%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%981">运营过程中出现的问题 - 权限问题[1]</a></li>
<li><a href="#taint-%E5%92%8C-tolerance%E7%9A%84%E9%97%AE%E9%A2%98-4">taint 和 tolerance的问题 [4]</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
<li><a href="#troubleshoot-calico-self">Troubleshoot - calico [self]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="kubectl-get-pod-返回状态">kubectl get pod 返回状态</span><a href="#kubectl-get-pod-返回状态" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/06/08/k8sTroubleshoot/podStatus.png" class width="700" height="500" title="返回状态">


<h2><span id="运营过程中出现的问题-权限问题1">运营过程中出现的问题 - 权限问题[1]</span><a href="#运营过程中出现的问题-权限问题1" class="header-anchor">#</a></h2><ul>
<li>案例1<ul>
<li>现象<br> 研发人员为提高系统效率，将update方法修改为patch<br> 研发人员本地非安全测试环境测试通过<br> 上生产，发现不work</li>
<li>原因<br>忘记更新rolebinding，对应的serviceaccount没有patch权限</li>
</ul>
</li>
<li>案例2:<ul>
<li>现象<br>研发人员创建CRD，并针对该CRD编程<br>上生产后不工作</li>
<li>原因<br>该CRD未授权，对应的组件get不到对应的CRD资源</li>
</ul>
</li>
</ul>
<h2><span id="taint-和-tolerance的问题-4">taint 和 tolerance的问题 [4]</span><a href="#taint-和-tolerance的问题-4" class="header-anchor">#</a></h2><ul>
<li>现象<ul>
<li>通过dashboard，能看到哪些用户的什么应用跑在哪些节点上</li>
</ul>
</li>
<li>原因 <ul>
<li>用户会忘记打tolerance， 导致pod无法调度， pending</li>
<li>其他用户会get node detail， 查到taints， 偷用资源</li>
</ul>
</li>
<li>解决办法 <ul>
<li>对于违规用户，批评教育为主</li>
</ul>
</li>
</ul>
<h2><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h2><p>[4] P26 P36 P90</p>
<h2><span id="troubleshoot-calico-self">Troubleshoot - calico [self]</span><a href="#troubleshoot-calico-self" class="header-anchor">#</a></h2><p>calico cpu memory 使用overload</p>
<ul>
<li>原因: 使用了full mesh的模式</li>
<li>解决: 换成RR模式 和 ipip模式</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>模块六</li>
<li>模块八-Kubernetes 控制平面组件：生命周期管理和服务发现</li>
<li>模块九：生产化集群的管理   chapter5 </li>
<li>模块七</li>
<li><a href="https://github.com/www6v/netshoot">netshoot</a> git<br>a Docker + Kubernetes network trouble-shooting swiss-army container</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/www6vHomeHexo/2022/06/07/aiMachineLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="机器学习算法">机器学习算法</span><a href="#机器学习算法" class="header-anchor">#</a></h2><ul>
<li><p>监督式学习</p>
<ul>
<li><p>Linear Models</p>
<ul>
<li>逻辑回归 (Logistic Regression)<br><strong>离散</strong><br>逻辑回归其实是一个分类算法而不是回归算法。</li>
<li>线性回归 (Linear Regression)<br><strong>连续</strong></li>
</ul>
</li>
<li><p>Nearest Neighbors</p>
<ul>
<li>K邻近算法，KNN</li>
</ul>
</li>
<li><p>决策树 Decision Trees</p>
</li>
<li><p>Support Vector Machines, SVM [2]</p>
<ul>
<li>可分 <ul>
<li>线性可分</li>
<li>线性不可分</li>
</ul>
</li>
<li>超平面<ul>
<li>低纬升到高纬</li>
</ul>
</li>
</ul>
</li>
<li><p>Naive Bayes</p>
</li>
<li><p>随机森林</p>
</li>
</ul>
</li>
<li><p>无监督式学习</p>
<ul>
<li>关联规则 </li>
<li>K-means聚类算法<br>质心（centroids），距离</li>
</ul>
</li>
<li><p>强化学习</p>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>Classification<br>Identifying which category an object belongs to.</li>
<li>Regression<br>  Predicting a continuous-valued attribute associated with an object.</li>
<li>Clustering<br>  Automatic grouping of similar objects into sets.  </li>
<li>Dimensionality reduction<br>  Reducing the number of random variables to consider.</li>
</ul>
<img src="/www6vHomeHexo/2022/06/07/aiMachineLearning/scikit-learn.png" class title="scikit-learn overview">



<h2><span id="按学习模型划分-4">按学习模型划分 [4]</span><a href="#按学习模型划分-4" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/479973669">【机器学习算法】10种常见机器学习算法+Python代码</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/b8227eac1fa6">机器学习–有监督–支持向量机SVM</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/supervised_learning.html">Supervised learning</a></p>
</li>
<li><p><a href="https://blog.csdn.net/hustlei/article/details/121803226">人工智能导论(6)——机器学习(Machine Learning)</a> ***</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>LSM-Tree  KV分离</title>
    <url>/www6vHomeHexo/2022/06/05/lsmTreeKeyValueSeparation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%89%8D%E6%8F%90">前提</a></li>
<li><a href="#kv%E5%88%86%E7%A6%BB3">KV分离[3]</a></li>
<li><a href="#%E4%B8%9A%E7%95%8C%E5%AE%9E%E7%8E%B0-titan2">业界实现-Titan[2]</a><ul>
<li><a href="#%E5%86%99%E6%B5%81%E7%A8%8B">写流程</a></li>
<li><a href="#%E8%AF%BB%E6%B5%81%E7%A8%8B">读流程</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="前提">前提</span><a href="#前提" class="header-anchor">#</a></h1><p>常见的 LSM 存储引擎，如 LevelDB 和 RocksDB，将用户写入的一组的 key 和 value 存放在一起，按顺序写入 SST。在 compaction 过程中，引擎将上层的 SST 与下层 SST 合并，产生新的 SST 文件。这一过程中，SST 里面的 key 和 value 都会被重写一遍，带来较大的写放大。<strong>如果 value 的大小远大于 key，compaction 过程带来的写放大会引入巨大的开销。</strong></p>
<h1><span id="kv分离3">KV分离[3]</span><a href="#kv分离3" class="header-anchor">#</a></h1><ul>
<li>Wisckey</li>
</ul>
<h1><span id="业界实现-titan2">业界实现-Titan[2]</span><a href="#业界实现-titan2" class="header-anchor">#</a></h1><h3><span id="写流程">写流程</span><a href="#写流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/06/05/lsmTreeKeyValueSeparation/write-titan.png" class>

<h3><span id="读流程">读流程</span><a href="#读流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/06/05/lsmTreeKeyValueSeparation/get-titan.png" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>bili </li>
<li><a href="https://www.skyzh.dev/posts/articles/2021-08-07-lsm-kv-separation-overview/">LSM 存储引擎中 KV 分离的实现</a>  *** </li>
<li><a href="https://zhuanlan.zhihu.com/p/428270334">LSM-Tree:从入门到放弃——放弃：LSM Tree的Compaction机制探讨和分析</a>   Overview</li>
<li><a href="https://zhuanlan.zhihu.com/p/423565251">论文翻译 WiscKey: Separating Keys from Values in SSD-Conscious Storage</a>  中文翻译</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>存储引擎</category>
      </categories>
      <tags>
        <tag>LSM-Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Serverless 优化</title>
    <url>/www6vHomeHexo/2022/06/03/serverlessOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h2><h3><span id="平台侧优化-1">平台侧优化 [1]</span><a href="#平台侧优化-1" class="header-anchor">#</a></h3><ul>
<li><p>冷启动（Cold Start）的耗时大部分和以下 4 个方面有关。</p>
<ul>
<li>1.资源调度和容器创建过程： 一般在秒级别时间完成；<ul>
<li>按需加载</li>
<li>P2P 加速</li>
</ul>
</li>
<li>2.代码和依赖层的下载过程：取决于代码大小以及是否有加速，一般从毫秒到秒级别不等；<ul>
<li>常用依赖的内置化</li>
</ul>
</li>
<li>3.VPC 网络的打通过程：主要是弹性网卡和路由下发耗时，通常在秒级别；<ul>
<li>在集群 VPC 内部创建代理</li>
</ul>
</li>
<li>4.运行时与用户代码初始化过程：和用户比较贴近，依据不同的语言，启动时间会有所影响，大概在毫秒到秒之间不等。 -加快函数运行环境的创建<ul>
<li>自动扩缩容<ul>
<li>基于函数互相调用的场景进行提前预测</li>
<li>基于函数的版本进行预测</li>
</ul>
</li>
<li>根据提前加载镜像的方式，部署到 Node 中</li>
</ul>
</li>
</ul>
</li>
<li><p>4.1  阿里Cloud Service Engine(CSE) 内部产品， 应用容器启动加速[3]</p>
<ul>
<li>L1: 容器提前启动，并且对容器进行冻结。<br> 应用实例 CPU 占用率为0，RAM 占用相当于之前的1&#x2F;20。</li>
<li>L2: 磁盘快照： 容器持久化到磁盘<br>时间消耗主要是数据的网络传输时间+内存拷贝时间。</li>
<li>L3: 完全冷启动<div style="text-align: center; width: 80%; height: 80%"></div></li>
</ul>
<p> <img src="https://user-images.githubusercontent.com/5608425/66549078-18b92a80-eb75-11e9-9c23-f86a1e711934.jpg" alt="ali-serverless-optimiaze"></p>
</li>
</ul>

CSE 则通过将一个应用的多个实例启动后，共享相同的指令数据，抽取出不同的指令数据，每次启动实例只需要加载多实例的**差异部分**

<ul>
<li><p>4.2 AWS Lambda 中使用的方法是维护一个热的 VM 实例池[5]</p>
</li>
<li><p>4.3 AWS Lamdba的延迟因素</p>
<ul>
<li>函数运行的并发限制： 1000</li>
<li>冷启动时间<br>预配置并发，预热Lambda执行环境<br>预配置并发，使用环境来立即执行代码</li>
</ul>
</li>
</ul>
<h3><span id="5-用户侧的优化手段-1">5 用户侧的优化手段 [1]</span><a href="#5-用户侧的优化手段-1" class="header-anchor">#</a></h3><ul>
<li>合理控制代码包的大小；</li>
<li>选择性能较高的运行时；</li>
<li>成本可控范围内合理使用<strong>预留实例</strong>；<br>阿里云  函数计算的预留实例[11]<br>预留实例的执行环境是长驻的，彻底消除冷启动对业务的影响</li>
<li><strong>定时任务激活</strong>延时敏感较高的函数实例；</li>
<li>本地缓存的的合理利用。</li>
</ul>
<h5><span id="混合部署">混合部署</span><a href="#混合部署" class="header-anchor">#</a></h5><p>更小的资源占用率， 更短的运行时间更易于混合部署。[3][5]</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>1.《04｜冷启动：如何加快函数的第一次调用过程？》 《Serverless进阶实战课》<br>3. <a href="https://mp.weixin.qq.com/s/Gj_qPPTn6KN065qUu6e-mw">研发运维效率提升100%，机器成本下降50%，阿里巴巴在 Serverless 计算领域的探索</a>  阿里<br>   I. 提高启动时间 II. 混合部署  优化<br>      <a href="https://yq.aliyun.com/articles/702070">CSE：阿里在线应用如何演进成Serverless架构</a><br>5. <a href="https://mp.weixin.qq.com/s/7qJUzf8xrGihPPLsvwPEig">无服务计算的未来和挑战: A Berkeley View on Serverless Computing</a> ***<br>9. <a href="https://yq.aliyun.com/articles/706537">分享 KubeCon 2019 （上海）关于 Serverless 及 Knative 相关演讲会议</a><br>   《加速：无服务器平台中的冷启动优化》- <a href="https://cloud.tencent.com/developer/article/1461709">腾讯云函数计算冷启动优化实践</a><br>    《Knative Serving 内部介绍》<br>11. <a href="https://help.aliyun.com/document_detail/138103.html">预留实例简介</a>  优化</p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serverless</category>
      </categories>
      <tags>
        <tag>serverless</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 热点Hotkey</title>
    <url>/www6vHomeHexo/2022/06/03/redisHotkey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="hotkey">HotKey</span><a href="#hotkey" class="header-anchor">#</a></h2><ul>
<li><p>寻找热点Key [2]</p>
<ul>
<li>客户端  [1]<br>缺点: 只能统计单个客户端</li>
<li>代理<br>缺点:  代理端的开发部署成本</li>
<li>服务端<br>缺点:  monitor命令,  短时间用</li>
<li>机器<br>packetbeat 抓包<br>缺点:  运维部署和机器成本</li>
</ul>
</li>
<li><p>解决热点key [2]</p>
<ul>
<li>拆分复杂数据结构</li>
<li>迁移热点key</li>
<li>本地缓存加通知机制  [3]<br>使用二级缓存  [1]</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTY5MDMxNA==&mid=2455759090&idx=1&sn=f9f0b49d7c1916672f9d4f63dab0c2b6">有赞透明多级缓存解决方案（TMC）</a></li>
<li>《Redis 开发与运维》  12.5</li>
<li><a href="https://github.com/shiyindaxiaojie/hotkey">京东 Redis 热点数据探测工具镜像</a><br><a href="https://mp.weixin.qq.com/s/xOzEj5HtCeh_ezHDPHw6Jw">京东毫秒级热key探测框架设计与实践，已实战于618大促</a></li>
<li><a href="https://help.aliyun.com/document_detail/67252.html">热点 Key 问题的发现与解决</a> 阿里 文档  已失效</li>
</ol>
<blockquote>
<p>通常的解决方案主要集中在对<strong>客户端</strong>和<strong>Server端</strong>进行相应的改造。<br>I. 服务端本地缓存.<br>II. 服务端分布式缓存。</p>
</blockquote>
<blockquote>
<p>阿里云方案 (整体看是用中间件 负载均衡 水平扩展)<br>I. 读写分离方案解决热读<br>写 热备; 读 存储水平扩展<br>II. 热点数据解决方案<br>该方案通过主动<strong>发现热点</strong>并对其进行<strong>存储</strong>来解决热点Key的问题。</p>
</blockquote>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 安装方式</title>
    <url>/www6vHomeHexo/2022/06/03/k8sSetupSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<table>
<thead>
<tr>
<th align="center">官方-安装方式</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">二进制</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">kubespray</td>
<td align="center">ansible</td>
</tr>
<tr>
<td align="center">KOPS</td>
<td align="center">cluster-api</td>
</tr>
<tr>
<td align="center">kubeadm安装-kubeadmin</td>
<td align="center"><a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></td>
</tr>
<tr>
<td align="center">minikube安装</td>
<td align="center"><a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a></td>
</tr>
<tr>
<td align="center">kind</td>
<td align="center"><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">非官方-安装方式</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">ansible安装 ansible - kubeasz</td>
<td align="center"><a href="https://github.com/easzlab/kubeasz">https://github.com/easzlab/kubeasz</a><br><a href="https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md">https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md</a></td>
</tr>
<tr>
<td align="center">kubekey</td>
<td align="center">kubesphere, golang</td>
</tr>
<tr>
<td align="center">sealos</td>
<td align="center"><a href="https://www.sealyun.com/">https://www.sealyun.com/</a></td>
</tr>
</tbody></table>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>模块九</p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes和VM</title>
    <url>/www6vHomeHexo/2022/06/03/k8sVM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://blog.csdn.net/qihoo_tech/article/details/113153640">kubevirt在360的探索之路（k8s接管虚拟化）</a><br><a href="https://www.jianshu.com/p/b2a35f31b88c">后Kubernetes时代的虚拟机管理技术之kubevirt篇</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis LazyFree</title>
    <url>/www6vHomeHexo/2022/06/01/redisLazyFree/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9C%BA%E6%99%AF-1">场景 [1]</a></li>
<li><a href="#%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE">相关配置</a></li>
<li><a href="#%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h2><span id="场景-1">场景 [1]</span><a href="#场景-1" class="header-anchor">#</a></h2><ul>
<li>场景一：客户端执行的<strong>显示删除&#x2F;清除命令</strong>，比如 del，flushdb 等；</li>
<li>场景二：某些指令带有的隐式删除命令，比如 move , rename 等；</li>
<li>场景三：到达<strong>过期</strong>时间的数据需要删除；</li>
<li>场景四：使用内存达到 maxmemory 后被选出来要<strong>淘汰</strong>的数据需要删除；</li>
<li>场景五：在主从同步全量同步阶段，从库收到主库的 RDB 文件后要先删除现有的数据再加载 RDB 文件；</li>
</ul>
<h2><span id="相关配置">相关配置</span><a href="#相关配置" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">惰性删除相 关的配置项</span><br><span class="line"></span><br><span class="line">lazyfree-lazy-eviction：对应缓存淘汰时的数据删除场景。</span><br><span class="line">lazyfree-lazy-expire：对应过期 key 的删除场景。</span><br><span class="line">lazyfree-lazy-server-del：对应会隐式进行删除操作的 server 命令执行场景。</span><br><span class="line">replica-lazy-flush：对应从节点完成全量同步后，删除原有旧数据的场景。</span><br></pre></td></tr></table></figure>


<h2><span id="源代码">源代码</span><a href="#源代码" class="header-anchor">#</a></h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">evict.c</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">freeMemoryIfNeeded</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    <span class="comment">/*......*/</span></span><br><span class="line">    <span class="comment">/* Finally remove the selected key. */</span></span><br><span class="line">        <span class="keyword">if</span> (bestkey) &#123;</span><br><span class="line">            db = server.db+bestdbid;</span><br><span class="line">            robj *keyobj = createStringObject(bestkey,sdslen(bestkey));</span><br><span class="line">            propagateExpire(db,keyobj,server.lazyfree_lazy_eviction);  ### <span class="number">1</span></span><br><span class="line">            <span class="comment">/* We compute the amount of memory freed by db*Delete() alone.</span></span><br><span class="line"><span class="comment">             * It is possible that actually the memory needed to propagate</span></span><br><span class="line"><span class="comment">             * the DEL in AOF and replication link is greater than the one</span></span><br><span class="line"><span class="comment">             * we are freeing removing the key, but we can&#x27;t account for</span></span><br><span class="line"><span class="comment">             * that otherwise we would never exit the loop.</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * AOF and Output buffer memory will be freed eventually so</span></span><br><span class="line"><span class="comment">             * we only care about memory used by the key space. */</span></span><br><span class="line">            delta = (<span class="type">long</span> <span class="type">long</span>) zmalloc_used_memory();    <span class="comment">//获取当前内存使用量</span></span><br><span class="line">            latencyStartMonitor(eviction_latency);</span><br><span class="line">            <span class="keyword">if</span> (server.lazyfree_lazy_eviction)   <span class="comment">/////. 如果 lazyfree_lazy_eviction 被设置为 1，也就是启用了缓存淘汰时的惰性删除，</span></span><br><span class="line">                dbAsyncDelete(db,keyobj);        <span class="comment">/////.  那么，删除操作对应的命令就是 UNLINK；</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                dbSyncDelete(db,keyobj);         <span class="comment">/////.  否则的话，命令就是 DEL。</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">/*......*/</span></span><br><span class="line">            delta -= (<span class="type">long</span> <span class="type">long</span>) zmalloc_used_memory();   <span class="comment">///根据当前内存使用量计算数据删除前后释放....</span></span><br><span class="line">            mem_freed += delta;    <span class="comment">//更新已释放的内存量</span></span><br><span class="line">            <span class="comment">/*......*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">db.c ### <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Propagate expires into slaves and the AOF file.</span></span><br><span class="line"><span class="comment"> * When a key expires in the master, a DEL operation for this key is sent</span></span><br><span class="line"><span class="comment"> * to all the slaves and the AOF file if enabled.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This way the key expiry is centralized in one place, and since both</span></span><br><span class="line"><span class="comment"> * AOF and the master-&gt;slave link guarantee operation ordering, everything</span></span><br><span class="line"><span class="comment"> * will be consistent even if we allow write operations against expiring</span></span><br><span class="line"><span class="comment"> * keys. */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">propagateExpire</span><span class="params">(redisDb *db, robj *key, <span class="type">int</span> lazy)</span> &#123;</span><br><span class="line">    robj *argv[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    argv[<span class="number">0</span>] = lazy ? shared.unlink : shared.del;    <span class="comment">// 如果server启用了lazyfree-lazy</span></span><br><span class="line">    argv[<span class="number">1</span>] = key;                <span class="comment">//被淘汰的key对象</span></span><br><span class="line">    incrRefCount(argv[<span class="number">0</span>]);</span><br><span class="line">    incrRefCount(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (server.aof_state != AOF_OFF)   <span class="comment">///  是否启用了 AOF 日志 /// 如果启用了AOF日志</span></span><br><span class="line">        feedAppendOnlyFile(server.delCommand,db-&gt;id,argv,<span class="number">2</span>);  <span class="comment">// 把被淘汰 key 的删除操作记录到 AOF 文件中，以保证后续使用 AOF 文件进行 Redis 数据库恢复时，可以和恢复前保持一致</span></span><br><span class="line">    replicationFeedSlaves(server.slaves,db-&gt;id,argv,<span class="number">2</span>);   <span class="comment">//// 把删除操作同步给从节点，以保证主从节点的数据一致</span></span><br><span class="line"></span><br><span class="line">    decrRefCount(argv[<span class="number">0</span>]);</span><br><span class="line">    decrRefCount(argv[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">子操作一</span><br><span class="line"></span><br><span class="line">子操作一：将被淘汰的键值对从哈希表中去除，这里的哈希表既可能是设置了过期 key</span><br><span class="line">的哈希表，也可能是全局哈希表。</span><br><span class="line">子操作二：释放被淘汰键值对所占用的内存空间。</span><br></pre></td></tr></table></figure>

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">子操作一</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Search and remove an element. This is an helper function for</span></span><br><span class="line"><span class="comment"> * dictDelete() and dictUnlink(), please check the top comment</span></span><br><span class="line"><span class="comment"> * of those functions. */</span></span><br><span class="line"><span class="type">static</span> dictEntry *<span class="title function_">dictGenericDelete</span><span class="params">(dict *d, <span class="type">const</span> <span class="type">void</span> *key, <span class="type">int</span> nofree)</span> &#123;</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">    h = dictHashKey(d, key);  <span class="comment">//计算key的哈希值 </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (table = <span class="number">0</span>; table &lt;= <span class="number">1</span>; table++) &#123;</span><br><span class="line">        idx = h &amp; d-&gt;ht[table].sizemask;  <span class="comment">//根据key的哈希值获取它所在的哈希桶编号</span></span><br><span class="line">        he = d-&gt;ht[table].table[idx];   <span class="comment">//获取key所在哈希桶的第一个哈希项</span></span><br><span class="line">        prevHe = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">while</span>(he) &#123;        <span class="comment">//在哈希桶中逐一查找被删除的key是否存在</span></span><br><span class="line">            <span class="keyword">if</span> (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123;</span><br><span class="line">                <span class="comment">/* Unlink the element from the list */</span></span><br><span class="line">                <span class="comment">//如果找见被删除key了，那么将它从哈希桶的链表中去除</span></span><br><span class="line">                <span class="keyword">if</span> (prevHe)</span><br><span class="line">                    prevHe-&gt;next = he-&gt;next;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    d-&gt;ht[table].table[idx] = he-&gt;next;</span><br><span class="line">                <span class="keyword">if</span> (!nofree) &#123;        <span class="comment">//如果要同步删除，那么就释放key和value的内存空间</span></span><br><span class="line">                    dictFreeKey(d, he);     <span class="comment">//调用dictFreeKey释放</span></span><br><span class="line">                    dictFreeVal(d, he);</span><br><span class="line">                    zfree(he);</span><br><span class="line">                &#125;</span><br><span class="line">                d-&gt;ht[table].used--;</span><br><span class="line">                <span class="keyword">return</span> he;</span><br><span class="line">            &#125;</span><br><span class="line">            prevHe = he;</span><br><span class="line">            he = he-&gt;next;           <span class="comment">//当前key不是要查找的key，再找下一个</span></span><br><span class="line">        &#125;</span><br><span class="line">        ......</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Remove an element, returning DICT_OK on success or DICT_ERR if the</span></span><br><span class="line"><span class="comment"> * element was not found. */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">dictDelete</span><span class="params">(dict *ht, <span class="type">const</span> <span class="type">void</span> *key)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> dictGenericDelete(ht,key,<span class="number">0</span>) ? DICT_OK : DICT_ERR;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Remove an element from the table, but without actually releasing</span></span><br><span class="line"><span class="comment"> * the key, value and dictionary entry. The dictionary entry is returned</span></span><br><span class="line"><span class="comment"> * if the element was found (and unlinked from the table), and the user</span></span><br><span class="line"><span class="comment"> * should later call `dictFreeUnlinkedEntry()` with it in order to release it.</span></span><br><span class="line"><span class="comment"> * Otherwise if the key is not found, NULL is returned.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This function is useful when we want to remove something from the hash</span></span><br><span class="line"><span class="comment"> * table but want to use its value before actually deleting the entry.</span></span><br><span class="line"><span class="comment"> * Without this function the pattern would require two lookups:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  entry = dictFind(...);</span></span><br><span class="line"><span class="comment"> *  // Do something with entry</span></span><br><span class="line"><span class="comment"> *  dictDelete(dictionary,entry);</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Thanks to this function it is possible to avoid this, and use</span></span><br><span class="line"><span class="comment"> * instead:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * entry = dictUnlink(dictionary,entry);</span></span><br><span class="line"><span class="comment"> * // Do something with entry</span></span><br><span class="line"><span class="comment"> * dictFreeUnlinkedEntry(entry); // &lt;- This does not need to lookup again.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">dictEntry *<span class="title function_">dictUnlink</span><span class="params">(dict *ht, <span class="type">const</span> <span class="type">void</span> *key)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> dictGenericDelete(ht,key,<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">子操作二</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">基于异步删除的数据淘汰</span><br><span class="line">dbAsyncDelete</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Delete a key, value, and associated expiration entry if any, from the DB.</span></span><br><span class="line"><span class="comment"> * If there are enough allocations to free the value object may be put into</span></span><br><span class="line"><span class="comment"> * a lazy free list instead of being freed synchronously. The lazy free list</span></span><br><span class="line"><span class="comment"> * will be reclaimed in a different bio.c thread. */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LAZYFREE_THRESHOLD 64</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">dbAsyncDelete</span><span class="params">(redisDb *db, robj *key)</span> &#123;</span><br><span class="line">    <span class="comment">/* Deleting an entry from the expires dict will not free the sds of</span></span><br><span class="line"><span class="comment">     * the key, because it is shared with the main dictionary. */</span></span><br><span class="line">    <span class="keyword">if</span> (dictSize(db-&gt;expires) &gt; <span class="number">0</span>) dictDelete(db-&gt;expires,key-&gt;ptr);  <span class="comment">/// 在过期 key 的哈希表中同步删除被淘汰的键值对</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* If the value is composed of a few allocations, to free in a lazy way</span></span><br><span class="line"><span class="comment">     * is actually just slower... So under a certain limit we just free</span></span><br><span class="line"><span class="comment">     * the object synchronously. */</span></span><br><span class="line">    dictEntry *de = dictUnlink(db-&gt;dict,key-&gt;ptr);  <span class="comment">/// 在全局哈希表中异步删除被淘汰的键值对</span></span><br><span class="line">    <span class="keyword">if</span> (de) &#123;</span><br><span class="line">        robj *val = dictGetVal(de);</span><br><span class="line">        <span class="type">size_t</span> free_effort = lazyfreeGetFreeEffort(val);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* If releasing the object is too much work, do it in the background</span></span><br><span class="line"><span class="comment">         * by adding the object to the lazy free list.</span></span><br><span class="line"><span class="comment">         * Note that if the object is shared, to reclaim it now it is not</span></span><br><span class="line"><span class="comment">         * possible. This rarely happens, however sometimes the implementation</span></span><br><span class="line"><span class="comment">         * of parts of the Redis core may call incrRefCount() to protect</span></span><br><span class="line"><span class="comment">         * objects, and then call dbDelete(). In this case we&#x27;ll fall</span></span><br><span class="line"><span class="comment">         * through and reach the dictFreeUnlinkedEntry() call, that will be</span></span><br><span class="line"><span class="comment">         * equivalent to just calling decrRefCount(). */</span></span><br><span class="line">        <span class="keyword">if</span> (free_effort &gt; LAZYFREE_THRESHOLD &amp;&amp; val-&gt;refcount == <span class="number">1</span>) &#123; <span class="comment">/// 计算释放被淘汰键值对内存空间的开销///当被淘汰键值对是包含超过 64 个元素的集合类型时</span></span><br><span class="line">            atomicIncr(lazyfree_objects,<span class="number">1</span>);</span><br><span class="line">            bioCreateBackgroundJob(BIO_LAZY_FREE,val,<span class="literal">NULL</span>,<span class="literal">NULL</span>); <span class="comment">/// 会调用 bioCreateBackgroundJob 函数，来实际创建后台任务执行惰性删除</span></span><br><span class="line">            dictSetVal(db-&gt;dict,de,<span class="literal">NULL</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Release the key-val pair, or just the key if we set the val</span></span><br><span class="line"><span class="comment">     * field to NULL in order to lazy free it later. */</span></span><br><span class="line">    <span class="keyword">if</span> (de) &#123;</span><br><span class="line">        dictFreeUnlinkedEntry(db-&gt;dict,de);</span><br><span class="line">        <span class="keyword">if</span> (server.cluster_enabled) slotToKeyDel(key);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Return the amount of work needed in order to free an object.</span></span><br><span class="line"><span class="comment"> * The return value is not always the actual number of allocations the</span></span><br><span class="line"><span class="comment"> * object is compoesd of, but a number proportional to it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For strings the function always returns 1.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For aggregated objects represented by hash tables or other data structures</span></span><br><span class="line"><span class="comment"> * the function just returns the number of elements the object is composed of.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Objects composed of single allocations are always reported as having a</span></span><br><span class="line"><span class="comment"> * single item even if they are actually logical composed of multiple</span></span><br><span class="line"><span class="comment"> * elements.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For lists the function returns the number of elements in the quicklist</span></span><br><span class="line"><span class="comment"> * representing the list. */</span></span><br><span class="line"><span class="type">size_t</span> <span class="title function_">lazyfreeGetFreeEffort</span><span class="params">(robj *obj)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (obj-&gt;type == OBJ_LIST) &#123;</span><br><span class="line">        quicklist *ql = obj-&gt;ptr;</span><br><span class="line">        <span class="keyword">return</span> ql-&gt;len;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj-&gt;type == OBJ_SET &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_HT) &#123;</span><br><span class="line">        dict *ht = obj-&gt;ptr;</span><br><span class="line">        <span class="keyword">return</span> dictSize(ht);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj-&gt;type == OBJ_ZSET &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_SKIPLIST)&#123;</span><br><span class="line">        zset *zs = obj-&gt;ptr;</span><br><span class="line">        <span class="keyword">return</span> zs-&gt;zsl-&gt;length;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj-&gt;type == OBJ_HASH &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_HT) &#123;</span><br><span class="line">        dict *ht = obj-&gt;ptr;</span><br><span class="line">        <span class="keyword">return</span> dictSize(ht);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj-&gt;type == OBJ_STREAM) &#123;</span><br><span class="line">        <span class="type">size_t</span> effort = <span class="number">0</span>;</span><br><span class="line">        stream *s = obj-&gt;ptr;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Make a best effort estimate to maintain constant runtime. Every macro</span></span><br><span class="line"><span class="comment">         * node in the Stream is one allocation. */</span></span><br><span class="line">        effort += s-&gt;rax-&gt;numnodes;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Every consumer group is an allocation and so are the entries in its</span></span><br><span class="line"><span class="comment">         * PEL. We use size of the first group&#x27;s PEL as an estimate for all</span></span><br><span class="line"><span class="comment">         * others. */</span></span><br><span class="line">        <span class="keyword">if</span> (s-&gt;cgroups) &#123;</span><br><span class="line">            raxIterator ri;</span><br><span class="line">            streamCG *cg;</span><br><span class="line">            raxStart(&amp;ri,s-&gt;cgroups);</span><br><span class="line">            raxSeek(&amp;ri,<span class="string">&quot;^&quot;</span>,<span class="literal">NULL</span>,<span class="number">0</span>);</span><br><span class="line">            <span class="comment">/* There must be at least one group so the following should always</span></span><br><span class="line"><span class="comment">             * work. */</span></span><br><span class="line">            serverAssert(raxNext(&amp;ri));</span><br><span class="line">            cg = ri.data;</span><br><span class="line">            effort += raxSize(s-&gt;cgroups)*(<span class="number">1</span>+raxSize(cg-&gt;pel));</span><br><span class="line">            raxStop(&amp;ri);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> effort;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>; <span class="comment">/* Everything else is a single allocation. */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">子操作二</span><br><span class="line"></span><br><span class="line">基于同步删除的数据淘汰</span><br><span class="line">dbSyncDelete</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/LIFE_PLAN/article/details/127786442">redis惰性删除 lazy free 源码剖析，干货满满</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>Pulsar</title>
    <url>/www6vHomeHexo/2022/05/31/mqPulsar/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="物理架构-1">物理架构 [1]</span><a href="#物理架构-1" class="header-anchor">#</a></h2><ul>
<li><p>broker &lt;-&gt; bundle负载均衡 &lt;-&gt; topic</p>
<ul>
<li><p>bundle负载均衡</p>
<p>  一致性hash映射<br>  以Bundle为单位进行Topic迁移</p>
</li>
<li><p>brocker leader<br>负责为其他几个Broker分配Bundle</p>
</li>
</ul>
</li>
<li><p>bundle负载均衡</p>
<ul>
<li>bundle的分割</li>
<li>offload unload 卸载</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2022/05/31/mqPulsar/bundle.png" class title="bundle">      

<h2><span id="逻辑架构-1">逻辑架构 [1]</span><a href="#逻辑架构-1" class="header-anchor">#</a></h2><ul>
<li>tenant  <ul>
<li>namespace<ul>
<li>topic</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h2> 


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s/MHrrqldqtT_XOiwIPINRGQ">【9张图】带你认识pulsar负载均衡利器Bundle </a> jinjunzhu</li>
<li>《Pulsar In Action中文版》 第二章</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Pulsar</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux-学习资源</title>
    <url>/www6vHomeHexo/2022/05/30/linuxStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="内核">内核</span><a href="#内核" class="header-anchor">#</a></h2><ul>
<li><a href="http://arthurchiao.art/blog/intro-to-io-uring-zh/">[译] Linux 异步 I&#x2F;O 框架 io_uring：基本原理、程序示例与性能压测（2020）</a> </li>
<li><a href="http://arthurchiao.art/blog/cgroupv2-zh/">[译] Control Group v2（cgroupv2 权威指南）（KernelDoc, 2021）</a> </li>
<li><a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现</a></li>
<li><a href="http://arthurchiao.art/blog/bbr-paper-zh/">[译] [论文] BBR：基于拥塞（而非丢包）的拥塞控制（ACM, 2017）</a></li>
<li><a href="http://www.pagefault.info/">pagefault</a>  kernel， mysql  2019停更</li>
</ul>
<h2><span id="list">list</span><a href="#list" class="header-anchor">#</a></h2><ul>
<li><a href="https://github.com/www6v/linux_kernel_wiki">Linux内核学习资料：经典内核文章，内核论文，内核项目，内核面试题，内核讲解视频</a></li>
</ul>
<h2><span id="linux-源代码">linux 源代码</span><a href="#linux-源代码" class="header-anchor">#</a></h2><ul>
<li><a href="https://elixir.bootlin.com/linux/v5.18-rc7/source/mm/huge_memory.c">linux 源代码</a></li>
</ul>
]]></content>
      <categories>
        <category>linux</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 调度</title>
    <url>/www6vHomeHexo/2022/05/29/linuxSceduling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="操作系统中有">操作系统中有</span><a href="#操作系统中有" class="header-anchor">#</a></h2><ul>
<li>进程调度器（Process Scheduler）、</li>
<li>网络调度器（Network Scheduler）和 </li>
<li>I&#x2F;O 调度器（I&#x2F;O Scheduler）</li>
</ul>
<h2><span id="进程调度策略">进程调度策略</span><a href="#进程调度策略" class="header-anchor">#</a></h2><ul>
<li>实时调度策略<br>SCHED_FIFO、SCHED_RR、SCHED_DEADLINE</li>
<li>普通调度策略<br>SCHED_NORMAL、SCHED_BATCH、SCHED_IDLE</li>
<li>完全公平调度算法<br>CFS: Completely Fair Scheduling </li>
<li>调度方式<br>主动调度, 抢占式调度</li>
</ul>
<h2><span id="进程调度分类">进程调度分类</span><a href="#进程调度分类" class="header-anchor">#</a></h2><ul>
<li>短期调度</li>
<li>中期调度</li>
<li>长期调度</li>
</ul>
<h2><span id="linux的io调度算法">Linux的IO调度算法</span><a href="#linux的io调度算法" class="header-anchor">#</a></h2><ul>
<li>Deadline </li>
<li>CFQ </li>
<li>Noop</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><a href>趣谈Linux操作系统 - 15-调度（上）：如何制定项目管理流程？</a>  刘超</li>
<li><a href="https://draveness.me/system-design-scheduler/">调度系统设计精要</a></li>
</ul>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据 学习资源</title>
    <url>/www6vHomeHexo/2022/05/28/bigDataStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="big-data">Big Data</span><a href="#big-data" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.zhihu.com/org/datafuntalk/posts">DataFunTalk</a> ***<br>DataFun社区专注于大数据和AI方向的技术分享<br>关注微信公众号：DataFunSummit</li>
<li><a href="http://dongxicheng.org/">董西成  大数据&amp;算法</a>  停更</li>
</ul>
<h2><span id="flink">Flink</span><a href="#flink" class="header-anchor">#</a></h2><ul>
<li><a href="http://www.54tianzhisheng.cn/tags/Flink/">tianzhisheng</a>    ***</li>
<li><a href="https://www.infoq.cn/theme/28">Apache Flink零基础入门到进阶</a> ***</li>
<li><a href="https://developer.aliyun.com/learning/course/58">Apache Flink 入门</a> ***</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 高级调度</title>
    <url>/www6vHomeHexo/2022/05/27/k8sAdvancedScheduling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="高级调度-overview-2">高级调度 Overview [2]</span><a href="#高级调度-overview-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/05/27/k8sAdvancedScheduling/k8sScheduler1.jpg" class title="k8s高级调度"> 

<h3><span id="affinity-vs-taint-2">Affinity vs. Taint [2]</span><a href="#affinity-vs-taint-2" class="header-anchor">#</a></h3><p>就<strong>污点</strong>而言,它的使用通常是<strong>负向的</strong>, 也就说, 污点常用在某Node不让大多数Pod调度只让少部分Pod调度时,又或者节点根本不参加工作负载时。比如:我们常见的master节点上不调度负载pod,保证master组件的稳定性；节点有特殊资源，大部分应用不需要而少部分应用需要,如GPU。</p>
<p>就<strong>Node Affinity</strong>来说,他的使用可以<strong>正向的</strong>,也就是说,我们想让某个应用的Pod部署在指定的一堆节点上。当然,也可以是负向的,比如说我们常说的Node 反亲和性,只需要把操作符设置为NotIn就能达成预期目标。</p>
<p>就<strong>污点</strong>而言，如果节点设置的污点效果是NoSchedule或者NoExecute,意味着没有设置污点容忍的Pod绝不可能调度到这些节点上。</p>
<p>就<strong>Node Affinity</strong>而言,如果节点设置了Label,但是Pod没有任何的Node Affinity设置,那么Pod是可以调度到这些节点上的。</p>
<table>
<thead>
<tr>
<th></th>
<th>特性</th>
<th>默认</th>
<th>优&#x2F;劣势</th>
</tr>
</thead>
<tbody><tr>
<td>taint</td>
<td>负向的</td>
<td>设置NoSchedule，  默认不可调度</td>
<td>不要改现有pod[2]</td>
</tr>
<tr>
<td>亲和，反亲和</td>
<td>正向的</td>
<td>设置了Label， 默认可调度</td>
<td>要改现有pod[2]</td>
</tr>
</tbody></table>
<h1><span id="亲和性">亲和性</span><a href="#亲和性" class="header-anchor">#</a></h1><p>NodeAffinity配置[1]</p>
<ul>
<li>NodeAffinity配置分类:<ul>
<li>requiredDuringSchedulingIgnoredDuringExecution (强亲和性)</li>
<li>preferredDuringSchedulingIgnoredDuringExecution (首选亲和性)</li>
</ul>
</li>
</ul>
<h1><span id="topology-4">Topology [4]</span><a href="#topology-4" class="header-anchor">#</a></h1><p>topologyKey</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s/oL7_a9a_V913IR78_dZfaA">Kubernetes高级调度- Taint和Toleration、Node Affinity分析</a></li>
<li><a href="https://mp.weixin.qq.com/s/iv60pNiLsIoWdAVVAA4Dpg">详解 K8S Pod 高级调度</a></li>
<li><a href="http://dockone.io/article/2635">Kubernetes之Pod调度 </a> 未</li>
<li><a href="https://www.bilibili.com/video/BV16t4y1w7r6">kubernetes架构师课程</a>   P97  P98 ***<br><a href="https://edu.51cto.com/course/23845.html">【2023】云原生Kubernetes全栈架构师：基于世界500强的k8s实战课程</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 集群扩容时NodeId问题</title>
    <url>/www6vHomeHexo/2022/05/23/redisNodeId/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#1-%E8%83%8C%E6%99%AF">1. 背景</a></li>
<li><a href="#2-redis%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E8%BF%87%E7%A8%8B%E6%A2%B3%E7%90%86">2. Redis集群扩容过程梳理</a><ul>
<li><a href="#21-%E5%90%AF%E5%8A%A8redis%E5%AE%9E%E4%BE%8B">2.1 启动Redis实例</a></li>
<li><a href="#22-redis%E9%9B%86%E7%BE%A4%E6%89%80%E6%9C%89%E4%B8%BB%E8%8A%82%E7%82%B9meet%E5%BE%85%E5%8A%A0%E5%85%A5%E8%8A%82%E7%82%B9">2.2 Redis集群所有主节点meet待加入节点</a></li>
<li><a href="#23-%E7%A1%AE%E5%AE%9A%E4%B8%BB%E4%BB%8E%E8%A7%92%E8%89%B2">2.3 确定主从角色</a></li>
</ul>
</li>
<li><a href="#3-redis%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E6%9C%9F%E9%97%B4%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81">3. Redis集群扩容期间节点状态</a><ul>
<li><a href="#31-%E6%89%A9%E5%AE%B9%E5%89%8D%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81">3.1 扩容前集群节点状态</a></li>
<li><a href="#32-redis%E5%90%AF%E5%8A%A8%E5%90%8E%E5%8D%95%E8%BA%AB%E7%8A%B6%E6%80%81">3.2 Redis启动后“单身”状态</a></li>
<li><a href="#33-meet-redis%E5%AE%9E%E4%BE%8B1">3.3 Meet Redis实例1</a></li>
<li><a href="#34-meet-redis%E5%AE%9E%E4%BE%8B2">3.4 Meet Redis实例2</a></li>
<li><a href="#35-%E6%8C%87%E5%AE%9A%E6%96%B0%E5%A2%9E%E5%AE%9E%E4%BE%8B%E4%B8%BB%E4%BB%8E%E5%85%B3%E7%B3%BB">3.5 指定新增实例主从关系</a></li>
</ul>
</li>
<li><a href="#5-%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB">5. 延伸阅读</a><ul>
<li><a href="#51-redis-%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">5.1 Redis 集群数据结构</a></li>
<li><a href="#52-redis-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B">5.2 Redis 启动过程</a></li>
<li><a href="#53-cluster-meet%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B">5.3 cluster meet处理过程</a></li>
<li><a href="#53-%E6%8F%AD%E6%99%93-node_id%E6%98%AF%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E7%9A%84%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%8F%98%E5%8C%96%E5%91%A2">5.3 揭晓 node_id是如何生成的？为什么会变化呢？</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="1-背景">1. 背景</span><a href="#1-背景" class="header-anchor">#</a></h1><p>有一次扩容 Redis 集群，往集群加入节点，这个操作了很多次的功能居然失败了。查看到异常日志：addAppClusterSharding:10.3.28.7:10240:10.3.28.70, result is false<br>观察发现，Redis实例已启动，并且加入到集群，但是主从角色不正确<br>本地调试发现，在从节点执行cluster replicate <master_node_id>报错，提示***，发现命令的<master_node_id>和Redis实例的<node_id>不一致</node_id></master_node_id></master_node_id></p>
<ul>
<li>Redis node_id是如何生成的，怎么会变化呢？</li>
</ul>
<p>临时查看最近修改代码，<br>com.tuhu.renault.portal.redis.impl.RedisCenterImpl  getNodeId方法<br>原来：<br>从目标Redis实例获取自己的node_id<br>改成了：<br>可以从其他Redis实例获取（因为cluster nodes命令可以拿到集群所有节点信息）<br>原因是：<br>当目标节点挂了，会获取不到node_id，页面操作故障节点下线会失败（用于下线故障节点）<br>修复方法：<br>临时恢复从目标实例获取自己的node_id;下线故障节点时候从其他节点获取node_id</p>
<img src="/www6vHomeHexo/2022/05/23/redisNodeId/img.png" class>

<p>下文简单梳理 Redis 集群扩容过程，主要关注 Redis node_id 变化及相关源码。</p>
<h1><span id="2-redis集群扩容过程梳理">2. Redis集群扩容过程梳理</span><a href="#2-redis集群扩容过程梳理" class="header-anchor">#</a></h1><p>Renault管理平台入口：</p>
<ul>
<li>com.tuhu.renault.portal.controller.ClusterController   doAddHorizontalNodes 方法<br>主要包括三个步骤：（1）启动 Redis 实例；（2）meet Redis 实例；（3）确定主从角色。</li>
</ul>
<h3><span id="21-启动redis实例">2.1 启动Redis实例</span><a href="#21-启动redis实例" class="header-anchor">#</a></h3><ul>
<li>根据模板生成Redis配置文件，并拷贝到目标机器</li>
<li>执行：redis-server &#x2F;usr&#x2F;local&#x2F;renault&#x2F;conf&#x2F;redis-cluster-6379.conf</li>
<li>没有返回值，检测端口被占用则认为成功</li>
</ul>
<h3><span id="22-redis集群所有主节点meet待加入节点">2.2 Redis集群所有主节点meet待加入节点</span><a href="#22-redis集群所有主节点meet待加入节点" class="header-anchor">#</a></h3><ul>
<li>确定Redis实例为”单身”</li>
<li>执行：cluster meet <ip> <port>，<a href="https://redis.io/commands/cluster-meet">https://redis.io/commands/cluster-meet</a></port></ip></li>
<li>返回“OK”即成功</li>
</ul>
<h3><span id="23-确定主从角色">2.3 确定主从角色</span><a href="#23-确定主从角色" class="header-anchor">#</a></h3><ul>
<li>获取主节点NodeId</li>
<li>从节点实例执行：cluster replicate <node_id>，<a href="https://redis.io/commands/cluster-replicate">https://redis.io/commands/cluster-replicate</a></node_id></li>
<li>返回“OK”即成功</li>
</ul>
<h1><span id="3-redis集群扩容期间节点状态">3. Redis集群扩容期间节点状态</span><a href="#3-redis集群扩容期间节点状态" class="header-anchor">#</a></h1><h3><span id="31-扩容前集群节点状态">3.1 扩容前集群节点状态</span><a href="#31-扩容前集群节点状态" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@work-arch-renault-redis-3 (16:06:02) ~ # redis-cli -p 6436 cluster nodes</span><br><span class="line">798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642579573335 66 connected 0-5454</span><br><span class="line">68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 myself,master - 0 1642579573000 69 connected 5455-10923</span><br><span class="line">9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642579575338 67 connected 10924-16383</span><br><span class="line">86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642579574337 66 connected</span><br><span class="line">ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642579573000 69 connected</span><br><span class="line">ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642579572335 67 connected</span><br></pre></td></tr></table></figure>
<p>每行的组成结构：<code>&lt;id&gt; &lt;ip:port&gt; &lt;flags&gt; &lt;master&gt; &lt;ping-sent&gt; &lt;pong-recv&gt; &lt;config-epoch&gt; &lt;link-state&gt; &lt;slot&gt; &lt;slot&gt; ... &lt;slot&gt;</code><br>每项的含义如下:</p>
<ul>
<li>1.id: 节点ID</li>
<li>2.ip:port：客户端与节点通信使用的地址</li>
<li>3.flags：逗号分割的标记位，可能的值如下 <ul>
<li>a. myself：当前连接的节点</li>
<li>b. master：节点是master</li>
<li>c. slave：节点是slave</li>
<li>d. fail?：节点处于PFAIL 状态。 当前节点无法联系，但逻辑上是可达的 (非 FAIL 状态)</li>
<li>e. fail：节点处于FAIL 状态. 大部分节点都无法与其取得联系将会将改节点由 PFAIL 状态升级至FAIL状态</li>
<li>f. handshake：还未取得信任的节点，当前正在与其进行握手</li>
<li>g. noaddr：没有地址的节点</li>
<li>h. noflags：连个标记都没有</li>
</ul>
</li>
<li><ol start="4">
<li>master_id：如果节点是slave，并且已知master节点，则这里列出master节点ID,否则的话这里列出”-“</li>
</ol>
</li>
<li><ol start="5">
<li>ping-sent：最近一次发送ping的时间，这个时间是一个unix毫秒时间戳，0代表没有发送过</li>
</ol>
</li>
<li><ol start="6">
<li>pong-recv：最近一次收到pong的时间，使用unix时间戳表示</li>
</ol>
</li>
<li><ol start="7">
<li>config-epoch：节点的epoch值（从节点值随主节点）</li>
</ol>
<ul>
<li>a. 每当节点发生失败切换时，都会创建一个新的，独特的，递增的epoch</li>
<li>b. 如果多个节点竞争同一个哈希槽时，epoch值更高的节点会抢夺到</li>
</ul>
</li>
<li><ol start="8">
<li>link-state：node-to-node集群总线使用的链接的状态，我们使用这个链接与集群中其他节点进行通信</li>
</ol>
<ul>
<li>a. 值可以是 connected 和 disconnected</li>
</ul>
</li>
<li><ol start="9">
<li>slot：哈希槽值或者一个哈希槽范围，代表当前节点可以提供服务的哈希槽值</li>
</ol>
</li>
</ul>
<h3><span id="32-redis启动后单身状态">3.2 Redis启动后“单身”状态</span><a href="#32-redis启动后单身状态" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@work-arch-docker-3 (16:14:17) ~ # redis-cli -p 6379 cluster nodes</span><br><span class="line">77199f3d3b63360117c352f51f0262627a53d215 :6379@16379 myself,master - 0 0 0 connected</span><br><span class="line">root@work-arch-docker-3 (16:14:30) ~ #</span><br><span class="line">root@work-arch-docker-3 (16:14:30) ~ # redis-cli -p 6380 cluster nodes</span><br><span class="line">60445f6231213b2285f216eac53f67408bdeae28 :6380@16380 myself,master - 0 0 0 connected</span><br></pre></td></tr></table></figure>

<h3><span id="33-meet-redis实例1">3.3 Meet Redis实例1</span><a href="#33-meet-redis实例1" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@work-arch-docker-3 (16:15:16) ~ # redis-cli -p 6379 cluster nodes</span><br><span class="line">ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580117000 69 connected</span><br><span class="line">86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580117750 66 connected</span><br><span class="line">ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580115000 67 connected</span><br><span class="line">798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580115742 66 connected 0-5454</span><br><span class="line">68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580115000 69 connected 5455-10923</span><br><span class="line">9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580117000 67 connected 10924-16383</span><br><span class="line">77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 myself,master - 0 1642580115000 0 connected</span><br></pre></td></tr></table></figure>

<h3><span id="34-meet-redis实例2">3.4 Meet Redis实例2</span><a href="#34-meet-redis实例2" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@work-arch-docker-3 (16:15:22) ~ # redis-cli -p 6380 cluster nodes</span><br><span class="line">86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580129887 66 connected</span><br><span class="line">ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580132000 69 connected</span><br><span class="line">ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580130000 67 connected</span><br><span class="line">798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580130590 66 connected 0-5454</span><br><span class="line">68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580133595 69 connected 5455-10923</span><br><span class="line">9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580132593 67 connected 10924-16383</span><br><span class="line">77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 master - 0 1642580131592 0 connected</span><br><span class="line">60445f6231213b2285f216eac53f67408bdeae28 10.100.140.152:6380@16380 myself,master - 0 1642580133000 73 connected</span><br></pre></td></tr></table></figure>

<h3><span id="35-指定新增实例主从关系">3.5 指定新增实例主从关系</span><a href="#35-指定新增实例主从关系" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@work-arch-docker-3 (16:16:55) ~ # redis-cli -p 6379 cluster nodes</span><br><span class="line">ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580214980 69 connected</span><br><span class="line">86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580215985 66 connected</span><br><span class="line">60445f6231213b2285f216eac53f67408bdeae28 10.100.140.152:6380@16380 slave 77199f3d3b63360117c352f51f0262627a53d215 0 1642580213000 73 connected</span><br><span class="line">ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580212000 67 connected</span><br><span class="line">798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580212000 66 connected 0-5454</span><br><span class="line">68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580213578 69 connected 5455-10923</span><br><span class="line">9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580214000 67 connected 10924-16383</span><br><span class="line">77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 myself,master - 0 1642580214000 0 connected</span><br></pre></td></tr></table></figure>

<h1><span id="5-延伸阅读">5. 延伸阅读</span><a href="#5-延伸阅读" class="header-anchor">#</a></h1><h3><span id="51-redis-集群数据结构">5.1 Redis 集群数据结构</span><a href="#51-redis-集群数据结构" class="header-anchor">#</a></h3><p>初始化 cluster 中有三个重要数据结构：</p>
<ul>
<li>clusterState：记录当前节点视角下，集群目前的状态</li>
<li>clusterNode：记录节点的状态</li>
<li>clusterLink：为clusterNode的属性，记录了连接该节点所需的有关信息<br>源码文件：cluster.h cluster.c<br><strong>struct clusterState</strong> 展开源码<br><strong>struct clusterNode</strong> 展开源码<br><strong>struct clusterLink</strong> 展开源码</li>
</ul>
<h3><span id="52-redis-启动过程">5.2 Redis 启动过程</span><a href="#52-redis-启动过程" class="header-anchor">#</a></h3><p>5.2.1 启动流程</p>
<img src="/www6vHomeHexo/2022/05/23/redisNodeId/img1.jpg" class>

<p>5.2.2 启动日志<br>Redis实例1日志<br> 展开源码<br>Redis实例2日志<br> 展开源码</p>
<h3><span id="53-cluster-meet处理过程">5.3 cluster meet处理过程</span><a href="#53-cluster-meet处理过程" class="header-anchor">#</a></h3><p>5.3.1 meet 命令处理<br>大致分为 3 个阶段：</p>
<ul>
<li>A 通过 meet msg 的 pong 回包，更改 A 对 B 的认识</li>
<li>B 通过 ping msg 的 pong 回包，更改 B 对 A 的认识</li>
<li>来自 A 的 ping or pong msg， B 更新自己看到的 A 的 slot 信息</li>
</ul>
<img src="/www6vHomeHexo/2022/05/23/redisNodeId/img2.png" class>

<p>5.3.2 Gossip消息结构（MEET、PING、PONG）<br><strong>MEET PING PNG message</strong> 展开源码</p>
<h3><span id="53-揭晓-node_id是如何生成的为什么会变化呢">5.3 揭晓 node_id是如何生成的？为什么会变化呢？</span><a href="#53-揭晓-node_id是如何生成的为什么会变化呢" class="header-anchor">#</a></h3><p>5.3.1 node_id 如何生成?<br>新建 redis 节点时候生成，几乎不会改变，除非执行：cluster reset hard</p>
<ul>
<li>cluster.c  line462 随机40字节字符串，这个值在节点启动的时候，从节点配置文件获取或者创建<ul>
<li><code>clusterLoadConfig  /* Load or create a new nodes configuration. */</code> </li>
<li><code>getRandomHexChars(node-&gt;name, CLUSTER_NAMELEN);</code></li>
</ul>
</li>
<li>cluster.c  line532  <strong>Only for hard reset: a new Node ID is generated.</strong><ul>
<li>hard reset多用于测试</li>
</ul>
</li>
<li>cluster reset 命令说明：<a href="http://www.redis.cn/commands/cluster-reset.html">http://www.redis.cn/commands/cluster-reset.html</a><br>5.3.2 Redis 集群扩容时候 node_id 为什么会变化?</li>
<li>其实 Redis 节点的 node_id 是没有变化的</li>
<li>扩容期间，meet 新节点后，立刻从其他节点获取该新节点的 nodei_id，可能拿到的是随机生成的 node_id (不认识的时候是随机生成node_id，认识之后再更新次node_id）</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>eBPF</title>
    <url>/www6vHomeHexo/2022/05/22/linux-ebpf/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8E%86%E5%8F%B2">历史</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84%E5%92%8C%E7%BB%84%E4%BB%B612">架构和组件[1][2]</a></li>
<li><a href="#bpf%E8%BF%90%E8%A1%8C%E6%97%B6%E7%BB%93%E6%9E%842">bpf运行时结构[2]</a></li>
<li><a href="#ebpf%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%B7%A5%E5%85%B74">eBPF的应用场景和对应的工具[4]</a></li>
<li><a href="#ebpf%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE">eBPF开源项目</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="历史">历史</span><a href="#历史" class="header-anchor">#</a></h2><p>转折点 2014年 - ebpf扩展到用户空间</p>
<h2><span id="架构和组件12">架构和组件[1][2]</span><a href="#架构和组件12" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/22/linux-ebpf/ebpf.png" class title="eBPF技术架构图">

<ul>
<li><p>架构</p>
<ul>
<li><strong>数据(Map 用户态控制和加载)与功能分离(bpf内核字节码)</strong></li>
</ul>
</li>
<li><p>LLVM</p>
<ul>
<li>LLVM编译出加载到内核中执行的汇编代码。</li>
<li>BPF是高级虚拟机, LLVM将C代码编译成BPF指令。</li>
</ul>
</li>
<li><p>BPF验证器</p>
</li>
<li><p>JIT编译器<br>直接将BPF字节码转换为机器码，减少运行时的时间开销。</p>
</li>
<li><p>BPF 映射Map</p>
<ul>
<li>负责在内核和用户空间之间共享数据</li>
<li>双向的数据共享, 可以分别从内核和用户空间写入和读取数据</li>
</ul>
</li>
</ul>
<h2><span id="bpf运行时结构2">bpf运行时结构[2]</span><a href="#bpf运行时结构2" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/22/linux-ebpf/bpf-runtime.png" class title="bpf运行时各模块">

<ul>
<li><p>bpf虚拟机实现包括一个解释器和JIT编译器</p>
<ul>
<li>JIT编译器负责生成处理器可直接执行的机器指令</li>
<li>指令集[4]<ul>
<li>从cBPF的33个扩展到87个</li>
<li>最大指令数，初始值4096，现在放大到了100万条</li>
</ul>
</li>
<li>11个寄存器</li>
</ul>
</li>
<li><p>bpf辅助函数</p>
</li>
<li><p>跟踪支持的事件</p>
<ul>
<li>动态跟踪<ul>
<li>kprobes&#x2F;kretprobes<br>内核中 , 数量多,非稳定ABI, fentry&#x2F;fexit替代</li>
<li>uprobes&#x2F;uretprobes<br>用户态</li>
</ul>
</li>
<li>内核静态跟踪<ul>
<li>tracepoints<br>静态探针, 静态内核跟踪点<br>稳定ABI, 数量和场景可能受限</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="ebpf的应用场景和对应的工具4">eBPF的应用场景和对应的工具[4]</span><a href="#ebpf的应用场景和对应的工具4" class="header-anchor">#</a></h2><ul>
<li>观察和跟踪<ul>
<li>memleak<br>内存使用情况</li>
<li>bcc<ul>
<li>execsnoop - 系统调用，观察短时临时进程</li>
</ul>
</li>
<li>pwru<br>网络数据包的路径</li>
<li>profile<br>火焰图  </li>
<li>kubectl trace</li>
</ul>
</li>
<li>网络<ul>
<li>Cilium eBPF Datapath 短路处理 [6]<img src="/www6vHomeHexo/2022/05/22/linux-ebpf/cilium-ebpf.png" class> </li>
<li>istio 使用 eBPF 优化流量劫持 [7]</li>
</ul>
</li>
<li>安全</li>
</ul>
<h2><span id="ebpf开源项目">eBPF开源项目</span><a href="#ebpf开源项目" class="header-anchor">#</a></h2><ul>
<li>tools<ul>
<li>bcc </li>
<li>BPFTrace</li>
<li>kubectl-trace</li>
</ul>
</li>
<li>安全<ul>
<li>Tracee - Aqua</li>
<li>Falco - Sysdig</li>
<li>Cilium Tetragon</li>
<li>ELkeid - 字节</li>
<li>eHIDS - 美团</li>
</ul>
</li>
<li>可观测<ul>
<li>eBPF Exporter - eBPF+Prometheus</li>
<li>Pixie</li>
<li>Kindling - 谐云</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《Linux内核观察技术BPF》</li>
<li><a href="https://www.bilibili.com/video/BV1LX4y157Gp?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">高效入门eBPF</a> bilibili good - 西安邮电大学 贺东升<br><a href="http://kerneltravel.net/blog/2021/ebpf_beginner/ebpf.pdf">高效入门eBPF</a> 相关的ppt  </li>
<li><a href="https://www.tcpdump.org/papers/bpf-usenix93.pdf">The BSD Packet Filter:A New Architecture for User-level Packet Capture</a> bpf论文-1992</li>
<li><a href="https://www.bilibili.com/video/BV1BT4y1q7wx?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">ebpf技术简介（上）</a> *** bili</li>
<li><a href="https://blog.csdn.net/alisystemsoftware/article/details/125753307">深入浅出 eBPF｜你要了解的 7 个核心问题</a>  阿里炎寻<br>K8s问题排查全景图 </li>
<li><a href="https://cloud.tencent.com/developer/article/1916561">Cilium eBPF 网络解析</a></li>
<li><a href="https://jimmysong.io/blog/beyond-istio-oss/#ebpf">Beyond Istio OSS —— Istio 服务网格的现状与未来</a></li>
</ol>
<ul>
<li><p><a href="https://www.ebpf.top/categories/BPF/">深入浅出 eBPF</a> 未</p>
</li>
<li><p><a href="https://feisky.xyz/posts/2021-01-06-ebpf-learn-path/">BPF 学习路径总结</a> 未<br><a href="https://www.brendangregg.com/ebpf.html">Linux Extended BPF (eBPF) Tracing Tools</a>  brendangregg</p>
</li>
<li><p><a href="https://davidlovezoe.club/wordpress/archives/tag/bpf">davidlovezoe</a> 未</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1CL411777R/">【云原生学院#25】云原生应用可观测性实践</a>  未 ***</p>
</li>
</ul>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes安全-Security</title>
    <url>/www6vHomeHexo/2022/05/22/k8sSecurity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="k8s安全加固建议-2">K8S安全加固建议 [2]</span><a href="#k8s安全加固建议-2" class="header-anchor">#</a></h2><ul>
<li><p>Kubernetes Pod 安全</p>
<ul>
<li>使用构建的容器，以非 root 用户身份运行应用程序 </li>
<li>在可能的情况下，用不可变的文件系统运行容器</li>
<li>扫描容器镜像，以发现可能存在的漏洞或错误配置</li>
<li>使用 Pod 安全政策来执行最低水平的安全，包括:<br>   防止有特权的容器<br>   拒绝经常被利用来突破的容器功能，如 hostPID、hostIPC、hostNetwork、allowedHostPath 等<br>   拒绝以 root 用户身份执行或允许提升为根用户的容器<br>   使用安全服务，如 SELinux®、AppArmor® 和 seccomp，加固应用程序，防止被利用。 @</li>
</ul>
</li>
<li><p>网络隔离和加固</p>
<ul>
<li>使用防火墙和基于角色的访问控制（RBAC）锁定对控制平面节点的访问</li>
<li>进一步限制对 Kubernetes etcd 服务器的访问</li>
<li>配置控制平面组件，使用传输层安全（TLS）证书进行认证、加密通信</li>
<li>设置网络策略来隔离资源。不同命名空间的 Pod 和服务仍然可以相互通信，除非执行额外的隔离，如网络策略 @</li>
<li>将所有凭证和敏感信息放在 Kubernetes Secret 中，而不是配置文件中。使用强大的加密方法对 Secret 进行加密</li>
</ul>
</li>
<li><p>认证和授权</p>
<ul>
<li>禁用匿名登录（默认启用）</li>
<li>使用强大的用户认证</li>
<li>创建 RBAC 策略以限制管理员、用户和服务账户活动 @</li>
</ul>
</li>
<li><p>日志审计</p>
<ul>
<li>启用审计记录（默认为禁用）</li>
<li>在节点、Pod 或容器级故障的情况下，持续保存日志以确保可用性</li>
<li>配置一个 metric logger</li>
</ul>
</li>
<li><p>升级和应用安全实践</p>
<ul>
<li>立即应用安全补丁和更新</li>
<li>定期进行漏洞扫描和渗透测试</li>
<li>当组件不再需要时，将其从环境中移除</li>
</ul>
</li>
</ul>
<h2><span id="k8s安全加固最佳实践-1">K8S安全加固最佳实践 [1]</span><a href="#k8s安全加固最佳实践-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/22/k8sSecurity/harden.jpg" class title="安全加固最佳实践">

<h2><span id="kubernetes-安全机制-6">Kubernetes 安全机制 [6]</span><a href="#kubernetes-安全机制-6" class="header-anchor">#</a></h2><h5><span id="k8s-api-安全-限制访问kubernetes-api">K8S API 安全 @限制访问Kubernetes API</span><a href="#k8s-api-安全-限制访问kubernetes-api" class="header-anchor">#</a></h5><ul>
<li>所有API交互使用TLS</li>
<li>API 认证<ul>
<li>Kubernetes支持的请求认证方式<ul>
<li>Basic 认证（不建议）</li>
<li>X509 证书认证</li>
<li>Bearer Tokens(JSON Web Tokens)：<br>Service Account &#x2F; OpenID Connect &#x2F; Webhooks</li>
</ul>
</li>
</ul>
</li>
<li>API 鉴权 - RBAC @使用基于角色的访问控制来最小化暴露<ul>
<li>三要素， 权限粒度<ul>
<li>Role， RoleBinding</li>
<li>ClusterRole， ClusterRoleBinding</li>
<li>Default ClusterRoleBinding(预置角色)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="容器能力限制">容器能力限制</span><a href="#容器能力限制" class="header-anchor">#</a></h5><ul>
<li>限制容器特权<ul>
<li>Security Context<br>限制容器运行时的用户、用户组，对容器特权进行限制</li>
<li>PSP(Pod Security Policy)<br>会在 1.25 之后被后面提到的 pod security admission webhook 替代</li>
</ul>
</li>
<li>限制资源用量<ul>
<li>Resource Quota </li>
<li>Limit Range</li>
</ul>
</li>
<li>限制资源访问  <ul>
<li>network policy @使用网络安全策略来限制集群级别的访问<br>网络隔离策略，设置黑名单或者白名单，为 namespace 去分配一独立的 IP 池</li>
</ul>
</li>
<li>限制调度节点      <ul>
<li>node selector</li>
<li>Taint</li>
<li>限制容器能够调度的节点，实现一定程度的物理隔离</li>
</ul>
</li>
</ul>
<h5><span id="安全增强">安全增强</span><a href="#安全增强" class="header-anchor">#</a></h5><ul>
<li>审计日志</li>
<li>pod security admission webhook<br>GateKeeper 开源</li>
<li>Key Management Service<br>借助 KMS 来加密 etcd 中的数据，在容器运行时进行解密</li>
</ul>
<h2><span id="安全容器-1">安全容器 [1]</span><a href="#安全容器-1" class="header-anchor">#</a></h2><ul>
<li>kata container(轻量级虚拟机)</li>
<li>gVisor(大部分是userspace的调用)</li>
</ul>
<p>参考：</p>
<ol>
<li>CNCF × Alibaba 云原生技术公开课<br><a href="https://mp.weixin.qq.com/s/nPErpcghHih5-dGPQkStJA?spm=a2c6h.12873639.article-detail.60.67905225MCDpLx">第27 章 ： Kubernetes安全之访问控制</a><br>第29 章 ： 安全容器技术</li>
<li><a href="https://jimmysong.io/docs/kubernetes-hardening-guidance/">Kubernetes 加固指南</a>    ***</li>
<li>Kubernetes in Action - 12章， 13章 （未）</li>
<li><a href="https://corvo.myseu.cn/2021/03/23/2021-03-23-%E8%AE%B0%E4%B8%80%E6%AC%A1Kubernetes%E4%B8%AD%E4%B8%A5%E9%87%8D%E7%9A%84%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/">记一次Kubernetes中严重的安全问题</a> 未</li>
<li><a href="https://icloudnative.io/posts/security-best-practices-for-kubernetes-pods/">Kubernetes 最佳安全实践指南</a> 未</li>
<li><a href="https://kubesphere.com.cn/blogs/k8s-security/">K8s 安全策略最佳实践</a>  文字稿<br><a href="https://www.bilibili.com/video/BV12Y4y1p7cp?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">火线沙龙第24期——K8s 安全策略最佳实践</a> 视频 </li>
<li><a href="https://kubesphere.com.cn/blogs/neuvector-cloud-native-security/">云原生安全产品 NeuVector 简介</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>security</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算产品-存储Storage</title>
    <url>/www6vHomeHexo/2022/05/22/cloudProduct-Storage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="存储-12">存储 [1][2]</span><a href="#存储-12" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">AWS service</th>
<th align="center">Azure service</th>
<th align="center">阿里云</th>
<th align="center">腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Object storage</td>
<td align="center"><a href="https://aws.amazon.com/s3/">Simple Storage Services (S3)</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">Blob storage</a></td>
<td align="center">OSS</td>
<td align="center">COS</td>
</tr>
<tr>
<td align="center">Virtual server disks</td>
<td align="center"><a href="https://aws.amazon.com/ebs/">Elastic Block Store (EBS)</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/storage/disks/">managed disks</a></td>
<td align="center">EBS</td>
<td align="center">CBS</td>
</tr>
<tr>
<td align="center">Shared files</td>
<td align="center"><a href="https://aws.amazon.com/efs/">Elastic File System</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/storage/files/">Files</a></td>
<td align="center">NAS</td>
<td align="center">CFS</td>
</tr>
</tbody></table>
<h3><span id="archiving-and-backup">Archiving and backup</span><a href="#archiving-and-backup" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>AWS service</th>
<th>Azure service</th>
<th>阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://aws.amazon.com/s3/storage-classes">S3 Infrequent Access (IA)</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview">Storage cool tier</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/s3/storage-classes">S3 Glacier</a>, Deep Archive-S3 Glacier</td>
<td><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview">Storage archive access tier</a></td>
<td>OSS Archive Storage</td>
<td>COS Archive Storage</td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/backup/">Backup</a></td>
<td><a href="https://azure.microsoft.com/services/backup/">Backup</a></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="hybrid-storage">Hybrid storage</span><a href="#hybrid-storage" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>AWS service</th>
<th>Azure service</th>
<th>阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://aws.amazon.com/storagegateway/">Storage Gateway</a></td>
<td><a href="https://azure.microsoft.com/services/storsimple/">StorSimple</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/datasync/">DataSync</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/storage/files/storage-sync-files-planning">File Sync</a></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="bulk-data-transfer">Bulk data transfer</span><a href="#bulk-data-transfer" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>AWS service</th>
<th>Azure service</th>
<th>阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://aws.amazon.com/snowball/disk/details/">Import&#x2F;Export Disk</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/storage/common/storage-import-export-service">Import&#x2F;Export</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/snowball/">Import&#x2F;Export Snowball</a>, <a href="https://aws.amazon.com/snowball-edge/">Snowball Edge</a>, <a href="https://aws.amazon.com/snowmobile/">Snowmobile</a></td>
<td><a href="https://azure.microsoft.com/services/storage/databox/">Data Box</a></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/158035354">从AWS到阿里云： 产品体系差异分析</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/architecture/aws-professional/storage">Compare storage on Azure and AWS</a> </li>
<li><a href="https://www.bilibili.com/video/BV1tD4y1977x?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里云系列课程</a> *** 有ppt链接</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>产品</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算产品-网络Network</title>
    <url>/www6vHomeHexo/2022/05/22/cloudProduct-Network/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





<h2><span id="网络-1-5">网络 [1] [5]</span><a href="#网络-1-5" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">云上网络[场景]</th>
<th align="center">AWS</th>
<th align="center">Azure</th>
<th align="center">阿里云</th>
<th align="center">腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Cloud virtual networking</td>
<td align="center"><a href="https://aws.amazon.com/vpc">Virtual Private Cloud (VPC)</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/virtual-network">Virtual Network</a></td>
<td align="center">VPC</td>
<td align="center">VPC</td>
</tr>
<tr>
<td align="center">EIP IPv4&#x2F;IPv6</td>
<td align="center">EIP</td>
<td align="center"></td>
<td align="center">EIP IPv4&#x2F;IPv6</td>
<td align="center">EIP IPv4&#x2F;IPv6</td>
</tr>
<tr>
<td align="center">NAT gateways</td>
<td align="center"><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">NAT Gateways</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/nat-overview">Virtual Network NAT</a></td>
<td align="center">NAT Gateway</td>
<td align="center">NAT Gateway</td>
</tr>
<tr>
<td align="center">Load balancing</td>
<td align="center">ELB(NLB，ALB)<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/load-balancer">Load Balancer</a></td>
<td align="center">SLB 默认是多可用区的，一主一备<br> CLB: 4层+7层, standby一个可用区中的实例处于工作状态 <br></td>
<td align="center">CLB</td>
</tr>
<tr>
<td align="center">Application-level load balancing</td>
<td align="center"><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">Application Load Balancer</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/application-gateway">Application Gateway</a></td>
<td align="center">ALB: 7层,在所有可用区同时工作[3]</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Route table</td>
<td align="center"><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html">Custom Route Tables</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-udr-overview">User Defined Routes</a></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Private link</td>
<td align="center"><a href="https://aws.amazon.com/privatelink">PrivateLink</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/private-link">Azure Private Link</a></td>
<td align="center">PrivateLink</td>
<td align="center">Private Link</td>
</tr>
<tr>
<td align="center">Private PaaS connectivity</td>
<td align="center"><a href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html">VPC endpoints</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview">Private Endpoint</a></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Virtual network peering</td>
<td align="center"><a href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html">VPC Peering</a></td>
<td align="center"><a href="https://azure.microsoft.com/resources/videos/virtual-network-vnet-peering">VNET Peering</a></td>
<td align="center"></td>
<td align="center">对等连接（Peering Connection)</td>
</tr>
<tr>
<td align="center">Content delivery networks</td>
<td align="center"><a href="https://aws.amazon.com/cloudfront">CloudFront</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/frontdoor">Front Door</a></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">计费模式</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">共享带宽包&#x2F;共享流量包</td>
<td align="center">共享带宽包&#x2F;共享流量包</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">跨地域网络[场景]</th>
<th align="center">AWS</th>
<th align="center">Azure</th>
<th align="center">阿里云</th>
<th align="center">腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td align="center">多VPC互联</td>
<td align="center">Transit Gateway(TGW)</td>
<td align="center"></td>
<td align="center">云企业网（CEN）</td>
<td align="center">云联网（CCN）</td>
</tr>
<tr>
<td align="center">动态加速网络</td>
<td align="center">Global accelerator</td>
<td align="center"></td>
<td align="center">Global accelerator</td>
<td align="center">Anycast Internet Acceleration</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">混合云网络[场景]</th>
<th align="center">AWS</th>
<th align="center">Azure</th>
<th align="center">阿里云</th>
<th align="center">腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Cross-premises connectivity</td>
<td align="center">VPN <a href="https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html">VPN Gateway</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways">VPN Gateway</a></td>
<td align="center">VPN</td>
<td align="center">VPN</td>
</tr>
<tr>
<td align="center">Dedicated network</td>
<td align="center"><a href="https://aws.amazon.com/directconnect">Direct Connect</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/expressroute">ExpressRoute</a></td>
<td align="center">Express Connect<br>高速通道-专线</td>
<td align="center">Direct Connect</td>
</tr>
<tr>
<td align="center">DNS management</td>
<td align="center"><a href="https://aws.amazon.com/route53">Route 53</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/dns/">DNS</a></td>
<td align="center">DNS&#x2F;Private Zone</td>
<td align="center">DNSPod</td>
</tr>
<tr>
<td align="center">DNS-based routing</td>
<td align="center"><a href="https://aws.amazon.com/route53">Route 53</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/traffic-manager">Traffic Manager</a></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">SD-WAN</td>
<td align="center">CloudWan</td>
<td align="center"></td>
<td align="center">智能接入网关 SAG</td>
<td align="center"></td>
</tr>
</tbody></table>
<p>ENI，</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/158035354">从AWS到阿里云： 产品体系差异分析</a></li>
<li>无</li>
<li>&lt;&lt;云网络-数字经济的连接&gt;&gt;  3.7 阿里</li>
<li><a href="https://www.bilibili.com/video/BV1tD4y1977x?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里云系列课程</a> *** 有ppt链接</li>
<li><a href="https://learn.microsoft.com/en-us/azure/architecture/aws-professional/networking">Networking on Azure and AWS</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>产品</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算计费</title>
    <url>/www6vHomeHexo/2022/05/21/cloudComputingBilling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h3><span id="云服务器计费方式1">云服务器计费方式[1]</span><a href="#云服务器计费方式1" class="header-anchor">#</a></h3><ul>
<li>包年&#x2F;包月 按时长计费（EC2）  [fixed]<br>  包年&#x2F;包月的计费模式也称为包周期计费模式，是一种预付费方式</li>
<li>按量付费   [on-demand]<br>  按量付费是后付费方式，可以随时开通&#x2F;删除弹性云服务器，</li>
</ul>
<h3><span id="公网计费方式2">公网计费方式[2]</span><a href="#公网计费方式2" class="header-anchor">#</a></h3><ul>
<li>按固定带宽收费   [fixed]<ul>
<li>单EIP实例(独占带宽)</li>
<li>多EIP实例(共享带宽)</li>
</ul>
</li>
<li>按流量计费  [on-demand]<br>一般情况， 只收取出云流量费用</li>
<li>按95线去峰带宽计费   [on-demand]</li>
</ul>
<h3><span id="云存储计费方式-on-demand">云存储计费方式   [on-demand]</span><a href="#云存储计费方式-on-demand" class="header-anchor">#</a></h3><ul>
<li>按存储大小计费<br>  S3</li>
<li>按请求数计费</li>
</ul>
<h3><span id="省钱工具">省钱工具</span><a href="#省钱工具" class="header-anchor">#</a></h3><ul>
<li>阿里云  -&gt;   ECS中长期成本节省方案（简称：节省计划）</li>
<li>AWS -&gt; Savings Plans</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://help.aliyun.com/document_detail/25370.html?source=5176.11533457&userCode=xbifxhv7">计费方式概述</a></li>
<li>&lt;&lt;云网络&gt;&gt; 3.2</li>
<li><a href="https://www.bilibili.com/video/BV1dP411u7js/">精讲云计算系列-成本优化章之综述</a> V</li>
<li><a href="/www6vHomeHexo/2022/03/30/aliCloudBill/" title="阿里云-计费方式">阿里云-计费方式</a> self</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>计费</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 学习资源</title>
    <url>/www6vHomeHexo/2022/05/21/k8sStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="个人">个人</span><a href="#个人" class="header-anchor">#</a></h2><h5><span id="倪朋飞">倪朋飞</span><a href="#倪朋飞" class="header-anchor">#</a></h5><p><a href="https://feisky.xyz/kubernetes-handbook/">Kubernetes 指南</a>   ***<br><a href="https://sdn.feisky.xyz/">SDN</a> ***</p>
<p><a href="https://feisky.xyz/">倪朋飞 微软</a><br><a href="https://www.zhihu.com/people/feisky/posts">倪朋飞@知乎 微软</a>  </p>
<h5><span id="宋净超-jimmy-song">宋净超-Jimmy Song</span><a href="#宋净超-jimmy-song" class="header-anchor">#</a></h5><p><a href="https://jimmysong.io/kubernetes-handbook/">Kubernetes Handbook——Kubernetes 中文指南&#x2F;云原生应用架构实战手册</a>    ***<br><a href="https://jimmysong.io/docs/istio-handbook/">Istio 基础教程</a>     ***<br><a href="https://jimmysong.io/docs/kubernetes-hardening-guidance/">Kubernetes 加固指南</a>    ***<br><a href="https://jimmysong.io/docker-handbook/">Docker Handbook</a><br><a href="https://jimmysong.io/serverless-handbook/">Serverless Handbook——无服务架构实践手册</a>  </p>
<h5><span id="others">others</span><a href="#others" class="header-anchor">#</a></h5><p><a href="http://arthurchiao.art/categories/">ArthurChiao’s Blog</a> cliumn , bpf, network  ***<br><a href="https://www.yuque.com/wei.luo/cni">wei.luo</a>  network  ***<br><a href="https://icloudnative.io/">云原生实验室</a>   米开朗基杨 @知乎 ，腾讯云开发者社区， ***<br><a href="https://skyao.io/#posts">傲小剑  service mesh</a>  service mesh ***<br><a href="https://zhaohuabing.com/">赵化冰@ZTE</a>  service mesh ***<br><a href="https://draveness.me/">draveness</a>   golang, k8s  ***<br><a href="http://www.xuyasong.com/?page_id=1827">Vermouth</a> Prometheus, Kubernetes, ETCD *** 2020 停更<br><a href="https://blog.fleeto.us/">崔秀龙</a> service mesh **<br><a href="https://morven.life/">morven</a> Kubernetes, go **<br><a href="https://www.cnblogs.com/popsuper1982/">刘超的通俗云计算  网易云首席架构</a> *<br><a href="https://imroc.io/">陈鹏</a> 腾讯云工程师 k8s  失效<br><a href="https://imfox.io/">钟华 腾讯云</a>  失效<br><a href="https://cizixs.com/">cizixs@蚂蚁金服</a>  2018 停更<br><a href="https://www.huweihuang.com/kubernetes-notes/">Kubernetes 学习笔记</a> 胡伟煌  2018 停更</p>
<h2><span id="公司">公司</span><a href="#公司" class="header-anchor">#</a></h2><h5><span id="团队分享">团队分享</span><a href="#团队分享" class="header-anchor">#</a></h5><p><a href="https://tencentcloudcontainerteam.github.io/">腾讯云容器团队</a>  k8s  2020停更<br><a href="https://cloud.tencent.com/developer/column/1075/tag-0">腾讯云容器服务团队的专栏</a><br><a href="https://zhuanlan.zhihu.com/qingcloud">青云QingCloud 知乎</a>  知乎 干货少<br><a href="https://www.zhihu.com/org/daocloud-3">DaoCloud 道客</a>  知乎 干货多</p>
<h5><span id="公司落地">公司落地</span><a href="#公司落地" class="header-anchor">#</a></h5><p><a href="https://mp.weixin.qq.com/s/hV8oT13J4DFtpe7JsxSONA">唯品会Noah云平台实现内幕披露</a><br>[点评]</p>
<h2><span id="公开课">公开课</span><a href="#公开课" class="header-anchor">#</a></h2><h5><span id="cncf-alibaba-云原生技术公开课">CNCF × Alibaba 云原生技术公开课</span><a href="#cncf-alibaba-云原生技术公开课" class="header-anchor">#</a></h5><p><a href="https://edu.aliyun.com/course/1651">CNCF × Alibaba 云原生技术公开课</a>  video+text+ppt  seen (30章节)<br><a href="https://developer.aliyun.com/article/765059?utm_content=g_1000142140">阿里云-K8s 资源全汇总 | K8s 大咖带你 31 堂课从零入门 K8s</a><br><a href="https://edu.aliyun.com/roadmap/cloudnative?spm=a2c6h.12873581.1367615.1.e9cf115eVcBAsC">阿里云-云原生技术公开课</a><br><a href="https://edu.aliyun.com/course/1651/lesson/list?spm=a2c6h.12873581.1367615.2.e9cf115eVcBAsC">阿里云-云原生技术公开课（备用地址）</a>  </p>
<ul>
<li>第9 章 ： 应用存储和持久化数据卷：核心知识 - seen</li>
<li>第10 章 ： 应用存储和持久化数据卷：存储快照与拓扑调度 </li>
<li>第11 章 ： 可观测性：你的应用健康吗 - seen<br>readiness， liveness， 应用故障排查</li>
<li>第12 章 ： 可观测性：监控与日志<br>prometheus， 监控大图</li>
<li>第13 章 ： Kubernetes网络概念及策略控制 -seen<br>约法三章</li>
<li>第14 章 ： Kubernetes Services - seen<br>架构图</li>
<li>第15 章 ： 深入剖析 Linux 容器<br> cgroup ，overlay， dockershim, containerd</li>
<li>第16 章 ： 深入理解 etcd - 基本原理解析 </li>
<li>第17 章 ： 深入理解etcd：etcd性能优化实践 </li>
<li>第18 章 ： Kubernetesdi调度和资源管理<br>QoS(requests,limits) , 亲和， taint&#x2F;toleration, 优先级调度</li>
<li>第19 章 ： 调度器的调度流程和算法介绍 </li>
<li>第21 章 ： Kubernetes 存储架构及插件使用<br>  第一部分讲述了Kubernetes存储架构，主要包括存储卷概念、挂载流程、系统组件等相关知识；<br>  第二部分讲述了Flexvolume插件的实现原理、部署架构、使用示例等；<br>  第三部分讲述了CSI插件的实现原理、资源对象、功能组件、使用示例等；</li>
<li>第22 章 ： 有状态应用编排：StatefulSet </li>
<li>第23 章 ： KubernetesAPI编程范式 </li>
<li>第24 章 ： KubernetesAPI编程利器：Operator和OperatorFramework<br>kubebuilder 和 operator-sdk</li>
<li>第25 章 ： Kubernetes网络模型进阶 – seen<br>架构图（重要）</li>
<li>第26 章 ： 理解CNI和CNI插件 – seen</li>
<li>第27 章 ： Kubernetes安全之访问控制 – seen</li>
<li>第28 章 ： 理解容器运行时接口CRI – seen </li>
<li>第29 章 ： 安全容器技术 – seen<br>Kata Containers， gVisor</li>
<li>第30 章 ： 理解RuntimeClass与使用多容器运行时</li>
</ul>
<h5><span id="华为云云原生王者课程集训营黄金gt钻石gt王者">华为云云原生王者课程集训营（黄金–&gt;钻石–&gt;王者）</span><a href="#华为云云原生王者课程集训营黄金gt钻石gt王者" class="header-anchor">#</a></h5><p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIzNzU5NTYzMA==&action=getalbum&album_id=2082025559781376005&scene=173&from_msgid=2247494514&from_itemidx=1&count=3&nolastread=1#wechat_redirect">华为云云原生王者课程集训营</a> - (黄金+钻石)  文字稿<br><a href="https://zhuanlan.zhihu.com/p/400092006">华为云云原生王者之路钻石集训营–学习笔记</a> good<br><a href="https://www.bilibili.com/video/BV1ZR4y1E7rb/?spm_id_from=333.788.recommend_more_video.2">华为云云原生王者之路_钻石</a> bilibili视频课<br><a href="https://www.bilibili.com/video/BV1Qr4y1278d/?spm_id_from=333.788.recommend_more_video.5">华为云云原生王者之路集训营_黄金课程</a> bilibili视频课<br><a href="https://edu.huaweicloud.com/activity/Cloud-native3.html?utm_source=hwynewbanner&utm_medium=sm-huaweiyun&utm_campaign=edu&utm_content=activity&utm_term=2">华为云云原生王者课程集训营</a>  视频课<br><a href="https://education.huaweicloud.com/programs/63384278-52ab-42e9-8e67-5dff5a9f37fd/about?utm_source=zhihu&utm_medium=bbs-ex&utm_campaign=edu&utm_content=courses&utm_term=16">华为云云原生王者课程集训营</a> 视频课</p>
<h5><span id="华为云-cloud-native-lives">华为云-Cloud Native Lives</span><a href="#华为云-cloud-native-lives" class="header-anchor">#</a></h5><p><a href="https://bbs.huaweicloud.com/webinar/100009">华为云-Cloud Native Lives</a>  </p>
<h5><span id="腾讯云-云原生正发声">腾讯云-云原生正发声</span><a href="#腾讯云-云原生正发声" class="header-anchor">#</a></h5><p><a href="https://mp.weixin.qq.com/s/7RoYOX9PBp79bwsLmTiOsQ">云原生技术实践 | 16位专家视频讲解合集（抢新年限定红包封面）</a><br><a href="https://cloud.tencent.com/developer/special/cloudnative">云原生正发声</a><br><a href="https://docs.qq.com/doc/DRmtFZkFNRlpqeVdR">【云原生正发声】直播PPT汇总</a></p>
<h2><span id="kube-study-platform">kube-study platform</span><a href="#kube-study-platform" class="header-anchor">#</a></h2><p><a href="https://www.katacoda.com/courses/kubernetes">Learn Kubernetes using Interactive Browser-Based Scenarios</a><br><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 性能优化</title>
    <url>/www6vHomeHexo/2022/05/19/streamingSparkPerformance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="spark优化">Spark优化</span><a href="#spark优化" class="header-anchor">#</a></h2><h3><span id="1-数据倾斜处理">1. 数据倾斜处理</span><a href="#1-数据倾斜处理" class="header-anchor">#</a></h3><p>把用户日志和用户表通过用户ID进行join，但是日志表有几亿条记录的<strong>用户ID是null</strong>，Hive把<strong>null当作一个字段值shuffle到同一个Reduce，结果这个Reduce跑了两天也没跑完，SQL当然也执行不完。像这种情况的数据倾斜，因为null字段没有意义，</strong>所以可以在where条件里加一个userID !&#x3D; null过滤掉就可以了**。</p>
<h3><span id="2-配置参数优化">2. 配置参数优化。</span><a href="#2-配置参数优化" class="header-anchor">#</a></h3><ul>
<li>case1<br>当时使用的这些服务器的CPU的核心数是48核，而应用配置的最大Executor数目是120，每台服务器30个任务，虽然30个任务在每个CPU核上都100%运行，但是总的CPU使用率仍只有60%多。</li>
</ul>
<p>具体优化也很简单，设置应用启动参数的Executor数为48×4&#x3D;192即可。</p>
<ul>
<li>case2  spark和kafka的优化
</li>
</ul>
<h3><span id="3-mapreduce-spark代码优化">3. MapReduce、Spark代码优化</span><a href="#3-mapreduce-spark代码优化" class="header-anchor">#</a></h3><p><a href="https://www6v.github.io/www6vHomeHexo/2019/03/10/sparkTrain/sparkTrain.pptx">Spark公司内部培训</a></p>
<h3><span id="4-sql语句优化">4. SQL语句优化</span><a href="#4-sql语句优化" class="header-anchor">#</a></h3><p>典型的就是Hive的<strong>MapJoin</strong>语法，如果join的一张表比较小，比如只有几MB，那么就可以用MapJoin进行连接，Hive会将这张小表当作Cache数据全部加载到所有的Map任务中，在Map阶段完成join操作，无需shuffle。</p>
<h3><span id="5-操作系统配置优化">5. 操作系统配置优化</span><a href="#5-操作系统配置优化" class="header-anchor">#</a></h3><blockquote>
<p>当transparent huge pages打开的时候，sys态CPU消耗就会增加，而不同Linux版本的transparent huge pages默认是否打开是不同的，对于默认打开transparent huge pages的Linux执行下面的指令，关闭transparent huge pages。</p>
</blockquote>
<h2><span id="spark优化">Spark优化</span><a href="#spark优化" class="header-anchor">#</a></h2><ul>
<li><p>Spark2.0 CBO<br>spark.sql.cbo.enabled</p>
</li>
<li><p>广播Join<br>大表 Join 小表(&lt;10M)<br>MapJoin - 规避shuffle<br>强制广播</p>
</li>
<li><p>SMB Join（sort merge bucket）<br>大表 Join 大表<br>分桶 - 分桶表join</p>
</li>
<li><p>数据倾斜 - 单表数据倾斜<br>产生原因： groupby 产生 shuffle<br>spark sql 自己的优化 - hashAggrartaion预聚合<br>解决方案： 二次聚合（加随机数打散）</p>
</li>
<li><p>拆分大key，打散大表， 扩容小表<br>A表（有大key的表） - A1表（有大key），A2表<br>B表（小表）<br>方案：<br>A1表 的key加随机数 ”Join“ B1表（key加随机数）<br>A2表 ”Join“ B表</p>
</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol start="5">
<li><a href>从0开始学大数据 - Spark的性能优化案例分析（上）</a>  李智慧</li>
<li><a href>从0开始学大数据 - Spark的性能优化案例分析（下）</a>  李智慧<br>bilibili - 尚硅谷-大数据Spark3.0调优</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云 汇总</title>
    <url>/www6vHomeHexo/2022/05/16/aliyunSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ul>
<li><a href="/www6vHomeHexo/2022/03/15/aliCloudNetwork/" title="阿里云-网络">阿里云-网络</a></li>
<li><a href="/www6vHomeHexo/2022/07/13/aliyunDB/" title="阿里云 数据库">阿里云 数据库</a></li>
<li><a href="/www6vHomeHexo/2022/01/15/aliyunCDN/" title="阿里云-CDN">阿里云-CDN</a></li>
<li><a href="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/" title="阿里云-云迁移Migrate">阿里云-云迁移Migrate</a></li>
<li><a href="/www6vHomeHexo/2022/01/04/aliyunHybridCloud/" title="阿里云-混合云HybridCloud">阿里云-混合云HybridCloud</a></li>
<li><a href="/www6vHomeHexo/2022/06/26/aliyunDisasterRecovery/" title="阿里云-容灾恢复DR">阿里云-容灾恢复DR</a></li>
<li><a href="/www6vHomeHexo/2022/03/24/aliyunBestPractice/" title="阿里云最佳实践-BestPractice">阿里云最佳实践-BestPractice</a></li>
<li><a href="/www6vHomeHexo/2022/03/30/aliCloudBill/" title="阿里云-计费方式">阿里云-计费方式</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Producer生产者</title>
    <url>/www6vHomeHexo/2022/05/15/kafkaProducer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="producer-架构">Producer 架构</span><a href="#producer-架构" class="header-anchor">#</a></h2><h3><span id="overviw-1-1">Overviw 1 [1]</span><a href="#overviw-1-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/05/15/kafkaProducer/kafka-producer1.jpg" class title="Producer架构">

<p>整个生产者客户端是由主线程和Sender线程协调运行的, 主线程创建消息, 然后通过 拦截器、元信息更新、序列化、分区器、缓存消息等等流程。<br>Sender线程在初始化的时候就已经运行了,并且是一个while循环。</p>
<h3><span id="overviw-2">Overviw 2</span><a href="#overviw-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/05/15/kafkaProducer/kafka-producer.jpg" class title="Producer架构">

<h2><span id="producer-分区策略2">Producer 分区策略[2]</span><a href="#producer-分区策略2" class="header-anchor">#</a></h2><ul>
<li>DefaultPartitioner 默认分区策略<br>粘性分区Sticky Partitioner</li>
<li>UniformStickyPartitioner 纯粹的粘性分区策略</li>
<li>RoundRobinPartitioner 分区策略</li>
</ul>
<h2><span id="kafka-生产者-里的buffer3">kafka 生产者 里的buffer[3]</span><a href="#kafka-生产者-里的buffer3" class="header-anchor">#</a></h2><p>kafka producer中配置的 buffer.memory （参数在文末有详细说明）参数是缓冲区的大小，这个缓存区大家也就是RecordAccmulator所用的内存大小。默认是32MB。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247493639&idx=1&sn=4bd43c8137a701e73dddd9ccdf58ad45&c">图解kafka生产者流程,超详细！</a>  石臻臻   kafka contributor  </li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247493592&idx=1&sn=4a4f536b21f1b6b1d506dd1bdfa07e80">Kafka生产者的3种分区策略 </a> 石臻臻   kafka contributor  </li>
<li><a href="https://blog.csdn.net/xixiqiuqiu/article/details/122806693">Kafka Producer全流程分析和思考</a></li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis NVM</title>
    <url>/www6vHomeHexo/2022/05/14/redisNVM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="特点">特点</span><a href="#特点" class="header-anchor">#</a></h2><ul>
<li>能持久化保存数据</li>
<li>读写速度和 DRAM 接近</li>
<li>容量大</li>
</ul>
<h2><span id="intel-optane-aep-内存条简称-aep-内存">Intel Optane AEP 内存条(简称 AEP 内存)</span><a href="#intel-optane-aep-内存条简称-aep-内存" class="header-anchor">#</a></h2><ul>
<li><p>Memory 模式<br>软件系统能使用到 的内存空间，就是 AEP 内存条的空间容量。<br>在 Memory 模式时，Redis 可以利用 NVM 容量大的特点，实现大容量实例，保存更 多数据。</p>
</li>
<li><p>App Direct 模式<br>使用了 App Direct 模式的 AEP 内存，也叫 做持久化内存(Persistent Memory，PM)。</p>
<p>Redis 可以直接在持久化内存上进行数据读写，在这 种情况下，Redis 不用再使用 RDB 或 AOF 文件了，数据在机器掉电后也不会丢失。<br>而且，实例可以直接使用持久化内存上的数据进行恢复，恢复速度特别快。</p>
</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>40 | Redis的下一步:基于NVM内存的实践</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>多主-CRDT</title>
    <url>/www6vHomeHexo/2022/05/14/multiMasterCRDT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>多主</category>
      </categories>
      <tags>
        <tag>db</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Serverless</title>
    <url>/www6vHomeHexo/2022/05/12/awsServerless/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="serverless">Serverless</span><a href="#serverless" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">领域</th>
<th align="center">Serverless 服务</th>
</tr>
</thead>
<tbody><tr>
<td align="center">计算</td>
<td align="center">Lambda， Fargate</td>
</tr>
<tr>
<td align="center">程序集成</td>
<td align="center">EventBridge, SNS, Step Functions,  SQS,  API Gateway,  AppSync</td>
</tr>
<tr>
<td align="center">存储</td>
<td align="center">S3， DynamoDB， RDS，Aurora Serverless</td>
</tr>
<tr>
<td align="center">分析</td>
<td align="center">Redshift Serverless, EMR Serverless，MSK Serverless</td>
</tr>
</tbody></table>
<h5><span id="lambda1">Lambda[1]</span><a href="#lambda1" class="header-anchor">#</a></h5><ul>
<li>场景</li>
<li>并发 流量</li>
<li>版本</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/sammyliu/p/15739301.html">AWS 15 年（1）：从 Serverful 到 Serverless</a><br><a href="https://www.infoq.cn/article/bTU5aTNWu2jyHlS5e6Qq">数据库技术新版图 -Serverless 数据库</a>  5种<br><a href="https://aws.amazon.com/cn/serverless/">AWS 上的无服务器</a>  计算 集成 存储<br><a href="https://serverlessland.com/">Welcome to Serverless Land</a> ***  AWS Serverless官方</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>MQ总结(Kafka, Rocketmq, Rabbitmq)</title>
    <url>/www6vHomeHexo/2022/05/12/mqCompare/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<table>
<thead>
<tr>
<th align="center">功能</th>
<th align="center">RocketMQ</th>
<th align="center">Kafka</th>
<th align="center">RabbitMQ</th>
</tr>
</thead>
<tbody><tr>
<td align="center">可靠性*</td>
<td align="center">- 同步刷盘<br> - 异步刷盘</td>
<td align="center">异步刷盘，丢数据概率高</td>
<td align="center">同步刷盘</td>
</tr>
<tr>
<td align="center">横向扩展能力</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">- 集群扩容依赖前端 <br> - LVS 负载均衡调度</td>
</tr>
<tr>
<td align="center">消费模型*</td>
<td align="center">Push&#x2F;Pull</td>
<td align="center">Pull</td>
<td align="center">Push&#x2F;Pull</td>
</tr>
<tr>
<td align="center">定时消息*</td>
<td align="center">支持（只支持18个固定 Level）</td>
<td align="center">不支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">顺序消息*</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">不支持</td>
</tr>
<tr>
<td align="center">消息堆积能力</td>
<td align="center">百亿级别 影响性能</td>
<td align="center">影响性能</td>
<td align="center">影响性能</td>
</tr>
<tr>
<td align="center">消息堆积查询</td>
<td align="center">支持</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
</tr>
<tr>
<td align="center">消息回溯</td>
<td align="center">支持</td>
<td align="center">支持（位置，时间）</td>
<td align="center">不支持</td>
</tr>
<tr>
<td align="center">消息重试</td>
<td align="center">支持</td>
<td align="center">生产者有重试机制</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">死信队列</td>
<td align="center">支持</td>
<td align="center">不支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">性能（常规）*</td>
<td align="center">非常好 十万级 QPS</td>
<td align="center">非常好 百万级 QPS</td>
<td align="center">一般 万级 QPS</td>
</tr>
<tr>
<td align="center">性能（万级 Topic 场景）</td>
<td align="center">非常好 十万级 QPS</td>
<td align="center">低</td>
<td align="center">低</td>
</tr>
<tr>
<td align="center">性能（海量消息堆积场景）</td>
<td align="center">非常好 十万级 QPS</td>
<td align="center">低</td>
<td align="center">低</td>
</tr>
<tr>
<td align="center">全链路消息轨迹</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
</tr>
</tbody></table>
<h2><span id="mq比较3">MQ比较[3]</span><a href="#mq比较3" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/12/mqCompare/mqCompare.jpg" class title="MQ比较"> 

<h2><span id="重点3">重点[3]</span><a href="#重点3" class="header-anchor">#</a></h2><ul>
<li>功能级别不具备一票否决权</li>
<li>选型时要特别注意中间件的性能与扩展性</li>
<li>需要注重团队技术栈与中间件编程语言的匹配度</li>
</ul>
<h2><span id="参数">参数</span><a href="#参数" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/belvine/article/details/80842240">Kafka、RabbitMQ、RocketMQ等消息中间件的对比</a></li>
<li><a href="https://honeypps.com/mq/kafka-vs-rabbitmq/">https://honeypps.com/mq/kafka-vs-rabbitmq/</a>   未</li>
<li>13 | 技术选型：如何根据应用场景选择合适的消息中间件？   丁威</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>MVCC</title>
    <url>/www6vHomeHexo/2022/05/12/mvcc/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#mvcc">MVCC</a><ul>
<li><a href="#%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE-4">并发控制协议 [4]</a></li>
<li><a href="#%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84mvcc-2">本地数据库中的MVCC [2]</a></li>
<li><a href="#mysql%E4%B8%AD%E7%9A%84mvcc-3">MySQL中的MVCC [3]</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%ADmvcc5">分布式数据库中MVCC[5]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="mvcc">MVCC</span><a href="#mvcc" class="header-anchor">#</a></h1><ul>
<li>四个关键模块设计 [1]<ul>
<li>并发控制协议 ***<ul>
<li>Two-phase Locking (MV2PL)</li>
<li>MVOCC</li>
<li>Timestamp Ordering (MVTO)</li>
</ul>
</li>
<li>版本数据存储  ***<ul>
<li>Append-only</li>
<li>Time-Travel</li>
<li>Delta</li>
</ul>
</li>
<li>垃圾清理机制</li>
<li>索引管理</li>
</ul>
</li>
</ul>
<h3><span id="并发控制协议-4">并发控制协议 [4]</span><a href="#并发控制协议-4" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/05/12/mvcc/mvccProtocol.jpg" class>

<h3><span id="本地数据库中的mvcc-2">本地数据库中的MVCC [2]</span><a href="#本地数据库中的mvcc-2" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>DB</th>
<th>Protocol</th>
<th>Version Storage</th>
<th>Garbage Collection</th>
<th></th>
<th>Index Management</th>
</tr>
</thead>
<tbody><tr>
<td>Oracle</td>
<td>MV2PL</td>
<td>Delta</td>
<td>Tuple-level (VAC)</td>
<td></td>
<td>Logical Pointers (TupleId)</td>
</tr>
<tr>
<td>Postgres</td>
<td>MV2PL&#x2F;SSI</td>
<td>Append-only (O2N)</td>
<td>Tuple-level (VAC)</td>
<td></td>
<td>Physical Pointers</td>
</tr>
<tr>
<td>MySQL-InnoDB</td>
<td>MV2PL</td>
<td>Delta</td>
<td>Tuple-level (VAC)</td>
<td></td>
<td>Logical Pointers (PKey)</td>
</tr>
</tbody></table>
<h3><span id="mysql中的mvcc-3">MySQL中的MVCC [3]</span><a href="#mysql中的mvcc-3" class="header-anchor">#</a></h3><h3><span id="分布式数据库中mvcc5">分布式数据库中MVCC[5]</span><a href="#分布式数据库中mvcc5" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://jiekun.dev/posts/mvcc/">Paper Reading：聊一聊MVCC</a> ***</li>
<li>[1] Y. Wu, An Empirical Evaluation of In-Memory Multi-Version Concurrency Control. In VLDB 2017.</li>
<li><a href="/www6vHomeHexo/2020/08/14/mysqlTransactionAndLock/" title="MySQL 事务-隔离性">MySQL 事务-隔离性</a> self 
MySQL MVCC 快照读 vs 当前读 </li>
<li><a href="https://www.zhihu.com/question/27876575">乐观锁和 MVCC 的区别？</a> </li>
<li><a href="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/" title="TiKV Transaction-MVCC+TSO">TiKV Transaction-MVCC+TSO</a> self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MVCC</category>
      </categories>
      <tags>
        <tag>db</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka消费者-Rebalance机制</title>
    <url>/www6vHomeHexo/2022/05/11/kafkaRebalance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rebalance-what">Rebalance (What)</a></li>
<li><a href="#rebalance-%E5%8F%91%E7%94%9F%E7%9A%84%E6%97%B6%E6%9C%BA%E6%9C%89%E4%B8%89%E4%B8%AA-when-1">Rebalance 发生的时机有三个 (when) [1]</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">问题和解决方案</a><ul>
<li><a href="#rebalance-%E7%9A%84-%E5%BC%8A%E7%AB%AF-1">Rebalance 的 弊端 [1]</a></li>
<li><a href="#consumer-rebalance-%E7%9A%84%E9%97%AE%E9%A2%98">consumer rebalance 的问题</a></li>
<li><a href="#%E4%B8%8D%E5%BF%85%E8%A6%81%E7%9A%84%E7%9A%84rebalance-solution-1">“不必要的”的Rebalance (Solution) [1]</a></li>
</ul>
</li>
<li><a href="#qa">Q&amp;A</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="rebalance-what">Rebalance (What)</span><a href="#rebalance-what" class="header-anchor">#</a></h1><ul>
<li><p>定义<br>再均衡：在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者</p>
</li>
<li><p>原理</p>
<ul>
<li><strong>重平衡的通知机制正是通过心跳线程来完成的</strong> [7]</li>
</ul>
</li>
<li><p>角色<br>     - consumer leader<br>     - cordinator[8]</p>
</li>
</ul>
<h1><span id="rebalance-发生的时机有三个-when-1">Rebalance 发生的时机有三个 (when) [1]</span><a href="#rebalance-发生的时机有三个-when-1" class="header-anchor">#</a></h1><ul>
<li>重平衡的 3 个触发条件：<br><strong>组成员数量发生变化。(最常遇到)</strong><br>订阅主题数量发生变化。<br>订阅主题的分区数发生变化。</li>
</ul>
<h1><span id="问题和解决方案">问题和解决方案</span><a href="#问题和解决方案" class="header-anchor">#</a></h1><h3><span id="rebalance-的-弊端-1">Rebalance 的 弊端 [1]</span><a href="#rebalance-的-弊端-1" class="header-anchor">#</a></h3><ul>
<li>Rebalance 影响 Consumer 端 TPS<br>在 Rebalance 期间，Consumer 会停下手头的事情，什么也干不了</li>
<li>如果你的 Group 下成员很多， Rebalance 会很慢。</li>
<li>Rebalance 效率不高<br>Group 下的 所有成员都要参与进来，而且通常不会考虑局部性原理</li>
</ul>
<h3><span id="consumer-rebalance-的问题">consumer rebalance 的问题</span><a href="#consumer-rebalance-的问题" class="header-anchor">#</a></h3><p>           Rebalance 过程也和这个类似，在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。<br>           这是 Rebalance 为人诟病的一个方面。<br>         【消费者重平衡的时候， 所有的消费者是不能消费数据的。】</p>
<h3><span id="不必要的的rebalance-solution-1">“不必要的”的Rebalance (Solution) [1]</span><a href="#不必要的的rebalance-solution-1" class="header-anchor">#</a></h3><ul>
<li><p>第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被“踢出”Group 而引发的。<br>因此，你需要仔细地设置session.timeout.ms 和 heartbeat.interval.ms的 值。</p>
</li>
<li><p>第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。<br>max.poll.interval.ms参数值的设置显得尤为关键。</p>
</li>
</ul>
<img src="/www6vHomeHexo/2022/05/11/kafkaRebalance/consumer1.jpg" class>


<h1><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h1><ul>
<li>消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《17 | 消费者组重平衡能避免吗? 》  胡夕</li>
<li>《15丨消费者组到底是什么？》  胡夕</li>
<li>《25 | 消费者组重平衡全流程解析》  胡夕</li>
<li><a href="https://blog.csdn.net/lzxlfly/article/details/106246879">Kafka的Rebalance机制可能造成的影响及解决方案</a></li>
<li><a href="https://www.cnblogs.com/chanshuyi/p/kafka_rebalance_quick_guide.html">线上Kafka突发rebalance异常，如何快速解决？</a></li>
<li><a href="https://help.aliyun.com/knowledge_detail/154454.html">为什么消费客户端频繁出现Rebalance？</a>  石臻臻</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247494990&idx=1&sn=54f626f66b8d0c1330a586ca800b1609">Kafka消费者客户端心跳请求</a>  石臻臻</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247494917&idx=1&sn=e1b18b70b58a3e9797a0110972a9d43e">什么是Kafka消费组协调器</a>  石臻臻</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps-Tekton</title>
    <url>/www6vHomeHexo/2022/05/10/devopsTekton/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="事件触发的自动化流水线1">事件触发的自动化流水线[1]</span><a href="#事件触发的自动化流水线1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/10/devopsTekton/tekton-pipeline.JPG" class>

<h2><span id="example1-chat">Example1 [chat]</span><a href="#example1-chat" class="header-anchor">#</a></h2><p>下面是一个包含EventListener、TriggerTemplate和PipelineRun的Tekton YAML模板示例，其中包含interceptors、bindings和template，旨在从Git中拉取代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: tekton.dev/v1beta1</span><br><span class="line">kind: EventListener  ##</span><br><span class="line">metadata:</span><br><span class="line">  name: my-event-listener</span><br><span class="line">spec:</span><br><span class="line">  interceptors:  ##</span><br><span class="line">  - cel:</span><br><span class="line">      filter: &quot;body.repository.full_name == &#x27;my-org/my-repo&#x27;&quot;</span><br><span class="line">  triggers:</span><br><span class="line">  - name: my-trigger</span><br><span class="line">    bindings:   ##</span><br><span class="line">    - name: my-binding</span><br><span class="line">      ref: my-binding</span><br><span class="line">    template:   ##</span><br><span class="line">      name: my-trigger-template</span><br><span class="line">---</span><br><span class="line">apiVersion: tekton.dev/v1beta1</span><br><span class="line">kind: TriggerTemplate</span><br><span class="line">metadata:</span><br><span class="line">  name: my-trigger-template</span><br><span class="line">spec:</span><br><span class="line">  params:</span><br><span class="line">  - name: repoUrl</span><br><span class="line">    description: The URL of the git repository to clone</span><br><span class="line">  resourcetemplates:</span><br><span class="line">  - apiVersion: tekton.dev/v1alpha1</span><br><span class="line">    kind: PipelineRun    </span><br><span class="line">    metadata:</span><br><span class="line">      name: my-pipeline-run ##</span><br><span class="line">    spec:</span><br><span class="line">      pipelineRef:</span><br><span class="line">        name: my-pipeline   ##</span><br><span class="line">      params:</span><br><span class="line">      - name: repoUrl</span><br><span class="line">        value: $(tt.params.repoUrl)</span><br><span class="line">---</span><br><span class="line">apiVersion: tekton.dev/v1beta1</span><br><span class="line">kind: Pipeline</span><br><span class="line">metadata:</span><br><span class="line">  name: my-pipeline  ##</span><br><span class="line">spec:</span><br><span class="line">  tasks:</span><br><span class="line">  - name: git-clone   ##</span><br><span class="line">    taskRef:</span><br><span class="line">      name: git-clone</span><br><span class="line">    params:</span><br><span class="line">    - name: REPO_URL</span><br><span class="line">      value: $(inputs.params.repoUrl)</span><br><span class="line">---</span><br><span class="line">apiVersion: tekton.dev/v1beta1</span><br><span class="line">kind: Task</span><br><span class="line">metadata:</span><br><span class="line">  name: git-clone   ##</span><br><span class="line">spec:</span><br><span class="line">  inputs:</span><br><span class="line">    params:</span><br><span class="line">    - name: REPO_URL</span><br><span class="line">      type: string</span><br><span class="line">  steps:</span><br><span class="line">  - name: git-clone</span><br><span class="line">    image: alpine/git</span><br><span class="line">    script:</span><br><span class="line">    - apk add --no-cache git</span><br><span class="line">    - git clone $(inputs.params.REPO_URL)</span><br><span class="line">---</span><br><span class="line">apiVersion: tekton.dev/v1beta1</span><br><span class="line">kind: PipelineRun</span><br><span class="line">metadata:</span><br><span class="line">  name: my-pipeline-run   ##</span><br><span class="line">spec:</span><br><span class="line">  pipelineRef:</span><br><span class="line">    name: my-pipeline</span><br><span class="line">  params:</span><br><span class="line">  - name: repoUrl</span><br><span class="line">    value: https://github.com/my-org/my-repo.git</span><br></pre></td></tr></table></figure>



<h2><span id="example2-2">Example2  [2]</span><a href="#example2-2" class="header-anchor">#</a></h2><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《10丨模块十：Kubernetes的生产化运维》  云原生训练营 </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1e94y117tY/">第一季：第 5 集 云原生 CI 中的强者 Tekton</a> 未</p>
<p><a href="https://github.com/DevopsChina/lab/tree/main/ci/lab05-tekton">相关的git-代码</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
        <category>Tekton</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-工具和应用</title>
    <url>/www6vHomeHexo/2022/05/09/gpt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#platform">Platform</a></li>
<li><a href="#tools-mix">Tools &amp; Mix</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE">思维导图</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E5%88%86%E6%9E%90%E7%A7%91%E7%A0%94">文档分析&amp;科研</a></li>
<li><a href="#%E8%A7%86%E9%A2%91">视频</a></li>
<li><a href="#%E8%8B%B1%E8%AF%AD">英语</a></li>
</ul>
</li>
<li><a href="#%E5%AE%A2%E6%88%B7%E7%AB%AF">客户端</a></li>
<li><a href="#chrome-plugin">Chrome plugin</a></li>
<li><a href="#%E5%88%9B%E4%B8%9A">创业</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="platform">Platform</span><a href="#platform" class="header-anchor">#</a></h1><ul>
<li>国外<br><a href="https://poe.com/ChatGPT">Poe</a> ***</li>
<li>国内<br><a href="https://saas.edu360.cn/system/chatgpt">实战云</a> gpt3.5  gpt4<br><a href="https://www.feijix.com/n/y0BnXI">ChatGPT使用指南！</a>   ***<br><a href="https://www.1888ai.com/base/chat">灵犀百通</a>  gpt3.5<br><a href="https://gpt.91chat-ai.cn/chat">ChatGpt PLUS</a><br><a href="https://yiyan.baidu.com/">文心一言</a></li>
</ul>
<h1><span id="tools-amp-mix">Tools &amp; Mix</span><a href="#tools-amp-mix" class="header-anchor">#</a></h1><ul>
<li><p>GPT学习宝典</p>
<ul>
<li>聚合<ul>
<li><a href="https://gpt.candobear.com/toolbox">GPT  工具箱</a></li>
</ul>
</li>
<li>教程<ul>
<li><a href="https://gpt.candobear.com/courses">学习资料</a></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe">极客时间 AIGC 知识库</a> *** </p>
<ul>
<li>聚合<ul>
<li><a href="https://gp477l8icq.feishu.cn/wiki/M1uCwFNjkiAGC7k30TaclZqknPh">AI工具大全</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/RpabwPG9niFEu9kwJAQcAGxenDg">AI主流工具精选</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/VJ9ewqfOgiyrbQksbyLcrODtnkb">AI经典项目</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/QVV6w3XstiR7hlkK53Bc8f9DnMf">AI导航站</a></li>
</ul>
</li>
<li><a href="https://longalong.feishu.cn/wiki/wikcneAKpN3u473N7J9EAC4Ga0b">应用与变现案例</a></li>
</ul>
</li>
<li><p><a href="https://www.ailookme.com/">AI 工具箱</a>  *** </p>
</li>
<li><p><a href="https://gptdoc.sparkai.chat/">ChatGPT Tutorial 101</a></p>
</li>
</ul>
<h1><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h1><h3><span id="思维导图">思维导图</span><a href="#思维导图" class="header-anchor">#</a></h3><p><a href="https://albus.org/">albus</a></p>
<h3><span id="文档分析amp科研">文档分析&amp;科研</span><a href="#文档分析amp科研" class="header-anchor">#</a></h3><ul>
<li><p>在线文档分析<br>Microsoft Edge Dev + new Bing  ***</p>
</li>
<li><p>文献查找 + 润色<br>Skype + new Bing  ***</p>
</li>
<li><p>本地文档分析</p>
<ul>
<li><p>VPN</p>
<ul>
<li>chatpdf  收费  ***<br><a href="https://www.chatpdf.com/">Chat with any PDF</a> 总结文献</li>
<li><a href="https://chatdoc.com/">Chat with documents</a></li>
</ul>
</li>
<li><p>非VPN</p>
<ul>
<li><a href="https://chatpaper.org/">ChatPaper</a>  Paper 中文<br><a href="https://github.com/kaixindelele/ChatPaper">ChatPaper</a> git</li>
<li><a href="https://chat2doc.cn/">阅读文档的好帮手</a>  收费</li>
<li><a href="https://lightpdf.com/chatdoc">LightPDF AI Tools</a> *** </li>
<li><a href="https://docalysis.com/files/hwylw4">docalysis</a> ***</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="视频">视频</span><a href="#视频" class="header-anchor">#</a></h3><p><a href="https://b.jimmylv.cn/">BibiGPT</a><br><a href="https://crucible.docnavigator.in/">Youtube tools</a></p>
<h3><span id="英语">英语</span><a href="#英语" class="header-anchor">#</a></h3><p><a href="https://callannie.ai/signin">callannie</a></p>
<h1><span id="客户端">客户端</span><a href="#客户端" class="header-anchor">#</a></h1><ul>
<li>ChatGPT 客户端<br> windows， mac</li>
</ul>
<h1><span id="chrome-plugin">Chrome plugin</span><a href="#chrome-plugin" class="header-anchor">#</a></h1><ul>
<li><p>WebChatGPT[instatlled]</p>
</li>
<li><p>AIPRM for ChatGPT[instatlled]</p>
</li>
<li><p>ChatGPT Sidebar<br>要注册账号, 需要api token</p>
</li>
<li><p>ChatHub  [instatlled]<br> chatgpt + new bing<br><a href="https://github.com/chathub-dev/chathub">ChatHub </a></p>
</li>
<li><p>OpenAI Translator<br><a href="https://github.com/yetone/openai-translator">openai-translator</a><br>要注册账号, 需要api token</p>
</li>
</ul>
<h1><span id="创业">创业</span><a href="#创业" class="header-anchor">#</a></h1><ul>
<li><a href="https://gpt3demo.com/map">GPT-3 Demo</a><ul>
<li>聊天机器人</li>
<li>代码辅助</li>
<li>写作应用</li>
<li>游戏</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 多集群管理</title>
    <url>/www6vHomeHexo/2022/05/08/k8sMultiCluster/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目标">目标：</span><a href="#目标" class="header-anchor">#</a></h2><p>让用户像使用单集群一样来使用多集群。</p>
<h2><span id="多集群部署需要解决哪些问题">多集群部署需要解决哪些问题</span><a href="#多集群部署需要解决哪些问题" class="header-anchor">#</a></h2><ul>
<li>而多集群管理需要解决以下问题：<br>多集群服务的分发部署（deployment、daemonset等）<br>跨集群自动迁移与调度（当某个集群异常，服务可以在其他集群自动部署）<br>多集群服务发现，网络通信及负载均衡（service，ingress等）</li>
</ul>
<h2><span id="开源和解决方案">开源和解决方案</span><a href="#开源和解决方案" class="header-anchor">#</a></h2><ul>
<li><p>KubeFed 或 Federation v2<br><a href="https://github.com/kubernetes-sigs/kubefed">https://github.com/kubernetes-sigs/kubefed</a><br><a href="https://jimmysong.io/kubernetes-handbook/practice/federation.html">集群联邦（Cluster Federation）</a>   jimmysong</p>
</li>
<li><p>virtual-kubelet Virtual Kubelet<br><a href="https://www.modb.pro/db/166209">阿里云virtual-kubelet-autoscaler实现ECI作为弹性补充</a><br><a href="https://www.alibabacloud.com/help/zh/elastic-container-instance/latest/schedule-pods-to-a-virtual-node-through-the-virtual-kubelet-autoscaler-add-on">通过 virtual-kubelet-autoscaler 将Pod自动调度到虚拟节点</a><br><a href="https://help.aliyun.com/document_detail/97527.html">自建Kubernetes集群部署Virtual Kubelet（ECI）</a> 未<br><a href="https://v.qq.com/x/page/d0816t4u183.html">简介：Virtual Kubelet</a> video 2018 kubecon   未<br><a href="https://v.qq.com/x/page/q0827olfrlx.html">深入了解：Virtual Kubelet</a> video   2018 kubecon  未  </p>
</li>
<li><p>Karmada（Kubernetes Armada）[5]</p>
</li>
<li><p>clusternet - 腾讯云</p>
</li>
<li><p>OCM(Open Cluster Management) - RedHat </p>
</li>
<li><p>Gardener [3]</p>
</li>
<li><p>Crossplane [3]</p>
</li>
<li><p>cluster-api [4]</p>
</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.huweihuang.com/kubernetes-notes/multi-cluster/k8s-multi-cluster-thinking.html">k8s多集群的思考</a></li>
<li><a href="https://kubesphere.com.cn/blogs/kubernetes-multicluster-kubesphere/">混合云下的 Kubernetes 多集群管理与应用部署</a> 未</li>
<li><a href="https://www.bilibili.com/video/BV14f4y1T7LY/">【PingCAP Infra Meetup】No.141 Kubernetes 开发设计模式在 TiDB Cloud 中的应用</a></li>
<li><a href="https://www.bilibili.com/video/BV12e4y1M7KM?spm_id_from=333.880.my_history.page.click">基于 ClusterAPI 的集群管理</a></li>
<li><a href="https://www.bilibili.com/video/BV1rX4y1c72s?spm_id_from=333.999.0.0">Karmada: 开源的云原生多云容器编排引擎</a> 华为</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>负载均衡-算法</title>
    <url>/www6vHomeHexo/2022/05/06/loadBalance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95">负载均衡算法</a><ul>
<li><a href="#lvs12">LVS[1][2]</a></li>
<li><a href="#nginx-3">Nginx [3]</a></li>
<li><a href="#haproxy">HAproxy</a></li>
</ul>
</li>
<li><a href="#lvs%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F11-16">LVS工作模式[11-16]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#lvs%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F">LVS工作模式</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="负载均衡算法">负载均衡算法</span><a href="#负载均衡算法" class="header-anchor">#</a></h1><h3><span id="lvs12">LVS[1][2]</span><a href="#lvs12" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>组件</th>
<th>类型</th>
<th>负载均衡算法</th>
<th>场景</th>
</tr>
</thead>
<tbody><tr>
<td>LVS</td>
<td><strong>静态</strong></td>
<td>RR：roundrobin [常用]</td>
<td>轮询机制</td>
</tr>
<tr>
<td></td>
<td></td>
<td>WRR：Weighted RR [常用]</td>
<td>加权轮询，权重越大承担负载越大</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SH：Source Hashing 源地址哈希</td>
<td>实现session sticky<br>缺点：调度粒度大，对负载均衡效果差；</td>
</tr>
<tr>
<td></td>
<td></td>
<td>DH：Destination Hashing 目标地址哈希</td>
<td></td>
</tr>
<tr>
<td></td>
<td><strong>动态</strong></td>
<td>LC：least connections</td>
<td>适用于长连接应用<br><code>Overhead=activeconns*256+inactiveconns</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td>WLC：Weighted LC  [常用]</td>
<td>默认调度方法,较常用（加上了权重）<br><code>Overhead=(activeconns*256+inactiveconns)/weight</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td><a href="https://so.csdn.net/so/search?q=SED&spm=1001.2101.3001.7020">SED</a>：Shortest Expection Delay</td>
<td><code>Overhead=(activeconns+1)*256/weight</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td>NQ：Never Queue，</td>
<td>第一轮均匀分配，后续SED<br>****SED****算法改进</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LBLC：Locality-Based LC</td>
<td>动态的 ****DH****连接算法</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LBLCR：LBLC with Replication</td>
<td>带复制功能的LBLC</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="nginx-3">Nginx [3]</span><a href="#nginx-3" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>组件</th>
<th>类型</th>
<th>负载均衡算法</th>
<th>场景</th>
</tr>
</thead>
<tbody><tr>
<td>Nginx</td>
<td><strong>静态</strong><br></td>
<td>轮询<br></td>
<td>每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除</td>
</tr>
<tr>
<td></td>
<td></td>
<td>加权轮询<br></td>
<td>指定轮询几率，weight 和访问比率成正比，用于后端服务器性能不均的情况</td>
</tr>
<tr>
<td></td>
<td></td>
<td>IP Hash<br></td>
<td>每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题</td>
</tr>
<tr>
<td></td>
<td><strong>动态</strong><br></td>
<td>Fair （第三方）<br></td>
<td>按后端服务器的响应时间来分配请求，响应时间短的优先分配</td>
</tr>
<tr>
<td></td>
<td></td>
<td>url_hash（第三方）</td>
<td>按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Least Connections</td>
<td></td>
</tr>
</tbody></table>
<h3><span id="haproxy">HAproxy</span><a href="#haproxy" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>组件</th>
<th>类型</th>
<th>负载均衡算法</th>
<th>场景</th>
</tr>
</thead>
<tbody><tr>
<td>HAproxy</td>
<td><strong>静态</strong><br></td>
<td>轮询<br></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>加权轮询<br></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>IP Hash<br></td>
<td></td>
</tr>
<tr>
<td></td>
<td><strong>动态</strong><br></td>
<td>Least Connections<br></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Source</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="lvs工作模式11-16">LVS工作模式[11-16]</span><a href="#lvs工作模式11-16" class="header-anchor">#</a></h1><ul>
<li>DR</li>
<li>TUN模式</li>
<li>NAT模式</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://demo.dandelioncloud.cn/article/details/1547407561087266818">LVS负载均衡集群服务搭建详解</a>      </li>
<li><a href="https://blog.csdn.net/aa896517050/article/details/125399055">LVS调度算法总结</a></li>
<li><a href="https://blog.csdn.net/weixin_42073629/article/details/109440892">LVS、Nginx 及 HAProxy 的区别</a><br> <a href="http://www.linuxvirtualserver.org/zh/lvs4.html">LVS集群的负载调度</a>  ***  未</li>
</ol>
<h3><span id="lvs工作模式">LVS工作模式</span><a href="#lvs工作模式" class="header-anchor">#</a></h3><ol start="11">
<li><a href="https://www.likecs.com/show-739972.html">LVS三种模式的区别及负载均衡算法</a></li>
<li><a href="https://www.likecs.com/show-307061337.html">LVS 介绍以及配置应用</a> ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/363346400">深入浅出 LVS 负载均衡（一）NAT、FULLNAT 模型原理</a>  未</li>
<li><a href="https://zhuanlan.zhihu.com/p/377090230">深入浅出 LVS 负载均衡（二）DR、TUN 模型原理</a>  未</li>
<li><a href="https://zhuanlan.zhihu.com/p/381297341">深入浅出 LVS 负载均衡（三）实操 NAT、TUNNEL 模型</a>  未</li>
<li><a href="https://zhuanlan.zhihu.com/p/356354676">深入浅出 LVS 负载均衡（四）实操 DR 模型、Keepalived DR 模型的高可用</a>  未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-Transit Gateway</title>
    <url>/www6vHomeHexo/2022/05/05/awsNetworkTGW/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="transit-gateway">Transit Gateway</span><a href="#transit-gateway" class="header-anchor">#</a></h2><h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.iloveaws.cn/3810.html">52-Transit VPC&amp;Transit Gateway</a> ***<br><a href="https://www.bilibili.com/video/BV1na4y147Gc/">AWS 云上网络构建和新功能揭秘 (Level 200)</a> ***</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 分布式锁</title>
    <url>/www6vHomeHexo/2022/05/05/redisDistKey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="一-基于redis的分布式锁">一. 基于redis的分布式锁</span><a href="#一-基于redis的分布式锁" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/05/redisDistKey/distributedLock-redis.jpg" class title="图1.redis分布式锁">
<img src="/www6vHomeHexo/2022/05/05/redisDistKey/redis-lock.jpg" class title="图2.redis分布式锁">


<h2><span id="锁的特性">锁的特性:</span><a href="#锁的特性" class="header-anchor">#</a></h2><ol>
<li>排它性</li>
<li>超时释放锁<br>redis expire</li>
<li>高可用，锁集群容错[图2]，<br>安全性[7]，</li>
<li>可重入锁, 避免死锁[8]</li>
<li>乐观锁, 悲观锁[10][图2]</li>
</ol>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://tech.meituan.com/2016/09/29/distributed-system-mutually-exclusive-idempotence-cerberus-gtis.html">分布式系统互斥性与幂等性问题的分析与解决</a> 点评 蒋谞 </li>
<li>漫画：什么是分布式锁？ 程序员小灰</li>
<li>《从Paxos到Zookeeper分布式一致性原理与实践》 倪超 6.1.7节</li>
<li>Redlock：Redis分布式锁最牛逼的实现 阿飞的博客</li>
<li><a href="https://mp.weixin.qq.com/s/ahcbgxWVVmRwrH9Y4-gXBA">SOFAJRaft-RheaKV 分布式锁实现剖析 | SOFAJRaft 实现原理</a>   SOFALab 米麒麟 未</li>
<li><a href="/www6vHomeHexo/2016/11/12/redis/" title="Redis 总结">Redis 总结</a>  self</li>
</ol>
<hr>
<ol start="7">
<li><a href="https://www.jianshu.com/p/31e85a18a9e7">分布式服务总结 分布式锁</a><br>通过栅栏(fencing)使得锁更安全, fencing token<br><a href="http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">How to do distributed locking</a> Martin Kleppmann  </li>
<li><a href="https://www.jianshu.com/p/1c5c1a592088">Redis实现分布式锁，以及可重入锁思路</a><br>唯一id I. uuid  II. 分布式线程中标识唯一线程：MAC地址 + jvm进程ID + 线程ID  </li>
<li><a href="https://www.cnblogs.com/jasonZh/p/9522772.html">Redis分布式锁实现秒杀业务(乐观锁、悲观锁)</a>  最后<br>乐观锁: jedis的watch方法</li>
</ol>
<hr>
<p>from redis<br>《Redis 深度历险：核心原理与应用实践》 钱文品<br>3. 应用 1：千帆竞发 —— 分布式锁<br>4. 拓展 3：拾遗漏补 —— 再谈分布式锁</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 幂等性和事务</title>
    <url>/www6vHomeHexo/2022/05/04/kafkaTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="kafka-幂等性">Kafka 幂等性</span><a href="#kafka-幂等性" class="header-anchor">#</a></h2><ul>
<li><p>为了实现生产者的幂等性， 引入了producer id（PID）和 序列号（sequence number）<br><strong>PID</strong>: producer 初始化的时候分配<br><strong>序列号</strong>： producer每发送一条消息，就会将&lt;PID, 分区&gt;对应的序列号的值+1.</p>
</li>
<li><p>局限性： Kafka 幂等性只能保证单个producer 回话（session）中单分区的幂等</p>
</li>
</ul>
<h2><span id="kafka-事务">Kafka 事务</span><a href="#kafka-事务" class="header-anchor">#</a></h2><ul>
<li>Kafka 幂等性不能跨多个分区运作，而事务可以保证对多个分区写入操作的原子性。</li>
</ul>
<h5><span id="事务性实现的关键">事务性实现的关键</span><a href="#事务性实现的关键" class="header-anchor">#</a></h5><ul>
<li><p>事务要求producer 开启幂等特性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">enable.idempotence = true</span><br></pre></td></tr></table></figure>
</li>
<li><p>transactionalId：<br>一个Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？<br>之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<br>Kafka 在 Producer 端引入了一个 transactionalId 来解决这个问题，这个 txn.id 是由应用来配置的；</p>
</li>
</ul>
<h5><span id="架构和组件">架构和组件</span><a href="#架构和组件" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/05/04/kafkaTransaction/kafka-tranaction1.png" class title="kafka事务-Data Flow">

<ul>
<li><p>transactionalId和PID一一对应，transactionalId用户显示设置，PID由Kafka内部分配；</p>
</li>
<li><p>跨producer会话的消息<strong>幂等发送</strong>: 新的producer启动后，具有相同transactionalId的旧producer会立即失效；</p>
</li>
<li><p>跨producer会话的<strong>事务恢复</strong>: producer宕机后，新的producer可以保证未完成的旧事务要么commit，要么Abort。</p>
</li>
<li><p>TransactionCoordinator(coordinate 协调者)</p>
</li>
<li><p>事务日志</p>
</li>
</ul>
<h5><span id="语义">语义</span><a href="#语义" class="header-anchor">#</a></h5><ul>
<li>Kafka 的事务机制，更多的情况下被用来配合Kafka的幂等机制来实现 Kafka 的 <strong>Exactly Once</strong> 语义。</li>
<li>Kafka 的 Exactly Once 机制，是为了解决在**”consume - transform - produce”（流计算）**这样的计算过程中数据不重不丢，而不是我们通常理解的使用消息队列进行消息生产消费过程中的 Exactly Once。</li>
<li>”consume - transform - produce“模式<img src="/www6vHomeHexo/2022/05/04/kafkaTransaction/consume-transform-produce.png" class title="consume - transform - produce模式"></li>
</ul>
<h2><span id="总结">总结：</span><a href="#总结" class="header-anchor">#</a></h2><p>幂等性、事务都是0.11.0.0之后引入的特性, 以此来实现EOS（Exactly-Once semantics 精确一次性语义）</p>
<h2><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h2><ul>
<li>Kafka中的幂等是怎么实现的 </li>
<li>Kafka中的事务是怎么实现的（这题我去面试6家被问4次）<br><a href="../../../../2022/05/04/kafkaTransaction/">Kafka 幂等性和事务</a></li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>消息队列高手课 - 25 | RocketMQ与Kafka中如何实现事务？</a> 李玥</li>
<li><a href="http://matt33.com/2018/11/04/kafka-transaction/">Kafka Exactly-Once 之事务性实现</a>  Matt’s Blog-柳年思水</li>
<li>&lt;&lt;深入理解Kafka：核心设计与实践原理&gt;&gt;  7.4节</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>事务</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-CDN</title>
    <url>/www6vHomeHexo/2022/05/03/awsNetworkCDN/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="cloudfront">CloudFront</span><a href="#cloudfront" class="header-anchor">#</a></h2><ul>
<li><p>function </p>
<ul>
<li>Improves read performance</li>
<li>DDoS protection, integration with Shield,<br>AWS Web Application  Firewall</li>
</ul>
</li>
<li><p>Origins</p>
<ul>
<li>S3 Bucket</li>
<li>Custom Origin (HTTP)<ul>
<li>EC2 instance</li>
<li>Elastic Load Balancer</li>
<li>API Gateway</li>
<li>Any HTTP backend</li>
</ul>
</li>
</ul>
</li>
<li><p>Geo Restriction</p>
<ul>
<li>Allow list</li>
<li>Block list</li>
</ul>
</li>
<li><p>CloudFront vs. S3 CRR  </p>
<ul>
<li>CloudFront<br>Great for static content</li>
<li>S3 CRR<br>Great for dynamic content</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>SAP-2  Caching Section</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Network-DNS</title>
    <url>/www6vHomeHexo/2022/05/03/awsNetworkDNS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="route-53-12">Route 53 [1][2]</span><a href="#route-53-12" class="header-anchor">#</a></h2><ul>
<li><p>Routing Policies</p>
<ul>
<li>Simple</li>
<li>Weighted </li>
<li>Latency-based</li>
<li>Failover (Active-Passive)</li>
<li>Geolocation</li>
<li>Geoproximity<ul>
<li>Traffic flow</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2022/05/03/awsNetworkDNS/routingPolicy.JPG" class title="Routing Policy">
</li>
<li><p>Hosted Zones<br>hosted zone: a set of records belonging to a domain</p>
<ul>
<li>Public Hosted Zones</li>
<li>Private Hosted Zones<br>Association with VPC</li>
</ul>
</li>
<li><p>Health Checks<br>Automated DNS Failover </p>
<ul>
<li>for public resources<ul>
<li>Private Hosted Zones<br>create a CloudWatch Metric and associate a CloudWatch Alarm</li>
</ul>
</li>
<li>Resolver Endpoints<ul>
<li>Inbound Endpoint</li>
<li>Outbound Endpoint<ul>
<li>Resolver Rules</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>CNAME vs 别名</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>SAP-1  </li>
<li>SAP-2  DNS Section</li>
<li><a href="http://www.cloudbin.cn/?p=2349">AWS学习笔记（十八） Route53</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Calico</title>
    <url>/www6vHomeHexo/2022/05/03/k8sCalico/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="一-介绍和原理">一. 介绍和原理</span><a href="#一-介绍和原理" class="header-anchor">#</a></h2><ul>
<li><p>calico 是容器网络的又一种解决方案，和其他虚拟网络最大的不同是，它没有采用 overlay 网络做报文的转发，提供了纯 3 层的网络模型。三层通信模型表示每个容器都通过 IP 直接通信，中间通过路由转发找到对方。在这个过程中，容器所在的节点类似于传统的路由器，提供了路由查找的功能。</p>
</li>
<li><p>要想路由工作能够正常，每个虚拟路由器（容器所在的主机节点）必须有某种方法知道整个集群的路由信息，calico 采用的是 BGP 路由协议，全称是 Border Gateway Protocol。</p>
</li>
<li><p>BGP(Border Gateway Protocol 边界网关协议): 就是在大规模网络中实现节点路由信息共享的一种协议</p>
</li>
</ul>
<code>
BGP 协议传输的消息

<p>1 [BGP 消息]<br>2 我是宿主机 192.168.1.3<br>3 10.233.2.0&#x2F;24 网段的容器都在我这里<br>4 这些容器的下一跳地址是我<br></p></code><p></p>
<h2><span id="二-组件">二. 组件</span><a href="#二-组件" class="header-anchor">#</a></h2><ul>
<li><p>Calico 的 CNI 插件</p>
</li>
<li><p>Felix<br>它是一个 DaemonSet，负责在宿主机上插入路由规则(即:写入 Linux 内核的 FIB 转发信息库)，以及维护 Calico 所需的网络设备等工作。</p>
</li>
</ul>
<ol>
<li><p>路由规则(核心)  </p>
<code>  
<目的容器 ip 地址段> via <网关的 ip 地址> dev eth0
</网关的></目的容器></code>

</li>
<li><p>iptables的配置组件Felix;<br>基于iptable&#x2F;linux kernel包转发;<br>根据iptables规则进行路由转发;</p>
</li>
</ol>
<ul>
<li>BIRD， 路由广播组件BGP Speaker<br>BIRD是 BGP 的客户端，专门负责在集群里分发路由规则信息。</li>
</ul>
<h2><span id="三-架构">三. 架构</span><a href="#三-架构" class="header-anchor">#</a></h2><h5><span id="1-node-to-node-mesh模式小规模">1. Node-to-Node Mesh模式（小规模）</span><a href="#1-node-to-node-mesh模式小规模" class="header-anchor">#</a></h5><ul>
<li>默认配置下，是一个被称为“Node-to-Node Mesh”的模式，一般推荐用在少于 100 个节点的集群里<br>Node 称为 BGP Peer</li>
</ul>
  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/bgp-peer.png" class title="BGP Peer"> 
  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/ippool.png" class title="ippool-CIDR 网段分配">       

  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/mesh-mode1.png" class title="Node-to-Node Mesh模式（图中Route为路由规则）">


<ul>
<li>非overlay, Calico 没有使用 CNI 的网桥模式;</li>
</ul>
<code> 
宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示: 

<p>10.233.2.3 dev cali5863f3 scope link<br>即:发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。<br></p></code> <p></p>
  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/mesh-mode.png" class title="Node-to-Node Mesh模式(有iptable规则)">


  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/link.png" class title="网路互通">


<h5><span id="2-route-reflector模式-ipip模式大规模">2. Route Reflector模式 + IPIP模式（大规模）</span><a href="#2-route-reflector模式-ipip模式大规模" class="header-anchor">#</a></h5>  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/route-reflector+ipip.png" class title="Route Reflector模式+IPIP模式">

<p>  默认情况下，每个 calico 节点会和集群中其他所有节点建立 BGP peer 连接，也就是说这是一个 O(n^2) 的增长趋势。在集群规模比较小的情况下，这种模式是可以接受的，但是当集群规模扩展到百个节点、甚至更多的时候，这样的连接数无疑会带来很大的负担。为了解决集群规模较大情况下 BGP client 连接数膨胀的问题，calico 引入了 RR（Router Reflector） 的功能。</p>
<p>  RR 的基本思想是选择一部分节点（一个或者多个）作为 Global BGP Peer，它们和所有的其他节点互联来交换路由信息，其他的节点只需要和 Global BGP Peer 相连就行，不需要之间再两两连接。更多的组网模式也是支持的，不管怎么组网，最核心的思想就是所有的节点能获取到整个集群的路由信息。</p>
  <img src="/www6vHomeHexo/2022/05/03/k8sCalico/route-reflector.png" class title="Route Reflector模式+IPIP模式">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><p>《31容器网络之Calico：为高效说出善意的谎言》  趣谈网络协议  刘超</p>
</li>
<li><p>《35  解读Kubernetes三层网络方案》  深入剖析Kubernetes  张磊</p>
</li>
<li><p><a href="https://blog.csdn.net/ccy19910925/article/details/82423452">kubernetes网络之—Calico原理解读</a>  看图</p>
</li>
<li><p><a href="https://developer.aliyun.com/article/68558">容器网络Calico进阶实践 | 褚向阳</a>  “看看 Calico 是如何实现跨主机互通”</p>
</li>
<li><p><a href="https://www.cnblogs.com/netonline/p/9720279.html">Calico网络方案</a> 安装</p>
</li>
<li><p><a href="https://cizixs.com/2017/10/19/docker-calico-network/">docker 容器网络方案：calico 网络模型</a> 安装+原理 - 阿里人 - ”报文流程“</p>
</li>
<li><p>&lt;&lt;kubernetes网络权威指南&gt;&gt;  5.4节</p>
</li>
<li><p><a href="https://www.yuque.com/wei.luo/cni/uf5hyp">20210806-Calico基础架构</a> 未</p>
</li>
<li><p><a href="https://projectcalico.docs.tigera.io/archive/v3.20/networking/bgp">Configure BGP peering</a> 未<br>Full-mesh , Route reflectors</p>
</li>
<li><p><a href="https://projectcalico.docs.tigera.io/archive/v3.20/networking/vxlan-ipip">Overlay networking</a>  未<br>公有云环境中（aws）<br>ipipMode field (IP in IP encapsulation)， ipipMode 必须with BGP<br>vxlanMode field (VXLAN encapsulation)， vxlanMode 可以without BGP<br>两种模式不能一起运行，只能运行其中的一种</p>
</li>
<li><p><a href="https://kubesphere.com.cn/blogs/calico-guide/">Calico 路由反射模式权威指南</a> 未</p>
</li>
</ul>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE 五大根基-报警</title>
    <url>/www6vHomeHexo/2022/05/02/sreWorkbookBasicAlert/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="告警设定考量">告警设定考量</span><a href="#告警设定考量" class="header-anchor">#</a></h3><ul>
<li>精准率<br>减少误告警</li>
<li>查全率<br>减少漏告警<img src="/www6vHomeHexo/2022/05/02/sreWorkbookBasicAlert/precision-recall.JPG" class></li>
</ul>
<ul>
<li>检测用时<br>过长  影响错误预算</li>
<li>重置用时<br>过长  增长内存和IO开销</li>
</ul>
<h3><span id="告警设定方法">告警设定方法</span><a href="#告警设定方法" class="header-anchor">#</a></h3><ul>
<li>基础<ul>
<li>方法1  目标错误率 &gt;&#x3D; SLO阈值</li>
</ul>
</li>
<li>window<ul>
<li>方法2  延长报警时间窗口</li>
<li>方法3  延长告警触发前的持续时间</li>
</ul>
</li>
<li>燃烧率   <ul>
<li>方法4 根据燃烧率发出告警</li>
<li>方法5 基于多个燃烧率的告警</li>
<li>方法6 基于多个窗口 多个燃烧率的告警</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1ov4y197N1/">《Google SRE工作手册》第四期基于SLO的告警配置及实践分享</a> V</li>
<li>《Google SRE工作手册》  第5章</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>sre</category>
      </categories>
      <tags>
        <tag>sre</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE 五大根基-SLO</title>
    <url>/www6vHomeHexo/2022/05/02/sreWorkbookBasicSLO/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#sre-%E4%BA%94%E5%A4%A7%E6%A0%B9%E5%9F%BA-%E4%B9%8B-slo1">SRE 五大根基 之 SLO[1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A41-%E5%88%B6%E5%AE%9Aslo">步骤1. 制定SLO</a><ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E7%9A%84slo">服务的SLO</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E7%9A%84slo">数据服务的SLO</a></li>
<li><a href="#sli">SLI</a></li>
</ul>
</li>
<li><a href="#%E6%AD%A5%E9%AA%A42-%E8%8E%B7%E5%BE%97%E5%B9%B2%E7%B3%BB%E4%BA%BA%E8%AE%A4%E5%90%8C">步骤2. 获得干系人认同</a><ul>
<li><a href="#slo-%E4%BB%AA%E8%A1%A8%E6%9D%BF%E8%B6%8B%E5%8A%BF">SLO 仪表板[趋势]</a></li>
</ul>
</li>
<li><a href="#%E6%AD%A5%E9%AA%A43-%E6%8C%81%E7%BB%AD%E7%9B%91%E6%8E%A7-%E6%94%B9%E8%BF%9Bslo">步骤3. 持续监控 改进SLO</a></li>
<li><a href="#%E6%AD%A5%E9%AA%A44-%E9%94%99%E8%AF%AF%E9%A2%84%E7%AE%97-slo%E5%86%B3%E7%AD%96">步骤4. 错误预算  SLO决策</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8Eslo%E5%92%8C%E9%94%99%E8%AF%AF%E9%A2%84%E7%AE%97%E7%9A%84%E5%86%B3%E7%AD%96">基于SLO和错误预算的决策</a></li>
</ul>
</li>
<li><a href="#%E6%AD%A5%E9%AA%A45-%E8%BF%9B%E9%98%B6">步骤5. 进阶</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="sre-五大根基-之-slo1">SRE 五大根基 之 SLO[1]</span><a href="#sre-五大根基-之-slo1" class="header-anchor">#</a></h1><h3><span id="步骤1-制定slo">步骤1.  制定SLO</span><a href="#步骤1-制定slo" class="header-anchor">#</a></h3><h5><span id="服务的slo">服务的SLO</span><a href="#服务的slo" class="header-anchor">#</a></h5><ul>
<li>VALET[Home Depot]<ul>
<li>Volume</li>
<li>Avail</li>
<li>Latency</li>
<li>Errors</li>
<li>Ticket</li>
</ul>
</li>
</ul>
<h5><span id="数据服务的slo">数据服务的SLO</span><a href="#数据服务的slo" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2022/04/27/redisSLO/" title="Redis SLO">Redis SLO</a></li>
<li><a href="/www6vHomeHexo/2022/06/16/kafkaSLO/" title="Kafka SLO">Kafka SLO</a></li>
</ul>
<h5><span id="sli">SLI</span><a href="#sli" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>服务类型</th>
<th>SLI类型</th>
</tr>
</thead>
<tbody><tr>
<td>请求驱动</td>
<td>可用性，延迟，质量</td>
</tr>
<tr>
<td>流水线</td>
<td>时效性，正确率，覆盖率</td>
</tr>
<tr>
<td>存储</td>
<td>持久性</td>
</tr>
</tbody></table>
<h3><span id="步骤2-获得干系人认同">步骤2.  获得干系人认同</span><a href="#步骤2-获得干系人认同" class="header-anchor">#</a></h3><h5><span id="slo-仪表板趋势">SLO 仪表板[趋势]</span><a href="#slo-仪表板趋势" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/05/02/sreWorkbookBasicSLO/slo-trend.JPG" class>

<h3><span id="步骤3-持续监控-改进slo">步骤3. 持续监控 改进SLO</span><a href="#步骤3-持续监控-改进slo" class="header-anchor">#</a></h3><ul>
<li>变更SLO</li>
<li>变更SLI实现</li>
<li>着手于现实的SLO</li>
<li>迭代</li>
</ul>
<h3><span id="步骤4-错误预算-slo决策">步骤4. 错误预算  SLO决策</span><a href="#步骤4-错误预算-slo决策" class="header-anchor">#</a></h3><h5><span id="基于slo和错误预算的决策">基于SLO和错误预算的决策</span><a href="#基于slo和错误预算的决策" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/05/02/sreWorkbookBasicSLO/slo-decision.JPG" class>

<h3><span id="步骤5-进阶">步骤5. 进阶</span><a href="#步骤5-进阶" class="header-anchor">#</a></h3><ul>
<li>用户旅程建模 </li>
<li>依赖关系建模</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1ZK41127WY/">《Google SRE工作手册》第二期SRE五大根基之一：SLO</a>  V ***</li>
<li>《Google SRE工作手册》 第二章</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>sre</category>
      </categories>
      <tags>
        <tag>sre</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 三层架构</title>
    <url>/www6vHomeHexo/2022/05/01/awsArch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="3-tier-iaas">3 Tier IaaS</span><a href="#3-tier-iaas" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/01/awsArch/iaas.png" class title="IaaS">

<h2><span id="3-tier-paas">3 Tier PaaS</span><a href="#3-tier-paas" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/05/01/awsArch/paas.png" class title="PasS">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><p>bili<br><a href="https://www.bilibili.com/video/BV1t4411i73x/">AWS基础服务介绍及上手实践（web三层架构）</a><br><a href="https://www.bilibili.com/video/BV1zU4y1w75e/">快速上手训练营-第四课：高可用架构与架构完善框架</a></p>
</li>
<li><p>其他<br> 《AWS in Action》 Andreas Wittig , Chaptor 11<br><a href="https://bp.aliyun.com/detail/23">互联网行业高弹性系统构建</a>  阿里云<br><a href="https://bp.aliyun.com/detail/57">同城跨可用区容灾迁移及演练</a> 阿里云</p>
</li>
</ul>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算产品-计算</title>
    <url>/www6vHomeHexo/2022/04/30/cloudProduct/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="计算-15">计算 [1][5]</span><a href="#计算-15" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">AWS service</th>
<th align="center">Azure service</th>
<th align="center">阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td align="center">VM</td>
<td align="center"><a href="https://aws.amazon.com/ec2/instance-types">Amazon EC2 Instance Types</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/virtual-machines">Azure Virtual Machines</a></td>
<td align="center">ECS</td>
<td>CVM</td>
</tr>
<tr>
<td align="center">Autoscaling</td>
<td align="center"><a href="https://aws.amazon.com/autoscaling">AWS Auto Scaling</a></td>
<td align="center"><a href="https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview">Virtual machine scale sets</a>, <a href="https://learn.microsoft.com/en-us/azure/app-service/web-sites-scale">App Service autoscale</a></td>
<td align="center">Auto Scaling</td>
<td>Auto Scaling</td>
</tr>
<tr>
<td align="center">Batch processing</td>
<td align="center"><a href="https://aws.amazon.com/batch">AWS Batch</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/batch">Azure Batch</a></td>
<td align="center"></td>
<td></td>
</tr>
<tr>
<td align="center">Serverless</td>
<td align="center"><a href="https://aws.amazon.com/lambda">AWS Lambda</a></td>
<td align="center"><a href="https://azure.microsoft.com/services/functions">Azure Functions</a>, <a href="https://learn.microsoft.com/en-us/azure/app-service/web-sites-create-web-jobs">WebJobs</a> in Azure App Service</td>
<td align="center">Function Compute</td>
<td>云函数 SCF</td>
</tr>
</tbody></table>
<h3><span id="storage">Storage</span><a href="#storage" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>AWS service</th>
<th>Azure service</th>
<th>阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td>Disk volumes on <a href="https://aws.amazon.com/ebs">Amazon Elastic Block Store (EBS)</a></td>
<td>Data disks in <a href="https://azure.microsoft.com/services/storage/blobs">Azure Blob Storage</a>.</td>
<td>todo</td>
<td>todo</td>
</tr>
<tr>
<td><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">Amazon EC2 instance store</a></td>
<td><a href="https://learn.microsoft.com/en-us/archive/blogs/mast/understanding-the-temporary-drive-on-windows-azure-virtual-machines">Azure temporary storage</a></td>
<td>todo</td>
<td>todo</td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/ebs/provisioned-iops">Amazon EBS Provisioned IOPS Volume</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/premium-storage-performance">Azure premium storage</a></td>
<td>todo</td>
<td>todo</td>
</tr>
<tr>
<td><a href="https://aws.amazon.com/efs">Amazon Elastic File System (EFS)</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction">Azure Files</a></td>
<td>todo</td>
<td>todo</td>
</tr>
</tbody></table>
<h3><span id="containers-and-container-orchestrators">Containers and container orchestrators</span><a href="#containers-and-container-orchestrators" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th></th>
<th>AWS service</th>
<th>Azure service</th>
<th>阿里云</th>
<th>腾讯云</th>
</tr>
</thead>
<tbody><tr>
<td>容器（托管）</td>
<td><a href="https://aws.amazon.com/eks">Amazon Elastic Kubernetes Service (EKS)</a></td>
<td><a href="https://azure.microsoft.com/services/kubernetes-service">Azure Kubernetes Service (AKS)</a></td>
<td>ACK</td>
<td>TKE</td>
</tr>
<tr>
<td>容器（serverless）</td>
<td><a href="https://aws.amazon.com/ecs">Amazon Elastic Container Service (Amazon ECS)</a>, <a href="https://aws.amazon.com/fargate">AWS Fargate</a></td>
<td><a href="https://azure.microsoft.com/products/container-apps/">Azure Container Apps</a></td>
<td>ECI（Container Instance）&#x2F;ASK</td>
<td>EKS(Kubernetes Service)</td>
</tr>
<tr>
<td>service mesh</td>
<td><a href="https://aws.amazon.com/app-mesh">AWS App Mesh</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/aks/open-service-mesh-integrations">Open Service Mesh on AKS</a></td>
<td>ASM</td>
<td></td>
</tr>
<tr>
<td>边缘托管</td>
<td></td>
<td></td>
<td>ACK@Edge[OpenYurt]</td>
<td>TKE for Edge<br></td>
</tr>
</tbody></table>
<img src="/www6vHomeHexo/2022/04/30/cloudProduct/product.jpg" class title="Kubernetes产品形态[2]">

<h2><span id="edge-边缘容器-3">Edge-边缘容器 [3]</span><a href="#edge-边缘容器-3" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>平台</th>
<th>开源</th>
</tr>
</thead>
<tbody><tr>
<td>百度云</td>
<td>OpenEdge[BAETYL]- Linux Foundation Edge</td>
</tr>
<tr>
<td>华为   华为</td>
<td>kubeedge-  CNCF</td>
</tr>
<tr>
<td>腾讯</td>
<td>SuperEdge- CNCF</td>
</tr>
<tr>
<td>阿里云</td>
<td>OpenYurt  - CNCF</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/158035354">从AWS到阿里云： 产品体系差异分析</a></li>
<li><a href="https://mp.weixin.qq.com/s/o_dPKP_6dL92Q4jiG4097A">Serverless Kubernetes：理想，现实与未来</a>  易立、张维</li>
<li><a href="https://www.easemob.com/news/7832">盘点：2022年值得关注的十大边缘计算开源项目</a></li>
<li><a href="https://www.bilibili.com/video/BV1tD4y1977x?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里云系列课程</a> *** 有ppt链接</li>
<li><a href="https://learn.microsoft.com/en-us/azure/architecture/aws-professional/compute">Compute services on Azure and AWS</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>产品</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE 五大根基</title>
    <url>/www6vHomeHexo/2022/04/27/sreWorkbookBasic/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#sre%E4%BA%94%E5%A4%A7%E6%A0%B9%E5%9F%BA">SRE五大根基</a><ul>
<li><a href="#%E5%AE%9E%E8%B7%B5slo">实践SLO</a></li>
<li><a href="#%E7%9B%91%E6%8E%A7">监控</a></li>
<li><a href="#%E5%91%8A%E8%AD%A6">告警</a></li>
<li><a href="#%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B">减少琐事</a></li>
<li><a href="#%E7%AE%80%E5%8D%95%E5%8C%96">简单化</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="sre五大根基">SRE五大根基</span><a href="#sre五大根基" class="header-anchor">#</a></h1><h3><span id="实践slo">实践SLO</span><a href="#实践slo" class="header-anchor">#</a></h3><h3><span id="监控">监控</span><a href="#监控" class="header-anchor">#</a></h3><h3><span id="告警">告警</span><a href="#告警" class="header-anchor">#</a></h3><h3><span id="减少琐事">减少琐事</span><a href="#减少琐事" class="header-anchor">#</a></h3><h3><span id="简单化">简单化</span><a href="#简单化" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1ZK41127WY/">《Google SRE工作手册》第二期SRE五大根基之一：SLO</a>  V ***</li>
<li><a href="https://www.bilibili.com/video/BV1JY411o7AS/">《Google SRE工作手册》第二期SRE五大根基之二：监控</a>  V *** </li>
<li><a href="/www6vHomeHexo/2023/02/01/sreWorkbook/" title="《SRE 工作手册》">《SRE 工作手册》</a> self</li>
<li><a href="/www6vHomeHexo/2022/05/02/sreWorkbookBasicSLO/" title="SRE 五大根基-SLO">SRE 五大根基-SLO</a></li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>sre</category>
      </categories>
      <tags>
        <tag>sre</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis SLO</title>
    <url>/www6vHomeHexo/2022/04/27/redisSLO/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="slo">SLO</span><a href="#slo" class="header-anchor">#</a></h2><ul>
<li>命中率 Hit Ratio<ul>
<li>每秒命中数量   </li>
<li>每秒未命中数量</li>
</ul>
</li>
<li>吞吐 Throughput<ul>
<li>操作数</li>
</ul>
</li>
<li>延迟<ul>
<li>命令执行平均耗时</li>
</ul>
</li>
<li>容量 <ul>
<li>expiring&#x2F;not expiring   keys</li>
<li>expired&#x2F;evicted keys</li>
</ul>
</li>
<li>内存使用率 Memory Utilization</li>
<li>活动连接数 Active Connections</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.51cto.com/wutengfei/6026334">部署一个redis exporter监控所有的Redis实例</a></li>
<li><a href="https://scalegrid.io/blog/6-crucial-redis-monitoring-metrics/">6 Crucial Redis Monitoring Metrics You Need To Watch</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd 部署</title>
    <url>/www6vHomeHexo/2022/04/24/etcdDeploy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="单机docker部署">单机Docker部署</span><a href="#单机docker部署" class="header-anchor">#</a></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">image</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker pull bitnami/etcd:latest</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">network</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker network create app-tier --driver bridge</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">server start</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -d --name etcd-server \</span></span><br><span class="line"><span class="language-bash">    --network app-tier \</span></span><br><span class="line"><span class="language-bash">    --publish 2379:2379 \</span></span><br><span class="line"><span class="language-bash">    --publish 2380:2380 \</span></span><br><span class="line"><span class="language-bash">    --<span class="built_in">env</span> ALLOW_NONE_AUTHENTICATION=<span class="built_in">yes</span> \</span></span><br><span class="line"><span class="language-bash">    --<span class="built_in">env</span> ETCD_ADVERTISE_CLIENT_URLS=http://etcd-server:2379 \</span></span><br><span class="line"><span class="language-bash">    bitnami/etcd:latest</span></span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">client-&gt;put</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -it --<span class="built_in">rm</span> \</span></span><br><span class="line"><span class="language-bash">    --network app-tier \</span></span><br><span class="line"><span class="language-bash">    --<span class="built_in">env</span> ALLOW_NONE_AUTHENTICATION=<span class="built_in">yes</span> \</span></span><br><span class="line"><span class="language-bash">    bitnami/etcd:latest etcdctl --endpoints http://etcd-server:2379 put       testKey helloEtcd!</span></span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">client-&gt;get</span>    </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -it --<span class="built_in">rm</span> \</span></span><br><span class="line"><span class="language-bash">    --network app-tier \</span></span><br><span class="line"><span class="language-bash">    --<span class="built_in">env</span> ALLOW_NONE_AUTHENTICATION=<span class="built_in">yes</span> \</span></span><br><span class="line"><span class="language-bash">    bitnami/etcd:latest etcdctl --endpoints=http://etcd-server:2379 get testKey</span>    </span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/zjdxr-up/p/15409176.html">docker 安装 ETCD 及 etcd 使用</a></p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title>流计算-Beam</title>
    <url>/www6vHomeHexo/2022/04/21/streamingBeam/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h1><h3><span id="pcollection-1">PCollection [1]</span><a href="#pcollection-1" class="header-anchor">#</a></h3><ul>
<li>Parallel Collection，意思是可并行计算的数据集</li>
<li>特性<ul>
<li>无序的</li>
<li>没有固定大小<br>不一定有固定的边界<br>可以是有界的，也可以是无界的</li>
<li>不可变性  </li>
<li><strong>延迟执行（deferred execution）</strong>的模式</li>
</ul>
</li>
</ul>
<h3><span id="transform-2">Transform [2]</span><a href="#transform-2" class="header-anchor">#</a></h3><ul>
<li><p>基本概念</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">pcollection.apply(ParDo.of(<span class="keyword">new</span> <span class="title class_">DoFn</span>()))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Stateful Transform 和 side input&#x2F;side output</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">FindUserNameFn</span> <span class="keyword">extends</span> <span class="title class_">DoFn</span>&lt;String, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@ProcessElement</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(<span class="meta">@Element</span> String userId, OutputReceiver&lt;String&gt; out)</span> &#123;</span><br><span class="line"></span><br><span class="line">    out.output(database.FindUserName(userId));</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  Database database;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="pipeline">Pipeline</span><a href="#pipeline" class="header-anchor">#</a></h3><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《24 | PCollection：为什么Beam要如此抽象封装数据？ 》  蔡元楠</li>
<li>《25 | Transform：Beam数据转换操作的抽象方法 》 蔡元楠</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>Beam</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>BoltDB</title>
    <url>/www6vHomeHexo/2022/04/12/kvBoltDB/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="boltdb-磁盘布局和索引结构2">Boltdb 磁盘布局和索引结构[2]</span><a href="#boltdb-磁盘布局和索引结构2" class="header-anchor">#</a></h2>

<ul>
<li>BoltDB 的磁盘布局<ul>
<li>元数据页</li>
<li>B+树索引节点页</li>
<li>B+树叶子节点页</li>
<li>空闲页管理页和空闲页</li>
</ul>
</li>
</ul>
<p>BoltDB 通过 B+树来管理分支&#x2F;叶子页，实现快速查找和写入 key-value 数据。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="2">
<li>《10 | boltdb：如何持久化存储你的key-value数据？》  etcd实战课  唐聪</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>BoltDB</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库迁移</title>
    <url>/www6vHomeHexo/2022/04/11/dbMigrate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%81%E7%A7%BB%E6%96%B9%E5%BC%8F">迁移方式</a></li>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B9">关注点</a><ul>
<li><a href="#%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7-1">迁移工具 [1]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E8%B1%A1%E6%98%A0%E5%B0%84">数据库对象映射</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E5%9C%BA%E6%99%AF-1">数据库迁移场景 [1]</a></li>
<li><a href="#%E8%BF%81%E7%A7%BB%E6%97%B6%E9%97%B4">迁移时间</a></li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E5%8F%AF%E4%BB%A5%E6%8E%A5%E5%8F%97%E7%9A%84%E5%81%9C%E6%9C%BA%E6%97%B6%E9%97%B4-1">业务可以接受的停机时间 [1]</a></li>
<li><a href="#%E8%BF%81%E7%A7%BB%E7%AD%96%E7%95%A5-1">迁移策略 [1]</a></li>
<li><a href="#%E8%BF%81%E7%A7%BB%E5%9C%BA%E6%99%AF">迁移场景</a></li>
</ul>
</li>
<li><a href="#sqlserver%E8%BF%81%E7%A7%BBmysql%E5%8F%8C%E5%86%99%E6%96%B9%E6%A1%88-2">SQLServer迁移MySQL(双写方案) [2]</a></li>
<li><a href="#shardingsphere-%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB-3">ShardingSphere 数据迁移 [3]</a><ul>
<li><a href="#%E5%87%86%E5%A4%87%E9%98%B6%E6%AE%B5">准备阶段</a></li>
<li><a href="#%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">重复数据处理</a></li>
<li><a href="#%E4%B8%80%E8%87%B4%E6%80%A7%E6%A0%A1%E9%AA%8C">一致性校验</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="迁移方式">迁移方式</span><a href="#迁移方式" class="header-anchor">#</a></h1><ul>
<li>全量迁移</li>
<li>全量 + 增量迁移</li>
</ul>
<h1><span id="关注点">关注点</span><a href="#关注点" class="header-anchor">#</a></h1><h3><span id="迁移工具-1">迁移工具 [1]</span><a href="#迁移工具-1" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">工具名称</th>
<th align="center">结构</th>
<th align="center">全量</th>
<th align="center">增量</th>
<th align="center">数据校验</th>
<th align="center">在线迁移</th>
</tr>
</thead>
<tbody><tr>
<td align="center">DTS</td>
<td align="center">P</td>
<td align="center">P</td>
<td align="center">P</td>
<td align="center">P</td>
<td align="center">P</td>
</tr>
<tr>
<td align="center">redis-shake</td>
<td align="center">无</td>
<td align="center">P</td>
<td align="center">P</td>
<td align="center">O</td>
<td align="center">P</td>
</tr>
<tr>
<td align="center">mongo-shake</td>
<td align="center">无</td>
<td align="center">P</td>
<td align="center">P</td>
<td align="center">O</td>
<td align="center">P</td>
</tr>
</tbody></table>
<h3><span id="数据库对象映射">数据库对象映射</span><a href="#数据库对象映射" class="header-anchor">#</a></h3><ul>
<li>数据库类型映射</li>
<li>库表列映射</li>
</ul>
<h3><span id="数据库迁移场景-1">数据库迁移场景 [1]</span><a href="#数据库迁移场景-1" class="header-anchor">#</a></h3><ul>
<li>迁移拓扑<br>1:1， 1:n， n:1, n:m</li>
<li>同构迁移<br>MySQL -&gt; MySQL</li>
<li>异构迁移<br>Oracle -&gt; MySQL</li>
</ul>
<h3><span id="迁移时间">迁移时间</span><a href="#迁移时间" class="header-anchor">#</a></h3><h3><span id="业务可以接受的停机时间-1">业务可以接受的停机时间 [1]</span><a href="#业务可以接受的停机时间-1" class="header-anchor">#</a></h3><ul>
<li>理想停机时间是 0</li>
<li>借助增量日志来实现</li>
</ul>
<h3><span id="迁移策略-1">迁移策略 [1]</span><a href="#迁移策略-1" class="header-anchor">#</a></h3><ul>
<li>流量切换策略<br>逐渐切流量<br>一次性切流量</li>
<li>先迁同构，再迁异构</li>
<li>迁移顺序<br>先迁移表对象结构<br>再迁移全量数据<br>再迁移增量数据<br>最后迁移触发器等对象数据</li>
</ul>
<h3><span id="迁移场景">迁移场景</span><a href="#迁移场景" class="header-anchor">#</a></h3><ul>
<li><p>无依赖关系 [1]<br>一对一迁移( 1:1)<br>一对多高耦合业务迁移( 1:n )<br>多对一异构迁移( n:1 )</p>
</li>
<li><p>有依赖关系<br><a href="https://zhuanlan.zhihu.com/p/68377907">多表存在因果联系更新下的数据库迁移</a></p>
</li>
</ul>
<h1><span id="sqlserver迁移mysql双写方案-2">SQLServer迁移MySQL(双写方案)  [2]</span><a href="#sqlserver迁移mysql双写方案-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/04/11/dbMigrate/dbMigrate.png" class title="SQLServer迁移MySQL-双写方案">



<h1><span id="shardingsphere-数据迁移-3">ShardingSphere 数据迁移 [3]</span><a href="#shardingsphere-数据迁移-3" class="header-anchor">#</a></h1><h3><span id="准备阶段">准备阶段</span><a href="#准备阶段" class="header-anchor">#</a></h3><ul>
<li>检查源端数据库所需的用户权限和数据库配置项是否开启,  比如MySQL需要打开binlog,  PG的wal_level &gt;&#x3D; logical </li>
<li>目标端建表 { and schema} </li>
<li>检查目标端表是否为空</li>
<li>初始化<strong>增量迁移</strong>的Task, 不同的数据库具有不同的实现, MySQL是伪装成Slave, 进行数据同步</li>
<li>初始化<strong>全量迁移</strong>的Task, 计算 每个Task 负责迁移的数据范围<br>[ 注意: 先初始化 增量,  再初始化全量]</li>
</ul>
<h3><span id="重复数据处理">重复数据处理</span><a href="#重复数据处理" class="header-anchor">#</a></h3><ul>
<li>问题:  <ul>
<li><strong>情况1  先增量，再全量，可能会有一定的重复数据</strong></li>
<li>情况2</li>
</ul>
</li>
<li>解决办法:  类似于<strong>幂等</strong>消费MQ重复消息， 使用数据库提供的<strong>insert or update</strong>进行幂等插入<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">MySQL: <span class="keyword">insert</span> <span class="keyword">into</span> ... <span class="keyword">on</span> duplicate key <span class="keyword">update</span> </span><br><span class="line">PG: <span class="keyword">insert</span> <span class="keyword">into</span> ... <span class="keyword">on</span> conflict do <span class="keyword">update</span> </span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="一致性校验">一致性校验</span><a href="#一致性校验" class="header-anchor">#</a></h3><p>前提条件: 当增量数据同步持续一段时间没有同步新数据 </p>
<ol>
<li>数据一致性校验之前, 可以停写源端库, 或者不停写<br>如果<strong>停写</strong>, 那所有校验算法都可以全面校验所有数据的一致性<br>如果<strong>不停写</strong>, 需要允许部分增量数据不校验。 支持增量数据变更的一致性校验还在规划中</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>&lt;&lt;云数据库架构&gt;&gt; 第四章</p>
</li>
<li><p>SQL Server 迁移到 MySQL<br>从 SQL Server 到 MySQL（一）：异构数据库迁移   停机迁移<br>从 SQL Server 到 MySQL（二）：在线迁移，空中换发动机<br>从 SQL Server 到 MySQL（三）：愚公移山 - 开源力量<br><a href="https://github.com/alswl/yugong">https://github.com/alswl/yugong</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1FW4y1n7R8/">ShardingSphere 数据迁移功能 &amp; 实战-郭信泽</a> V</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13s4y1t74t/">基于Apache ShardingSphere 改造单机数据库为分布式数据库实战</a> V 未</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/455976887">数据迁移的一致性探讨</a>  腾讯  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>迁移</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式数据库-全局时钟</title>
    <url>/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-2">Overview [2]</a></li>
<li><a href="#tso-%E5%92%8C-hlc-%E7%9A%84%E5%8C%BA%E5%88%ABn-ai">TSO 和 HLC 的区别[N AI]</a><br>  * <a href="#%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5">时间同步</a><br>  * <a href="#%E6%97%B6%E9%92%9F%E6%BC%82%E7%A7%BB">时钟漂移</a><br>  * <a href="#%E6%97%B6%E9%97%B4%E7%B2%BE%E5%BA%A6">时间精度</a></li>
<li><a href="#hlc">HLC</a><br>  * <a href="#hlc%E7%9A%84%E7%89%B9%E7%82%B9%E5%92%8C%E7%BC%BA%E7%82%B9-1">HLC的特点和缺点 [1]</a><br>  * <a href="#hlc%E7%AE%97%E6%B3%952">HLC算法[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview-2">Overview [2]</span><a href="#overview-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/globalTime.JPG" class>

<h1><span id="tso-和-hlc-的区别n-ai">TSO 和 HLC 的区别[N AI]</span><a href="#tso-和-hlc-的区别n-ai" class="header-anchor">#</a></h1><p>TSO (Timestamp Ordering) 和 HLC (Hybrid Logical Clock) 都是两种分布式系统中用于解决事件顺序问题的算法。<br>虽然两种算法都是基于时间戳的，但它们也有着一些区别。</p>
<h5><span id="时间同步">时间同步</span><a href="#时间同步" class="header-anchor">#</a></h5><p>TSO 需要依赖全局的时间同步服务，如 NTP (Network Time Protocol)，保证所有机器的时钟是一致的。而 HLC 可以在没有全局时钟同步服务的情况下，通过增加逻辑时钟来维护全局时钟的顺序。<br>这就使得 <strong>HLC 比 TSO 更加灵活和适用于更多的场景</strong>。</p>
<h5><span id="时钟漂移">时钟漂移</span><a href="#时钟漂移" class="header-anchor">#</a></h5><p>TSO 可能会受到时钟漂移的影响，当时钟出现漂移时，事件的顺序可能会受到影响。<strong>而 HLC 可以通过逻辑时钟来解决时钟漂移的问题</strong>。这使得 <strong>HLC 更加鲁棒</strong>，并且可以在更长的时间内保持正确的事件顺序。</p>
<h5><span id="时间精度">时间精度</span><a href="#时间精度" class="header-anchor">#</a></h5><p>TSO 采用 64 位的时间戳，精度为纳秒级别。而 <strong>HLC 采用 64 位的时间戳和 16 位的逻辑时钟</strong>，精度为微秒级别，比 TSO 更高。这意味着 HLC 可以更准确地记录事件发生的顺序。</p>
<h1><span id="hlc">HLC</span><a href="#hlc" class="header-anchor">#</a></h1><h5><span id="hlc的特点和缺点-1">HLC的特点和缺点 [1]</span><a href="#hlc的特点和缺点-1" class="header-anchor">#</a></h5><p>HLC（Hybrid Logical Clock 混合逻辑时钟）结合了物理时间和逻辑时钟的优点，提供了下面3个特性：</p>
<ol>
<li><strong>如果a happened-before b, 则hlc(a) &lt; hlc(b)。不过反之不成立</strong></li>
<li>HLC的时间戳占用的bit数不变</li>
<li>误差有上届。（其实就是NTP的最大误差，也就是所有机器的物理时间的最大误差）</li>
</ol>
<p>HLC本质上来说还是一个逻辑时钟，所以它只能<strong>提供partial ordering</strong>，而<strong>不能提供total  ordering</strong>。所以2个节点中发生的独立事件a和b，如果他们的timestamp在误差的上届范围内，是无法排序的。</p>
<h5><span id="hlc算法2">HLC算法[2]</span><a href="#hlc算法2" class="header-anchor">#</a></h5><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/265226466">CockRoachDB分布式事务 - HLC和MVCC的相映成趣</a></li>
<li>《05 | 全局时钟：物理时钟和逻辑时钟你Pick谁？》 分布式数据库30讲  王磊</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>KV  存储引擎比较[Boltdb Rocksdb]</title>
    <url>/www6vHomeHexo/2022/04/11/kvCompare/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="boltdb-vs-rocksdbn-ai">Boltdb vs.  Rocksdb[N AI]</span><a href="#boltdb-vs-rocksdbn-ai" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>特征</th>
<th>Boltdb</th>
<th>Rocksdb</th>
</tr>
</thead>
<tbody><tr>
<td>内存管理</td>
<td>MMap</td>
<td>MMap+预读取</td>
</tr>
<tr>
<td>索引结构</td>
<td><strong>B+树</strong>[2]</td>
<td><strong>LSM树</strong>[3]</td>
</tr>
<tr>
<td>读写性能<br>读写场景[1]</td>
<td>读性能好，写性能差<br><strong>适合‘读多写少’</strong></td>
<td>读写性能均衡<br><strong>适合‘写多读少’</strong></td>
</tr>
<tr>
<td>数据安全<br>事务</td>
<td>具有ACID特性<br>支持事务[1]</td>
<td>具有ACID特性<br>支持事务</td>
</tr>
<tr>
<td>备份和恢复</td>
<td>支持快照备份和恢复</td>
<td>支持增量备份和恢复</td>
</tr>
<tr>
<td>扩展性</td>
<td>不支持集群扩展</td>
<td>支持集群扩展</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>1.《云原生分布式存储基石-etcd深入理解》<br>2. <a href="/www6vHomeHexo/2022/04/12/kvBoltDB/" title="BoltDB">BoltDB</a>  self<br>2. <a href="/www6vHomeHexo/2022/04/05/rocksdbLsm/" title="RocksDB- LSM-Tree">RocksDB- LSM-Tree</a>  self</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Prometheus</title>
    <url>/www6vHomeHexo/2022/04/10/observabilityPrometheus/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="metric-之-prometheus-9">Metric 之 Prometheus [9]</span><a href="#metric-之-prometheus-9" class="header-anchor">#</a></h2><ul>
<li>非分布式， 联邦</li>
<li>pushgataway</li>
<li>服务发现</li>
<li>拉模式</li>
</ul>
<img src="/www6vHomeHexo/2022/04/10/observabilityPrometheus/metric.JPG" class title="metric的类型">



<h2><span id="业务场景">业务场景</span><a href="#业务场景" class="header-anchor">#</a></h2><ul>
<li>业务指标</li>
<li>metric</li>
</ul>
<h2><span id="多云-多地域监控">多云, 多地域监控</span><a href="#多云-多地域监控" class="header-anchor">#</a></h2><p>在远端云上部署Prometheus agent</p>
<h2><span id="prometheus-存储层的演进-7">Prometheus 存储层的演进 [7]</span><a href="#prometheus-存储层的演进-7" class="header-anchor">#</a></h2><ul>
<li>1st Generation: Prototype<ul>
<li>key<ul>
<li>metric name</li>
<li>labels</li>
<li>timestamp</li>
</ul>
</li>
<li>value</li>
</ul>
</li>
<li>2nd Generation: Prometheus V1<ul>
<li>压缩<ul>
<li>Timestamp Compression: Double Delta</li>
<li>Value Compression</li>
</ul>
</li>
<li>Chunk Encoding<br>1KB</li>
</ul>
</li>
<li>3rd Generation: Prometheus V2</li>
</ul>
<h2><span id="prometheus服务发现机制1011">Prometheus服务发现机制[10][11]</span><a href="#prometheus服务发现机制1011" class="header-anchor">#</a></h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">  <span class="attr">kubernetes_sd_configs:</span> <span class="comment">#基于 Kubernetes API实现的服务发现，让prometheus动态发现kubernetes中被监控的目标 </span></span><br><span class="line">  <span class="attr">static_configs:</span> <span class="comment">#静态服务发现，基于prometheus配置文件指定的监控目标</span></span><br><span class="line">  <span class="attr">dns_sd_configs:</span> <span class="comment">#DNS服务发现监控目标</span></span><br><span class="line">  <span class="attr">consul_sd_configs:</span> <span class="comment">#Consul服务发现，基于consul服务动态发现监控目标</span></span><br><span class="line">  <span class="attr">file_sd_configs:</span> <span class="comment">#基于指定的文件实现服务发现，基于指定的文件发现监控目标</span></span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol start="3">
<li><a href>第十八期: 玩转云原生容器场景的Prometheus监控</a>  腾讯云 云原生正发声  #todo 重看一遍</li>
</ol>
<h5><span id="存储层">存储层</span><a href="#存储层" class="header-anchor">#</a></h5><ol start="7">
<li><a href="https://cloud.tencent.com/developer/article/1847798">Prometheus 存储层的演进</a>  ***</li>
</ol>
<h5><span id="metric">Metric</span><a href="#metric" class="header-anchor">#</a></h5><ol start="8">
<li><a href>深入剖析Kubernetes - 48 | Prometheus、Metrics Server与Kubernetes监控体系</a> 张磊</li>
<li><a href>微服务架构实战160讲 第七模块 ：微服务监控告警Prometheus架构和实践 119.监控模式分类</a> 杨波 partial</li>
</ol>
<h5><span id="服务发现机制">服务发现机制</span><a href="#服务发现机制" class="header-anchor">#</a></h5><ol start="10">
<li><a href="https://www.bilibili.com/video/BV16t4y1w7r6">kubernetes架构师课程</a> ***</li>
<li><a href="https://www.cnblogs.com/punchlinux/p/16773486.html">prometheus服务发现</a></li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>metric</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>VPC</title>
    <url>/www6vHomeHexo/2022/04/09/vpc/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="概念">概念</span><a href="#概念" class="header-anchor">#</a></h2><ul>
<li>虚拟私有网络(Virtual Private Cloud，简称 VPC)</li>
<li>网段: 通常用 CIDR 来表达;</li>
<li>子网: 阿里云中把子网形象地称为“交换机”;</li>
<li>路由表: 每个子网都必须有一张关联的路由表;</li>
<li>网关: 对进出私有网络的流量进行把守和分发的重要节点;</li>
<li>安全组: 虚拟机进出流量的通行或拦截规则, 可以起到虚拟机网络防火墙的作用;</li>
</ul>
<h2><span id="高可用性">高可用性</span><a href="#高可用性" class="header-anchor">#</a></h2><p><strong>VPC的子网可以跨可用区(AZ), 也就是跨同区域内不同数据中心的私有网络</strong></p>
<h2><span id="弹性网卡elastic-network-interface-eni">弹性网卡(Elastic Network Interface-ENI)</span><a href="#弹性网卡elastic-network-interface-eni" class="header-anchor">#</a></h2><h5><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h5><ul>
<li>一个虚拟机可以绑定多块网卡，有主网卡和辅助网卡之分;</li>
<li>一块网卡隶属于一个子网，可以配置同一子网内的多个私有 IP;</li>
<li>辅助网卡可以动态解绑，还能够绑定到另一台虚拟机上。</li>
</ul>
<h5><span id="绑定">绑定</span><a href="#绑定" class="header-anchor">#</a></h5><p>新虚拟机自动生成的主网卡，接入了 所选 VPC 的所选子网。</p>
<h2><span id="弹性-ipelastic-ip-eip">弹性 IP(Elastic IP-EIP)</span><a href="#弹性-ipelastic-ip-eip" class="header-anchor">#</a></h2><ul>
<li>EIP的IP 是固定</li>
<li>弹性：指可以非常自由地解绑和再次绑定到任意目标</li>
<li>DNS服务绑定EIP</li>
</ul>
<h2><span id="在-vpc-上开口子">在 VPC 上“开口子”</span><a href="#在-vpc-上开口子" class="header-anchor">#</a></h2><h5><span id="需求">需求</span><a href="#需求" class="header-anchor">#</a></h5><ul>
<li>弹性 IP 带来的是双向的开放，有时我们只想允许单向的连接</li>
<li>如何允许多台没有公有 IP 的虚拟机访问外网</li>
</ul>
<h5><span id="方案">方案</span><a href="#方案" class="header-anchor">#</a></h5><p>NAT(Network Address Translation)网关<br>VPN 网关</p>
<h2><span id="多网连接">多网连接</span><a href="#多网连接" class="header-anchor">#</a></h2><h5><span id="对等连接vpc-peering">对等连接(VPC Peering)</span><a href="#对等连接vpc-peering" class="header-anchor">#</a></h5><p>不具备传递性</p>
<h5><span id="专用网络设施">专用网络设施</span><a href="#专用网络设施" class="header-anchor">#</a></h5><p>AWS 的 Transit Gateway<br>阿里云的云企业网</p>
<h2><span id="混合云架构复杂">混合云架构(复杂)</span><a href="#混合云架构复杂" class="header-anchor">#</a></h2><h5><span id="vpn-轻量方式">VPN 轻量方式</span><a href="#vpn-轻量方式" class="header-anchor">#</a></h5><h5><span id="专线">专线</span><a href="#专线" class="header-anchor">#</a></h5><p>AWS 的 Direct Connect<br>Azure 的 ExpressRoute<br>阿里云的“高速通道”</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>06 | 云上虚拟网络:开合有度，编织无形之网</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>VPC</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd 总结</title>
    <url>/www6vHomeHexo/2022/04/06/etcd/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="读流程-3">读流程 [3]</span><a href="#读流程-3" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/06/etcd/read1.png" class title="读流程">

<p>一个读请求从 client 通过 Round-robin 负载均衡算法，选择一个-etcd server 节点，发出 gRPC 请求，经过 etcd server 的 KVServer 模块、线性读模块、MVCC 的 treeIndex 和 boltdb 模块紧密协作，完成了一个读请求。</p>
<p>etcd 提供的两种读机制 (串行读和线性读) </p>
<h2><span id="写流程4">写流程[4]</span><a href="#写流程4" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/06/etcd/write.png" class title="写流程">

<p> etcd 的写请求流程，重点介绍了 Quota、WAL、Apply 和 MVCC 模块。Quota 模块会防止 db 大小超限，WAL 模块保证了集群的一致性和可恢复性，Apply 模块实现了幂等性，MVCC 模块维护索引版本号和内存索引结构。etcd 通过异步、批量提交事务机制提升写 QPS 和吞吐量。这些模块相互协作，实现了在节点遭遇 crash 等异常情况下，不丢任何已提交的数据、不重复执行任何提案。[N AI]</p>
<h2><span id="客户端lease-5">客户端Lease [5]</span><a href="#客户端lease-5" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/06/etcd/lease.png" class title="客户端Lease">

<p>etcd 的 Lessor 模块在启动时会创建两个常驻 goroutine，分别用于检查过期 Lease 并撤销，以及定时更新 Lease 的剩余到期时间。该模块提供了 Grant、Revoke、LeaseTimeToLive 和 LeaseKeepAlive API，用于创建、撤销、获取有效期和续期 Lease。[N AI]</p>
<h2><span id="mvcc-和-revision2">MVCC 和 revision[2]</span><a href="#mvcc-和-revision2" class="header-anchor">#</a></h2><ul>
<li>etcd revision<ul>
<li>定义 <ul>
<li>计数器</li>
<li>键值空间发生变化， revision相应增加</li>
</ul>
</li>
<li>用途<ul>
<li>逻辑时钟</li>
<li><strong>MVCC</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://edu.aliyun.com/lesson_1651_18365#_18365">第16 章 ： 深入理解 etcd - 基本原理解析</a> 未</li>
<li>《云原生分布式存储基石-etcd深入解析》</li>
<li>《02 | 基础架构：etcd一个读请求是如何执行的？》</li>
<li>《03丨基础架构：etcd一个写请求是如何执行的？》</li>
<li>《06 | 租约：如何检测你的客户端存活？》</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title>RocksDB- LSM-Tree</title>
    <url>/www6vHomeHexo/2022/04/05/rocksdbLsm/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="22-rocksdb-architecture-and-its-use-of-lsm-trees">2.2 RocksDB Architecture and Its Use of LSM-trees</span><a href="#22-rocksdb-architecture-and-its-use-of-lsm-trees" class="header-anchor">#</a></h1><hr>
<p> <strong>the write process in RocksDB</strong> </p>
<p>Data is written to an in-memory write buffer called MemTable and a non-disk Write Ahead Log (WAL). Once the MemTable reaches a configured size, its contents are flushed to a Sorted String Table (SSTable) data file on disk. Each SSTable is immutable and stores data in sorted order, with an index block for binary search.</p>
<p> RocksDB 中的写入过程</p>
<p>数据被写入到一个内存写缓冲区 MemTable 和一个非磁盘写前日志 (WAL) 中。一旦 MemTable 达到配置的大小，它的内容就会被刷新到磁盘上的一个排序字符串表 (SSTable) 数据文件中。每个 SSTable 都是不可变的，并且按排序方式存储数据，具有用于二进制搜索的索引块。</p>
<hr>
<p>This document describes the compaction process in Rocksdb, which involves merging SSTables from different levels to optimize for read performance and space efficiency. The process gradually migrates written data from Level-0 to the last level, and is efficient due to parallelization and bulk reads and writes of entire files.</p>
<p>本文介绍了Rocksdb中的压缩流程，该流程涉及将来自不同级别的SSTables合并以优化读取性能和空间效率。该流程逐渐将写入的数据从Level-0迁移到最后一级，并且由于整个文件的并行化读写和批量读写而变得高效。</p>
<hr>
<p>MemTables 和 level-0 SSTables 具有重叠的键范围，因为它们包含键空间中的任何键。每个较旧的级别，即 level-1 或更旧的级别，由覆盖键空间的非重叠分区的 SSTables 组成。为了节省磁盘空间，较旧级别中的 SSTables 块可以选择进行压缩。</p>
<hr>
<p><strong>the read path in Rocksdb</strong></p>
<p>The read path involves searching MemTables and SSTables in successively older levels until the key is found or determined to be absent. Binary search is used in each case, and Bloom filters are used to reduce unnecessary searches. Hot SSTable blocks are cached in a memory-based block cache to reduce I&#x2F;O and decompression overheads.</p>
<p>Rocksdb 中的读取路径</p>
<p>读取路径涉及到在不断递减的级别中搜索 MemTables 和 SSTables，直到找到键或确定其不存在为止。每次都使用二进制搜索，并使用 Bloom 过滤器来减少不必要的搜索。热门 SSTable 块被缓存在基于内存的块缓存中，以减少 I&#x2F;O 和解压缩开销。</p>
<hr>
<p>This document describes the different types of compaction supported by RocksDB, including Leveled Compaction, Tiered Compaction, and FIFO Compaction. </p>
<p>Leveled Compaction assigns exponentially increasing size targets to levels, while Tiered Compaction lazily compacts multiple SSTables together. FIFO Compaction discards old SSTables and is designed for in-memory caching applications.</p>
<p>本文介绍了RocksDB支持的不同类型的压缩，包括<strong>分层压缩、分层压缩和FIFO压缩</strong>。</p>
<p>分层压缩将指数增长的大小目标分配给层级，而分层压缩则将多个SSTable懒惰地压缩在一起。FIFO压缩会丢弃旧的SSTable，适用于内存缓存应用程序。</p>
<h2><span id="rocksdb的多种压缩类型">RocksDB的多种压缩类型</span><a href="#rocksdb的多种压缩类型" class="header-anchor">#</a></h2><ul>
<li>Leveled Compaction<ul>
<li>LevelDB的改进版</li>
<li>指数增加大小目标</li>
<li>预防性压缩</li>
</ul>
</li>
<li>Tiered Compaction (RocksDB中称为Universal Compaction)<ul>
<li>与Apache Cassandra或HBase相似</li>
<li>延迟压缩</li>
<li>在非零级别的SSTables数量和级别0文件的总和超过可配置的阈值时，或者总DB大小与最大级别大小的比率超过阈值时进行压缩</li>
</ul>
</li>
<li>FIFO Compaction<ul>
<li>只执行轻量级压缩</li>
<li>在达到大小限制时丢弃旧的SSTables</li>
<li>适用于内存缓存应用</li>
</ul>
</li>
</ul>
<hr>
<p>This document discusses the benefits of configuring the type of compaction in RocksDB, which can be adjusted to be <strong>read-friendly, write-friendly, or very write-friendly</strong> depending on the use case. </p>
<p>However, there are tradeoffs to consider, such as write amplification and read performance. Different services require different setups, with logging or stream processing benefiting from a write-heavy approach and database services requiring a balanced approach.</p>
<p>本文讨论了在RocksDB中配置紧缩类型的好处，这可以根据用例调整为读取友好、写入友好或非常写入友好。</p>
<p>然而，需要考虑权衡，例如写入放大和读取性能。不同的服务需要不同的设置，日志记录或流处理受益于写入重型方法，而数据库服务需要平衡方法。</p>
<table>
<thead>
<tr>
<th>Compaction</th>
<th>Write Amplification</th>
<th>Space Overhead</th>
</tr>
</thead>
<tbody><tr>
<td>Leveled</td>
<td>大</td>
<td>小</td>
</tr>
<tr>
<td>Tiered</td>
<td>小</td>
<td>大</td>
</tr>
<tr>
<td>FIFO</td>
<td>小</td>
<td>N&#x2F;A</td>
</tr>
</tbody></table>
<hr>
<p> the benefits of using <strong>column families</strong> in Rocksdb </p>
<p>Column families allow different independent key spaces to co-exist in one DB, and each column family can be configured independently with different compaction, compression, and merge operators. The shared write-ahead log enables atomic writes to different column families, and existing column families can be removed and new ones created dynamically and efficiently. Column families are widely used to allow different compaction strategies for different classes of data in the same database and to remove obsolete data efficiently.</p>
<p>在Rocksdb中使用<strong>列族</strong>的好处</p>
<p>列族允许不同的独立键空间在一个DB中共存，每个列族都可以配置不同的压缩、合并和压实操作。共享的预写日志使得可以对不同的列族进行原子写操作，并且可以动态高效地删除现有的列族并创建新的列族。列族广泛用于允许在同一数据库中为不同类型的数据使用不同的压实策略，并高效地删除过时的数据。</p>
<details><summary>点击 原文</summary><p>Writes. Whenever data is written to RocksDB, the written data is added to an in-memory write buffer called MemTable,aswellasanon-diskWrite Ahead Log (WA L ). MemTable is implemented as a skiplist to keep the data ordered with O (log n) insert and search overheads. The WAL is used for recovery after a failure, but is not mandatory. Once the size of the MemTable reaches a configured size, then (i) the MemTable and WAL become immutable, (ii) a new MemTable and WAL are allocated for subsequent writes, (iii) the contents of the MemTable are flushed to a Sorted String Table (SSTable) data file on disk, and (iv) the flushed MemTable and associated WAL are discarded. Each SSTable stores data in sorted order, divided into uniformly sized blocks. Once written, each SSTable is immutable. Every SSTable also has an index block with one index entry per SSTable block for binary search.</p>
<p>Compaction. The LSM-tree has multiple levels, as shown in Figure 1. The newest SSTables are created by MemTable flushes, as described above, and are placed in Level-0. The other levels are created by a process called compaction. The maximum size of each level is limited by configuration parameters. When level-L’s size target is exceeded, some SSTables in level-L are selected and merged with the overlapping SSTables in level-(L+1) to create a new SSTable in level-(L+1). In doing so, deleted and overwritten data is removed, and the new SSTable is optimized for read performance and space efficiency. This process gradually migrates written data from Level-0 to the last level. Compaction I&#x2F;O is efficient, as it can be parallelized and only involves bulk reads and writes of entire files. (To avoid confusion with the terms “higher” and “lower,” given that levels with a higher number are generally located lower in images depicting multiple levels, we will refer to levels with a higher number as older levels.)</p>
<p>MemTables and level-0 SSTables have overlapping key ranges, since they contain keys anywhere in the keyspace. Each older level, i.e., a level-1 or older level, consists of SSTables covering nonoverlapping partitions of the keyspace. To save disk space, the blocks of SSTables in older levels may optionally be compressed.</p>
<p>Reads. In the read path, a key lookup occurs by first searching all MemTables, followed by searching all Level-0 SSTables, followed by the SSTables in successively older levels whose partition covers the lookup key. Binary search is used in each case. The search continues until the key is found, or it is determined that the key is not present in the oldest level.1 Hot SSTable blocks are cached in a memory-based block cache to reduce I&#x2F;O as well as decompression overheads. Bloom filters are used to eliminate most unnecessary searches within SSTables.</p>
<p>RocksDB supports multiple different types of compaction [23]. Leveled Compaction was adapted from LevelDB and then improved [19]. In this compaction style, levels are assigned exponentially increasing size targets as exemplified by the dashed boxes in Figure 1. Compactions are initiated proactively to ensure the target sizes are not exceeded. Tiered Compaction (called Universal Compaction in RocksDB [26]) is similar to what is used by Apache Cassandra or HBase [36, 37, 58]. Multiple SSTables are lazily compacted together, either when the sum of the number of level-0 files and the number of non-zero levels exceeds a configurable threshold or when the ratio between total DB size over the size of the largest level exceeds a threshold. In effect, compactions are delayed until either read performance or space efficiency degenerates, so more data can be compacted altogether. Finally, FIFO Compaction simply discards old SSTables once the DB hits a size limit and only performs lightweight compactions. It targets in-memory caching applications.</p>
<p>Being able to configure the type of compaction allows RocksDB to serve a wide range of use cases. By using different compaction styles, RocksDB can be configured as read-friendly, write-friendly, or very write-friendly (for special cache workloads). However, application owners will need to consider tradeoffs among the different metrics for their specific use case[4]. A lazier compaction algorithm improves write amplification and write throughput, but read performance suffers. In contrast, a more aggressive compaction sacrifices write amplification but allows for faster reads. Services such as logging or stream processing can use a write-heavy setup, while database services need a balanced approach. Table 3 depicts this flexibility by way of micro-benchmark results.</p>
<p>In 2014, we added a feature called column family2 [22], which allows different independent key spaces to co-exist in one DB. Each KV pair is associated with exactly one column family (by default the default column family), while different column families can contain KV pairs with the same key. Each column family has its own set of MemTables and SSTables, but they share the WAL. Benefits of column families include the following:</p>
<p>(1) each column family can be configured independently; that is, they each can have different compaction, compression, merge operators (Section 6.2), and compaction filters (Section 6.2);</p>
<p>(2) the shared WAL enables atomic writes to different column families3;and</p>
<p>(3) existing column families can be removed, and new column families can be created, dynamically and efficiently.</p>
<p>Column families are widely used. One way they are used is to allow different compaction strategies for different classes of data in the same database; e.g., in a database, some data ranges might be write-heavy and other ranges might be read-heavy, in which case compaction can be made more effective overall by placing the two different classes of data into two different column families configured to use different compaction strategies. Another way column families are used is to exploit the fact that a column family can be removed efficiently: If the data known to become obsolete4 within a time period is placed in the same column family, then a column family can be removed at the appropriate time without having to explicitly delete the KV pairs contained therein.</p>
</details>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications   2.2</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>RocksDB</category>
      </categories>
      <tags>
        <tag>RocksDB</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S高可用-零停机[自主中断]</title>
    <url>/www6vHomeHexo/2022/04/05/k8sAvailable/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%87%AA%E4%B8%BB%E4%B8%AD%E6%96%AD%E5%92%8C%E9%9D%9E%E8%87%AA%E4%B8%BB%E4%B8%AD%E6%96%AD4">自主中断和非自主中断[4]</a></li>
<li><a href="#poddisruptionbudged-4">PodDisruptionBudged [4]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="自主中断和非自主中断4">自主中断和非自主中断[4]</span><a href="#自主中断和非自主中断4" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/04/05/k8sAvailable/interrupt.png" class title="自主中断和非自主中断">

<h1><span id="poddisruptionbudged-4">PodDisruptionBudged [4]</span><a href="#poddisruptionbudged-4" class="header-anchor">#</a></h1><ol>
<li>无状态应用：<br>maxUnavailable &#x3D; 40%</li>
<li>单实例有状态应用: </li>
<li>多实例有状态应用：<br>etcd N, maxUnavailable&#x3D;1 或者 minAvailable&#x3D;N</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="4">
<li>模块十一： 将应用迁移至Kubernetes平台</li>
<li><a href="https://zhuanlan.zhihu.com/p/360521649">使用 PDB 避免 Kubernetes 集群中断</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes CNI插件</title>
    <url>/www6vHomeHexo/2022/04/03/k8sCNI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="如何使用cni插件">如何使用CNI插件</span><a href="#如何使用cni插件" class="header-anchor">#</a></h2><ol>
<li>配置CNI配置文件<br>&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;xxnet.conf</li>
<li>安装CNI二进制插件<br>&#x2F;opt&#x2F;cni&#x2F;bin&#x2F;xxnet</li>
<li>在这个节点上创建Pod</li>
<li>kubelet 根据CNI配置文件执行CNI插件</li>
<li>Pod的网络配置完成</li>
</ol>
<h2><span id="cni插件-三种实现模式">CNI插件 三种实现模式</span><a href="#cni插件-三种实现模式" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/03/k8sCNI/types.png" class>

<ul>
<li>Overlay<br>容器有自己的ip段<br>通过隧道的方式，容器网段的包封装成物理主机之间的包<br>不依赖底层网络</li>
<li>路由<br>底层网络需要二层可达的能力</li>
<li>Underlay<br>强依赖底层网络</li>
</ul>
<h2><span id="cni插件的选择">CNI插件的选择</span><a href="#cni插件的选择" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/03/k8sCNI/chooseCNI.png" class>

<ul>
<li><p>环境限制</p>
<ul>
<li>虚拟化<br> 网络限制多：限制二层访问，只能三层ip转发（overlay）</li>
<li>物理机<br> 网络限制少： underlay或者路由</li>
</ul>
</li>
<li><p>功能需求</p>
<ul>
<li>安全<br> NetworkPolicy</li>
<li>集群外资源互联互通<br> underlay ， calico-bgp</li>
<li>服务发现与负载均衡</li>
</ul>
</li>
<li><p>性能需求  </p>
<ul>
<li>Pod创建速度</li>
<li>Pod网络性能<br>Overlay性能相对较差<br>Underlay和路由模式网络插件性能好</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://mp.weixin.qq.com/s/oC4PemXm6aupFNKKCkqOJQ">如何实现一个 Kubernetes 网络插件</a><br><a href="https://www.bilibili.com/video/BV1XJ411W7zZ?vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里技术大牛 30 分钟讲透 Kubernetes ： 理解 CNI 和 CNI 插件</a><br>​</p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubelet和PLEG</title>
    <url>/www6vHomeHexo/2022/04/03/k8sPLEG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="kubelet管理pod的核心流程">kubelet管理Pod的核心流程</span><a href="#kubelet管理pod的核心流程" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/03/k8sPLEG/startPodInKubelet.png" class title="kubelet管理Pod的核心流程">


<h2><span id="pod启动流程-23">Pod启动流程 [2][3]</span><a href="#pod启动流程-23" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/04/03/k8sPLEG/podStart.png" class title="Pod启动流程 819 439">

<img src="/www6vHomeHexo/2022/04/03/k8sPLEG/schedule.jpg" class width="720" height="426">

<h2><span id="pleg">PLEG</span><a href="#pleg" class="header-anchor">#</a></h2><p>   relist操作</p>
<p>参考：</p>
<ol>
<li><a href="http://www.xuyasong.com/?p=1819">kubelet 原理解析二：pleg</a></li>
<li>《模块七：Kubernetes控制平面组件》</li>
<li><a href="https://zhuanlan.zhihu.com/p/562805848">从架构层面了解Kubernetes</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Window</title>
    <url>/www6vHomeHexo/2022/03/31/streamingFlinkWindow/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="time-window">Time Window</span><a href="#time-window" class="header-anchor">#</a></h1><h3><span id="滚动窗口tumble">滚动窗口（TUMBLE）</span><a href="#滚动窗口tumble" class="header-anchor">#</a></h3><h3><span id="滑动窗口hop">滑动窗口（HOP）</span><a href="#滑动窗口hop" class="header-anchor">#</a></h3><h3><span id="会话窗口session">会话窗口（SESSION）</span><a href="#会话窗口session" class="header-anchor">#</a></h3><h3><span id="全局窗口global">全局窗口(GLOBAL)</span><a href="#全局窗口global" class="header-anchor">#</a></h3><h3><span id="渐进式窗口cumulate">渐进式窗口（CUMULATE）</span><a href="#渐进式窗口cumulate" class="header-anchor">#</a></h3><h1><span id="count-window">Count Window</span><a href="#count-window" class="header-anchor">#</a></h1><h3><span id="滚动-count">滚动 count</span><a href="#滚动-count" class="header-anchor">#</a></h3><h3><span id="滑动-count">滑动 count</span><a href="#滑动-count" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1.《Flink核心技术与实战》  第三章<br>2. <a href="https://blog.csdn.net/WindyQCF/article/details/125093370">万字详述 Flink SQL 4 种时间窗口语义！（收藏）</a></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-部署方式</title>
    <url>/www6vHomeHexo/2022/03/31/streamingFlinkDeploy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="flink-集群部署模式">Flink 集群部署模式</span><a href="#flink-集群部署模式" class="header-anchor">#</a></h2><ul>
<li>Session</li>
<li>Per-Job</li>
<li>Application</li>
</ul>
<h2><span id="集群资源管理器支持">集群资源管理器支持</span><a href="#集群资源管理器支持" class="header-anchor">#</a></h2><ul>
<li>Standalone</li>
<li>Yarn</li>
<li>Kubernetes<br>Flink on Kubernetes<br><a href="https://github.com/www6v/dev-ops/tree/master/yaml-prod/flink">Flink on Kubernetes</a>  Flink官方的方案</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>《Flink核心技术与实战》 极客时间  第二章</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-计费方式</title>
    <url>/www6vHomeHexo/2022/03/30/aliCloudBill/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="ecs计费方式">ECS计费方式</span><a href="#ecs计费方式" class="header-anchor">#</a></h3><ul>
<li>包年包月   [fixed]<br>7*24小时</li>
<li>按量付费  [on-demand]<br>爆发业务</li>
<li>抢占式<br>实例可被释放</li>
</ul>
<img src="/www6vHomeHexo/2022/03/30/aliCloudBill/ecs-bill.JPG" class>



<h3><span id="oss计费方式">OSS计费方式</span><a href="#oss计费方式" class="header-anchor">#</a></h3><ul>
<li>存  ， 取， 处理， 网络流量   [on-demand]</li>
</ul>
<img src="/www6vHomeHexo/2022/03/30/aliCloudBill/oss-bill.JPG" class>



<h3><span id="cdn-计费方式">CDN 计费方式</span><a href="#cdn-计费方式" class="header-anchor">#</a></h3><ul>
<li><p>基础</p>
<ul>
<li>峰值带宽   [on-demand]<br>带宽利用率高</li>
<li>流量   [on-demand]<br>带宽利用率低</li>
<li>流量包    [fixed]</li>
</ul>
</li>
<li><p>增值</p>
<ul>
<li>全站加速 [动态加速]<br>请求数计费</li>
</ul>
</li>
<li><p>CDN的计费方式 本质上是 网络的计费方式</p>
</li>
</ul>
<img src="/www6vHomeHexo/2022/03/30/aliCloudBill/cdn-bill.JPG" class>



]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 计算Computing</title>
    <url>/www6vHomeHexo/2022/03/30/awsComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="ec2">EC2</span><a href="#ec2" class="header-anchor">#</a></h2><h5><span id="types">Types</span><a href="#types" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/03/30/awsComputing/ec2-types.JPG" class title="EC2类型">

<h2><span id="ec2-auto-scaling-hand-on2">EC2 Auto Scaling [hand-on][2]</span><a href="#ec2-auto-scaling-hand-on2" class="header-anchor">#</a></h2><ul>
<li>Auto Scaling + CloudWatch<ul>
<li>Auto Scaling<ul>
<li>配置最大 最小 实例数</li>
<li>扩展策略<br>目标跟踪策略[简单]<br>步进扩展策略[复杂-根据CloudWatch来配置-优先建议采用]</li>
</ul>
</li>
<li>CloudWatch<br>配置平均cpu使用率, 如果大于或小于某个使用量, 做扩缩</li>
</ul>
</li>
</ul>
<p>参考：<br><a href="https://www.bilibili.com/video/BV1hJ411U7vd">AWS解决方案架构师认证 Professional(SAP)中文视频培训课程2022</a>  P23-25</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>bilibili video</li>
<li>aws中文教程</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Watermark &amp; Window</title>
    <url>/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="watermark-3">Watermark [3]</span><a href="#watermark-3" class="header-anchor">#</a></h1><h3><span id="定义-作用">定义 作用</span><a href="#定义-作用" class="header-anchor">#</a></h3><p>• Watermark 用于标记 Event-Time 的前进过程；<br>• Watermark 跟随 DataStream Event-Time 变动，并自身携带 TimeStamp；<br>• Watermark 用于表明所有较早的事件已经（可能）到达；<br>• Watermark 本身也属于特殊的事件；</p>
<h3><span id="目的">目的</span><a href="#目的" class="header-anchor">#</a></h3><p>用于处理乱序和延迟事件.   </p>
<h3><span id="类型">类型</span><a href="#类型" class="header-anchor">#</a></h3><ul>
<li><p>完美的Watermark<br>在顺序事件中没有太大的意义</p>
<img src="/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/perfectWaterwark.JPG" class>
</li>
<li><p>启发式的Watermark<br>EG    W(11) &#x3D;  15 - maxOutOfOrderness(4)<br>  W(7) &#x3D; 11 - maxOutOfOrderness(4)    </p>
<img src="/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/heuristicWatermark.JPG" class>
</li>
<li><p>迟到事件<br>迟到事件被丢弃  不计算在窗口中</p>
<img src="/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/lateEvent.JPG" class></li>
</ul>
<h3><span id="watermark触发时机-2">Watermark触发时机 [2]</span><a href="#watermark触发时机-2" class="header-anchor">#</a></h3><ol>
<li>Watermark时间 &gt; window_end_time<br>如果当前的watermark已经大于或等于窗口的最大时间戳（即窗口的endTime），那么就会触发窗口计算，并输出结果。</li>
<li>在[window_start_time, window_end_time)区间中有数据存在(左闭右开)</li>
</ol>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>muke体系课 - 大数据工程师</li>
<li><a href="https://www.jianshu.com/p/2a26a26d6599">Flink Window触发机制</a> *</li>
<li>《Flink核心技术与实战》  极客时间  第三章 ***</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis雪崩、击穿、穿透</title>
    <url>/www6vHomeHexo/2022/03/28/redisReliability-1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%97%AE%E9%A2%98%E6%96%B9%E6%A1%88-1">问题&amp;方案 [1]</a><ul>
<li><a href="#%E6%9C%89%E6%8D%9F%E6%96%B9%E6%A1%88">“有损”方案</a></li>
<li><a href="#%E9%A2%84%E9%98%B2%E5%BC%8F%E6%96%B9%E6%A1%88">预防式方案</a></li>
</ul>
</li>
<li><a href="#%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF">缓存击穿</a><ul>
<li><a href="#%E5%8E%9F%E5%9B%A0">原因</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
</ul>
</li>
<li><a href="#%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F">缓存穿透</a><ul>
<li><a href="#%E5%8E%9F%E5%9B%A0-1">原因</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-1">解决方案</a></li>
</ul>
</li>
<li><a href="#%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%8B%97%E6%A1%A9%E6%95%88%E5%BA%943">缓存穿透-狗桩效应[3]</a><ul>
<li><a href="#%E5%8E%9F%E5%9B%A0-2">原因</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-2">解决方案</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="问题amp方案-1">问题&amp;方案 [1]</span><a href="#问题amp方案-1" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/03/28/redisReliability-1/redisReliability-1.jpg" class title="Redis雪崩、击穿、穿透">

<h3><span id="有损方案">“有损”方案</span><a href="#有损方案" class="header-anchor">#</a></h3><ul>
<li>服务熔断、服务降级、请求限流这些方法都是属于<strong>“有损”方案</strong>，在保证数据库和整体系统稳定的同时，会对业务应用带来负面影响。</li>
</ul>
<h3><span id="预防式方案">预防式方案</span><a href="#预防式方案" class="header-anchor">#</a></h3><ul>
<li>建议是，尽量使用<strong>预防式方案</strong><ul>
<li>针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群；</li>
<li>针对缓存击穿，在缓存访问非常频繁的热点数据时，不要设置过期时间；</li>
<li>针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。</li>
</ul>
</li>
</ul>
<h1><span id="缓存击穿">缓存击穿</span><a href="#缓存击穿" class="header-anchor">#</a></h1><ul>
<li>缓存击穿</li>
</ul>
<h3><span id="原因">原因</span><a href="#原因" class="header-anchor">#</a></h3><p>热点数据过期<br>大量并发请求</p>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><p>唯一DB请求，共享结果<br>分布式锁</p>
<h1><span id="缓存穿透">缓存穿透</span><a href="#缓存穿透" class="header-anchor">#</a></h1><h3><span id="原因">原因</span><a href="#原因" class="header-anchor">#</a></h3><p>  指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。<br>  提交不在数据库中的查询，会击穿缓存，直接到达数据库。</p>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><ol>
<li>回种空值</li>
<li>使用bloomfilter</li>
</ol>
<h1><span id="缓存穿透-狗桩效应3">缓存穿透-狗桩效应[3]</span><a href="#缓存穿透-狗桩效应3" class="header-anchor">#</a></h1><h3><span id="原因">原因</span><a href="#原因" class="header-anchor">#</a></h3><p>当有一个极热点的缓存项，它一旦失效会有大量请求穿透到数据库，这会对数据库造成瞬时极大的压力，我们把这个场景叫做<strong>“dog-pile effect”（狗桩效应）</strong></p>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><ul>
<li><p>后台线程定时加载<br>在代码中，控制在某一个热点缓存项失效之后<strong>启动一个后台线程，穿透到数据库，将数<br>据加载到缓存中</strong>，在缓存未加载之前，所有访问这个缓存的请求都不再穿透而直接返回。</p>
</li>
<li><p>设置分布式锁<br>通过在 Memcached 或者 Redis 中设置<strong>分布式锁</strong>，只有获取到锁的请求才能够穿透到数<br>据库。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《26丨缓存异常（下）：如何解决缓存雪崩、击穿、穿透难题？》</li>
<li><a href="https://www.bilibili.com/video/BV1Gb4y187un?zw&vd_source=f6e8c1128f9f264c5ab8d9411a644036">【直播回放】海量并发微服务框架设计</a> V</li>
<li>《15 | 缓存的使用姿势（三）：缓存穿透了怎么办？》</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>流量治理-Sentinel</title>
    <url>/www6vHomeHexo/2022/03/28/ratelimitSentinel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#sentinel">Sentinel</a><ul>
<li><a href="#%E9%99%90%E6%B5%81-api-4">限流 API [4]</a></li>
<li><a href="#%E9%99%90%E6%B5%81-%E7%B1%BB%E5%9E%8B-2">限流 类型 [2]</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%99%90%E6%B5%81-1">分布式限流 [1]</a></li>
<li><a href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3-2">滑动窗口 [2]</a></li>
</ul>
</li>
<li><a href="#sentinel-vs-hystrix-vs-resilience4j-3">Sentinel vs. Hystrix vs. resilience4j [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="sentinel">Sentinel</span><a href="#sentinel" class="header-anchor">#</a></h1><h3><span id="限流-api-4">限流 API [4]</span><a href="#限流-api-4" class="header-anchor">#</a></h3><ol>
<li><p>定义资源<br>资源：可以是任何东西，一个服务，服务里的方法，甚至是一段代码。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> (<span class="type">Entry</span> <span class="variable">entry</span> <span class="operator">=</span> SphU.entry(<span class="string">&quot;HelloWorld&quot;</span>)) &#123;</span><br><span class="line">    <span class="comment">// Your business logic here.</span></span><br><span class="line">    System.out.println(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (BlockException e) &#123;</span><br><span class="line">    <span class="comment">// Handle rejected request.</span></span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// try-with-resources auto exit</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SentinelResource(&quot;HelloWorld&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">helloWorld</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 资源中的逻辑</span></span><br><span class="line">    System.out.println(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义规则<br>规则：Sentinel 支持以下几种规则：<br>流量控制规则、熔断降级规则、系统保护规则、来源访问控制规则和 热点参数规则。  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initSystemRule</span><span class="params">()</span> &#123;</span><br><span class="line">    List&lt;SystemRule&gt; rules = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="type">SystemRule</span> <span class="variable">rule</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SystemRule</span>();  <span class="comment">// 规则</span></span><br><span class="line">    rule.setHighestSystemLoad(<span class="number">10</span>);</span><br><span class="line">    rules.add(rule);</span><br><span class="line">    SystemRuleManager.loadRules(rules);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FlowRuleManager.loadRules(List&lt;FlowRule&gt; rules); <span class="comment">// 流控规则</span></span><br><span class="line">DegradeRuleManager.loadRules(List&lt;DegradeRule&gt; rules); <span class="comment">// 降级规则</span></span><br><span class="line">SystemRuleManager.loadRules(List&lt;SystemRule&gt; rules); <span class="comment">// 系统规则</span></span><br><span class="line">AuthorityRuleManager.loadRules(List&lt;AuthorityRule&gt; rules); <span class="comment">// 授权规则</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3><span id="限流-类型-2">限流 类型 [2]</span><a href="#限流-类型-2" class="header-anchor">#</a></h3><ul>
<li>直接失败  [滑动时间窗口]</li>
<li>Warmup 预热 [令牌桶算法]</li>
<li>限流排队  [漏桶算法]</li>
</ul>
<h3><span id="分布式限流-1">分布式限流 [1]</span><a href="#分布式限流-1" class="header-anchor">#</a></h3><h3><span id="滑动窗口-2">滑动窗口  [2]</span><a href="#滑动窗口-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/03/28/ratelimitSentinel/SlidingWindows.png" class>

<details><summary>核心代码 LeapArray.java</summary><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Get bucket item at given time from the array.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * (1) Bucket is absent, then just create a new bucket and CAS update to circular array.</span></span><br><span class="line"><span class="comment">     * (2) Bucket is up-to-date, then just return the bucket.</span></span><br><span class="line"><span class="comment">     * (3) Bucket is deprecated, then reset current bucket.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        WindowWrap&lt;T&gt; old = array.get(idx);</span><br><span class="line">        <span class="keyword">if</span> (old == <span class="literal">null</span>) &#123; <span class="comment">/// 初始化一个窗口</span></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             *     B0       B1      B2    NULL      B4</span></span><br><span class="line"><span class="comment">             * ||_______|_______|_______|_______|_______||___</span></span><br><span class="line"><span class="comment">             * 200     400     600     800     1000    1200  timestamp</span></span><br><span class="line"><span class="comment">             *                             ^</span></span><br><span class="line"><span class="comment">             *                          time=888</span></span><br><span class="line"><span class="comment">             *            bucket is empty, so create new and update</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * If the old bucket is absent, then we create a new bucket at &#123;@code windowStart&#125;,</span></span><br><span class="line"><span class="comment">             * then try to update circular array via a CAS operation. Only one thread can</span></span><br><span class="line"><span class="comment">             * succeed to update, while other threads yield its time slice.</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            WindowWrap&lt;T&gt; window = <span class="keyword">new</span> <span class="title class_">WindowWrap</span>&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis));</span><br><span class="line">            <span class="keyword">if</span> (array.compareAndSet(idx, <span class="literal">null</span>, window)) &#123;</span><br><span class="line">                <span class="comment">// Successfully updated, return the created bucket.</span></span><br><span class="line">                <span class="keyword">return</span> window;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Contention failed, the thread will yield its time slice to wait for bucket available.</span></span><br><span class="line">                Thread.<span class="keyword">yield</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (windowStart == old.windowStart()) &#123; <span class="comment">/// 返回老的窗口</span></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             *     B0       B1      B2     B3      B4</span></span><br><span class="line"><span class="comment">             * ||_______|_______|_______|_______|_______||___</span></span><br><span class="line"><span class="comment">             * 200     400     600     800     1000    1200  timestamp</span></span><br><span class="line"><span class="comment">             *                             ^</span></span><br><span class="line"><span class="comment">             *                          time=888</span></span><br><span class="line"><span class="comment">             *            startTime of Bucket 3: 800, so it&#x27;s up-to-date</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * If current &#123;@code windowStart&#125; is equal to the start timestamp of old bucket,</span></span><br><span class="line"><span class="comment">             * that means the time is within the bucket, so directly return the bucket.</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="keyword">return</span> old;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (windowStart &gt; old.windowStart()) &#123;  <span class="comment">/// 滚动: 重置老的窗口, 增加新的窗口</span></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             *   (old)</span></span><br><span class="line"><span class="comment">             *             B0       B1      B2    NULL      B4</span></span><br><span class="line"><span class="comment">             * |_______||_______|_______|_______|_______|_______||___</span></span><br><span class="line"><span class="comment">             * ...    1200     1400    1600    1800    2000    2200  timestamp</span></span><br><span class="line"><span class="comment">             *                              ^</span></span><br><span class="line"><span class="comment">             *                           time=1676</span></span><br><span class="line"><span class="comment">             *          startTime of Bucket 2: 400, deprecated, should be reset</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * If the start timestamp of old bucket is behind provided time, that means</span></span><br><span class="line"><span class="comment">             * the bucket is deprecated. We have to reset the bucket to current &#123;@code windowStart&#125;.</span></span><br><span class="line"><span class="comment">             * Note that the reset and clean-up operations are hard to be atomic,</span></span><br><span class="line"><span class="comment">             * so we need a update lock to guarantee the correctness of bucket update.</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * The update lock is conditional (tiny scope) and will take effect only when</span></span><br><span class="line"><span class="comment">             * bucket is deprecated, so in most cases it won&#x27;t lead to performance loss.</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="keyword">if</span> (updateLock.tryLock()) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// Successfully get the update lock, now we reset the bucket.</span></span><br><span class="line">                    <span class="keyword">return</span> resetWindowTo(old, windowStart);  <span class="comment">/// 清零重置old窗口</span></span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    updateLock.unlock();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Contention failed, the thread will yield its time slice to wait for bucket available.</span></span><br><span class="line">                Thread.<span class="keyword">yield</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (windowStart &lt; old.windowStart()) &#123;  <span class="comment">/// 时钟回拨</span></span><br><span class="line">            <span class="comment">// Should not go through here, as the provided time is already behind.</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WindowWrap</span>&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</details>



<h1><span id="sentinel-vs-hystrix-vs-resilience4j-3">Sentinel vs. Hystrix vs. resilience4j [3]</span><a href="#sentinel-vs-hystrix-vs-resilience4j-3" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th>Sentinel</th>
<th>Hystrix</th>
<th>resilience4j</th>
</tr>
</thead>
<tbody><tr>
<td>隔离策略</td>
<td>信号量隔离（并发线程数限流）</td>
<td>线程池隔离&#x2F;信号量隔离</td>
<td>信号量隔离</td>
</tr>
<tr>
<td>熔断降级策略</td>
<td>基于响应时间、异常比率、异常数等</td>
<td>异常比率模式、超时熔断</td>
<td>基于异常比率、响应时间</td>
</tr>
<tr>
<td>实时统计实现</td>
<td>滑动窗口（LeapArray）</td>
<td>滑动窗口（基于 RxJava）</td>
<td>Ring Bit Buffer</td>
</tr>
<tr>
<td>动态规则配置</td>
<td>支持<a href="https://github.com/alibaba/Sentinel/wiki/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%99%E6%89%A9%E5%B1%95#datasource-%E6%89%A9%E5%B1%95">多种配置源</a></td>
<td>支持多种数据源</td>
<td>有限支持</td>
</tr>
<tr>
<td>扩展性</td>
<td>丰富的 SPI 扩展接口</td>
<td>插件的形式</td>
<td>接口的形式</td>
</tr>
<tr>
<td>基于注解的支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>限流</td>
<td>基于 QPS，支持基于调用关系的限流</td>
<td>有限的支持</td>
<td>Rate Limiter</td>
</tr>
<tr>
<td>集群流量控制</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>流量整形</td>
<td>支持预热模式、匀速排队模式等多种复杂场景</td>
<td>不支持</td>
<td>简单的 Rate Limiter 模式</td>
</tr>
<tr>
<td>系统自适应保护</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>控制台</td>
<td>提供开箱即用的控制台，可配置规则、查看秒级监控、机器发现等</td>
<td>简单的监控查看</td>
<td>不提供控制台，可对接其它监控系统</td>
</tr>
<tr>
<td>多语言支持</td>
<td>Java &#x2F; C++</td>
<td>Java</td>
<td>Java</td>
</tr>
<tr>
<td>开源社区状态</td>
<td>活跃</td>
<td>停止维护</td>
<td>较活跃</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://sentinelguard.io/zh-cn/docs/cluster-flow-control.html">集群流量控制</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1MP4y1176q?p=11">【图灵学院】2022最新B站独家分布式限流算法原理与应用讲解视频短合集</a>   V</p>
</li>
<li><p><a href="https://github.com/alibaba/Sentinel/wiki/%E5%B8%B8%E7%94%A8%E9%99%90%E6%B5%81%E9%99%8D%E7%BA%A7%E7%BB%84%E4%BB%B6%E5%AF%B9%E6%AF%94">常用限流降级组件对比</a> Sentinel vs. Hystrix</p>
</li>
<li><p><a href="https://www.cnblogs.com/crazymakercircle/p/14285001.html">sentinel （史上最全+入门教程）</a> ***      </p>
</li>
<li><p><a href="https://github.com/www6v/StabilityGuide/blob/master/docs/prevention/resilience/%E6%B5%81%E6%8E%A7%E9%99%8D%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md">流控降级最佳实践</a>  阿里 未</p>
</li>
<li><a href="/www6vHomeHexo/2016/09/26/ratelimit/" title="限流-总结">限流-总结</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>流量治理</category>
      </categories>
      <tags>
        <tag>流量治理</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis Error-MOVED和ASK指令</title>
    <url>/www6vHomeHexo/2022/03/28/redisError/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<details><summary>相关代码</summary><p>cluster.c</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Return the pointer to the cluster node that is able to serve the command.</span></span><br><span class="line"><span class="comment"> * For the function to succeed the command should only target either:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1) A single key (even multiple times like LPOPRPUSH mylist mylist).</span></span><br><span class="line"><span class="comment"> * 2) Multiple keys in the same hash slot, while the slot is stable (no</span></span><br><span class="line"><span class="comment"> *    resharding in progress).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * On success the function returns the node that is able to serve the request.</span></span><br><span class="line"><span class="comment"> * If the node is not &#x27;myself&#x27; a redirection must be perfomed. The kind of</span></span><br><span class="line"><span class="comment"> * redirection is specified setting the integer passed by reference</span></span><br><span class="line"><span class="comment"> * &#x27;error_code&#x27;, which will be set to CLUSTER_REDIR_ASK or</span></span><br><span class="line"><span class="comment"> * CLUSTER_REDIR_MOVED.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * When the node is &#x27;myself&#x27; &#x27;error_code&#x27; is set to CLUSTER_REDIR_NONE.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If the command fails NULL is returned, and the reason of the failure is</span></span><br><span class="line"><span class="comment"> * provided via &#x27;error_code&#x27;, which will be set to:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * CLUSTER_REDIR_CROSS_SLOT if the request contains multiple keys that</span></span><br><span class="line"><span class="comment"> * don&#x27;t belong to the same hash slot.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * CLUSTER_REDIR_UNSTABLE if the request contains multiple keys</span></span><br><span class="line"><span class="comment"> * belonging to the same slot, but the slot is not stable (in migration or</span></span><br><span class="line"><span class="comment"> * importing state, likely because a resharding is in progress).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * CLUSTER_REDIR_DOWN_UNBOUND if the request addresses a slot which is</span></span><br><span class="line"><span class="comment"> * not bound to any node. In this case the cluster global state should be</span></span><br><span class="line"><span class="comment"> * already &quot;down&quot; but it is fragile to rely on the update of the global state,</span></span><br><span class="line"><span class="comment"> * so we also handle it here.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * CLUSTER_REDIR_DOWN_STATE if the cluster is down but the user attempts to</span></span><br><span class="line"><span class="comment"> * execute a command that addresses one or more keys. */</span></span><br><span class="line">clusterNode *<span class="title function_">getNodeByQuery</span><span class="params">(client *c, <span class="keyword">struct</span> redisCommand *cmd, robj **argv, <span class="type">int</span> argc, <span class="type">int</span> *hashslot, <span class="type">int</span> *error_code)</span> &#123;</span><br><span class="line">    clusterNode *n = <span class="literal">NULL</span>;</span><br><span class="line">    robj *firstkey = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">int</span> multiple_keys = <span class="number">0</span>;</span><br><span class="line">    multiState *ms, _ms;</span><br><span class="line">    multiCmd mc;</span><br><span class="line">    <span class="type">int</span> i, slot = <span class="number">0</span>, migrating_slot = <span class="number">0</span>, importing_slot = <span class="number">0</span>, missing_keys = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Allow any key to be set if a module disabled cluster redirections. */</span></span><br><span class="line">    <span class="keyword">if</span> (server.cluster_module_flags &amp; CLUSTER_MODULE_FLAG_NO_REDIRECTION)</span><br><span class="line">        <span class="keyword">return</span> myself;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Set error code optimistically for the base case. */</span></span><br><span class="line">    <span class="keyword">if</span> (error_code) *error_code = CLUSTER_REDIR_NONE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Modules can turn off Redis Cluster redirection: this is useful</span></span><br><span class="line"><span class="comment">     * when writing a module that implements a completely different</span></span><br><span class="line"><span class="comment">     * distributed system. */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* We handle all the cases as if they were EXEC commands, so we have</span></span><br><span class="line"><span class="comment">     * a common code path for everything */</span></span><br><span class="line">    <span class="keyword">if</span> (cmd-&gt;proc == execCommand) &#123;</span><br><span class="line">        <span class="comment">/* If CLIENT_MULTI flag is not set EXEC is just going to return an</span></span><br><span class="line"><span class="comment">         * error. */</span></span><br><span class="line">        <span class="keyword">if</span> (!(c-&gt;flags &amp; CLIENT_MULTI)) <span class="keyword">return</span> myself;</span><br><span class="line">        ms = &amp;c-&gt;mstate;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/* In order to have a single codepath create a fake Multi State</span></span><br><span class="line"><span class="comment">         * structure if the client is not in MULTI/EXEC state, this way</span></span><br><span class="line"><span class="comment">         * we have a single codepath below. */</span></span><br><span class="line">        ms = &amp;_ms;</span><br><span class="line">        _ms.commands = &amp;mc;</span><br><span class="line">        _ms.count = <span class="number">1</span>;</span><br><span class="line">        mc.argv = argv;</span><br><span class="line">        mc.argc = argc;</span><br><span class="line">        mc.cmd = cmd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Check that all the keys are in the same hash slot, and obtain this</span></span><br><span class="line"><span class="comment">     * slot and the node associated. */</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; ms-&gt;count; i++) &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">redisCommand</span> *<span class="title">mcmd</span>;</span></span><br><span class="line">        robj **margv;</span><br><span class="line">        <span class="type">int</span> margc, *keyindex, numkeys, j;</span><br><span class="line"></span><br><span class="line">        mcmd = ms-&gt;commands[i].cmd;</span><br><span class="line">        margc = ms-&gt;commands[i].argc;</span><br><span class="line">        margv = ms-&gt;commands[i].argv;</span><br><span class="line"></span><br><span class="line">        keyindex = getKeysFromCommand(mcmd,margv,margc,&amp;numkeys);</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; numkeys; j++) &#123;</span><br><span class="line">            robj *thiskey = margv[keyindex[j]];</span><br><span class="line">            <span class="type">int</span> thisslot = keyHashSlot((<span class="type">char</span>*)thiskey-&gt;ptr,</span><br><span class="line">                                       sdslen(thiskey-&gt;ptr));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (firstkey == <span class="literal">NULL</span>) &#123;</span><br><span class="line">                <span class="comment">/* This is the first key we see. Check what is the slot</span></span><br><span class="line"><span class="comment">                 * and node. */</span></span><br><span class="line">                firstkey = thiskey;</span><br><span class="line">                slot = thisslot;</span><br><span class="line">                n = server.cluster-&gt;slots[slot];</span><br><span class="line"></span><br><span class="line">                <span class="comment">/* Error: If a slot is not served, we are in &quot;cluster down&quot;</span></span><br><span class="line"><span class="comment">                 * state. However the state is yet to be updated, so this was</span></span><br><span class="line"><span class="comment">                 * not trapped earlier in processCommand(). Report the same</span></span><br><span class="line"><span class="comment">                 * error to the client. */</span></span><br><span class="line">                <span class="keyword">if</span> (n == <span class="literal">NULL</span>) &#123;</span><br><span class="line">                    getKeysFreeResult(keyindex);</span><br><span class="line">                    <span class="keyword">if</span> (error_code)</span><br><span class="line">                        *error_code = CLUSTER_REDIR_DOWN_UNBOUND;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">/* If we are migrating or importing this slot, we need to check</span></span><br><span class="line"><span class="comment">                 * if we have all the keys in the request (the only way we</span></span><br><span class="line"><span class="comment">                 * can safely serve the request, otherwise we return a TRYAGAIN</span></span><br><span class="line"><span class="comment">                 * error). To do so we set the importing/migrating state and</span></span><br><span class="line"><span class="comment">                 * increment a counter for every missing key. */</span></span><br><span class="line">                <span class="keyword">if</span> (n == myself &amp;&amp;</span><br><span class="line">                    server.cluster-&gt;migrating_slots_to[slot] != <span class="literal">NULL</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    migrating_slot = <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (server.cluster-&gt;importing_slots_from[slot] != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                    importing_slot = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">/* If it is not the first key, make sure it is exactly</span></span><br><span class="line"><span class="comment">                 * the same key as the first we saw. */</span></span><br><span class="line">                <span class="keyword">if</span> (!equalStringObjects(firstkey,thiskey)) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (slot != thisslot) &#123;</span><br><span class="line">                        <span class="comment">/* Error: multiple keys from different slots. */</span></span><br><span class="line">                        getKeysFreeResult(keyindex);</span><br><span class="line">                        <span class="keyword">if</span> (error_code)</span><br><span class="line">                            *error_code = CLUSTER_REDIR_CROSS_SLOT;</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">/* Flag this request as one with multiple different</span></span><br><span class="line"><span class="comment">                         * keys. */</span></span><br><span class="line">                        multiple_keys = <span class="number">1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* Migarting / Improrting slot? Count keys we don&#x27;t have. */</span></span><br><span class="line">            <span class="keyword">if</span> ((migrating_slot || importing_slot) &amp;&amp;</span><br><span class="line">                lookupKeyRead(&amp;server.db[<span class="number">0</span>],thiskey) == <span class="literal">NULL</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                missing_keys++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        getKeysFreeResult(keyindex);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* No key at all in command? then we can serve the request</span></span><br><span class="line"><span class="comment">     * without redirections or errors in all the cases. */</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="literal">NULL</span>) <span class="keyword">return</span> myself;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Cluster is globally down but we got keys? We can&#x27;t serve the request. */</span></span><br><span class="line">    <span class="keyword">if</span> (server.cluster-&gt;state != CLUSTER_OK) &#123;</span><br><span class="line">        <span class="keyword">if</span> (error_code) *error_code = CLUSTER_REDIR_DOWN_STATE;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Return the hashslot by reference. */</span></span><br><span class="line">    <span class="keyword">if</span> (hashslot) *hashslot = slot;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* MIGRATE always works in the context of the local node if the slot</span></span><br><span class="line"><span class="comment">     * is open (migrating or importing state). We need to be able to freely</span></span><br><span class="line"><span class="comment">     * move keys among instances in this case. */</span></span><br><span class="line">    <span class="keyword">if</span> ((migrating_slot || importing_slot) &amp;&amp; cmd-&gt;proc == migrateCommand)</span><br><span class="line">        <span class="keyword">return</span> myself;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* If we don&#x27;t have all the keys and we are migrating the slot, send</span></span><br><span class="line"><span class="comment">     * an ASK redirection. */</span></span><br><span class="line">    <span class="keyword">if</span> (migrating_slot &amp;&amp; missing_keys) &#123;</span><br><span class="line">        <span class="keyword">if</span> (error_code) *error_code = CLUSTER_REDIR_ASK;</span><br><span class="line">        <span class="keyword">return</span> server.cluster-&gt;migrating_slots_to[slot];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* If we are receiving the slot, and the client correctly flagged the</span></span><br><span class="line"><span class="comment">     * request as &quot;ASKING&quot;, we can serve the request. However if the request</span></span><br><span class="line"><span class="comment">     * involves multiple keys and we don&#x27;t have them all, the only option is</span></span><br><span class="line"><span class="comment">     * to send a TRYAGAIN error. */</span></span><br><span class="line">    <span class="keyword">if</span> (importing_slot &amp;&amp;</span><br><span class="line">        (c-&gt;flags &amp; CLIENT_ASKING || cmd-&gt;flags &amp; CMD_ASKING))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (multiple_keys &amp;&amp; missing_keys) &#123;</span><br><span class="line">            <span class="keyword">if</span> (error_code) *error_code = CLUSTER_REDIR_UNSTABLE;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> myself;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Handle the read-only client case reading from a slave: if this</span></span><br><span class="line"><span class="comment">     * node is a slave and the request is about an hash slot our master</span></span><br><span class="line"><span class="comment">     * is serving, we can reply without redirection. */</span></span><br><span class="line">    <span class="keyword">if</span> (c-&gt;flags &amp; CLIENT_READONLY &amp;&amp;</span><br><span class="line">        (cmd-&gt;flags &amp; CMD_READONLY || cmd-&gt;proc == evalCommand ||</span><br><span class="line">         cmd-&gt;proc == evalShaCommand) &amp;&amp;</span><br><span class="line">        nodeIsSlave(myself) &amp;&amp;</span><br><span class="line">        myself-&gt;slaveof == n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> myself;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Base case: just return the right node. However if this node is not</span></span><br><span class="line"><span class="comment">     * myself, set error_code to MOVED since we need to issue a rediretion. */</span></span><br><span class="line">    <span class="keyword">if</span> (n != myself &amp;&amp; error_code) *error_code = CLUSTER_REDIR_MOVED;</span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</details>


<p>代码展示了 getNodeByQuery 函数基本执行过程</p>
<img src="/www6vHomeHexo/2022/03/28/redisError/moved.jpg" class>


<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><p>Redis Cluster 会因为负载均衡或节点故障等原因而执行数据迁移，而</p>
<p>这就会导致客户端访问的 key 并不在接收到命令的集群节点上。因此，集群节点在命令执</p>
<p>行函数 processCommand 中，针对集群模式，就增加了额外的处理逻辑。这主要是包括</p>
<p>调用 <strong>getNodeByQuery 函数</strong>查询访问的 key 实际所属的节点，以及根据查询结果调用</p>
<p><strong>clusterRedirectClient 函数</strong>执行请求重定向。</p>
<p>事实上，对于分布式集群来说，Redis Cluster 设计实现的请求重定向机制是一个不错的参</p>
<p>考示例。其中，MOVED 和 ASK 两种重定向情况，就充分考虑了数据正在迁移的场景，这</p>
<p>种设计值得我们学习。而且，getNodeByQuery 函数在查询 key 所属的 slot 和节点时，</p>
<p>也充分考虑了 Redis 的事务操作，在对命令访问 key 进行查询时，巧妙地使用了<strong>同一个数</strong></p>
<p><strong>据结构 multiState</strong>，来封装事务涉及的多条命令和常规的单条命令，增加了代码的复用程</p>
<p>度，这一点也非常值得学习。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>27 | 从MOVED、ASK看集群节点如何处理命令？  蒋德钧</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 慢查询排查</title>
    <url>/www6vHomeHexo/2022/03/25/redisSlowResponse/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="慢查询-排查">慢查询 排查</span><a href="#慢查询-排查" class="header-anchor">#</a></h1><ul>
<li><p>获取 Redis 实例在当前环境下的基线性能。</p>
</li>
<li><p>是否用了慢查询命令？<br>如果是的话，就使用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。</p>
</li>
<li><p>是否对过期 key 设置了相同的过期时间？<br>对于批量删除的 key，可以在每个 key 的过期时间上加一个随机数，避免同时删除。</p>
</li>
<li><p>是否存在 bigkey？<br>对于 bigkey 的删除操作，如果你的 Redis 是 4.0 及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是 Redis 4.0 以前的版本，可以使用SCAN 命令迭代删除；对于 bigkey 的集合查询和聚合操作，可以使用 SCAN 命令在客户端完成。</p>
</li>
<li><p><strong>Redis AOF 配置级别是什么？业务层面是否的确需要这一可靠性级别？</strong><br><strong>如果我们需要高性能，同时也允许数据丢失，可以将配置项 no-appendfsync-on-rewrite 设置为  yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。<br>如果既需要高性能又需要高可靠性，最好使用高速固态盘作为 AOF 日志的写入盘。</strong></p>
</li>
<li><p>Redis 实例的内存使用是否过大？发生 swap 了吗？<br>如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现Redis 和其他内存需求大的应用共享机器的情况。</p>
</li>
<li><p>在 Redis 实例的运行环境中，是否启用了透明大页机制？<br>如果是的话，直接关闭内存大页机制就行了。</p>
</li>
<li><p>是否运行了 Redis 主从集群？<br>如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。</p>
</li>
<li><p>是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？<br>使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>19 | 波动的响应延迟：如何应对变慢的Redis？（下）<br><a href="https://redis.io/topics/latency">Diagnosing latency issues</a></p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云最佳实践-BestPractice</title>
    <url>/www6vHomeHexo/2022/03/24/aliyunBestPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><a href="https://bp.aliyun.com/">阿里云最佳实践</a><br><a href="https://zhuanlan.zhihu.com/p/446202318">阿里云解决方案汇总，24种上云场景，企业上云最佳实践</a></p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Seata 总结</title>
    <url>/www6vHomeHexo/2022/03/24/transactionSeata/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="seata-模式3">Seata 模式[3]</span><a href="#seata-模式3" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>功能名称</th>
<th>特点</th>
<th>基本原理</th>
<th>使用场景</th>
<th>存在问题</th>
<th>性能</th>
<th>复杂度</th>
</tr>
</thead>
<tbody><tr>
<td>AT 模式<br><a href="https://seata.io/zh-cn/docs/dev/mode/at-mode.html">AT模式官方文档 1</a></td>
<td>自动补偿事务</td>
<td>代理数据源，一阶段解析sql将数据写入undo_log表，二阶段根据undo_log表进行回滚</td>
<td><strong>支持 ACID 事务的关系型数据库</strong></td>
<td>开发时需注意：脏读</td>
<td>性能较高</td>
<td>开发简单</td>
</tr>
<tr>
<td>TCC 模式<br><a href="https://seata.io/zh-cn/docs/dev/mode/tcc-mode.html">TCC模式官方文档</a></td>
<td>手动补偿事务</td>
<td>代理数据源，一阶段执行prepare方法，二阶段执行commit或rollback方法</td>
<td><strong>不支持 ACID 事务的数据库</strong></td>
<td>开发时需注意：空回滚、幂等、悬挂</td>
<td>性能高</td>
<td>开发复杂</td>
</tr>
<tr>
<td>Saga 模式<br><a href="https://seata.io/zh-cn/docs/user/saga.html">SAGA模式官方文档</a></td>
<td></td>
<td>基于状态机引擎实现，用户设计状态图，开发rollback方法，状态机根据状态图 json 调用相关方法</td>
<td><strong>业务流程长、业务流程多</strong> <br>参与者包含其它公司或遗留系统服务，<strong>无法提供 TCC 模式要求的三个接口</strong></td>
<td>除了额外开发方法，需设计开发状态图 json</td>
<td>性能高</td>
<td>开发复杂</td>
</tr>
<tr>
<td>XA 模式<br><a href="https://seata.io/zh-cn/docs/dev/mode/xa-mode.html">XA模式官方文档</a></td>
<td>满足全局数据一致性. <br>AT、TCC、Saga 都是补偿型，无法做到真正的全局一致性.</td>
<td>由数据库XA协议完成提交、回滚</td>
<td>支持XA 事务的数据库</td>
<td>性能较低</td>
<td>性能较低</td>
<td>开发简单</td>
</tr>
</tbody></table>
<h3><span id="at模式-1">AT模式 [1]</span><a href="#at模式-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/03/24/transactionSeata/AT.jpg" class title="AT模式">

<ul>
<li>典型的分布式事务过程：<ul>
<li>TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID；</li>
<li>XID 在微服务调用链路的上下文中传播；</li>
<li>RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖；</li>
<li>TM 向 TC 发起针对 XID 的全局提交或回滚决议；</li>
<li>TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。</li>
</ul>
</li>
</ul>
<h3><span id="at模式-示例-4">AT模式 示例 [4]</span><a href="#at模式-示例-4" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OrderServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">OrderService</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="meta">@GlobalTransactional</span>   <span class="comment">///  TM</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> OperationResponse <span class="title function_">placeOrder</span><span class="params">(PlaceOrderRequestVO placeOrderRequestVO)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 扣减库存</span></span><br><span class="line">        DynamicDataSourceContextHolder.setDataSourceKey(DataSourceKey.STOCK);</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">operationStockResult</span> <span class="operator">=</span> stockService.reduceStock(placeOrderRequestVO.getProductId(), amount);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 扣减余额</span></span><br><span class="line">        DynamicDataSourceContextHolder.setDataSourceKey(DataSourceKey.PAY);</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">operationBalanceResult</span> <span class="operator">=</span> payService.reduceBalance(placeOrderRequestVO.getUserId(), price);</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PayServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">PayService</span> &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 事务传播特性设置为 REQUIRES_NEW 开启新的事务</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> userId 用户 ID</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> price  扣减金额</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRES_NEW)</span>   <span class="comment">/// RM</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">reduceBalance</span><span class="params">(Long userId, Integer price)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">&quot;开始扣减用户 &#123;&#125; 余额&quot;</span>, userId);</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">record</span> <span class="operator">=</span> accountDao.reduceBalance(price);</span><br><span class="line">        log.info(<span class="string">&quot;扣减用户 &#123;&#125; 余额结果:&#123;&#125;&quot;</span>, userId, record &gt; <span class="number">0</span> ? <span class="string">&quot;操作成功&quot;</span> : <span class="string">&quot;扣减余额失败&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> record &gt; <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="seata-同类产品">Seata 同类产品</span><a href="#seata-同类产品" class="header-anchor">#</a></h2><ul>
<li><p>TCC 模式</p>
<p>Eg: 支付宝DTS #3<br>蚂蚁 XTS(内部)&#x2F;DTX(蚂蚁金融云) #3 <br></p>
</li>
<li><p>两阶段</p>
<p>阿里 TXC(内部)&#x2F;GTS(阿里云) <br><strong>非入侵性</strong> <br><a href="https://github.com/seata/seata/wiki/AT-Mode">AT Mode</a> 基于 支持本地 ACID 事务 的 “关系型数据库” <br><a href="https://github.com/seata/seata/wiki/MT-Mode">MT Mode</a> 支持把”自定义”的分支事务纳入到全局事务的管理中</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU3MDAzNDg1MA==&mid=2247499421&idx=1&sn=a55797652284bafd9216ea981f4125e0">对比7种分布式事务方案，还是偏爱阿里开源的Seata，真香！(原理+实战)</a>  ***</p>
</li>
<li><p><a href="https://seata.io/zh-cn/docs/dev/mode/at-mode.html">Seata AT 模式</a>  官方文档</p>
</li>
<li><p><a href="http://koca.szkingdom.com/forum/t/topic/322"><a href="http://koca.szkingdom.com/forum/t/topic/322">Seata 使用调研</a></a></p>
</li>
<li><p><a href="https://github.com/www6v/seata-samples/tree/master/multiple-datasource">Seata AT模式-多数据源</a></p>
</li>
<li><p><a href="https://blog.csdn.net/abcd930704/article/details/121650029">seata的AT模式</a></p>
<p>早期</p>
</li>
<li><p>分布式事务之TCC事务 梁钟霖</p>
</li>
<li><p>分布式事务之TCC服务设计和实现注意事项 绍辉</p>
</li>
<li><p><a href="https://github.com/www6v/tcc-transaction">https://github.com/www6v/tcc-transaction</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/S0touTyVWfolEqgFaAjLxg">更开放的分布式事务 | Fescar 品牌升级，更名为 Seata</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/XTCZEZdmToWrETbR1GtR4g">关于开源分布式事务中间件Fescar，我们总结了开发者关心的13个问题</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>Seata</category>
      </categories>
      <tags>
        <tag>Seata</tag>
      </tags>
  </entry>
  <entry>
    <title>API 网关-灰度发布</title>
    <url>/www6vHomeHexo/2022/03/23/apiGatewayGray/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="灰度发布-策略-2">灰度发布 策略 [2]</span><a href="#灰度发布-策略-2" class="header-anchor">#</a></h2><ul>
<li>基于权重  百分比</li>
<li>version</li>
<li>…</li>
</ul>
<h2><span id="基于springcloud-gateway-nacos实现灰度发布1">基于springcloud gateway + nacos实现灰度发布[1]</span><a href="#基于springcloud-gateway-nacos实现灰度发布1" class="header-anchor">#</a></h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">gateway-reactor-gray</span></span><br><span class="line">  <span class="attr">cloud:</span></span><br><span class="line">     <span class="attr">nacos:</span></span><br><span class="line">       <span class="attr">discovery:</span></span><br><span class="line">        <span class="attr">server-addr:</span> <span class="string">localhost:8848</span></span><br><span class="line">     <span class="attr">gateway:</span></span><br><span class="line">       <span class="attr">discovery:</span></span><br><span class="line">         <span class="attr">locator:</span></span><br><span class="line">           <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">           <span class="attr">lower-case-service-id:</span> <span class="literal">true</span></span><br><span class="line">       <span class="attr">routes:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">hello-consumer</span></span><br><span class="line">           <span class="attr">uri:</span> <span class="string">grayLb://hello-consumer</span>  <span class="comment">## 灰度负载均衡</span></span><br><span class="line">           <span class="attr">predicates:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">Path=/hello/**</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GrayLoadBalancer</span> <span class="keyword">implements</span> <span class="title class_">ReactorServiceInstanceLoadBalancer</span> &#123;</span><br><span class="line"></span><br><span class="line">    ... </span><br><span class="line">    <span class="keyword">private</span> Response&lt;ServiceInstance&gt; <span class="title function_">getInstanceResponse</span><span class="params">(List&lt;ServiceInstance&gt; instances,HttpHeaders headers)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (instances.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">return</span> getServiceInstanceEmptyResponse();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> getServiceInstanceResponseWithWeight(instances);  <span class="comment">//</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据版本进行分发</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Response&lt;ServiceInstance&gt; <span class="title function_">getServiceInstanceResponseByVersion</span><span class="params">(List&lt;ServiceInstance&gt; instances, HttpHeaders headers)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">versionNo</span> <span class="operator">=</span> headers.getFirst(<span class="string">&quot;version&quot;</span>); <span class="comment">//</span></span><br><span class="line">        System.out.println(versionNo);</span><br><span class="line">        Map&lt;String,String&gt; versionMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        versionMap.put(<span class="string">&quot;version&quot;</span>,versionNo);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据在nacos中配置的权重值，进行分发</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Response&lt;ServiceInstance&gt; <span class="title function_">getServiceInstanceResponseWithWeight</span><span class="params">(List&lt;ServiceInstance&gt; instances)</span> &#123;</span><br><span class="line">        Map&lt;ServiceInstance,Integer&gt; weightMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (ServiceInstance instance : instances) &#123;</span><br><span class="line">            Map&lt;String,String&gt; metadata = instance.getMetadata();</span><br><span class="line">            <span class="keyword">if</span>(metadata.containsKey(<span class="string">&quot;weight&quot;</span>))&#123;   <span class="comment">//</span></span><br><span class="line">                weightMap.put(instance,Integer.valueOf(metadata.get(<span class="string">&quot;weight&quot;</span>)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GrayReactiveLoadBalancerClientFilter</span> <span class="keyword">implements</span> <span class="title class_">GlobalFilter</span>, Ordered &#123;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Mono&lt;Void&gt; <span class="title function_">filter</span><span class="params">(ServerWebExchange exchange, GatewayFilterChain chain)</span> &#123;</span><br><span class="line">        <span class="type">URI</span> <span class="variable">url</span> <span class="operator">=</span> (URI)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR);</span><br><span class="line">        <span class="type">String</span> <span class="variable">schemePrefix</span> <span class="operator">=</span> (String)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR);</span><br><span class="line">        <span class="keyword">if</span> (url != <span class="literal">null</span> &amp;&amp; (<span class="string">&quot;grayLb&quot;</span>.equals(url.getScheme()) || <span class="string">&quot;grayLb&quot;</span>.equals(schemePrefix))) &#123; <span class="comment">//</span></span><br><span class="line">            ServerWebExchangeUtils.addOriginalRequestUrl(exchange, url);</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">this</span>.choose(exchange).doOnNext((response) -&gt; &#123; <span class="comment">//</span></span><br><span class="line">              ... </span><br><span class="line">            &#125;).then(chain.filter(exchange));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> chain.filter(exchange);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Mono&lt;Response&lt;ServiceInstance&gt;&gt; <span class="title function_">choose</span><span class="params">(ServerWebExchange exchange)</span> &#123; <span class="comment">//</span></span><br><span class="line">        <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> (URI)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR);</span><br><span class="line">        <span class="type">GrayLoadBalancer</span> <span class="variable">loadBalancer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">GrayLoadBalancer</span>(clientFactory.getLazyProvider(uri.getHost(), ServiceInstanceListSupplier.class), uri.getHost());</span><br><span class="line">        <span class="keyword">if</span> (loadBalancer == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">NotFoundException</span>(<span class="string">&quot;No loadbalancer available for &quot;</span> + uri.getHost());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> loadBalancer.choose(<span class="built_in">this</span>.createRequest(exchange)); <span class="comment">//</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/linyb-geek/p/12774014.html">基于springcloud gateway + nacos实现灰度发布（reactive版）</a><br><a href="https://github.com/lyb-geek/gateway">相关的代码</a></li>
<li><a href="/www6vHomeHexo/2022/02/10/k8sIngressNginx/" title="Kubernetes Nginx Ingress">Kubernetes Nginx Ingress</a>   灰度发布  self</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>API Gateway</tag>
      </tags>
  </entry>
  <entry>
    <title>API 网关-SpringCloud Gateway</title>
    <url>/www6vHomeHexo/2022/03/22/apiGatawaySpringGateway/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#features-0">Features [0]</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5-12">核心概念 [1][2]</a></li>
<li><a href="#%E8%B7%AF%E7%94%B1route12">路由（Route）[1][2]</a><ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0-%E9%9B%86%E6%88%90nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83-2">服务发现-集成nacos服务注册中心 [2]</a></li>
<li><a href="#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1-%E6%95%B4%E5%90%88-apollo-2">动态路由-整合 Apollo [2]</a></li>
<li><a href="#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1-%E6%95%B4%E5%90%88nacos-3">动态路由-整合nacos [3]</a></li>
</ul>
</li>
<li><a href="#%E8%B0%93%E8%AF%8D-%E6%96%AD%E8%A8%80predicate12">谓词、断言（Predicate）[1][2]</a></li>
<li><a href="#%E8%BF%87%E6%BB%A4%E5%99%A8filter12">过滤器（Filter）[1][2]</a></li>
<li><a href="#%E7%A8%B3%E5%AE%9A%E6%80%A7">稳定性</a><ul>
<li><a href="#%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7-hystrix-3">熔断降级-Hystrix [3]</a></li>
<li><a href="#%E6%B5%81%E6%8E%A7%E5%92%8C%E9%99%8D%E7%BA%A7-sentinel-3">流控和降级-Sentinel [3]</a></li>
</ul>
</li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BD%91%E5%85%B31">高可用网关[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="features-0">Features [0]</span><a href="#features-0" class="header-anchor">#</a></h1><ul>
<li>Built on Spring Framework 5, Project Reactor and Spring Boot 2.0</li>
<li>Able to match <strong>routes</strong> on any request attribute.</li>
<li><strong>Predicates and filters</strong> are specific to <strong>routes</strong>.</li>
<li><strong>Circuit Breaker</strong> integration.</li>
<li>Spring Cloud <strong>DiscoveryClient</strong> integration</li>
<li>Easy to write <strong>Predicates and Filters</strong></li>
<li>Request <strong>Rate Limiting</strong></li>
<li><strong>Path Rewriting</strong></li>
</ul>
<h1><span id="核心概念-12">核心概念 [1][2]</span><a href="#核心概念-12" class="header-anchor">#</a></h1><ul>
<li><p>路由（Route）</p>
<ul>
<li>id：路由标识，要求唯一，名称任意（默认值 uuid，一般不用，需要自定义）</li>
<li>uri：请求最终被转发到的目标地址</li>
<li>order： 路由优先级，数字越小，优先级越高</li>
<li>predicates：断言数组，即判断条件，如果返回值是boolean，则转发请求到 uri 属性指定的服务中</li>
<li>filters：过滤器数组，在请求传递过程中，对请求做一些修改</li>
</ul>
</li>
<li><p>谓词、断言（Predicate）<br>允许开发人员匹配 HTTP 请求中的内容，比如请求头或请求参数，最后根据匹配结果返回一个<strong>布尔值</strong>。参照 Java8 的新特性Predicate.</p>
</li>
<li><p>过滤器（Filter）<br>可以在返回请求之前或之后<strong>修改请求和响应的内容</strong>。</p>
</li>
</ul>
<h1><span id="路由route12">路由（Route）[1][2]</span><a href="#路由route12" class="header-anchor">#</a></h1><h3><span id="服务发现-集成nacos服务注册中心-2">服务发现-集成nacos服务注册中心 [2]</span><a href="#服务发现-集成nacos服务注册中心-2" class="header-anchor">#</a></h3><ul>
<li><p>服务路由配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">cloud:</span></span><br><span class="line">    <span class="attr">gateway:</span></span><br><span class="line">      <span class="attr">routes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">gateway-provider_1</span></span><br><span class="line">          <span class="comment">## 使用了lb形式，从注册中心负载均衡的获取uri</span></span><br><span class="line">          <span class="attr">uri:</span> <span class="string">lb://gateway-provider</span></span><br><span class="line">          <span class="comment">## 配置断言</span></span><br><span class="line">          <span class="attr">predicates:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">Path=/gateway/provider/**</span></span><br><span class="line">          <span class="attr">filters:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">AddResponseHeader=X-Response-Foo,</span> <span class="string">Bar</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>自动路由配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># enabled：默认为false，设置为true表明spring cloud gateway开启服务发现和路由的功能，网关自动根据注册中心的服务名为每个服务创建一个router，将以服务名开头的请求路径转发到对应的服务</span></span><br><span class="line"><span class="string">spring.cloud.gateway.discovery.locator.enabled</span> <span class="string">=</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># lowerCaseServiceId：启动 locator.enabled=true 自动路由时，路由的路径默认会使用大写ID，若想要使用小写ID，可将lowerCaseServiceId设置为true</span></span><br><span class="line"><span class="string">spring.cloud.gateway.discovery.locator.lower-case-service-id</span> <span class="string">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="动态路由-整合-apollo-2">动态路由-整合 Apollo [2]</span><a href="#动态路由-整合-apollo-2" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Apollo路由更改监听刷新</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GatewayPropertRefresher</span> <span class="keyword">implements</span> <span class="title class_">ApplicationContextAware</span>, ApplicationEventPublisherAware</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 监听路由修改</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@ApolloConfigChangeListener(interestedKeyPrefixes = &quot;spring.cloud.gateway.&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onChange</span><span class="params">(ConfigChangeEvent changeEvent)</span></span><br><span class="line">    &#123;</span><br><span class="line">        refreshGatewayProperties(changeEvent);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 刷新路由信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">refreshGatewayProperties</span><span class="params">(ConfigChangeEvent changeEvent)</span></span><br><span class="line">    &#123;</span><br><span class="line">        logger.info(<span class="string">&quot;gateway网关配置 刷新开始！&quot;</span>);</span><br><span class="line"> </span><br><span class="line">        preDestroyGatewayProperties(changeEvent);</span><br><span class="line">        <span class="comment">//更新配置</span></span><br><span class="line">        <span class="built_in">this</span>.applicationContext.publishEvent(<span class="keyword">new</span> <span class="title class_">EnvironmentChangeEvent</span>(changeEvent.changedKeys()));</span><br><span class="line">        <span class="comment">//更新路由</span></span><br><span class="line">        refreshGatewayRouteDefinition();</span><br><span class="line"> </span><br><span class="line">        logger.info(<span class="string">&quot;gateway网关配置 刷新完成！&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="动态路由-整合nacos-3">动态路由-整合nacos  [3]</span><a href="#动态路由-整合nacos-3" class="header-anchor">#</a></h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">NacosDynamicRouteService</span> <span class="keyword">implements</span> <span class="title class_">ApplicationEventPublisherAware</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">String</span> <span class="variable">dataId</span> <span class="operator">=</span> <span class="string">&quot;gateway-router&quot;</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">String</span> <span class="variable">group</span> <span class="operator">=</span> <span class="string">&quot;DEFAULT_GROUP&quot;</span>;</span><br><span class="line">  <span class="meta">@Value(&quot;$&#123;spring.cloud.nacos.config.server-addr&#125;&quot;)</span></span><br><span class="line">  <span class="keyword">private</span> String serverAddr;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Autowired</span></span><br><span class="line">  <span class="keyword">private</span> RouteDefinitionWriter routeDefinitionWriter;</span><br><span class="line">  <span class="keyword">private</span> ApplicationEventPublisher applicationEventPublisher;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> List&lt;String&gt; ROUTE_LIST = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@PostConstruct</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">dynamicRouteByNacosListener</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">ConfigService</span> <span class="variable">configService</span> <span class="operator">=</span> NacosFactory.createConfigService(serverAddr);</span><br><span class="line">      configService.getConfig(dataId, group, <span class="number">5000</span>);</span><br><span class="line">      </span><br><span class="line">      configService.addListener(dataId, group, <span class="keyword">new</span> <span class="title class_">Listener</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">receiveConfigInfo</span><span class="params">(String configInfo)</span> &#123;</span><br><span class="line">          clearRoute();</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNullOrEmpty(configInfo)) &#123;<span class="comment">//配置被删除</span></span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            List&lt;RouteDefinition&gt; gatewayRouteDefinitions = JSONObject.parseArray(configInfo, RouteDefinition.class);</span><br><span class="line">            <span class="keyword">for</span> (RouteDefinition routeDefinition : gatewayRouteDefinitions) &#123;</span><br><span class="line">              addRoute(routeDefinition);</span><br><span class="line">            &#125;</span><br><span class="line">            publish();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;receiveConfigInfo error&quot;</span> + e);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Executor <span class="title function_">getExecutor</span><span class="params">()</span> &#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NacosException e) &#123;</span><br><span class="line">        log.error(<span class="string">&quot;dynamicRouteByNacosListener error&quot;</span> + e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">clearRoute</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (String id : ROUTE_LIST) &#123;</span><br><span class="line">      <span class="built_in">this</span>.routeDefinitionWriter.delete(Mono.just(id)).subscribe();</span><br><span class="line">    &#125;</span><br><span class="line">    ROUTE_LIST.clear();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">addRoute</span><span class="params">(RouteDefinition definition)</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      routeDefinitionWriter.save(Mono.just(definition)).subscribe();</span><br><span class="line">      ROUTE_LIST.add(definition.getId());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"> log.error(<span class="string">&quot;addRoute error&quot;</span> + e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1><span id="谓词-断言predicate12">谓词、断言（Predicate）[1][2]</span><a href="#谓词-断言predicate12" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2022/03/22/apiGatawaySpringGateway/Predicate.png" class title="Predicate">

<h1><span id="过滤器filter12">过滤器（Filter）[1][2]</span><a href="#过滤器filter12" class="header-anchor">#</a></h1><ul>
<li><p>生命周期</p>
<ul>
<li>PRE</li>
<li>POST</li>
</ul>
</li>
<li><p>作用范围</p>
<ul>
<li>GatewayFilter 局部过滤器 <ul>
<li>默认预定义<ul>
<li><strong>限流</strong></li>
</ul>
</li>
</ul>
</li>
<li>GlobalFilter 全局过滤器 <ul>
<li>自定义全局过滤器<ul>
<li><strong>统一鉴权过滤器</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="稳定性">稳定性</span><a href="#稳定性" class="header-anchor">#</a></h1><h3><span id="熔断降级-hystrix-3">熔断降级-Hystrix [3]</span><a href="#熔断降级-hystrix-3" class="header-anchor">#</a></h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">server.port:</span> <span class="number">8082</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">gateway</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">      <span class="attr">host:</span> <span class="string">localhost</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">password:</span> <span class="number">123456</span></span><br><span class="line">  <span class="attr">cloud:</span></span><br><span class="line">    <span class="attr">gateway:</span></span><br><span class="line">      <span class="attr">routes:</span> <span class="comment">##</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">rateLimit_route</span></span><br><span class="line">          <span class="attr">uri:</span> <span class="string">http://localhost:8000</span></span><br><span class="line">          <span class="attr">order:</span> <span class="number">0</span></span><br><span class="line">          <span class="attr">predicates:</span>  <span class="comment">##</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">Path=/test/**</span></span><br><span class="line">          <span class="attr">filters:</span>   <span class="comment">##</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">StripPrefix=1</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Hystrix</span></span><br><span class="line">              <span class="attr">args:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">fallbackCmdA</span></span><br><span class="line">                <span class="attr">fallbackUri:</span> <span class="string">forward:/fallbackA</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">hystrix.command.fallbackCmdA.execution.isolation.thread.timeoutInMilliseconds:</span> <span class="number">5000</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="流控和降级-sentinel-3">流控和降级-Sentinel [3]</span><a href="#流控和降级-sentinel-3" class="header-anchor">#</a></h3><h1><span id="高可用网关1">高可用网关[1]</span><a href="#高可用网关1" class="header-anchor">#</a></h1><p>Nginx负载均衡到部署的多个Gateway</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://spring.io/projects/spring-cloud-gateway">spring-cloud-gateway</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV11i4y1F7eu?p=8">2021最新(完整版)Gateway教学-第二代微服务网关组件SpringCloud-Gateway</a> *** V</p>
</li>
<li><p><a href="https://blog.csdn.net/a745233700/article/details/122917167">Spring Cloud Gateway 服务网关的部署与使用详细介绍</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/crazymakercircle/p/11704077.html">SpringCloud gateway （史上最全）</a> 尼恩 </p>
</li>
<li><p><a href="https://www.cnblogs.com/crazymakercircle/p/17436191.html">3W字吃透：微服务网关SpringCloud gateway底层原理和实操</a>   尼恩 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>API Gateway</tag>
      </tags>
  </entry>
  <entry>
    <title>API 网关-apisix</title>
    <url>/www6vHomeHexo/2022/03/22/apiGatawayApisix/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="apisix特性">apisix特性</span><a href="#apisix特性" class="header-anchor">#</a></h2><ul>
<li>Core<ul>
<li>api聚合</li>
<li>灰度发布</li>
</ul>
</li>
<li>稳定性<ul>
<li>服务熔断</li>
<li>故障注入</li>
<li>流量复制</li>
</ul>
</li>
<li>云原生<ul>
<li>多云，混合云</li>
<li>容器友好</li>
<li>随意扩缩容</li>
</ul>
</li>
</ul>
<h2><span id="apisix功能">apisix功能</span><a href="#apisix功能" class="header-anchor">#</a></h2><ul>
<li>动态配置，不用reload<br>路由, ssl证书，上游，插件…</li>
<li>插件化(40个)<br>身份验证, 安全, 日志, 可观察性…  </li>
<li>对接Prom，zipkin， skywalking</li>
<li>grpc代理和协议转换(rest &lt;-&gt; gprc)</li>
<li>apisix只用了nginx的网络层</li>
</ul>
<h2><span id="apisix使用场景">apisix使用场景</span><a href="#apisix使用场景" class="header-anchor">#</a></h2><ul>
<li>处理L4, L7层流量</li>
<li>代替nginx处理南北流量</li>
<li>代替envoy处理东西流量</li>
<li>k8s ingress controller</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1Gt4y1q7qC?vd_source=f6e8c1128f9f264c5ab8d9411a644036">【云原生学院 #3】基于 Apache APISIX 的全流量 API 网关</a> ***</li>
</ul>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>API Gateway</tag>
      </tags>
  </entry>
  <entry>
    <title>gRPC</title>
    <url>/www6vHomeHexo/2022/03/21/grpc/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="grpc">gRPC</span><a href="#grpc" class="header-anchor">#</a></h2><h5><span id="通信1">通信[1]</span><a href="#通信1" class="header-anchor">#</a></h5><ul>
<li>基于HTTP2连接，发送消息</li>
<li>protobuf做消息序列化</li>
</ul>
<h5><span id="http2-2">HTTP2 [2]</span><a href="#http2-2" class="header-anchor">#</a></h5><ul>
<li>优点<ul>
<li>头部压缩、多路复用   </li>
<li>stream,  frame</li>
</ul>
</li>
</ul>
<h5><span id="grpc的通信模式-1">gRPC的通信模式 [1]</span><a href="#grpc的通信模式-1" class="header-anchor">#</a></h5><ul>
<li>一元RPC模式</li>
<li>服务端 流式RPC模式</li>
<li>客户端 流式RPC模式</li>
<li>双向流RPC模式</li>
</ul>
<h2><span id="grpc">gRPC</span><a href="#grpc" class="header-anchor">#</a></h2><h5><span id="grpc-java-1">grpc-java [1]</span><a href="#grpc-java-1" class="header-anchor">#</a></h5><h5><span id="grpc-go-34">gRPC-go [3][4]</span><a href="#grpc-go-34" class="header-anchor">#</a></h5><details><summary>gRPC-go</summary><p><a href="https://github.com/grpc/grpc-go/blob/master/examples/route_guide/server/server.go">grpc-go server.go</a><br>&#x2F;&#x2F; Package main implements a simple gRPC server that demonstrates how to use gRPC-Go libraries<br>&#x2F;&#x2F; to perform unary, client streaming, server streaming and full duplex RPCs.</p>
</details>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://space.bilibili.com/359351574/channel/collectiondetail?sid=412936">IT老齐的gRPC实战课</a></li>
<li>《透视HTTP协议》《33 | 我应该迁移到HTTP&#x2F;2吗？》</li>
<li><a href="https://github.com/grpc/grpc-go/tree/master/examples">grpc-go examples</a></li>
<li><a href="https://www.bilibili.com/video/BV1ht41187Wh/">#17 grpc 开发及 grpcp 的源码分析 【 Go 夜读 】</a></li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>rpc</category>
      </categories>
      <tags>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Skywalking</title>
    <url>/www6vHomeHexo/2022/03/18/observabilitySkywalking/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="定位">定位</span><a href="#定位" class="header-anchor">#</a></h2><p>APM: Tracing + Metric</p>
<h2><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/03/18/observabilitySkywalking/arch.jpeg" class title="架构">

<h2><span id="agent">agent</span><a href="#agent" class="header-anchor">#</a></h2><ul>
<li>rpc<br>dubbo，springboot</li>
<li>mesh<br>istio, envoy  </li>
<li>中间件<br>nginx, kong<br>tomcat   </li>
<li>db<br>mysql</li>
<li>linux<br>ebpf  </li>
<li>k8s</li>
</ul>
<h2><span id="live-demo">live demo</span><a href="#live-demo" class="header-anchor">#</a></h2><p><a href="http://demo.skywalking.apache.org/general">live demo</a></p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1ZJ411s7Mn">java基础教程全面的深入学习Skywalking</a> 入门</li>
<li><a href="https://www.cnblogs.com/itxiaoshen/p/16513711.html">SkyWalking分布式系统应用程序性能监控工具-上</a></li>
<li><a href="https://skyapm.github.io/document-cn-translation-of-skywalking/zh/8.0.0/setup/service-agent/java-agent/">安装Java agent</a></li>
<li><a href="https://skywalking.apache.org/zh/2022-04-13-skywalking-in-autonomous-driving/">SkyWalking 在无人驾驶领域的实践</a> 未</li>
<li><a href="https://space.bilibili.com/390683219">ApacheSkyWalking</a>  bilibili</li>
<li><a href="https://skywalking.apache.org/zh/2020-08-13-cloud-native-academy/">[视频] 云原生学院 - 后分布式追踪时代的性能问题定位——方法级性能剖析</a> 吴晟 未</li>
</ul>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>tracing</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-网络</title>
    <url>/www6vHomeHexo/2022/03/15/aliCloudNetwork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="vpc的组件">VPC的组件</span><a href="#vpc的组件" class="header-anchor">#</a></h2><ul>
<li><p>vSwitch 虚拟交换机</p>
<ul>
<li>对应一个子网</li>
</ul>
</li>
<li><p>vRouter 虚拟路由器</p>
<ul>
<li>功能<ul>
<li>连接vpc内的各个交换机</li>
<li>连接vpc和其他网络的网关设备</li>
</ul>
</li>
<li>路由表<ul>
<li>默认路由 vs. 自定义路由</li>
<li>主路由表 vs. 子路由表<ul>
<li>主路由表<br>每个vpc一个主路由表</li>
<li>子路由表<br>子网&#x2F;交换机粒度</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="vpc的安全">VPC的安全</span><a href="#vpc的安全" class="header-anchor">#</a></h2><ul>
<li>网络ACL<ul>
<li>功能<ul>
<li>ACL与交换机绑定</li>
</ul>
</li>
<li>边界  <ul>
<li>子网间的ACL</li>
<li>vpc边界的ACL</li>
</ul>
</li>
<li>无状态</li>
</ul>
</li>
<li>安全组<ul>
<li>主机粒度</li>
<li>虚拟防火墙</li>
<li>有状态</li>
</ul>
</li>
</ul>
<h2><span id="nat网关">NAT网关</span><a href="#nat网关" class="header-anchor">#</a></h2><ul>
<li>VPC公网网关<ul>
<li>SNAT<br>NAT Gateway绑定EIP;<br>配置子网的SNAT规则;<br>ECS通过EIP访问公网;</li>
<li>DNAT<br>从公网访问ECS</li>
</ul>
</li>
</ul>
<h2><span id="vpn网关2">VPN网关[2]</span><a href="#vpn网关2" class="header-anchor">#</a></h2><ul>
<li>协议<ul>
<li>IP-Sec<br>IPSec-VPN低成本快速构建混合云</li>
<li>SSL<br>远程用户访问公司敏感数据的方案[远程办公场景]<br>简单、安全</li>
</ul>
</li>
</ul>
<h2><span id="高速通道2">高速通道[2]</span><a href="#高速通道2" class="header-anchor">#</a></h2><ul>
<li>基于运营商的专线接入公有云<br>大带宽，稳定， 安全，私有， 混合云</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><ol>
<li><a href="https://www.bilibili.com/video/BV1tD4y1977x">云上常见架构设计及优化</a> 阿里云 ***</li>
</ol>
</li>
<li><ol start="2">
<li>《云网络》 阿里云</li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE 总结</title>
    <url>/www6vHomeHexo/2022/03/13/sre/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="sre-1">SRE [1]</span><a href="#sre-1" class="header-anchor">#</a></h2><ul>
<li>SRE &#x3D; PE（Production Engineer） + 工具平台开发 + 稳定性平台开发<ul>
<li>工具平台团队，负责效能工具的研发，比如实现 CMDB、运维自动化、持续交付流水线以<br>及部分技术运营报表的实现，为基础运维和应用运维提供效率平台支持。</li>
<li>稳定性平台团队，负责稳定性保障相关的标准和平台，比如监控、服务治理相关的限流降<br>级、全链路跟踪、容量压测和规划。</li>
</ul>
</li>
</ul>
<h2><span id="组织架构图-1">组织架构图 [1]</span><a href="#组织架构图-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/03/13/sre/rolesInCompany.JPG" class title="组织架构图"> 

<h2><span id="故障复盘-2">故障复盘 [2]</span><a href="#故障复盘-2" class="header-anchor">#</a></h2><ul>
<li>黄金三问<ul>
<li>第一问：故障原因有哪些？</li>
<li>第二问：我们做什么，怎么做才能确保下次不会再出现类似故障？</li>
<li>第三问：当时如果我们做了什么，可以用更短的时间恢复业务？</li>
</ul>
</li>
<li>故障判定的三原则<ul>
<li>健壮性原则。</li>
<li>第三方默认无责。  </li>
<li>分段判定原则。</li>
</ul>
</li>
<li>5W 分析法</li>
</ul>
<h2><span id="google-sre-principle-5">Google SRE Principle [5]</span><a href="#google-sre-principle-5" class="header-anchor">#</a></h2><ul>
<li>运营是软件问题 </li>
<li>服务水平目标SLO</li>
<li>减少琐事<br>用自动化的方式减少琐事</li>
<li>自动化 <ul>
<li>** 统一环境， IaC CaC，  **<br>IaC: Terraform, Ansible, Pulumi</li>
<li>生产环境中进行测试</li>
<li><strong>统一</strong> 版本管理，制品库，cmdb</li>
<li>可观测性</li>
</ul>
</li>
<li>降低失败成本<br>复盘  从失败中学习</li>
<li>共享所有权<br>个人安全，责任共担</li>
<li>拥抱风险 +<br>错误预算</li>
</ul>
<h2><span id="google-sre-实践总结-5">Google SRE 实践总结 [5]</span><a href="#google-sre-实践总结-5" class="header-anchor">#</a></h2><ul>
<li>确保长期关注研发工作 </li>
<li><strong>在保障SLO的前提下</strong>最大化迭代速度 </li>
<li>监控系统 +<br>insight，根因</li>
<li>应急事件处理 </li>
<li>变更管理<br>ITIL</li>
<li>需求预测和容量规划 +<br>AIOps</li>
<li>资源部署 </li>
<li>效率与性能</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《09｜案例：互联网典型的SRE组织架构是怎样的？》  赵成</li>
<li>《08｜故障复盘：黄金三问与判定三原则》  赵成</li>
<li>xxx</li>
<li><a href="https://www.kawabangga.com/posts/4481">SRE 的工作介绍</a>   SRE大佬  未 </li>
<li><a href="https://www.bilibili.com/video/BV1i84y147Xe/">SRE核心概念与可观测性介绍</a>   中国DevOps社区 刘峰</li>
</ol>
<p>+《SRE google 运维解密》</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/2010397">《SRE google 运维解密》读书笔记 （一）</a> 未</li>
<li><a href="https://cloud.tencent.com/developer/article/2010401">《SRE google 运维解密》读书笔记 （二）</a> 未</li>
<li><a href="https://cloud.tencent.com/developer/article/2010405">《SRE google 运维解密》读书笔记 （三）</a></li>
<li><a href="https://cloud.tencent.com/developer/article/2010408">《SRE google 运维解密》读书笔记 （四）</a></li>
</ul>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>sre</category>
      </categories>
      <tags>
        <tag>sre</tag>
      </tags>
  </entry>
  <entry>
    <title>容量保障与全链路压测</title>
    <url>/www6vHomeHexo/2022/03/13/capacityGuarantee/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="容量保障overview2">容量保障Overview[2]</span><a href="#容量保障overview2" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/03/13/capacityGuarantee/capacityGuarantee.JPG" class>

<ul>
<li>大促容量保障的三项重点工作：<ul>
<li><a href="#%E5%AE%B9%E9%87%8F%E9%A2%84%E6%B5%8B">大促流量预估</a></li>
<li><a href="#%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B">大促容量测试</a></li>
<li>大促容量保障预案</li>
</ul>
</li>
</ul>
<h2><span id="容量预测">容量预测</span><a href="#容量预测" class="header-anchor">#</a></h2><h5><span id="容量预测1">容量预测[1]</span><a href="#容量预测1" class="header-anchor">#</a></h5><ul>
<li><p>我首先给出了<strong>“皮尔逊相关系数”</strong>这个工具，对服务 TPS 和 CPU 利用率之间的相关度进<br>行了定量分析，根据相关度的强弱，分别采取不同策略。其中，重点讲到了在两者弱相关<br>时的应对策略，如果能够穷举出尽可能多的相关特征，可以通过特征选取的方式对服务进<br>行画像，提升预测准确率；如果特征非常难找，那么可以依靠概率表的方式曲线救国。</p>
</li>
<li><p>随着服务不断迭代，容量也在不断变化，我与你分析的第二个问题，就是如何平衡好服务<br>迭代和容量预测频率的关系。根据服务发布窗口（或其他变更时间点）建立滑动窗口机<br>制，既保证了在服务变更后能够尽快地更新模型，又不至于带来大量的计算量，是一个不<br>错的实践方式。</p>
</li>
<li><p>业务场景变化也会导致容量变化，针对这个问题，我结合之前提到的全链路压测工作，通<br>过建立全链路压测和容量预测双向校准的机制，提前对变化的业务场景进行预测，识别容<br>量风险。</p>
</li>
</ul>
<h2><span id="全链路压测">全链路压测</span><a href="#全链路压测" class="header-anchor">#</a></h2><h5><span id="核心功能4">核心功能[4]</span><a href="#核心功能4" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/03/13/capacityGuarantee/stressTest.png" class title="全链路压测">

<h5><span id="压测-部署架构">压测-部署架构</span><a href="#压测-部署架构" class="header-anchor">#</a></h5><ul>
<li><p>施压机的分布[3]</p>
<ul>
<li>大部分仍然是跟线上系统在<strong>同机房内</strong>，少量会在公有云节点上</li>
<li>以将全球（主要是国内）的 <strong>CDN 节点</strong>作为施压机<ul>
<li>更加真实地模拟真实用户从全球节点进入的真实访问流量</li>
<li>成本过高，技术条件和细节难</li>
</ul>
</li>
</ul>
</li>
<li><p>压测的读写流量[3]</p>
<ul>
<li>读流量</li>
<li>写流量 <ul>
<li>对压测的写请求做专门的<strong>标记</strong>。 </li>
<li>当请求要写数据库时，由分布式数据库的<strong>中间件框架中的逻辑来判断这个请求是否是压测请求</strong>，如果是压测写请求则路由到对应的<strong>影子库</strong>中，而不是直接写到线上正式的库中。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="改造-全链路压测5">改造 全链路压测[5]</span><a href="#改造-全链路压测5" class="header-anchor">#</a></h5><ul>
<li>数据隔离<br> 物理隔离  vs. 逻辑隔离 [见表1]     </li>
<li>中间件改造<br>  eg. MQ改造<br> ​     Producer:  判断请求带压测标识，转换到数据体（msg）中<br> ​     Consumer:  判断数据（msg）中有压测标识，  恢复压测标识至请求中 </li>
<li>应用服务改造<ul>
<li>绕开限制逻辑</li>
<li>数据隔离前置</li>
<li>Mock 逻辑</li>
</ul>
</li>
</ul>
<p>​                                                                              <strong>表1   逻辑隔离 vs. 物理隔离</strong></p>
<table>
<thead>
<tr>
<th>隔离类型</th>
<th>逻辑隔离</th>
<th>物理隔离</th>
</tr>
</thead>
<tbody><tr>
<td>中间件改造</td>
<td>小，几乎不需要改造</td>
<td>大，需要保证压测流量标识能一路透传不丢失</td>
</tr>
<tr>
<td>业务侵入性</td>
<td>大，会影响表结构设计</td>
<td>小， 对数据实体没有侵入</td>
</tr>
<tr>
<td>数据清洗难度</td>
<td>大， 需根据每个数据实体的标识单独定制清洗规则</td>
<td>小，压测数据都在影子表</td>
</tr>
<tr>
<td>可扩展性</td>
<td>弱， 新数据实体均需要设计新的压测标识</td>
<td>强， 流量标识为统一形式， 且与数据无关</td>
</tr>
<tr>
<td>安全性</td>
<td>弱， 与真实数据写入同一张表， 一旦隔离逻辑有疏漏， 会影响真实用户</td>
<td>强， 与真实数据分开存储， 即便考虑不周 ，也不会影响真实数据</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《09 | 容量预测（下）：为不同服务“画像”，提升容量预测准确性》  吴骏龙</li>
<li>《 13 | 大促容量保障体系建设：怎样做好大促活动的容量保障工作（下）》 吴骏龙   </li>
<li><a href="https://zhuanlan.zhihu.com/p/149538568">稳定性实践：容量规划之压测系统建设</a>  极客时间？</li>
<li><a href="https://blog.csdn.net/u013256816/article/details/123414839">全链路压测体系建设方案的思考与实践</a>  阿里 *** </li>
<li>《05 | 全链路压测：系统整体容量保障的“核武器”（上）》 吴骏龙</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>容量保障</category>
      </categories>
      <tags>
        <tag>容量保障</tag>
      </tags>
  </entry>
  <entry>
    <title>安全产品</title>
    <url>/www6vHomeHexo/2022/03/12/cyberSecurityTool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="安全防御工具-软件">安全防御工具-软件</span><a href="#安全防御工具-软件" class="header-anchor">#</a></h2><ul>
<li><p>安全防御工具[7]</p>
<img src="/www6vHomeHexo/2022/03/12/cyberSecurityTool/security-product.JPG" class title="安全防御工具">  
</li>
<li><p>防火墙[5]</p>
<img src="/www6vHomeHexo/2022/03/12/cyberSecurityTool/firewall.JPG" class title="防火墙">
</li>
<li><p>入侵检测-IDS[4]</p>
<img src="/www6vHomeHexo/2022/03/12/cyberSecurityTool/IDS.JPG" class title="入侵检测-IDS"></li>
</ul>
<h2><span id="安全产品-硬件h3c6">安全产品-硬件[H3C][6]</span><a href="#安全产品-硬件h3c6" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/03/12/cyberSecurityTool/securityProduct.JPG" class title="安全产品">  


<ul>
<li><p>网络层</p>
<ul>
<li>防火墙</li>
<li>AFC<br>抗DDoS</li>
<li>NGFW 下一代防火墙<br>应用层检查</li>
<li>NFV 软件<ul>
<li>用在IDC&#x2F;云安全中</li>
<li>vLB</li>
<li>vFW</li>
</ul>
</li>
<li>网管</li>
</ul>
</li>
<li><p>应用层</p>
<ul>
<li>IPS 入侵检测</li>
<li>应用控制<br>上网行为管理</li>
<li>负载均衡<ul>
<li>服务器负载均衡</li>
<li>链路负载均衡</li>
<li>全局负载均衡</li>
</ul>
</li>
<li>WAF<br>web防火墙</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="4">
<li>《21 | IDS：当黑客绕过了防火墙，你该如何发现？》 何为舟</li>
<li>《19 | 防火墙：如何和黑客“划清界限”？》 何为舟</li>
<li><a href="https://www.bilibili.com/video/BV1Lb4y127a8?p=22">网络安全</a></li>
<li>《模块串讲（三）丨安全防御工具：如何选择和规划公司的安全防御体系？》何为舟</li>
</ol>
]]></content>
      <categories>
        <category>安全</category>
        <category>安全产品</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式 数据库 比较</title>
    <url>/www6vHomeHexo/2022/03/11/distributedDatabaseCompare/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="分布式-数据库-比较-1">分布式 数据库 比较 [1]</span><a href="#分布式-数据库-比较-1" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th></th>
<th>PolarDB-X</th>
<th>TiDB</th>
<th>CockroachDB</th>
<th>Spanner</th>
</tr>
</thead>
<tbody><tr>
<td>分布式事务</td>
<td>MVCC+TSO</td>
<td>MVCC+TSO [Percolator][5][6]</td>
<td>MVCC+HLC[2][3][4]</td>
<td>MVCC+TrueTime API</td>
</tr>
<tr>
<td>存储引擎</td>
<td>InnoDB&#x2F;x-engine<br> HEX2(列存)</td>
<td>RocksDB<br>TiFlash(列存)</td>
<td>RocksDB</td>
<td>Colossus</td>
</tr>
<tr>
<td>高可用</td>
<td>计算无状态集群<br>存储Multi-Paxos</td>
<td>计算无状态集群<br>存储Raft</td>
<td>计算存储一体化 集群化 <br>存储Raft</td>
<td>计算存储一体化 集群化 <br>存储Multi-Paxos</td>
</tr>
<tr>
<td>数据分区</td>
<td>Hash&#x2F;List&#x2F;Range</td>
<td>Range&#x2F;Hash</td>
<td>Range&#x2F;Hash</td>
<td>Range&#x2F;Hash</td>
</tr>
<tr>
<td>全局索引</td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>HTAP</td>
<td>强隔离 强一致<br>MPP并行</td>
<td>强隔离 强一致<br>Spark</td>
<td>混合负载<br> MPP并行</td>
<td>混合负载<br> MPP并行</td>
</tr>
<tr>
<td>生态兼容性</td>
<td>基本兼容MySQL</td>
<td>基本兼容MySQL</td>
<td>基本兼容PGSQL</td>
<td>非标准SQL</td>
</tr>
<tr>
<td>元数据DDL</td>
<td>全局一致 + Online</td>
<td>全局一致 + Online</td>
<td>全局一致 + Online</td>
<td>全局一致 + Online</td>
</tr>
<tr>
<td>全局日志</td>
<td>√</td>
<td>√</td>
<td>√（企业版）</td>
<td>×</td>
</tr>
<tr>
<td>备份恢复</td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《云数据库架构》 2.3.1</li>
<li><a href="https://www.modb.pro/db/84156">CockroachDB分布式事务解密(一)：CockroachDB &amp; HLC</a><br>CockroachDB使用了一个软件实现的基于NTP时钟同步的混合逻辑时钟算法(Hybrid Logic Clock)——HLC追踪系统中事务的的hb关系(happen before)。</li>
<li><a href="https://www.modb.pro/db/84153">CockroachDB事务解密(二)：事务模型</a></li>
<li><a href="https://github.com/cockroachdb/cockroach/blob/master/pkg/util/hlc/hlc.go">Cockroach  HLC</a></li>
<li><a href="/www6vHomeHexo/2023/04/10/tikvMVCCTransaction/" title="TiKV Transaction-MVCC+TSO">TiKV Transaction-MVCC+TSO</a></li>
<li><a href="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/" title="分布式数据库-全局时钟">分布式数据库-全局时钟</a>  self</li>
<li><a href="/www6vHomeHexo/2023/06/05/globalSecondaryIndex/" title="全局二级索引-GSI">全局二级索引-GSI</a> self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 基础</title>
    <url>/www6vHomeHexo/2022/03/08/golangBasic/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rumtime-3">Rumtime [3]</a><ul>
<li><a href="#scheduler">Scheduler</a></li>
<li><a href="#netpoll">Netpoll</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#gc">GC</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80">基础</a><ul>
<li><a href="#%E5%86%85%E5%BB%BA%E5%87%BD%E6%95%B0">内建函数</a><ul>
<li><a href="#new-vs-make">New vs Make</a></li>
</ul>
</li>
<li><a href="#%E5%80%BC%E4%BC%A0%E9%80%92-%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92">值传递、引用传递</a></li>
<li><a href="#select-vs-switch">select vs. switch</a></li>
<li><a href="#method-function">Method &amp;  Function</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="rumtime-3">Rumtime [3]</span><a href="#rumtime-3" class="header-anchor">#</a></h1><h3><span id="scheduler">Scheduler</span><a href="#scheduler" class="header-anchor">#</a></h3><p>GMP</p>
<h3><span id="netpoll">Netpoll</span><a href="#netpoll" class="header-anchor">#</a></h3><h3><span id="memory">Memory</span><a href="#memory" class="header-anchor">#</a></h3><h3><span id="gc">GC</span><a href="#gc" class="header-anchor">#</a></h3><h1><span id="基础">基础</span><a href="#基础" class="header-anchor">#</a></h1><h3><span id="内建函数">内建函数</span><a href="#内建函数" class="header-anchor">#</a></h3><ul>
<li>init<ul>
<li>make()</li>
<li>new()</li>
</ul>
</li>
<li>slice  <ul>
<li>append()<br>用于切片(slice) 追加元素</li>
<li>copy()<br>只能用于数组切片内容赋值</li>
</ul>
</li>
<li>size <ul>
<li>len()<br> 计算数组(包括数组指针)、切片(slice)、map、channel、字符串等数据类型的长度</li>
<li>cap()<br> 返回指定类型的容量</li>
</ul>
</li>
<li>print() &amp; println()</li>
<li>resource reclaim<ul>
<li>close()</li>
<li>delete()</li>
</ul>
</li>
<li>num<ul>
<li>complex()</li>
<li>real() &amp; imag()</li>
</ul>
</li>
<li>error handle<ul>
<li>panic()</li>
<li>recover()</li>
</ul>
</li>
<li>others [6]<ul>
<li>clear()<br> 对于map对象: clear函数清空map对象的所有的元素<br>对于slice对象： clear函数将所有的元素设置为元素类型的零值，长度不变，容量不变</li>
<li>min()  &amp;&amp;  max()</li>
</ul>
</li>
</ul>
<h5><span id="new-vs-make">New vs Make</span><a href="#new-vs-make" class="header-anchor">#</a></h5><ul>
<li>new和make是内置函数,主要用来分配内存空间</li>
<li>make<br>make 仅用于 slice、map和 channel 的初始化，返回值为类型本身，而不是指针</li>
<li>new<br>  只用于内存分配，且把内存清零<br>  返回一个指向对应类型零值的指针<br>  new() 一般 显示返回指针</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">make</span><span class="params">(t Type, size ...IntegerType)</span></span> Type </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">new</span><span class="params">(Type)</span></span> *Type </span><br></pre></td></tr></table></figure>

<p><a href="https://www.golangroadmap.com/class/gointerview/1-9.html">9.Go 内置函数make和new的区别？</a></p>
<h3><span id="值传递-引用传递">值传递、引用传递</span><a href="#值传递-引用传递" class="header-anchor">#</a></h3><ul>
<li><p>Go语言中所有的传参都是<strong>值传递（传值），都是一个副本，一个拷贝</strong>。</p>
</li>
<li><p>是否可以修改原内容数据，和传值、传引用没有必然的关系。在C++中，传引用肯定是可以修改原内容数据的，在Go语言里，虽然只有传值，但是我们也可以修改原内容数据，因为参数是引用类型</p>
</li>
<li><p>引用类型和引用传递是2个概念，切记！！！</p>
</li>
<li><p><strong>类型</strong></p>
<ul>
<li>非引用类型（int、string、struct等这些）</li>
<li>引用类型（指针、map、slice、chan等这些）</li>
</ul>
</li>
<li><p><strong>值传递</strong></p>
<ul>
<li>将实参的值传递给形参，形参是实参的一份拷贝，实参和形参的内存地址不同。函数内对形参值内容的修改，是否会影响实参的值内容，取决于参数是否是引用类型</li>
<li>参数如果是非引用类型（int、string、struct等这些），这样就在函数中就无法修改原内容数据；如果是引用类型（指针、map、slice、chan等这些），这样就可以修改原内容数据。</li>
</ul>
</li>
<li><p>引用传递<br>Go语言是没有引用传递的<br>在C++中，函数参数的传递方式有引用传递</p>
</li>
</ul>
<p><a href="https://www.golangroadmap.com/class/gointerview/1-7.html">Go 函数参数传递到底是值传递还是引用传递？</a>  </p>
<h3><span id="select-vs-switch">select vs. switch</span><a href="#select-vs-switch" class="header-anchor">#</a></h3><ul>
<li>select只能应用于channel的操作<br>则会随机选取其中一个满足条件的分支</li>
<li>switch 分支是顺序执行的</li>
</ul>
<h3><span id="method-amp-function">Method &amp;  Function</span><a href="#method-amp-function" class="header-anchor">#</a></h3><ul>
<li>Method<br>方法(Method)是一个带有receiver的函数Function</li>
<li>Receiver[5]</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.golangroadmap.com/class/gointerview/">GOLANG ROADMAP</a><br> <a href="https://www.golangroadmap.com/">GOLANG ROADMAP</a><br> 邀请码：caspar<br> 邀请码：Gopher-10645-1382</li>
<li>极客时间 《Go 并发编程实战课》  鸟窝</li>
<li>《04 Go 程序是怎么跑起来的》</li>
<li>xxx</li>
<li><a href="https://zhuanlan.zhihu.com/p/522568859">golang 方法接收者</a>  mycode-go</li>
<li><a href="https://colobu.com/2023/05/28/go1-21-whats-new-builtin/">你知道吗？Go新增加了三个内建函数 </a></li>
<li><a href="https://cloud.tencent.com/developer/article/2329450">Golang高性能编程实践</a> *** 未</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Rumtime-GMP</title>
    <url>/www6vHomeHexo/2022/03/08/golangGMP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#goroutine%E7%9A%84%E8%B0%83%E5%BA%A6">Goroutine的调度</a><br>- <a href="#%E7%BB%84%E4%BB%B6">组件</a><br>- <a href="#%E5%9C%BA%E6%99%AF">场景</a><br>- <a href="#%E7%94%9F%E4%BA%A7%E7%AB%AF-2">生产端 [2]</a><br>- <a href="#%E6%B6%88%E8%B4%B9%E7%AB%AF-2">消费端 [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="goroutine的调度">Goroutine的调度</span><a href="#goroutine的调度" class="header-anchor">#</a></h2><ul>
<li>Overview<img src="/www6vHomeHexo/2022/03/08/golangGMP/goroutine.JPG" class title="goroutine的调度">
<strong>System Thread</strong> : kernel entity<br><strong>Processor</strong> : go实现的协程处理器<br><strong>Goroutine</strong> : 协程</li>
</ul>
<h5><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h5><ul>
<li><p>组件<br>G:  goroutine, 一个计算任务。 由需要<strong>执行的代码和其上下文组成</strong>, 上下文包括:  当前代码位置,  栈顶、 栈底地址，状态等。<br>M: machine,  <strong>系统线程</strong>, 执行实体,  想要在CPU上执行代码, 必须有线程<br>P:  processor, <strong>虚拟处理器</strong>,  <strong>M必须获得P才能执行代码</strong>, 否则必须陷入休眠(后台监控线程除外).</p>
</li>
<li><p>组件之间的关系<br>Processor在不同的系统线程里, 每个Processor挂载着一个协程队列，Processor依次调用Gorouine，<br>只有一个Gorouine是正在运行状态的。</p>
</li>
<li><p>P 和 M 的个数问题  [1]</p>
<ul>
<li>P 的数量<br>在程序执行的<strong>任意时刻</strong>都只有 <strong>$GOMAXPROCS 个</strong> goroutine 在同时运行</li>
<li>M 的数量<br>go 程序启动时，会设置 M 的最大数量，默认 10000<br><strong>一个 M 阻塞了，会创建新的 M。</strong></li>
</ul>
<p>M 与 P 的数量没有绝对关系，<strong>一个 M 阻塞，P 就会去创建或者切换另一个 M</strong>，所以，即使 P 的默认数量是 1，也有可能会创建很多个 M 出来。</p>
</li>
<li><p>P 和 M 何时会被创建  [1]</p>
<ul>
<li>P 何时创建<br>在确定了 P 的最大数量 n 后，运行时系统会根据这个数量创建 n 个 P。</li>
<li>M 何时创建<br><strong>没有足够的 M 来关联 P 并运行其中的可运行的 G。</strong> 比如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，而没有空闲的，就会去创建新的 M</li>
</ul>
</li>
<li><p>看参考文档里的图</p>
<img src="/www6vHomeHexo/2022/03/08/golangGMP/GMP.jpg" class title="GMP">

</li>
<li><p>协程跟线程是有区别的，线程由 CPU 调度是抢占式的，协程由用户态调度是协作式的，一个协程让出 CPU 后，才执行下一个协程。</p>
</li>
</ul>
<h5><span id="场景">场景</span><a href="#场景" class="header-anchor">#</a></h5><ul>
<li><p><strong>处理阻塞[能被runtime拦截到的阻塞-非阻塞]</strong>  [2]</p>
<ul>
<li>在线程发生阻塞的时候，不会无限制的创建线程</li>
<li>以下这些情况,  会把goroutine挂起，让g先进某个数据结构，待ready后再继续执行<br>不会占用线程， 线程会进入schedule，继续消费队列，执行其他的g<img src="/www6vHomeHexo/2022/03/08/golangGMP/feizusai.jpg" class></li>
</ul>
</li>
<li><p><strong>处理阻塞[不能被runtime拦截到的阻塞-阻塞]</strong>  [2]</p>
<ul>
<li>两种情况<ul>
<li>CGO</li>
<li>阻塞在syscall上时,  必须占用一个线程</li>
</ul>
</li>
<li><strong>sysmon</strong>,  在专有线程中执行, 不需要绑定P就可以执行<ul>
<li>checkdead<br>常见误解 这个可以检查死锁</li>
<li>netpoll<br>inject g list to global runqueue</li>
<li>retake<br>如果syscall卡了很久，就把p剥离(<strong>handoffp</strong>)  #1<br>如果用户g运行了很久, 就发信号SIGURG抢占</li>
</ul>
</li>
</ul>
</li>
<li><p>case</p>
<ul>
<li>case1<br><strong>一个协程运行时间运行的特别长</strong>。 守护线程计数， 如果processor完成的goroutine数量一直不变。在这个协程的任务栈里插入一个中断的标记，读到这个中断标记后，goroutine会把自己中断下来插入到等待队列的队尾。</li>
<li>case2<br><strong>某个协程被io中断后</strong>， Processer会把自己移动到另一个可用的系统线程当中， 执行队列里的任务。<br>当这个协程io中断被唤醒后，会把自己加入到某个Processor的队列里， 或者加入到全局的等待队列里。</li>
</ul>
</li>
<li><p>调度器的设计策略 [1]</p>
<ul>
<li>work stealing 机制</li>
<li>hand off 机制  #1</li>
</ul>
</li>
</ul>
<h5><span id="生产端-2">生产端 [2]</span><a href="#生产端-2" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/03/08/golangGMP/producer.JPG" class>

<h5><span id="消费端-2">消费端 [2]</span><a href="#消费端-2" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/03/08/golangGMP/consumer.JPG" class>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/qq_44205272/article/details/111565957">Golang的GMP原理与调度</a> ***</li>
<li>《04 Go 程序是怎么跑起来的》  *** </li>
<li><a href="https://www.bilibili.com/video/BV1oE411y7qG/">#64 深入浅出 Golang Runtime 【 Go 夜读 】</a>  goroutine ,  go network , gc *** 未</li>
<li><a href="https://www.bilibili.com/video/BV19r4y1w7Nx?spm_id_from=333.880.my_history.page.click">Golang深入理解GPM模型</a>  *** 未</li>
</ol>
]]></content>
      <categories>
        <category>Golang</category>
        <category>GMP</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP1-上云迁移</title>
    <url>/www6vHomeHexo/2022/03/03/tencentTCP1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-上云迁移10">一、上云迁移（10%）</span><a href="#一-上云迁移10" class="header-anchor">#</a></h2><h3><span id="1上云迁移背景">1.上云迁移背景</span><a href="#1上云迁移背景" class="header-anchor">#</a></h3><h5><span id="11-上云迁移的相关概念">1.1 上云迁移的相关概念</span><a href="#11-上云迁移的相关概念" class="header-anchor">#</a></h5><p>掌握上云迁移相关概念，包括：主要是迁移内容（应用、文件、数据库）、非结构化数据、结构化数据、上云迁移模式等	☆</p>
<h5><span id="12-上云迁移的原因">1.2 上云迁移的原因</span><a href="#12-上云迁移的原因" class="header-anchor">#</a></h5><p>了解传统企业架构面临的问题和云上架构的优势	☆</p>
<h5><span id="13-上云迁移的目的">1.3 上云迁移的目的</span><a href="#13-上云迁移的目的" class="header-anchor">#</a></h5><p>了解传统架构的特点；云上架构的特点；云上架构如何解决传统架构的问题。掌握传统资源和云上资源的差异，上云后应用架构、文件存储、数据库出现的变化	☆☆</p>
<h3><span id="2上云迁移评估">2.上云迁移评估</span><a href="#2上云迁移评估" class="header-anchor">#</a></h3><h5><span id="21-业务上云初步评估">2.1 业务上云初步评估</span><a href="#21-业务上云初步评估" class="header-anchor">#</a></h5><p>“判断业务是否适合直接上云、适合改造上云还是不适合上云”	☆☆</p>
<h5><span id="22-具体信息评估">2.2 具体信息评估</span><a href="#22-具体信息评估" class="header-anchor">#</a></h5><p>“对应用的各个指标进行评估：计算、网络、存储、数据库、应用等”	☆☆☆</p>
<h5><span id="23-业务评估">2.3 业务评估</span><a href="#23-业务评估" class="header-anchor">#</a></h5><p>对系统重要性、部署模式和业务状况进行评估	☆☆</p>
<h3><span id="3上云迁移流程">3.上云迁移流程</span><a href="#3上云迁移流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/03/03/tencentTCP1/migrate-process.jpg" class>

<h5><span id="31-系统调研">3.1 系统调研</span><a href="#31-系统调研" class="header-anchor">#</a></h5><p>包括：业务调研、系统架构调研、数据库调研和应用调研	☆☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/survey-1.jpg" class>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/survey-2.jpg" class>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/survey-3.jpg" class>

<h5><span id="32-风险评估">3.2 风险评估</span><a href="#32-风险评估" class="header-anchor">#</a></h5><p>“包括：平台兼容性评估、性能风险评估、系统改造风险评估和资源风险评估”	☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/risk.jpg" class>

<h5><span id="33-方案设计">3.3 方案设计</span><a href="#33-方案设计" class="header-anchor">#</a></h5><p>“包括：网络架构设计、云上运维管理架构设计、应用架构设计、应用改造设计和容量规划”	☆☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/solution1.jpg" class>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/solution2.jpg" class>

<h5><span id="34-基础架构环境搭建">3.4 基础架构环境搭建</span><a href="#34-基础架构环境搭建" class="header-anchor">#</a></h5><p>包括：系统架构、网络架构、数据库、应用程序搭建	☆☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/infra.jpg" class>

<h5><span id="35-系统割接">3.5 系统割接</span><a href="#35-系统割接" class="header-anchor">#</a></h5><p>“系统割接的内容及风险；系统割接的方法：一次性切换、灰度割接”	☆☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/cutover.jpg" class>

<h5><span id="36-性能和功能测试">3.6 性能和功能测试</span><a href="#36-性能和功能测试" class="header-anchor">#</a></h5><p>性能测试和功能测试的指标	☆☆</p>
<img src="/www6vHomeHexo/2022/03/03/tencentTCP1/perf.jpg" class>

<h3><span id="4迁移工具">4.迁移工具</span><a href="#4迁移工具" class="header-anchor">#</a></h3><h5><span id="41-迁移服务平台msp">4.1 迁移服务平台MSP</span><a href="#41-迁移服务平台msp" class="header-anchor">#</a></h5><p>迁移服务平台MSP基本概念、功能原理	☆☆</p>
<h5><span id="42-msp迁移工具">4.2 MSP迁移工具</span><a href="#42-msp迁移工具" class="header-anchor">#</a></h5><p>数据库迁移工具：DTS；文件迁移工具：COS本地上传、 COS在线迁移；主机迁移工具：对主机建立镜像，从镜像创建CVM；大数据迁移工具：HDFS_TO_COS；离线迁移工具：CDM_M30、CDM_L80	☆☆☆</p>
<h3><span id="5上云迁移的内容">5.上云迁移的内容</span><a href="#5上云迁移的内容" class="header-anchor">#</a></h3><h5><span id="51-数据库迁移">5.1 数据库迁移</span><a href="#51-数据库迁移" class="header-anchor">#</a></h5><p>数据库迁移场景、方式和工具使用	☆☆☆</p>
<h5><span id="52-文件迁移">5.2 文件迁移</span><a href="#52-文件迁移" class="header-anchor">#</a></h5><p>文件迁移场景、方式和工具使用	☆☆☆</p>
<h5><span id="53-主机迁移">5.3 主机迁移</span><a href="#53-主机迁移" class="header-anchor">#</a></h5><p>主机迁移场景、方式和工具使用	☆☆☆</p>
<h5><span id="54-大数据迁移">5.4 大数据迁移</span><a href="#54-大数据迁移" class="header-anchor">#</a></h5><p>大数据迁移的方法	☆☆</p>
<h3><span id="6全量迁移">6.全量迁移</span><a href="#6全量迁移" class="header-anchor">#</a></h3><h5><span id="61-全量迁移的概述">6.1 全量迁移的概述</span><a href="#61-全量迁移的概述" class="header-anchor">#</a></h5><p>全量迁移的优缺点及适用场景	☆☆</p>
<h5><span id="62-全量迁移流程">6.2 全量迁移流程</span><a href="#62-全量迁移流程" class="header-anchor">#</a></h5><p>掌握全量迁移的流程	☆☆☆</p>
<h3><span id="7平滑迁移">7.平滑迁移</span><a href="#7平滑迁移" class="header-anchor">#</a></h3><h5><span id="71-平滑迁移概述">7.1 平滑迁移概述</span><a href="#71-平滑迁移概述" class="header-anchor">#</a></h5><p>平滑迁移的优缺点及适用场景	☆☆</p>
<h5><span id="72-平滑迁移流程">7.2 平滑迁移流程</span><a href="#72-平滑迁移流程" class="header-anchor">#</a></h5><p>掌握平滑迁移的流程	☆☆☆</p>
<h5><span id="73-全量迁移和平滑迁移的差异">7.3 全量迁移和平滑迁移的差异</span><a href="#73-全量迁移和平滑迁移的差异" class="header-anchor">#</a></h5><p>“全量迁移处理增量数据的方法，平滑迁移处理增量数据的方法”	☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>系统设计 总结</title>
    <url>/www6vHomeHexo/2022/03/02/systemDesign/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<table>
<thead>
<tr>
<th></th>
<th>特点</th>
<th>设计</th>
</tr>
</thead>
<tbody><tr>
<td>Feed流</td>
<td>读写比例100:1<br> 消息必达性要求高<br> 非稳定的账号关系</td>
<td><a href="/www6vHomeHexo/2019/09/13/feed/" title="Feed流 总结">Feed流 总结</a></td>
</tr>
<tr>
<td>秒杀系统</td>
<td></td>
<td><a href="/www6vHomeHexo/2018/05/21/secKillSummary/" title="秒杀系统总结">秒杀系统总结</a>  <br>  <a href="/www6vHomeHexo/2018/05/06/seckill/" title="秒杀系统和商品详情页系统(培训讲义)">秒杀系统和商品详情页系统(培训讲义)</a></td>
</tr>
<tr>
<td>计数系统[2]</td>
<td>数据量巨大<br>  访问量大，性能要求高<br>对于可用性、数字的准确性要求高</td>
<td>+ 方案1 数据库 + 缓存 -&gt; 按照 weibo_id做分库分表。数据库和缓存之间无法保证数据的一致性 [2]<br>+方案2  全部写入redis -&gt; MQ异步写，批量合并写。redis昂贵， 存储优化，冷热分离 [2]</td>
</tr>
<tr>
<td>抢红包系统[3]</td>
<td>交易类信息:红包发、抢、拆、详情列表 <br> 展示类信息: 收红包列表、发红包列表</td>
<td>+ 架构设计，理论基础是快慢分离。红包入账是一个分布事务，属于慢接口。而拆红包凭证落地则速度快 [3]<br>+ 高并发   set化， 局部化-控制同一红包并发个数</td>
</tr>
<tr>
<td>排行榜系统</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><h5><span id="计数系统">计数系统</span><a href="#计数系统" class="header-anchor">#</a></h5><ol>
<li><p><a href="https://mp.weixin.qq.com/s/cCnPGRQ6LZHg8-7FRJLaKQ">计数系统架构实践一次搞定 | 架构师之路 </a>  未</p>
</li>
<li><p>&lt;&lt;37丨计数系统设计（一）：面对海量数据的计数器要如何做？&gt;&gt;  唐扬</p>
</li>
</ol>
<h5><span id="红包系统">红包系统</span><a href="#红包系统" class="header-anchor">#</a></h5><ol start="3">
<li><p><a href="https://blog.csdn.net/code52/article/details/51168854">揭秘微信红包架构、抢红包算法和高并发和降级方案</a>  微信红包</p>
<p><a href="https://blog.csdn.net/itfly8/article/details/100890926">揭秘微信红包：架构、抢红包算法、高并发和降级方案</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/1637408">微信红包后台系统设计</a> 未</p>
</li>
<li><p><a href="https://melonshell.github.io/2020/01/23/tech2_wx_red_packet/">微信高并发资金交易系统设计方案—–百亿红包背后的技术支撑</a> </p>
<p>SET化、请求排队串行化、双维度分库表 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统设计</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>feed</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式系统 模式</title>
    <url>/www6vHomeHexo/2022/03/01/distributedSystemPattern/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="fault-tolerant-consensus">Fault Tolerant Consensus</span><a href="#fault-tolerant-consensus" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>英文</th>
<th>中文</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html">Paxos</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/paxos.md">Paxos</a></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html">Quorum</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/quorum.md">Quorum</a></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html">Generation Clock</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/generation-clock.md">世代时钟（Generation Clock）</a></td>
</tr>
</tbody></table>
<h3><span id="pattern-sequence-for-implementing-replicated-log">Pattern Sequence for implementing replicated log</span><a href="#pattern-sequence-for-implementing-replicated-log" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>英文</th>
<th>中文</th>
<th>Example &amp; 扩展</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/replicated-log.md">复制日志（Replicated Log）</a></td>
<td>Raft</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html">Write-Ahead Log</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/write-ahead-log.md">预写日志（Write-Ahead Log）</a></td>
<td>MySQL redo log<br>HBase WAL<br>Redis aof</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/low-watermark.html">Low-Water Mark</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/low-water-mark.md">低水位标记（Low-Water Mark）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html">High-Water Mark</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/high-water-mark.md">高水位标记（High-Water Mark）</a></td>
<td>kafka HW <br><a href="/www6vHomeHexo/2022/03/29/streamingFlinkWatermarkWindow/" title="Flink-Watermark &amp; Window">Flink-Watermark &amp; Window</a></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html">HeartBeat</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/heartbeat.md">心跳（HeartBeat）</a></td>
<td><a href="/www6vHomeHexo/2019/10/12/crashDetect/" title="宕机检测-Lease、心跳">宕机检测-Lease、心跳</a></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html">Leader and Followers</a> ***<br>leader election</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/leader-and-followers.md">领导者和追随者（Leader and Followers）</a></td>
<td><a href="/www6vHomeHexo/2021/05/16/kafkaElection/" title="Kafka 中的选主">Kafka 中的选主</a> <br> <a href="/www6vHomeHexo/2019/06/21/raft/" title="Raft协议">Raft协议</a> <br><a href="/www6vHomeHexo/2015/11/29/zookeeperZab/" title="Zookeeper-Zab">Zookeeper-Zab</a><br>es 选主[todo]</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/follower-reads.html">Follower Reads</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/follower-reads.md">追随者读取（Follower Reads）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/log-segmentation.html">Segmented Log</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/segmented-log.md">分段日志（Segmented Log）</a></td>
<td>Kafka segment</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/request-pipeline.html">Request Pipeline</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/request-pipeline.md">请求管道（Request Pipeline）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/singular-update-queue.html">Singular Update Queue</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/singular-update-queue.md">单一更新队列（Singular Update Queue）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/single-socket-channel.html">Single Socket Channel</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/single-socket-channel.md">单一 Socket 通道（Single Socket Channel）</a></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="atomic-commit">Atomic Commit</span><a href="#atomic-commit" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>英文</th>
<th>中文</th>
<th>Example &amp; 扩展</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/two-phase-commit.html">Two Phase Commit</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/two-phase-commit.md">两阶段提交（Two Phase Commit）</a></td>
<td>两阶段变种</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/fixed-partitions.html">Fixed Partitions</a></td>
<td>&#x2F;</td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/key-range-partitions.html">Key-Range Partitions</a></td>
<td>&#x2F;</td>
<td></td>
</tr>
</tbody></table>
<h3><span id="kubernetes-or-kafka-control-plane">Kubernetes or Kafka Control Plane</span><a href="#kubernetes-or-kafka-control-plane" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>英文</th>
<th>中文</th>
<th>Example &amp; 扩展</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/time-bound-lease.html">Lease</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/lease.md">租约（Lease）</a></td>
<td><a href="/www6vHomeHexo/2019/10/12/crashDetect/" title="宕机检测-Lease、心跳">宕机检测-Lease、心跳</a> <br>ETCD  Lease <br>K8s Lease<br>Eureka Lease</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/state-watch.html">State Watch</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/state-watch.md">状态监控（State Watch）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html">Idempotent Receiver</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/idempotent-receiver.md">幂等接收者（Idempotent Receiver）</a></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="logical-timestamp-usage">Logical Timestamp usage</span><a href="#logical-timestamp-usage" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>英文</th>
<th>中文</th>
<th>Example &amp; 扩展</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html">Lamport Clock</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/lamport-clock.md">Lamport 时钟（Lamport Clock）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html">Versioned Value</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/versioned-value.md">有版本的值（Versioned Values）</a></td>
<td>tikv mvcc[TSO]<br><a href="https://www.cockroachlabs.com/docs/stable/">CockroachDB</a> mvcc[HLC]<br></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html">Hybrid Clock</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/hybrid-clock.md">混合时钟（Hybrid Clock）</a></td>
<td><a href="https://www.cockroachlabs.com/docs/stable/">CockroachDB</a></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/gossip-dissemination.html">Gossip Dissemination</a> ***</td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/gossip-dissemination.md">Gossip 传播（Gossip Dissemination）</a></td>
<td>Redis Gossip</td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html">Consistent Core</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/consistent-core.md">一致性内核（Consistent Core）</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html">Version Vector</a></td>
<td><a href="https://github.com/www6v/patterns-of-distributed-systems/blob/master/content/version-vector.md">版本向量（Version Vector）</a></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="others">Others</span><a href="#others" class="header-anchor">#</a></h3><ul>
<li><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/request-batch.html">Request Batch</a></li>
<li><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/clock-bound.html">Clock-Bound Wait</a></li>
<li><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/emergent-leader.html">Emergent Leader</a></li>
<li><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/request-waiting-list.html">Request Waiting List</a></li>
</ul>
<h2><span id="self">Self</span><a href="#self" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th></th>
<th>Example &amp; 扩展</th>
</tr>
</thead>
<tbody><tr>
<td>time wheel</td>
<td><a href="/www6vHomeHexo/2017/01/05/timedTask/" title="延迟消息 时间轮">延迟消息 时间轮</a></td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://martinfowler.com/articles/patterns-of-distributed-systems/">Patterns of Distributed Systems</a><br><a href="https://github.com/www6v/patterns-of-distributed-systems">《分布式系统模式》中文版</a></li>
<li><a href="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/" title="分布式数据库-全局时钟">分布式数据库-全局时钟</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>模式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL Logs</title>
    <url>/www6vHomeHexo/2022/02/27/mysqlLog/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#mysql-log%E5%92%8C%E4%BA%8B%E5%8A%A10">MySQL Log和事务[0]</a><br>- <a href="#redo-log">redo log</a><br>- <a href="#undo-log">undo log</a><br>- <a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#mysql-log%E5%92%8C%E5%8F%AF%E9%9D%A0%E6%80%A7">MySQL Log和可靠性</a><br>- <a href="#binlog%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A0%BC%E5%BC%8F1">binlog的三种格式[1]</a><br>- <a href="#redo-log-%E5%92%8C-undo-log-4">redo log 和 undo log [4]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="mysql-log和事务0">MySQL Log和事务[0]</span><a href="#mysql-log和事务0" class="header-anchor">#</a></h2><h5><span id="redo-log">redo log</span><a href="#redo-log" class="header-anchor">#</a></h5><ul>
<li>有了redolog之后，当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在redo log buffer中。在事务提交时，会将redo log buffer中的数据刷新到redo log磁盘文件中。过一段时间之后，如果刷新缓冲区的脏页到磁盘时，<strong>发生错误，此时就可以借助于redo log进行数据恢复，这样就保证了事务的持久性</strong>。</li>
<li>因为在业务操作中，我们操作数据一般都是<strong>随机读写磁盘</strong>的，而不是顺序读写磁盘。 而redo log在往磁盘文件中写入数据，由于是日志文件，所以都是<strong>顺序写</strong>的。顺序写的效率，要远大于随机写。 这种先写日志的方式，称之为** WAL（Write-Ahead Logging）**。</li>
</ul>
<h5><span id="undo-log">undo log</span><a href="#undo-log" class="header-anchor">#</a></h5><p>回滚日志，用于记录数据被修改前的信息 , 作用包含两个 : **提供回滚(保证事务的原子性) 和MVCC(多版本并发控制) **。</p>
<p>undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undolog中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。</p>
<h5><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>Log</th>
<th>性质</th>
<th>记录内容</th>
<th>ACID</th>
</tr>
</thead>
<tbody><tr>
<td>redo log</td>
<td>物理日志[记录内容]</td>
<td>wal</td>
<td>保证事务的持久性[D]</td>
</tr>
<tr>
<td>undo log</td>
<td>物理日志[记录内容]</td>
<td>被修改前的信息，提供回滚</td>
<td>保证事务的原子性[A]</td>
</tr>
</tbody></table>
<p>​    binlog                 逻辑日志[记录操作]</p>
<h2><span id="mysql-log和可靠性">MySQL Log和可靠性</span><a href="#mysql-log和可靠性" class="header-anchor">#</a></h2><h5><span id="binlog的三种格式1">binlog的三种格式[1]</span><a href="#binlog的三种格式1" class="header-anchor">#</a></h5><ul>
<li><p>statement</p>
</li>
<li><p>row格式</p>
</li>
<li><p>mixed格式: statement or row格式</p>
<p>因为有些<strong>statement格式</strong>的binlog可能会<strong>导致主备不一致</strong>，所以要使用row格式。<br>但<strong>row格式的缺点是，很占空间</strong>。比如你用一个delete语句删掉10万行数据，用statement的话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果用row格式的binlog，就要把这10万条记录都写到binlog中。这样做，不仅会占用更大的空间，同时写binlog也要耗费IO资源，影响执行速度。<br>所以，MySQL就取了个<strong>折中方案</strong>，也就是有了mixed格式的binlog。<strong>mixed格式</strong>的意思是，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式，否则就用statement格式。<br>也就是说，mixed格式<strong>可以利用statment格式的优点，同时又避免了数据不一致的风险</strong>.因此，如果你的线上MySQL设置的binlog格式是statement的话，那基本上就可以认为这是一个不合理的设置。你至少应该把binlog的格式设置为mixed。</p>
</li>
</ul>
<h5><span id="redo-log-和-undo-log-4">redo log 和 undo log [4]</span><a href="#redo-log-和-undo-log-4" class="header-anchor">#</a></h5><ul>
<li><p>redo Log </p>
<ul>
<li>WAL日志，保证事务持久性</li>
<li>buffer楼盘之前数据库意外宕机， 可以进行数据的恢复</li>
</ul>
</li>
<li><p>undo log</p>
<ul>
<li>保证事务原子性， 回滚或者事务异常，可以回滚到历史版本</li>
<li>实现MVCC的必要条件</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">事务开始.</span><br><span class="line">    记录A=1到undo log.</span><br><span class="line">    修改A=3.</span><br><span class="line">    记录A=3到redo log.( 先写内存， 后同步到磁盘中)</span><br><span class="line"></span><br><span class="line">    记录B=2到undo log.</span><br><span class="line">    修改B=4.</span><br><span class="line">    记录B=4到redo log.( 先写内存， 后同步到磁盘中)</span><br><span class="line"></span><br><span class="line">    将redo log写入磁盘。</span><br><span class="line">事务提交</span><br></pre></td></tr></table></figure>



<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="0">
<li><p><a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?p=78">黑马程序员 MySQL数据库入门到精通</a>  P138 - P140<br><a href="https://github.com/www6v/mysql_note">mysql_note</a> 笔记1<br><a href="https://frxcat.fun/database/MySQL/MySQL_Advanced_index/">MySQL 索引</a> 笔记2 ***</p>
</li>
<li><p>《MySQL是怎么保证主备一致的？》 MySQL实战45讲  丁奇</p>
</li>
<li><p>《云数据库架构》 1.1.5  1.1.7 - 阿里云</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis和数据库之间的一致性</title>
    <url>/www6vHomeHexo/2022/02/21/redisDbConsistent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h3><span id="数据的一致性含义"><strong>“数据的一致性”</strong>含义</span><a href="#数据的一致性含义" class="header-anchor">#</a></h3><p>  <strong>缓存中有数据，那么，缓存的数据值需要和数据库中的值相同；<br>  缓存中本身没有数据，那么，数据库中的值必须是最新值。</strong></p>
<h3><span id="对于读写缓存的情况">对于“读写缓存”的情况</span><a href="#对于读写缓存的情况" class="header-anchor">#</a></h3><p>  如果我们采用同步写回策略，那么可以保证缓存和数据库中的数据一致。<br>  所以，我们要在业务应用中使用<strong>事务机制</strong>，来保证缓存和数据库的更新具有<strong>原子性</strong>，<br>  也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写。</p>
<h3><span id="对于只读缓存的情况">对于”只读缓存”的情况</span><a href="#对于只读缓存的情况" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/02/21/redisDbConsistent/redis-db-consist.png" class title="Redis和数据库之间的一致性">


<h3><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h3><p>25丨缓存异常（上）：如何解决缓存和数据库的数据不一致问题？</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Operator-Redis</title>
    <url>/www6vHomeHexo/2022/02/17/k8sOperator-redis/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="deploy-redis-cluster-operator">Deploy redis cluster operator</span><a href="#deploy-redis-cluster-operator" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml</span><br><span class="line">kubectl create -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml</span><br><span class="line"></span><br><span class="line">// cluster-scoped</span><br><span class="line">$ kubectl create -f deploy/service_account.yaml</span><br><span class="line">$ kubectl create -f deploy/cluster/cluster_role.yaml</span><br><span class="line">$ kubectl create -f deploy/cluster/cluster_role_binding.yaml</span><br><span class="line">$ kubectl create -f deploy/cluster/operator.yaml</span><br></pre></td></tr></table></figure>

<h2><span id="deploy-a-sample-redis-cluster">Deploy a sample Redis Cluster</span><a href="#deploy-a-sample-redis-cluster" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## Custom Resource</span><br><span class="line">$ kubectl create -f deploy/example/custom-resources.yaml</span><br><span class="line"></span><br><span class="line">## Persistent Volume</span><br><span class="line">$ kubectl create -f deploy/example/persistent.yaml</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://github.com/ucloud/redis-cluster-operator">redis-cluster-operator</a><br><a href="https://www.bilibili.com/video/BV16t4y1w7r6?p=129">kubernetes架构师课程</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Deployment</title>
    <url>/www6vHomeHexo/2022/02/16/k8sDeployment/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#argo-rollouts-5">Argo Rollouts [5]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="deployment">Deployment</span><a href="#deployment" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/02/16/k8sDeployment/k8sDeployment.jpg" class title="Kubenetes服务部署"> 

<ol>
<li>Pod-template-hash label</li>
<li>Rolling Update Deployment<br>Max Surge<br>Max Unavailable</li>
</ol>
<h2><span id="argo-rollouts-5">Argo Rollouts [5]</span><a href="#argo-rollouts-5" class="header-anchor">#</a></h2><p>Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>Deployment D</p>
<ol>
<li><a href="https://www.infoq.cn/article/oyjoCIZBpxw*dI21AXPI">如何在 Kubernetes 中对无状态应用进行分批发布</a>  阿里 孙齐（代序）</li>
<li><a href="https://edu.aliyun.com/lesson_1651_13081?spm=5176.10731542.0.0.e7a120beywNIVX#_13081">第6 章 ： 应用编排与管理： Deployment</a>  阿里</li>
<li><a href="https://tencentcloudcontainerteam.github.io/2019/05/08/kubernetes-best-practice-grace-update/">kubernetes 最佳实践：优雅热更新</a>  陈鹏</li>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/deployment.html">Deployment</a> jimmysong</li>
<li><a href="https://lib.jimmysong.io/argo-rollouts/">Argo Rollouts 中文文档</a>  jimmysong</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps-Terraform</title>
    <url>/www6vHomeHexo/2022/02/15/devopsTerraform/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="terraform1">Terraform[1]</span><a href="#terraform1" class="header-anchor">#</a></h2><h5><span id="command2">Command[2]</span><a href="#command2" class="header-anchor">#</a></h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform init</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform <span class="built_in">fmt</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform validate</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform plan  -out</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform plan -detailed-exitcode</span></span><br><span class="line">  echo $?</span><br><span class="line">  0</span><br><span class="line">  drift </span><br><span class="line">  0 no drift</span><br><span class="line">  1 error</span><br><span class="line">  2 success ## infra drift,可能有人变更了infra并apply成功</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform apply/destroy -target</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 下面两条命令是相反的操作</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform state <span class="built_in">rm</span> <span class="comment">## 从状态文件中删除一个或多个对象</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">terraform import <span class="comment">## 已经存在的手动创建的资源对象导入Terraform，由terraform来控制</span></span></span><br></pre></td></tr></table></figure>

<h5><span id="module-3-4">Module [3] [4]</span><a href="#module-3-4" class="header-anchor">#</a></h5><ul>
<li>模块复用<br>module 可以嵌套 module</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/www6v/terraform-practice/tree/master/iac">Terraform 简介</a></li>
<li><a href="https://github.com/www6v/terraform-practice/tree/master/alicloud_ecs">alicloud_ecs</a>   example </li>
<li><a href="https://github.com/www6v/terraform-practice/tree/master/invoke_module">terraform-module</a></li>
<li><a href="https://github.com/terraform-alicloud-modules">aliyun提供的modules</a></li>
</ol>
<ul>
<li><a href="https://www.bilibili.com/video/BV1HU4y1e7wZ/">第一季：第 8 集  基础架构自动化编排工具 Terraform</a></li>
<li><a href="https://github.com/www6v/terraform-practice">terraform-practice</a></li>
</ul>
]]></content>
      <categories>
        <category>devops</category>
        <category>Terraform</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Prometheus  HA</title>
    <url>/www6vHomeHexo/2022/02/11/observabilityPrometheusHA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%881">高可用方案[1]</a><br>- <a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%A1%88">高可用的几种方案</a><br>- <a href="#%E9%97%AE%E9%A2%98">问题</a><br>- <a href="#%E5%8E%9F%E5%9B%A0">原因</a><br>- <a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
<li><a href="#thanos">Thanos</a><br>- <a href="#%E7%BB%84%E4%BB%B6">组件</a><br>- <a href="#%E7%89%B9%E7%82%B9">特点</a><br>- <a href="#%E6%A8%A1%E5%BC%8F">模式</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><br>- <a href="#ha">HA</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="高可用方案1">高可用方案[1]</span><a href="#高可用方案1" class="header-anchor">#</a></h2><h5><span id="高可用的几种方案">高可用的几种方案</span><a href="#高可用的几种方案" class="header-anchor">#</a></h5><ul>
<li>基本 HA：即两套 Prometheus 采集完全一样的数据，外边挂负载均衡</li>
<li>HA + 远程存储：除了基础的多副本 Prometheus，还通过 Remote Write 写入到远程存储，解决存储持久化问题</li>
<li>联邦集群：即 Federation，按照功能进行分区，不同的 Shard 采集不同的数据，由Global节点来统一存放，解决监控数据规模的问题。</li>
<li>使用 Thanos 或者 Victoriametrics，来解决全局查询、多副本数据 Join 问题。</li>
</ul>
<h5><span id="问题">问题</span><a href="#问题" class="header-anchor">#</a></h5><p>就算使用官方建议的多副本 + 联邦，仍然会遇到一些问题:</p>
<ul>
<li>官方建议数据做 Shard，然后通过Federation来实现高可用，</li>
<li>但是边缘节点和Global节点依然是单点，需要自行决定是否每一层都要使用双节点重复采集进行保活。<br>  也就是仍然会有单机瓶颈。</li>
<li>另外部分敏感报警尽量不要通过Global节点触发，毕竟从Shard节点到Global节点传输链路的稳定性会影响数据到达的效率，进而导致报警实效降低。</li>
<li>例如服务Updown状态，Api请求异常这类报警我们都放在Shard节点进行报警。</li>
</ul>
<h5><span id="原因">原因</span><a href="#原因" class="header-anchor">#</a></h5><p>本质原因是，Prometheus 的本地存储没有数据同步能力，要在保证可用性的前提下，再保持数据一致性是比较困难的，基础的 HA Proxy 满足不了要求，比如：</p>
<ul>
<li>集群的后端有 A 和 B 两个实例，A 和 B 之间没有数据同步。A 宕机一段时间，丢失了一部分数据，如果负载均衡正常轮询，请求打到A 上时，数据就会异常。</li>
<li>如果 A 和 B 的启动时间不同，时钟不同，那么采集同样的数据时间戳也不同，就不是多副本同样数据的概念了</li>
<li>就算用了远程存储，A 和 B 不能推送到同一个 TSDB，如果每人推送自己的 TSDB，数据查询走哪边就是问题了。</li>
</ul>
<h5><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h5><ul>
<li>在<strong>存储、查询</strong>两个角度上保证数据的一致<ul>
<li><strong>存储角度</strong>：如果使用 <strong>Remote Write 远程存储， A 和 B后面可以都加一个 Adapter，Adapter做选主逻辑，只有一份数据能推送到 TSDB，这样可以保证一个异常，另一个也能推送成功，数据不丢，同时远程存储只有一份，是共享数据</strong>。方案可以参考这篇文章 </li>
<li><strong>查询角度</strong>：上边的方案实现很复杂且有一定风险，因此现在的大多数方案在查询层面做文章，<strong>比如 Thanos 或者 Victoriametrics，仍然是两份数据，但是查询时做数据去重和 Join</strong>。只是 Thanos 是通过 Sidecar 把数据放在对象存储，Victoriametrics 是把数据 Remote Write 到自己的 Server 实例，但查询层 Thanos-Query 和 Victor 的 Promxy的逻辑基本一致。</li>
</ul>
</li>
</ul>
<h2><span id="thanos">Thanos</span><a href="#thanos" class="header-anchor">#</a></h2><h5><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h5><ul>
<li>Bucket</li>
<li>Check</li>
<li>Compactor</li>
<li>Query</li>
<li>Rule</li>
<li>Sidecar</li>
<li>Store</li>
<li>receive</li>
<li>downsample</li>
</ul>
<h5><span id="特点">特点</span><a href="#特点" class="header-anchor">#</a></h5><ul>
<li>为多集群Prometheus提供全局接口<br>全局视图</li>
<li>将监控数据存储到各种对象存储</li>
<li>为Prometheus提供高可用</li>
<li>易于与Prometheus集成并可模块化部署</li>
</ul>
<h5><span id="模式">模式</span><a href="#模式" class="header-anchor">#</a></h5><ul>
<li>Sidecar(默认的模式)<br>pull, remote read</li>
<li>Receive<br>push, remote rewrite</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><h5><span id="ha">HA</span><a href="#ha" class="header-anchor">#</a></h5><ol>
<li><a href="http://www.xuyasong.com/?p=1921">高可用 Prometheus：问题集锦</a></li>
<li><a href="http://www.xuyasong.com/?p=1925">高可用 Prometheus：Thanos 实践</a> </li>
<li><a href>第十八期: 玩转云原生容器场景的Prometheus监控</a>  腾讯云 云原生正发声  #todo 重看一遍</li>
<li><a href="http://dockone.io/article/6019">Thanos：开源的大规模Prometheus集群解决方案</a>   失效</li>
<li><a href="https://kubesphere.com.cn/live/jinaai0602-live/">基于 KubeSphere 和 Thanos 构建可持久化存储的多集群监控系统</a>  失效</li>
<li><a href="https://zhuanlan.zhihu.com/p/128658137">打造云原生大型分布式监控系统(二): Thanos 架构详解</a>   imroc@腾讯云 ***</li>
<li><a href="https://viva.gitbook.io/project/cloud-native/prometheus/cun-chu-fang-an-cha-yi-dui-bi">Thanos与Cortex方案对比</a> 未</li>
<li><a href="https://blog.csdn.net/sinat_32582203/article/details/121487648">Prometheus Thanos与Cortex组件比较</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>metric</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Nginx Ingress</title>
    <url>/www6vHomeHexo/2022/02/10/k8sIngressNginx/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="kubernetes-nginx-ingress">Kubernetes Nginx Ingress</span><a href="#kubernetes-nginx-ingress" class="header-anchor">#</a></h2><h5><span id="版本1-k8s官方维护-12">版本1-K8s官方维护 [1][2]</span><a href="#版本1-k8s官方维护-12" class="header-anchor">#</a></h5><ul>
<li>There are three ways to customize NGINX:<ul>
<li><strong>ConfigMap</strong>: using a Configmap to set global configurations in NGINX.<br>[滚动更新后生效， 针对全局的配置]</li>
<li><strong>Annotations</strong>: use this if you want a specific configuration for a particular Ingress rule. [立即生效，针对某个域名location进行配置]</li>
<li><strong>Custom template</strong>: when more specific settings are required, like open_file_cache, adjust listen options as rcvbuf or when is not possible to change the configuration through the ConfigMap.</li>
</ul>
</li>
</ul>
<h5><span id="版本2-nginx官方维护-34">版本2-Nginx官方维护 [3][4]</span><a href="#版本2-nginx官方维护-34" class="header-anchor">#</a></h5><h5><span id="版本1-vs-版本2-5">版本1 vs.  版本2 [5]</span><a href="#版本1-vs-版本2-5" class="header-anchor">#</a></h5><h2><span id="实践-last">实践 [last]</span><a href="#实践-last" class="header-anchor">#</a></h2><h5><span id="域名重定向">域名重定向 ***</span><a href="#域名重定向" class="header-anchor">#</a></h5><p>  <code>redirect</code> ，  http 重定向到https， 新旧域名替换</p>
<h5><span id="前后端分离">前后端分离 ***</span><a href="#前后端分离" class="header-anchor">#</a></h5><p><code>rewrite</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+ www.a.com /    --&gt; 前端服务</span><br><span class="line">+ www.a.com /api --&gt; 后端服务 /api  </span><br><span class="line">  www.a.com/api rewrite到  www.a.com </span><br></pre></td></tr></table></figure>
<h5><span id="ssl配置-p">SSL配置  [p]</span><a href="#ssl配置-p" class="header-anchor">#</a></h5><p>  验证  自签名证书</p>
<h5><span id="匹配请求头">匹配请求头</span><a href="#匹配请求头" class="header-anchor">#</a></h5><p>  Mobile 和 PC 的请求头不同，路由到不同的后端服务</p>
<h5><span id="实现灰度金丝雀发布67">实现灰度金丝雀发布[6][7]</span><a href="#实现灰度金丝雀发布67" class="header-anchor">#</a></h5><p>不同灰度方式的优先级由高到低为：<br>canary-by-header<code>&gt;</code>canary-by-cookie<code>&gt;</code>canary-weight</p>
<table>
<thead>
<tr>
<th>Annotation</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary</td>
<td>必须设置该Annotation值为<code>true</code>，否则其它规则将不会生效。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-by-header</td>
<td>表示基于请求头的名称进行灰度发布。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-by-header-value</td>
<td>表示基于请求头的值进行灰度发布。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-by-header-pattern</td>
<td>表示基于请求头的值进行灰度发布，并对请求头的值进行正则匹配。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-by-cookie</td>
<td>表示基于Cookie进行灰度发布。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-weight</td>
<td>表示基于权重进行灰度发布。</td>
</tr>
<tr>
<td>nginx.ingress.kubernetes.io&#x2F;canary-weight-total</td>
<td>表示设定的权重总值。</td>
</tr>
</tbody></table>
<details><summary>灰度发布 示例</summary><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+ 基于Header灰度（自定义header值）：当请求Header为ack: alibaba时将访问灰度服务；其它Header将根据灰度权重将流量分配给灰度服务。</span><br><span class="line">+ 基于Cookie灰度：当Header不匹配时，请求的Cookie为hangzhou_region=always时将访问灰度服务。 </span><br><span class="line"></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: nginx</span><br><span class="line">    nginx.ingress.kubernetes.io/canary: &quot;true&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/canary-weight: &quot;20&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/canary-by-header: &quot;ack&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/canary-by-header-value: &quot;alibaba&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/canary-by-cookie: &quot;hangzhou_region&quot;</span><br></pre></td></tr></table></figure></details>



<h5><span id="速率限制">速率限制</span><a href="#速率限制" class="header-anchor">#</a></h5><h5><span id="黑白名单">黑白名单</span><a href="#黑白名单" class="header-anchor">#</a></h5><p>  allow deny</p>
<h5><span id="自定义错误页面">自定义错误页面</span><a href="#自定义错误页面" class="header-anchor">#</a></h5><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">Ingress-Nginx Doc</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/ingress-nginx">Ingress-Nginx Github</a></p>
</li>
<li><p><a href="https://docs.nginx.com/nginx-ingress-controller/configuration/ingress-resources/advanced-configuration-with-annotations/">Nginx-ingress Doc</a></p>
</li>
<li><p><a href="https://github.com/nginxinc/kubernetes-ingress">Nginx-ingress GitHub</a></p>
</li>
<li><p><a href="https://github.com/nginxinc/kubernetes-ingress/blob/main/docs/content/intro/nginx-ingress-controllers.md">nginx-ingress-controllers.md</a>    表格 </p>
</li>
<li><p><a href="https://help.aliyun.com/document_detail/86533.html#section-gjm-dw6-hkn">Nginx Ingress高级用法</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV15o4y1Y7Bq/">【IT老齐294】大厂如何基于K8S实现金丝雀发布</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV16t4y1w7r6?p=162">kubernetes架构师课程</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式 数据库 总结</title>
    <url>/www6vHomeHexo/2022/02/09/distributedDatabase/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2022/02/09/distributedDatabase/dDatabase.jpg" class title="分布式数据库">

<img src="/www6vHomeHexo/2022/02/09/distributedDatabase/dDatabase1.jpg" class title="分布式数据库">



<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p>&lt;&lt;分布式数据库30讲&gt;&gt; </p>
</li>
<li><a href="/www6vHomeHexo/2022/04/11/distributedDatabaseGlobalTime/" title="分布式数据库-全局时钟">分布式数据库-全局时钟</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio Service Failover</title>
    <url>/www6vHomeHexo/2022/02/06/istioServiceFailover/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2022/02/06/istioServiceFailover/failover.JPG" class title="Istio Service Failover">

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV11P4y1p7kz?spm_id_from=333.880.my_history.page.click">基于云原生技术的服务最大化可用性，白西原（乐天创研）</a></p>
<p><a href="https://www.aliyundrive.com/s/dXxngxjTkZE">ppt</a></p>
<p><a href="https://www.solo.io/blog/open-source-service-mesh-hub-technical-overview/">Open Source Service Mesh Hub – Technical Overview</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Checkpoint-分布式快照方法</title>
    <url>/www6vHomeHexo/2022/02/03/streamingFlinkCheckpoint/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8E%9F%E7%90%86-1">原理 [1]</a><ul>
<li><a href="#checkpoint%E7%9A%84%E4%BF%9D%E5%AD%98">CheckPoint的保存</a></li>
<li><a href="#%E4%BB%8Echeckpoint%E6%81%A2%E5%A4%8D%E7%8A%B6%E6%80%81">从CheckPoint恢复状态</a></li>
<li><a href="#checkpoint-%E7%AE%97%E6%B3%95">CheckPoint 算法</a></li>
</ul>
</li>
<li><a href="#%E5%8E%9F%E7%90%86-5">原理 [5]</a><ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%9C%BA%E6%99%AF%E7%9A%84%E7%8A%B6%E6%80%81%E5%AE%B9%E9%94%99">分布式场景的状态容错</a></li>
<li><a href="#flink-%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E6%96%B9%E6%B3%95">Flink 的分布式快照方法</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E8%B7%B5">实践</a><ul>
<li><a href="#api-2-3">API [2] [3]</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C4n-ai">使用经验[4][N AI]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="原理-1">原理 [1]</span><a href="#原理-1" class="header-anchor">#</a></h1><h3><span id="checkpoint的保存">CheckPoint的保存</span><a href="#checkpoint的保存" class="header-anchor">#</a></h3><p>  <strong>当所有任务都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来</strong></p>
<h3><span id="从checkpoint恢复状态">从CheckPoint恢复状态</span><a href="#从checkpoint恢复状态" class="header-anchor">#</a></h3><p>（1）重启应用<br>（2）<strong>读取检查点，重置状态</strong><br>（3）<strong>重放数据</strong><br>    重置偏移量<br>（4）继续处理数据</p>
<h3><span id="checkpoint-算法">CheckPoint 算法</span><a href="#checkpoint-算法" class="header-anchor">#</a></h3><ul>
<li><p>检查点的“分界线”（Checkpoint Barrier）<br>watermark 指示的是“之前的数据全部到齐了”，而 barrier 指示的是“之前所有数据的状<br>态更改保存入当前检查点”：它们都是一个“截止时间”的标志。</p>
</li>
<li><p>分布式快照算法</p>
<ul>
<li>Flink 使用了 Chandy-Lamport 算法的一种变体，被称为<strong>“异步分界线快照” (asynchronous barrier snapshotting)算法</strong>。</li>
<li>算法的核心两个原则:<ol>
<li>当上游任务向多个并行下游任务发送** barrier** 时，需要广播出去;</li>
<li>而当多个上游任务向同一个下游任务传递 barrier 时， 需要在下游任务执行**“分界线对齐”(barrier alignment)**操作，也就是需要等到所有并行分区 的 barrier 都到齐，才可以开始状态的保存。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h1><span id="原理-5">原理 [5]</span><a href="#原理-5" class="header-anchor">#</a></h1><h3><span id="分布式场景的状态容错">分布式场景的状态容错</span><a href="#分布式场景的状态容错" class="header-anchor">#</a></h3><p>Flink 分布式场景的作业拓扑是<strong>有向无环并且是弱联通图</strong>。可以<strong>采用裁剪的Chandy-Lamport</strong>，记录所有输入的offset和各个算子状态，依赖 rewinding source 从而<strong>不需要存储 channel 的状态</strong>。这在存在聚合逻辑的情况下可以<strong>节省大量的存储空间</strong>。</p>
<p>最后做恢复，恢复就是把数据源的位置重新设定，然后每一个算子都从检查点恢复状态。</p>
<h3><span id="flink-的分布式快照方法">Flink 的分布式快照方法</span><a href="#flink-的分布式快照方法" class="header-anchor">#</a></h3><p>首先在源数据流里插入<strong>Checkpoint barrier</strong>，也就是上文提到的<strong>Chandy-Lamport算法里的marker message</strong>，不同的Checkpoint barrier会把流自然地切分多个段，每个段都包含了Checkpoint的数据；</p>
<p>Flink 里有一个<strong>全局的 Coordinator</strong>，它不像 Chandy-Lamport 对任意一个进程都可以发起快照，这个集中式的 Coordinator会把 Checkpoint barrier 注入到每个 source 里，然后启动快照。</p>
<p>当每个节点收到 barrier 后，因为 Flink 里面它不存储 Channel state，所以它<strong>只需存储本地的状态</strong>就好。</p>
<p>在做完了Checkpoint 后，<strong>每个算子的每个并发都会向Coordinator发送一个确认消息</strong>，当所有任务的确认消息都被Checkpoint Coordinator接收，快照就结束了。</p>
<h1><span id="实践">实践</span><a href="#实践" class="header-anchor">#</a></h1><h3><span id="api-2-3">API [2] [3]</span><a href="#api-2-3" class="header-anchor">#</a></h3><ul>
<li>默认不开启 checkpoint</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// start a checkpoint every 1000 ms</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// advanced options:</span></span><br><span class="line"><span class="comment">// set mode to exactly-once (this is the default)</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// checkpoints have to complete within one minute, or are discarded</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// make sure 500 ms of progress happen between checkpoints</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// allow only one checkpoint to be in progress at the same time</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// enable externalized checkpoints which are retained after job cancellation</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.</span></span><br><span class="line">env.getCheckpointConfig().setFailOnCheckpointingErrors(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 每 60s 做一次 checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 高级配置：</span></span><br><span class="line"><span class="comment">// checkpoint 语义设置为 EXACTLY_ONCE，这是默认语义</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 两次 checkpoint 的间隔时间至少为 1 s，默认是 0，立即进行下一次 checkpoint</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// checkpoint 必须在 60s 内结束，否则被丢弃，默认是 10 分钟</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 同一时间只能允许有一个 checkpoint</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 最多允许 checkpoint 失败 3 次</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">3</span>);</span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 checkpoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 允许实验性的功能：非对齐的 checkpoint，以提升性能</span></span><br><span class="line">env.getCheckpointConfig().enableUnalignedCheckpoints();</span><br></pre></td></tr></table></figure>

<h3><span id="使用经验4n-ai">使用经验[4][N AI]</span><a href="#使用经验4n-ai" class="header-anchor">#</a></h3><ol>
<li>当 Checkpoint 时间比设置的间隔时间长时，可以设置 Checkpoint 间最小时间间隔。这样不会立即进行下一次 Checkpoint，从而减少系统在 Checkpoint 方面的资源消耗，确保任务计算资源的充足。</li>
<li>如果 Flink 状态很大，在恢复时需要从远程存储读取状态，状态文件过大可能导致任务恢复很慢，浪费时间在网络传输方面。建议开启 Flink Task 本地状态恢复，加快状态恢复速度。</li>
<li>可以增加 Checkpoint 保存数，以便在最新的 Checkpoint 恢复失败时回滚到之前的 Checkpoint 进行恢复。</li>
<li>建议设置的 Checkpoint 间隔时间大于完成时间。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>尚硅谷 flink(Java) - bilibili<br> 《尚硅谷大数据技术之flink（java）》 第 10 章 容错机制</li>
<li><a href="https://blog.csdn.net/xc_zhou/article/details/124898733">[1143]Flink的Checkpoint和Savepoint</a> </li>
<li><a href="https://www.shuzhiduo.com/A/xl56qQM9Jr/">Flink Checkpoint 参数详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/112876846">有赞实时任务优化：Flink Checkpoint 异常解析与应用实践</a></li>
<li><a href="https://developer.aliyun.com/learning/course/58/detail/1071">课时5: 《 Fault-tolerance in Flink》</a>  阿里云  《Apache Flink 入门》 ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/43536305">(十)简单解释: 分布式数据流的异步快照(Flink的核心)</a> 原理 - aws SDE - 未 *** </li>
<li><a href="https://xie.infoq.cn/article/f89ea4c1e06004f617b7ae0a7">基于 Log 的通用增量 Checkpoint</a>  未  阿里</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>范式Paradigm-数据密集</title>
    <url>/www6vHomeHexo/2022/02/02/dataIntensiveParadigm/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="流范式-stream">流范式 Stream</span><a href="#流范式-stream" class="header-anchor">#</a></h2><h5><span id="大数据流">大数据流</span><a href="#大数据流" class="header-anchor">#</a></h5><p>Lamda架构</p>
<h5><span id="http2-流-tcp-stream">HTTP2 流， TCP-Stream</span><a href="#http2-流-tcp-stream" class="header-anchor">#</a></h5><h5><span id="pipeline">Pipeline</span><a href="#pipeline" class="header-anchor">#</a></h5><p>CPU流水线</p>
<h2><span id="复制范式-replication">复制范式 Replication</span><a href="#复制范式-replication" class="header-anchor">#</a></h2><h5><span id="主从">主从</span><a href="#主从" class="header-anchor">#</a></h5><p>MySQL</p>
<h5><span id="多主">多主</span><a href="#多主" class="header-anchor">#</a></h5><p>Git</p>
<h5><span id="无主">无主</span><a href="#无主" class="header-anchor">#</a></h5><p>Cassandra</p>
<h5><span id="kafka-isr">Kafka ISR</span><a href="#kafka-isr" class="header-anchor">#</a></h5><p>原理上是主从，但是从延迟太大会被踢出ISR</p>
<table>
<thead>
<tr>
<th>弱一致性</th>
<th>协议</th>
<th>特性</th>
<th>工程</th>
</tr>
</thead>
<tbody><tr>
<td>Master-Slave<br>(最终一致性)</td>
<td></td>
<td>延迟低，吞吐高<br>主动推送&#x2F;被动拉取</td>
<td><a href="/www6vHomeHexo/2020/06/21/mysqlReliability/" title="MySQL的主从 高可用 容灾">MySQL的主从 高可用 容灾</a> self</td>
</tr>
<tr>
<td>Master-Master<br>(最终一致性)</td>
<td><a href="/www6vHomeHexo/2022/05/14/multiMasterCRDT/" title="多主-CRDT">CRDT</a> self</td>
<td>延迟低，吞吐高</td>
<td><a href="/www6vHomeHexo/2020/06/21/mysqlReliability/" title="MySQL的主从 高可用 容灾">MySQL的主从 高可用 容灾</a> self</td>
</tr>
<tr>
<td>弱一致性</td>
<td>Backups（备份）</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2><span id="数据存储和处理">数据存储和处理</span><a href="#数据存储和处理" class="header-anchor">#</a></h2><h5><span id="oltp">OLTP</span><a href="#oltp" class="header-anchor">#</a></h5><p>MYSQL</p>
<h5><span id="olap">OLAP</span><a href="#olap" class="header-anchor">#</a></h5><p>HADOOP</p>
<h5><span id="hatpat">HATP（A，T）</span><a href="#hatpat" class="header-anchor">#</a></h5><p>TiDB， TiKV(行存)， Tiflash（列存）</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>范式</category>
      </categories>
      <tags>
        <tag>范式</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S 应用迁移至K8S</title>
    <url>/www6vHomeHexo/2022/02/02/k8sAppMigrate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="应用容器化">应用容器化</span><a href="#应用容器化" class="header-anchor">#</a></h2><h4><span id="启动速度">启动速度</span><a href="#启动速度" class="header-anchor">#</a></h4><h4><span id="健康检查">健康检查</span><a href="#健康检查" class="header-anchor">#</a></h4><h4><span id="启动参数">启动参数</span><a href="#启动参数" class="header-anchor">#</a></h4><h4><span id="dockerfile">Dockerfile</span><a href="#dockerfile" class="header-anchor">#</a></h4><ul>
<li>基础镜像，Utility Lib</li>
<li>主进程, Fork bomb</li>
<li>代码和配置分离<br>配置： 环境变量， 配置文件mount</li>
<li>分层控制</li>
<li>Entrypoint</li>
</ul>
<h4><span id="日志">日志</span><a href="#日志" class="header-anchor">#</a></h4><ul>
<li>Log Driver<br>Blocking mode<br>Non Blocking mode</li>
<li>sideCar模式</li>
<li>Node 模式<br> <a href="https://developer.aliyun.com/article/674327">容器日志采集利器Log-Pilot</a></li>
</ul>
<h4><span id="共享kernel">共享kernel</span><a href="#共享kernel" class="header-anchor">#</a></h4><ul>
<li>系统参数配置共享<br><a href="../../../../2020/08/16/kernelParam/">虚拟机和容器中的内核参数 kernel</a><br><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/">K8S 内核参数</a></li>
<li>进程数共享 - Fork bomb<br><a href="https://github.com/krallin/tini">https://github.com/krallin/tini</a></li>
<li>fd数共享<br>容器可能用完主机所有的fd</li>
<li>主机磁盘共享<br>lvm， 强IO</li>
</ul>
<h4><span id="容器化应用的资源监控">容器化应用的资源监控</span><a href="#容器化应用的资源监控" class="header-anchor">#</a></h4><ul>
<li>容器中看到的是主机资源<br>top<br>java runtime.GetAvailableProcess() - cpu数<br>cat &#x2F;proc&#x2F;cupinfo<br>cat &#x2F;proc&#x2F;meminfo<br>df -k</li>
<li>解决方案<ol>
<li>查询&#x2F;proc&#x2F;1&#x2F;cgroup是否包含kubepods关键字，表明运行在k8s之上。（参考1）</li>
<li>要得到单个容器的 CPU 使用率，我们可以从 CPU Cgroup 每个控制组里的统计文件 cpuacct.stat 中获取。单个容器 CPU 使用率 &#x3D;((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 &#x2F; (HZ * et * 1 )。（参考2）</li>
<li>其他 lxcfs</li>
</ol>
</li>
<li>对应用的影响<br>Java - Concurrent GC Thread， Heap Size， 线程数不可控</li>
</ul>
<h4><span id="pod-spec">Pod Spec</span><a href="#pod-spec" class="header-anchor">#</a></h4><ul>
<li><p>初始化需求(init container) </p>
</li>
<li><p>需要几个主container</p>
</li>
<li><p>权限？ Privilege和Security(PSP)</p>
</li>
<li><p>共享哪些namespace（PID，IPC，NET，UTS，MNT）<br>NET 默认共享</p>
</li>
<li><p>配置管理<br>环境变量<br>volumn mount</p>
</li>
<li><p>DNS策略及对resolv.conf的影响<br>default<br>clsuterFirst: 默认 coredns<br>clusterFirstWithHostNet<br>none</p>
</li>
<li><p>imagePullPolicy Image 拉取策略<br> Never，IfNotPresent， always</p>
</li>
<li><p>数据保存<br>local-ssd: 独占的本地磁盘， 独占io， 固定大小， 读写性能高;<br>local-dynamic: LVM,动态分配空间， 效率低;</p>
<img src="/www6vHomeHexo/2022/02/02/k8sAppMigrate/persistentChoose.png" class title="数据保存">
</li>
<li><p>可用性 self</p>
</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>模块十一： 将应用迁移至Kubernetes平台</li>
<li>06|容器CPU(2):如何正确地拿到容器CPU的开销?   -  李程远</li>
<li><a href="https://blog.csdn.net/qq_36441027/article/details/103527710">DEVOPS-迁移SpringMVC应用到生产K8S集群</a> 未  java项目</li>
<li><a href="http://t.zoukankan.com/code-craftsman-p-11746603.html">如何把应用程序迁移到k8s</a> 未 golang项目</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>存算分离-数据应用</title>
    <url>/www6vHomeHexo/2022/02/02/disaggregationOfComputeAndStorage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<table>
<thead>
<tr>
<th></th>
<th>存算分离</th>
<th>存算一体</th>
</tr>
</thead>
<tbody><tr>
<td>RMDB</td>
<td>MySQL Cluster</td>
<td>MySQL Group Replicatoin(MGR)</td>
</tr>
<tr>
<td>NewSQL,HTAP</td>
<td>TiDB[tidb,tikv] , openGaussDB[2], CockroachDB? <br> Aurora[10], PolarDB[3],</td>
<td>PGXC风格[1]</td>
</tr>
<tr>
<td>大数据</td>
<td>clickhouse, hbase</td>
<td>ES</td>
</tr>
<tr>
<td>MQ</td>
<td>Pulsar</td>
<td>kafka, rocketmq</td>
</tr>
<tr>
<td>文件系统</td>
<td>Ceph[PG, ODS], PolarFS[5]</td>
<td></td>
</tr>
<tr>
<td>KV</td>
<td>GaussDB(for Redis) [4], Codis</td>
<td>redis Cluster</td>
</tr>
<tr>
<td>其他</td>
<td>serverless[FasS, BaaS]</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>PolarDB [7]<br><strong>基于Redo Log物理复制实现的一写多读共享存储集群</strong></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>[1] 《04 | 架构风格：NewSQL和PGXC到底有啥不一样？》  王磊<br>[2]  <a href="https://docs.opengauss.org/zh/docs/3.1.1/docs/Description/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.html">opengauss系统架构</a><br>[3]  PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers<br>[4]  <a href="https://blog.csdn.net/devcloud/article/details/118360126">GaussDB(for Redis)揭秘：Redis存算分离架构最全解析</a><br>[5] <a href="https://zhuanlan.zhihu.com/p/42754631">阿里推出PolarFS分布式文件系统，存储与计算分开！附论文</a><br>[6] <a href="/www6vHomeHexo/2020/06/21/mysqlReliability/" title="MySQL的主从 高可用 容灾">MySQL的主从 高可用 容灾</a> self<br>[7] &lt;&lt;云原生数据库 原理与实践&gt;&gt; 5.2  5.3<br>[10] VERBITSKI A，GUPTA A，SAHA D，et al. Amazon aurora：design considerations for high throughput cloud-native relational databases ［C］. Proceedings of the 2017 ACM International Conference on Man⁃ agement of Data，2017 ：1041-1052.</p>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统架构</category>
        <category>存算分离</category>
      </categories>
      <tags>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 端到端Exactly-once</title>
    <url>/www6vHomeHexo/2022/02/01/streamingFlinkExactlyOnce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="实现端到端exactly-once123">实现端到端Exactly-once[1][2][3]</span><a href="#实现端到端exactly-once123" class="header-anchor">#</a></h2><p>(1)启动检查点保存<br>(2)算子任务对状态做快照<br>(3)Sink 任务开启事务，进行预提交<br>(4)检查点保存完成，提交事务</p>
<img src="/www6vHomeHexo/2022/02/01/streamingFlinkExactlyOnce/flinkExactlyOnce.png" class title="Flink Exactly-Once">

<p>  Once all of the operators complete their pre-commit, they issue a commit.<br>  If at least one pre-commit fails, all others are aborted, and we roll back to the previous successfully-completed checkpoint.<br>  After a successful pre-commit, the commit must be guaranteed to eventually succeed – both our operators and our external system need to make this guarantee. If a commit fails (for example, due to an intermittent network issue), the entire Flink application fails, restarts according to the user’s restart strategy, and there is another commit attempt. This process is critical because if the commit does not eventually succeed, data loss occurs.</p>
<p>[关键:  barrier , pre-commit，commit]</p>
<h2><span id="flink-端到端exactly-once-是怎么实现的-cursor">Flink 端到端Exactly-once 是怎么实现的？ [Cursor]</span><a href="#flink-端到端exactly-once-是怎么实现的-cursor" class="header-anchor">#</a></h2><p>To achieve end-to-end exactly-once semantics in Flink, the framework uses a combination of <strong>checkpointing and transactional sources&#x2F;sinks</strong>.</p>
<p>Checkpointing is a mechanism that allows Flink to take a snapshot of the state of the application at regular intervals. This state includes the state of all operators in the application, as well as the progress of any ongoing transactions. In the event of a failure, Flink can use the latest checkpoint to restore the state of the application and resume processing from where it left off.</p>
<p>Transactional sources and sinks are sources and sinks that support transactions. These sources and sinks ensure that data is only read or written if the transaction is committed. If a transaction fails, any changes made by that transaction are rolled back, ensuring that the data remains consistent.</p>
<p>By combining checkpointing with transactional sources and sinks, Flink can guarantee end-to-end exactly-once semantics. If a failure occurs, Flink can use the latest checkpoint to restore the state of the application, and any transactions that were in progress at the time of the failure will be rolled back. When the application resumes processing, it will start from the last successfully committed transaction, ensuring that no data is lost or duplicated.</p>
<p>For more information on how to implement exactly-once semantics in Flink, you can refer to the official documentation: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/transactional/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/transactional/</a></p>
<h3><span id="exactly-once-vs-at-least-once-67">Exactly Once vs.   At Least Once  [6][7]</span><a href="#exactly-once-vs-at-least-once-67" class="header-anchor">#</a></h3><p>barrier对齐就是Exactly Once，为什么barrier不对齐就是 At Least Once</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/68797265">Flink端到端Exactly-once</a> 王星辰</li>
<li><a href="https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a>   official</li>
<li>尚硅谷 flink(Java) - bilibili   ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/77677075">端到端一致性,流系统Spark&#x2F;Flink&#x2F;Kafka&#x2F;DataFlow对比总结(压箱宝具呕血之作)</a> 原理 - aws SDE - 未</li>
<li><a href="http://www.54tianzhisheng.cn/2019/06/20/flink-kafka-Exactly-Once/">Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 </a></li>
<li><a href="http://www.54tianzhisheng.cn/2019/08/21/Flink-Exactly_Once_vs_At_Least_Once/">一文搞懂 Flink 的 Exactly Once 和 At Least Once </a>  ***</li>
<li><a href="/www6vHomeHexo/2022/02/03/streamingFlinkCheckpoint/" title="Flink Checkpoint-分布式快照方法">Flink Checkpoint-分布式快照方法</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-数据中心</title>
    <url>/www6vHomeHexo/2022/01/30/cloudDatacenter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://arthurchiao.art/blog/spine-leaf-design-zh/">[译] 数据中心网络：Spine-Leaf 架构设计综述（2016）</a></li>
<li><a href="http://arthurchiao.art/blog/cloud-native-data-center-networking-notes-zh/">[笔记] Cloud Native Data Center Networking（云原生数据中心网络设计）(O’Reilly 2019)</a></li>
<li><a href="http://arthurchiao.art/blog/ctrip-network-arch-evolution-zh/">云计算时代携程的网络架构变迁（2019）</a></li>
<li><a href="http://arthurchiao.art/blog/network-evolves-zh/">计算规模驱动下的网络方案演进</a></li>
<li>HCIP Data Center</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>数据中心</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-Kubernetes</title>
    <url>/www6vHomeHexo/2022/01/30/k8sObservability/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="kubernetes可观察性-1">Kubernetes可观察性 [1]</span><a href="#kubernetes可观察性-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/30/k8sObservability/k8sObserver.png" class title="Kubernetes可观察性">

<ul>
<li>NPD： node problem detector</li>
<li>kube-eventer： Kubernetes 事件离线工具</li>
</ul>
<h2><span id="kubernetes-metrics监控方案-2">Kubernetes Metrics监控方案 [2]</span><a href="#kubernetes-metrics监控方案-2" class="header-anchor">#</a></h2><p>cadvisor&#x2F;exporter+prometheus+grafana</p>
<h2><span id="kubernetes-logs监控方案34">Kubernetes Logs监控方案[3][4]</span><a href="#kubernetes-logs监控方案34" class="header-anchor">#</a></h2><details><summary>es-statefulset.yaml</summary><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">    <span class="attr">version:</span> <span class="string">v7.10.2</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v7.10.2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v7.10.2</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">quay.io/fluentd_elasticsearch/elasticsearch:v7.10.2</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">          <span class="string">...</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9200</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">              <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9300</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">transport</span></span><br><span class="line">              <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;NAMESPACE&quot;</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;MINIMUM_MASTER_NODES&quot;</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">elasticsearch-logging</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;&#125;  <span class="comment">### 生产上不要挂在本地</span></span><br><span class="line">      <span class="comment"># Elasticsearch requires vm.max_map_count to be at least 262144.</span></span><br><span class="line">      <span class="comment"># If your OS already sets up this number to a higher value, feel free</span></span><br><span class="line">      <span class="comment"># to remove this init container.</span></span><br><span class="line">      <span class="attr">initContainers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">alpine:3.6</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&quot;/sbin/sysctl&quot;</span>, <span class="string">&quot;-w&quot;</span>, <span class="string">&quot;vm.max_map_count=262144&quot;</span>]</span><br><span class="line">          <span class="attr">name:</span> <span class="string">elasticsearch-logging-init</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></details>


<details><summary>fluentd-es-configmap.yaml</summary><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;source&gt;</span><br><span class="line">    @id fluentd-containers.log</span><br><span class="line">    @type tail</span><br><span class="line">    path /var/log/containers/*.log  ### 宿主机映射到容器的目录</span><br><span class="line">    pos_file /var/log/es-containers.log.pos ### position日志,消费偏移量</span><br><span class="line">    tag raw.kubernetes.*</span><br><span class="line">    read_from_head true</span><br><span class="line">    &lt;parse&gt;</span><br><span class="line">      @type multi_format</span><br><span class="line">      &lt;pattern&gt;</span><br><span class="line">        format json</span><br><span class="line">        time_key time</span><br><span class="line">        time_format %Y-%m-%dT%H:%M:%S.%NZ</span><br><span class="line">      &lt;/pattern&gt;</span><br><span class="line">      &lt;pattern&gt;</span><br><span class="line">        format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/</span><br><span class="line">        time_format %Y-%m-%dT%H:%M:%S.%N%:z</span><br><span class="line">      &lt;/pattern&gt;</span><br><span class="line">    &lt;/parse&gt;</span><br><span class="line">  &lt;/source&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output.conf: |-</span><br><span class="line">  &lt;match **&gt;</span><br><span class="line">    @id elasticsearch</span><br><span class="line">    @type elasticsearch</span><br><span class="line">    @log_level info</span><br><span class="line">    type_name _doc</span><br><span class="line">    include_tag_key true</span><br><span class="line">    host elasticsearch-logging ### service的名字[集群内部的]</span><br><span class="line">    port 9200</span><br><span class="line">    logstash_format true</span><br><span class="line">    &lt;buffer&gt;</span><br><span class="line">      @type file</span><br><span class="line">      path /var/log/fluentd-buffers/kubernetes.system.buffer</span><br><span class="line">      flush_mode interval</span><br><span class="line">      retry_type exponential_backoff</span><br><span class="line">      flush_thread_count 2</span><br><span class="line">      flush_interval 5s</span><br><span class="line">      retry_forever</span><br><span class="line">      retry_max_interval 30</span><br><span class="line">      chunk_limit_size 2M</span><br><span class="line">      total_limit_size 500M</span><br><span class="line">      overflow_action block</span><br><span class="line">    &lt;/buffer&gt;</span><br><span class="line">  &lt;/match&gt;</span><br></pre></td></tr></table></figure></details>


<details><summary>fluentd-es-ds.yaml</summary><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es-v3.1.1</span><br><span class="line">  namespace: logging</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    version: v3.1.1</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: fluentd-es</span><br><span class="line">      version: v3.1.1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: fluentd-es</span><br><span class="line">        version: v3.1.1</span><br><span class="line">    spec:</span><br><span class="line">      securityContext:</span><br><span class="line">        seccompProfile:</span><br><span class="line">          type: RuntimeDefault</span><br><span class="line">      priorityClassName: system-node-critical</span><br><span class="line">      serviceAccountName: fluentd-es</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd-es</span><br><span class="line">        image: quay.io/fluentd_elasticsearch/fluentd:v3.1.0</span><br><span class="line">        env:</span><br><span class="line">        - name: FLUENTD_ARGS</span><br><span class="line">          value: --no-supervisor -q</span><br><span class="line">        ...</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: /var/log</span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: /var/lib/docker/containers ### 收集打到控制台的日志</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/fluent/config.d</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 24231</span><br><span class="line">          name: prometheus</span><br><span class="line">          protocol: TCP</span><br><span class="line">          ...</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/log</span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/lib/docker/containers</span><br><span class="line">      - name: config-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: fluentd-es-config-v0.2.1</span><br></pre></td></tr></table></figure></details>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://edu.aliyun.com/lesson_1651_18360#_18360">第12 章 ： 可观测性：监控与日志</a></li>
<li><a href="https://www.jianshu.com/p/e76053b6f3f5">Prometheus监控k8s</a>     Prometheus监控方案</li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/release-1.23/cluster/addons/fluentd-elasticsearch">fluentd-elasticsearch</a> EFK监控方案</li>
<li><a href="https://www.bilibili.com/video/BV16t4y1w7r6?p=143">kubernetes架构师课程</a></li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 协议栈</title>
    <url>/www6vHomeHexo/2022/01/30/linuxNetwork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2022/01/30/linuxNetwork/send.png" class title="发送流程">

<img src="/www6vHomeHexo/2022/01/30/linuxNetwork/recv.png" class title="接收流程">


<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1vJ41127p2">#68 网络知识十全大补丸</a> bibibili<br><a href="https://talkgo.org/t/topic/87">网络知识十全大补丸</a></p>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 系统调用SystemCall</title>
    <url>/www6vHomeHexo/2022/01/30/linuxSystemCall/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63564517-e721db00-c597-11e9-86eb-2e5d502a2c52.jpg" alt="系统调用">  系统调用</p>
</div>

<h2><span id="关键系统调用">关键系统调用</span><a href="#关键系统调用" class="header-anchor">#</a></h2><ul>
<li>file<br>read()&#x2F;write()&#x2F;open()&#x2F;close()</li>
<li>process<br>fork()&#x2F;exec()</li>
<li>network<br>connect()&#x2F;accept()</li>
<li>memory<br>mmap()&#x2F;brk()</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>《性能之巅》 3.2.6 系统调用</p>
]]></content>
      <categories>
        <category>linux</category>
        <category>系统调用</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Kernel总结</title>
    <url>/www6vHomeHexo/2022/01/30/linuxKernel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>关键字: 内核, 系统调用</p>
<h1><span id="kernel">Kernel</span><a href="#kernel" class="header-anchor">#</a></h1><h3><span id="linux-内核架构图">Linux 内核架构图</span><a href="#linux-内核架构图" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/01/30/linuxKernel/kernelMap1.jpg" class title="kernel">


<h3><span id="linux-内核架构图-2626-版本-2">Linux 内核架构图- 2.6.26 版本  [2]</span><a href="#linux-内核架构图-2626-版本-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/01/30/linuxKernel/performance2.jpg" class>



<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63564514-e5f0ae00-c597-11e9-9d32-985d0771c207.png" alt="内核体系结构"> 内核体系结构</p>
</div>





<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《03 你可以把Linux内核当成一家软件外包公司的老板》 趣谈Linux操作系统   刘超</li>
<li>《15丨性能测试场景：如何进行监控设计？》 ***  高楼</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>RocksDB 总结</title>
    <url>/www6vHomeHexo/2022/01/29/rocksdb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="b树-vs-lsm-tree2">B+树 vs LSM-tree[2]</span><a href="#b树-vs-lsm-tree2" class="header-anchor">#</a></h2><ul>
<li>B+树<ul>
<li>读多写少的场景</li>
<li>牺牲写, 提高读</li>
</ul>
</li>
<li>LSM-tree<ul>
<li>写多读少的场景</li>
<li>不是一种数据结构，是一种数据的组织方式<ul>
<li>内存中”就地写”<br>到达阈值进行flush, 生成新的数据块, 不阻塞磁盘压缩合并</li>
<li>磁盘中”追加写”<br>内存顺序io &gt;&gt; 内存随机io &#x3D;&#x3D; 磁盘顺序io &gt;&gt; 磁盘随机io</li>
<li>数据块的存储方式<ul>
<li>拆分, 有序的快存储</li>
<li>磁盘压缩合并</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="rocksdb组件和架构23">RocksDB组件和架构[2][3]</span><a href="#rocksdb组件和架构23" class="header-anchor">#</a></h2><ul>
<li>分治<ul>
<li>按冷热数据分层</li>
<li>多路归并</li>
</ul>
</li>
<li>level<ul>
<li>重复<br>level0 单个sstable有序不重复<br>level1-leveln 每一层中没有数据重复，层与层之间有冗余数据<br>在所有层中，单个sstable有序不重复</li>
<li>有序<br>level1-leveln   sstable之间有序(range)</li>
<li>90% 的数据存储在最后一层, 空间放大不超过 1.11（L0 层的数据较少，可以忽略不计）</li>
</ul>
</li>
<li>内存数据结构[2]<ul>
<li>时间复杂度<ul>
<li>红黑树， 跳表 O(logn)</li>
<li>b+&#x2F;b O(m * logn)</li>
</ul>
</li>
<li>并发读写 互斥性<ul>
<li>红黑树(不推荐)<br>重新着色，旋转的方式</li>
<li>跳表 多层级有序链表(推荐)<br>插入的流程: 先随机层数, 再建立节点间关系<br>加锁的数据结构:<br>高度(atomic<int> max_heigh_) + 节点(atomic<node> next_)      <img src="/www6vHomeHexo/2022/01/29/rocksdb/skip-list.png" class>       
<img src="/www6vHomeHexo/2022/01/29/rocksdb/mtable-optimized.png" class title="写操作, 能指令重排">
<img src="/www6vHomeHexo/2022/01/29/rocksdb/mtable-optimized1.png" class title="写操作">
原子操作<br>max_heigh_.store(height, … relax) 性能优化点: 能指令重排,不影响正确性.  增加了并发性能<br>max_heigh_.store(height, … release)  不能指令重排<br>max_heigh_.load()</node></int></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="rocksdb-写x2f读流程-5">RocksDB 写&#x2F;读流程 [5]</span><a href="#rocksdb-写x2f读流程-5" class="header-anchor">#</a></h2><ul>
<li>写流程 </li>
<li>读流程</li>
</ul>
<h2><span id="rocksdb可改进点4">RocksDB可改进点[4]</span><a href="#rocksdb可改进点4" class="header-anchor">#</a></h2><ul>
<li><p>写放大</p>
<ul>
<li>说明：RocksDB 支持多种 Compaction。下面分析的是 Level Style Compaction。</li>
<li>RocksDB 的写放大分析：<br>+1 - redo log 的写入<br>+1 - Immutable Memtable 写入到 L0 文件<br>+2 - L0 和 L1 compaction（L0 SST 文件的 key 范围是重叠的，出于性能考虑，一般尽量保持 L0 和 L1 的数据大小是一样的，每次拿全量 L0 的数据和全量 L1 的数据进行 compaction）<br>+11 - Ln-1 和 Ln 合并的写入（n &gt;&#x3D; 2，默认情况下，Ln 的数据大小是 Ln-1 的 10 倍，见 max_bytes_for_level_multiplier ）。</li>
<li>所以，总的写放大是 4 + 11 * (n-1)  &#x3D; 11 * n - 7 倍。关键是 n 的取值。</li>
</ul>
</li>
<li><p>读放大<br>LSM-Tree 的读操作需要从新到旧（从上到下）一层一层查找，直到找到想要的数据。这个过程可能需要不止一次 I&#x2F;O。特别是 range query 的情况，影响很明显。</p>
</li>
<li><p>空间放大（Space Amplification）<br>因为所有的写入都是顺序写（append-only）的，不是 in-place update ，所以过期数据不会马上被清理掉。</p>
</li>
</ul>
<h2><span id="rocksdb的应用">RocksDB的应用</span><a href="#rocksdb的应用" class="header-anchor">#</a></h2><ul>
<li>pika<br>redis协议的rocksdb引擎</li>
<li>myrocks<br>mysql innodb换成rocksdb<ul>
<li>磁盘利用率<br>eg. innodb 150G(空间放大严重: 叶子节点的分裂, 磁盘利用率低(50%左右)), rocksdb只需要50G，</li>
</ul>
</li>
<li>Tidb</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://developer.aliyun.com/article/669316">看图了解RocksDB</a> 未</li>
<li><a href="https://www.bilibili.com/video/BV1Jr4y1W7Wn?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">RocksDB 是什么？解决了什么问题？是怎么解决的？解决了哪些具体问题？</a> bili  ***</li>
<li><a href="https://docs.pingcap.com/zh/tidb/dev/rocksdb-overview">RocksDB 简介</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1352666?ivk_sa=1024320u">LSM-Tree 的写放大写放大、读放大、空间放大RockDB 写放大简单分析参考文档</a></li>
<li><a href="/www6vHomeHexo/2022/04/05/rocksdbLsm/" title="RocksDB- LSM-Tree">RocksDB- LSM-Tree</a> self</li>
<li><a href="https://www.infoq.cn/article/u3leu3EmGJjWflQLtJHs">字节跳动在 RocksDB 存储引擎上的改进实践</a> *** 未</li>
<li><a href="https://www.cnblogs.com/lygin/p/17158774.html">Rocksdb 调优指南</a>  中文翻译 未<br>   <a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide">RocksDB Tuning Guide</a>原文</li>
<li><a href="https://zhuanlan.zhihu.com/p/616209332">深入RocksDB原理</a> *** 未</li>
<li><a href="https://gaomf.cn/2021/11/28/Paper_RocksDB_Developemnt/">【读读论文】RocksDB 发展回顾及展望 </a>  论文 未</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>RocksDB</category>
      </categories>
      <tags>
        <tag>RocksDB</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps 总结</title>
    <url>/www6vHomeHexo/2022/01/29/devops/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#devops-roadmap">Devops Roadmap</a></li>
<li><a href="#devops">DevOps</a><br>- <a href="#%E5%88%86%E6%94%AF%E7%AD%96%E7%95%A5-4">分支策略 [4]</a><br>- <a href="#%E6%9C%AC%E5%9C%B0%E5%BC%80%E5%8F%91-6">本地开发 [6]</a><br>- <a href="#%E6%8A%80%E6%9C%AF%E5%80%BA%E5%8A%A1-7">技术债务 [7]</a><br>- <a href="#%E4%BB%A3%E7%A0%81%E9%A2%84%E6%A3%80%E6%9F%A5-8">代码预检查 [8]</a><br>- <a href="#%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7">编程工具</a></li>
<li><a href="#monitorself">Monitor[self]</a></li>
<li><a href="#devsecops">DevSecOps</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="devops-roadmap">Devops Roadmap</span><a href="#devops-roadmap" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/29/devops/devopsRoadmap.png" class title="DevopsRoadmap">

<h2><span id="devops">DevOps</span><a href="#devops" class="header-anchor">#</a></h2><p>一种理念，文化</p>
<h5><span id="分支策略-4">分支策略 [4]</span><a href="#分支策略-4" class="header-anchor">#</a></h5><ul>
<li>主干开发，分支发布</li>
<li>分支开发，主干发布</li>
<li>主干开发，主干发布<br>大道至简</li>
</ul>
<h5><span id="本地开发-6">本地开发 [6]</span><a href="#本地开发-6" class="header-anchor">#</a></h5><p>KSync<br>Skaffold-谷歌<br>Nocalhost-腾讯CODING</p>
<h5><span id="技术债务-7">技术债务 [7]</span><a href="#技术债务-7" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/29/devops/debt.JPG" class title="技术债务">

<h5><span id="代码预检查-8">代码预检查 [8]</span><a href="#代码预检查-8" class="header-anchor">#</a></h5><ul>
<li>本地检查<ul>
<li>IDE插件<br>SonarLint&#x2F;FindBugs&#x2F;CheckStyle&#x2F;PMD&#x2F;阿里规范插件</li>
<li>本地构建</li>
<li>本地测试</li>
</ul>
</li>
<li>本地提交<ul>
<li>工具<br>pre-commit 的多语言包管理工具——pre-commit</li>
</ul>
</li>
<li>远程提交<ul>
<li>工具<br>SonarQube+增量检查</li>
<li>人工评审</li>
<li>自动化测试<br>jacoco 检查单元测试覆盖率<br>一定范围的集成测试</li>
</ul>
</li>
<li>分支合并<ul>
<li>工具<br>GitHub的PR<br>GitHub的MR</li>
</ul>
</li>
<li>CodeView<br>工具 <strong>Gerrit</strong></li>
</ul>
<h5><span id="编程工具">编程工具</span><a href="#编程工具" class="header-anchor">#</a></h5><ul>
<li>codota<br>codota是以色列人开发的一个软件,以深度学习为技术背景</li>
</ul>
<h2><span id="monitorself">Monitor[self]</span><a href="#monitorself" class="header-anchor">#</a></h2><ul>
<li>Metric </li>
<li>APM</li>
<li>Log</li>
</ul>
<h2><span id="devsecops">DevSecOps</span><a href="#devsecops" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>xxx</li>
<li>xxx</li>
<li><a href="https://lib.jimmysong.io/service-mesh-devsecops/">利用服务网格为基于微服务的应用程序实施 DevSecOps</a>  jimmy song 未</li>
<li><a href>11 | 分支策略：让研发高效协作的关键要素</a> 石雪峰</li>
<li>xxx</li>
<li>《16 | 环境管理：一切皆代码是一种什么样的体验？ 》  石雪峰</li>
<li><a href>15 | 技术债务：那些不可忽视的潜在问题</a> 石雪峰</li>
<li>《06  代码预检查：提高入库代码质量的神兵利器》  lg2064-DevOps 落地笔记-拉钩专栏</li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性-OpenTelemetry</title>
    <url>/www6vHomeHexo/2022/01/29/observabilityOpenTelemetry/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV18K4y1M7bL">可观测性技术生态和 OpenTelemetry 原理及实践，陈一枭</a>  ***<br><a href="https://lib.jimmysong.io/opentelemetry-obervability/">OpenTelemetry 可观测性的未来</a> Jimmy Song - Ted Young ***</p>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>OpenTelemetry</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>github资源分类</title>
    <url>/www6vHomeHexo/2022/01/27/categoryOfGithub/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="language">Language</span><a href="#language" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/concurrency-programming-via-rust">concurrency-programming-via-rust</a>  鸟窝 大佬 rust  </p>
<p><a href="https://github.com/www6v/read">read</a>   *****<br>Go 学习之路：Go 开发者博客、Go 微信公众号、Go 学习资料（文档、书籍、视频） </p>
<p><a href="https://github.com/www6v/gopl-zh">gopl-zh</a>  Go语言圣经中文版  ***</p>
<p><a href="https://github.com/www6v/learn-go-with-tests">learn-go-with-tests</a>  ***</p>
<p><a href="https://github.com/www6v/go-advice">go-advice</a><br>List of advice and tricks for Go - 面试  *</p>
<p><a href="https://github.com/www6v/awesome-go-cn">awesome-go-cn</a>   Go 资源大全中文版 *</p>
<p><a href="https://github.com/www6v/book-1">book-1</a>  学习笔记 golang *</p>
<p><a href="https://github.com/www6v/go-perfbook">go-perfbook</a>  Thoughts on Go performance optimization  </p>
<p><a href="https://github.com/www6v/under-the-hood">under-the-hood</a>  Go: Under The Hood | Go 语言原本 | <a href="https://golang.design/under-the-hood">https://golang.design/under-the-hood</a>     </p>
<h2><span id="devops">Devops</span><a href="#devops" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/devops-exercises">devops-exercises</a> DevOps Interview Questions ***</p>
<p><a href="https://github.com/www6v/StabilityGuide">StabilityGuide</a>  打造国内稳定性领域知识库 ***</p>
<p><a href="https://github.com/www6v/awesome-chaosblade">awesome-chaosblade</a>     </p>
<h2><span id="k8s-容器">K8S 容器</span><a href="#k8s-容器" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/k8s_awesome_document">k8s_awesome_document</a> *** </p>
<p><a href="https://github.com/www6v/101">101</a>   A basic instruction for Kubernetes setup and understanding.  ***</p>
<p><a href="https://github.com/www6v/kubernetes-the-hard-way">kubernetes-the-hard-way</a>   </p>
<p><a href="https://github.com/www6v/kiosk">kiosk</a>  Multi-Tenancy Extension For Kubernetes </p>
<p><a href="https://github.com/www6v/kubernetes-practice-guide">kubernetes-practice-guide</a>   Kubernetes Practice Guide (Kubernetes 实践指南)        </p>
<p><a href="https://github.com/www6v/k8s-tool">k8s-tool</a>  k8s工具，适用于cka考试，主要包括脚本和yaml文件   </p>
<p><a href="https://github.com/www6v/k8sToolSkaffold">k8sToolSkaffold</a> Easy and Repeatable Kubernetes Development    </p>
<p><a href="https://github.com/www6v/curriculum">curriculum</a>   Open Source Curriculum for CNCF Certification Courses   </p>
<p><a href="https://github.com/www6v/gardener">gardener</a>   多云管理</p>
<p><a href="https://github.com/www6v/cloud-native-istio">cloud-native-istio</a>  华为云原生丛书之《云原生服务网格Istio：原理、实践、架构与源码解析》 </p>
<p><a href="https://github.com/www6v/prometheus-book">prometheus-book</a>  Prometheus操作指南  </p>
<p><a href="https://github.com/www6v/container-monitor">container-monitor</a> 容器监控方案汇总  </p>
<p><a href="https://github.com/www6v/community-1">community-1</a> kubernetes&#x2F;community</p>
<p><a href="https://github.com/www6v/Kubernetes-Certified-Administrator">Kubernetes-Certified-Administrator</a>  CNCF CKA</p>
<p><a href="https://github.com/www6v/Certified-Kubernetes-Security-Specialist">Certified-Kubernetes-Security-Specialist</a>  CNCF  CKS</p>
<h2><span id="linux">Linux</span><a href="#linux" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/linux_kernel_wiki">linux_kernel_wiki</a>   linux内核学习资料   0voice ***</p>
<p><a href="https://github.com/www6v/kernel_awsome_feature">kernel_awsome_feature</a>   深入研究 kvm,ceph,fuse特性 ***</p>
<p><a href="https://github.com/www6v/perf-tools">perf-tools</a>  Performance analysis tools based on Linux perf_events (aka perf) and ftrace    ***</p>
<p><a href="https://github.com/www6v/linux-perf-examples">linux-perf-examples</a>  《Linux 性能优化实战》案例     </p>
<h2><span id="algo">Algo</span><a href="#algo" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/algorithm">algorithm</a></p>
<p><a href="https://github.com/www6v/Java">Java</a>  All Algorithms implemented in Java</p>
<p><a href="https://github.com/www6v/leetcode-1">leetcode-1</a></p>
<p><a href="https://github.com/www6v/LeetCodeAnimation">LeetCodeAnimation</a> 用动画的形式呈现解LeetCode题目的思路</p>
<p><a href="https://github.com/www6v/awesome-java-leetcode">awesome-java-leetcode</a>   LeetCode of algorithms with java</p>
<p><a href="https://github.com/www6v/algorithms">algorithms</a> </p>
<p><a href="https://github.com/www6v/LeetCode-Go">LeetCode-Go</a></p>
<h2><span id="bigdataampstreaming">BigData&amp;Streaming</span><a href="#bigdataampstreaming" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/flink-tutorials">flink-tutorials</a></p>
<p><a href="https://github.com/www6v/flink-learning">flink-learning</a>  flink learning blog. *** <a href="http://www.54tianzhisheng.cn/">http://www.54tianzhisheng.cn</a>  </p>
<p><a href="https://github.com/www6v/big-data-study">big-data-study</a></p>
<h2><span id="interview">Interview</span><a href="#interview" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/JavaGuide">JavaGuide</a>   Java学习+面试指南 ***</p>
<p><a href="https://github.com/www6v/reverse-interview">reverse-interview</a>   Questions to ask the company during your interview  </p>
<p><a href="https://github.com/www6v/JCSprout">JCSprout</a>  Java Core Sprout : basic, concurrent, algorithm面试</p>
<p><a href="https://github.com/www6v/system-design-primer">system-design-primer</a> ***   </p>
<p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.  </p>
<h2><span id="community">community</span><a href="#community" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/talkgo-night">talkgo-night</a>    Go 夜读 ***</p>
<p><a href="https://github.com/www6v/conference">gopherchina  conference</a>  conference-ppt ***</p>
<p><a href="https://github.com/www6v/academy">academy</a>  云原生学院|直播|活动 ***</p>
<h2><span id="综合">综合</span><a href="#综合" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/patterns-of-distributed-systems">patterns-of-distributed-systems</a>  分布式设计模式 *** </p>
<p><a href="https://github.com/www6v/blog_demos">blog_demos</a>    all demos for my blogs : <a href="http://blog.csdn.net/boling_cavalry">http://blog.csdn.net/boling_cavalry</a>    ***</p>
<p><a href="https://github.com/www6v/cloud-atlas">cloud-atlas</a>   云计算指南</p>
<p><a href="https://github.com/www6v/open_source_team">open_source_team</a>     国内顶尖团队的开源地址  </p>
<p><a href="https://github.com/www6v/issue-case">issue-case</a>   问题&#x2F;故障案例总结与整理</p>
<p><a href="https://github.com/www6v/Qix">Qix</a> Machine Learning、Deep Learning、PostgreSQL、Distributed System、Node.Js、Golang  </p>
<h2><span id="speciality">speciality</span><a href="#speciality" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/ddd-starter-modelling-process">ddd-starter-modelling-process</a>   DDD </p>
<p><a href="https://github.com/www6v/geektime-nginx">geektime-nginx</a>   极客时间：nginx核心知识100讲配置文件与代码分享</p>
<p><a href="https://github.com/www6v/staffjoy">staffjoy</a> 微服务(Microservices)和云原生架构教学案例项目，基于Spring Boot和Kubernetes技术栈   </p>
<h2><span id="databases">databases</span><a href="#databases" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/awesome-database-learning">awesome-database-learning</a>   A list of learning materials to understand databases internals  </p>
<p><a href="https://github.com/www6v/meetup">meetup</a>    pingcap meetup</p>
<h2><span id="serverless">serverless</span><a href="#serverless" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/serverless">serverless</a>    Serverless Framework</p>
<p><a href="https://github.com/www6v/funcraft">funcraft</a>  (have) Fun with Serverless(API Gateway &amp; Function Compute)  </p>
<p><a href="https://github.com/www6v/ibm-opentech-ma">ibm-opentech-ma</a>   IBM open technology mini academy-knative</p>
<p><a href="https://github.com/www6v/knative101">knative101</a>  Forked from IBM&#x2F;knative101</p>
<h2><span id="ai">AI</span><a href="#ai" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/aiops-handbook">aiops-handbook</a>  Collection of slides, repositories, papers about AIOps</p>
<p><a href="https://github.com/www6v/d2l-zh">d2l-zh</a>  《动手学深度学习》：面向中文读者、能运行、可讨论</p>
<p><a href="https://github.com/www6v/Machine-learning-learning-notes">Machine-learning-learning-notes</a></p>
<p><a href="https://github.com/www6v/aiops">aiops</a>  All things about aiops  </p>
<p><a href="https://github.com/www6v/awesome-AIOps">awesome-AIOps</a>    AIOps学习资料汇总</p>
<p><a href="https://github.com/www6v/berkeley-stat-157">berkeley-stat-157</a>   Homepage for STAT 157 at UC Berkeley </p>
<h2><span id="paper-amp-大学课程">Paper &amp; 大学课程</span><a href="#paper-amp-大学课程" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/cn">cn</a> learn-sys ***</p>
<p><a href="https://github.com/www6v/CS-Notes">CS-Notes</a>   Computer Science Learning Notes  </p>
<p><a href="https://github.com/www6v/distributed_system_readings">distributed_system_readings</a>   distributed systems</p>
<p><a href="https://github.com/www6v/XRP">XRP</a>    XRP: In-Kernel Storage Functions with eBPF        </p>
<p><a href="https://github.com/www6v/awesome-serverless-papers">awesome-serverless-papers</a>  Collect papers about serverless computing research   </p>
<p><a href="https://github.com/www6v/streaming-readings">streaming-readings</a>  Streaming System 相关的论文读物    </p>
<p><a href="https://github.com/www6v/paper-note">paper-note</a>  paper note  </p>
<p><a href="https://github.com/www6v/papers-we-love">papers-we-love</a>  Papers from the computer science community to read and discuss.</p>
<p><a href="https://github.com/www6v/services-engineering">services-engineering</a>  A reading list for services engineering</p>
<p><a href="https://github.com/www6v/SJTU-Courses">SJTU-Courses</a>  上海交通大学课程资料分享</p>
<p><a href="https://github.com/www6v/zju-icicles">zju-icicles</a>  浙江大学课程攻略共享计划        </p>
<p><a href="https://github.com/www6v/REKCARC-TSC-UHT">REKCARC-TSC-UHT</a>   清华大学计算机系课程攻略</p>
<p><a href="https://github.com/www6v/USTC-Course">USTC-Course</a>  中国科学技术大学课程资源 </p>
<h2><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/hangzhou_house_knowledge">hangzhou_house_knowledge</a>    2017年买房经历总结出来的买房购房知识分享给大家</p>
<p><a href="https://github.com/www6v/main-runner">main-runner</a>   wrap java maven project by command line start script    </p>
<p> <a href="https://github.com/www6v/hacker-laws-zh">hacker-laws-zh</a>  对开发人员有用的定律、理论、原则和模式。 ***       </p>
<p><a href="https://github.com/www6v/aws-sap-code">aws-sap-code</a>   Code used in our AWS Certified Solutions Architect Professional course  </p>
<h2><span id="self">Self</span><a href="#self" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/awesome-awesomes"> awesome-awesomes</a>  ***</p>
<p><a href="https://github.com/www6v/demo">demo</a></p>
<p><a href="https://github.com/www6v/dev-ops">dev-ops</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>DPDK</title>
    <url>/www6vHomeHexo/2022/01/25/linuxDPDK/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="dpdk的优化方式">DPDK的优化方式</span><a href="#dpdk的优化方式" class="header-anchor">#</a></h2><ul>
<li>cache和内存<br>huge page，cacheline align， 线程绑定， 预取， NUMA </li>
<li>并行计算<ul>
<li>多核</li>
<li>亲和性</li>
<li>指令并发与数据并行<br>SIMD 单指令多数据</li>
</ul>
</li>
<li>流分类 多队列<ul>
<li>网卡多队列  多队列网卡<br> 不同核操作不同的队列</li>
<li>流分类<br> RSS</li>
</ul>
</li>
</ul>
<h2><span id="dpdk可以调用的内核接口">DPDK可以调用的内核接口</span><a href="#dpdk可以调用的内核接口" class="header-anchor">#</a></h2><p>  KNI（Kernel NIC Interface）      </p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1cy4y117UG?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">为什么dpdk越来越受欢迎，看完以后，让人醍醐灌顶</a>  V</li>
<li>&lt;&lt;深入浅出DPDK&gt;&gt;</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>DPDK</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP3-构建腾讯云上高可用架构</title>
    <url>/www6vHomeHexo/2022/01/25/tencentTCP3/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="三-构建腾讯云上高可用架构15">三、构建腾讯云上高可用架构(15%)</span><a href="#三-构建腾讯云上高可用架构15" class="header-anchor">#</a></h2><h3><span id="1云上高可用概述">1.云上高可用概述</span><a href="#1云上高可用概述" class="header-anchor">#</a></h3><h5><span id="11-构建腾讯云上高可用架构相关概念">1.1 构建腾讯云上高可用架构相关概念</span><a href="#11-构建腾讯云上高可用架构相关概念" class="header-anchor">#</a></h5><p>可用性的含义，可用性的计算方法	☆☆<br>同城灾备、异地灾备，消息队列：MQ、Kafka	☆☆</p>
<h5><span id="12-面临的挑战">1.2 面临的挑战</span><a href="#12-面临的挑战" class="header-anchor">#</a></h5><p>高可用面临的挑战	☆</p>
<ul>
<li>高可用面临的挑战<ul>
<li>高可用的目标<br>99.99%系统可用性年不可用时间-0.876小时<br>基本可用: 99%<br>较高可用性: 99.9%<br>高可用性: 99.99%<br>极高可用性: 99.999%</li>
<li>事故及问题归类<ul>
<li>内部<br>代码问题<br>配置问题<br>性能问题  </li>
<li>供应商<br>通道&#x2F;专线抖动<br>运营商故障<br>机房整体故障</li>
<li>外部威胁<br>攻击<br>劫持</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="13-构建高可用的架构目的">1.3 构建高可用的架构目的</span><a href="#13-构建高可用的架构目的" class="header-anchor">#</a></h5><p>为什么需要高可用架构？	☆☆</p>
<ul>
<li>高可用的整体规划<ul>
<li>成本曲线 反比 业务中断影响(损失)</li>
<li>不同阶段客户推荐容灾方案<ul>
<li>初创期<ol>
<li>数据容灾</li>
<li>环境容灾</li>
<li>安全保障</li>
</ol>
</li>
<li>成长期<ol>
<li>数据&#x2F;环境容灾</li>
<li>安全保障</li>
<li>同机房多集群</li>
<li>同区域多机房</li>
</ol>
</li>
<li>稳定器<ol>
<li>数据&#x2F;环境容灾</li>
<li>安全保障</li>
<li>同机房多集群</li>
<li>同区域多机房</li>
<li>跨区域多机房</li>
</ol>
</li>
<li>总体规划<ul>
<li>分层设计思路<br>接入和服务分层解耦<br>服务和数据分层解耦<br>分层高内聚 低耦合<br>分层支持灵活扩展<br>分层支持冗余容灾部署<br>分层支持set化部署</li>
</ul>
</li>
<li>分层解析<img src="/www6vHomeHexo/2022/01/25/tencentTCP3/layer-analysis.png" class title="分层解析"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="2构建外层高可用架构">2.构建外层高可用架构</span><a href="#2构建外层高可用架构" class="header-anchor">#</a></h3><h5><span id="21-外层高可用架构的设计原则">2.1 外层高可用架构的设计原则</span><a href="#21-外层高可用架构的设计原则" class="header-anchor">#</a></h5><p>外层网络面临的问题	☆<br>外层高可用的设计思路：DNS的解析过程、出现解析问题如何解决？为什么会出现访问慢的问题？应该如何解决？	☆</p>
<ul>
<li><p>外层网络面临的问题</p>
<ul>
<li>外层网络<br>从用户发起请求到服务接入之间的网络</li>
<li>用户外层访问, 面临的问题<ul>
<li>域名解析异常<img src="/www6vHomeHexo/2022/01/25/tencentTCP3/error1.png" class></li>
<li>访问速度慢  <img src="/www6vHomeHexo/2022/01/25/tencentTCP3/error2.png" class></li>
</ul>
</li>
</ul>
</li>
<li><p>外层高可用的设计思路</p>
<ul>
<li>域名解析问题的解决方法<img src="/www6vHomeHexo/2022/01/25/tencentTCP3/dns-error-solution.png" class></li>
<li>针对访问速度慢的问题<img src="/www6vHomeHexo/2022/01/25/tencentTCP3/slow-response-solution.png" class></li>
</ul>
</li>
</ul>
<h5><span id="22-设计外层的高可用架构">2.2 设计外层的高可用架构</span><a href="#22-设计外层的高可用架构" class="header-anchor">#</a></h5><p>外层设计相关产品	☆☆<br>云解析、HttpDNS和CDN架构	☆☆☆</p>
<h3><span id="3-构建接入层高可用架构">3. 构建接入层高可用架构</span><a href="#3-构建接入层高可用架构" class="header-anchor">#</a></h3><h5><span id="31-接入层高可用架构的设计原则">3.1 接入层高可用架构的设计原则</span><a href="#31-接入层高可用架构的设计原则" class="header-anchor">#</a></h5><p>接入层面临的问题、接入层高可用设计思路	☆☆</p>
<ul>
<li><p>接入层面临的问题</p>
<ul>
<li>接入层<br>指的是提供服务的负载均衡层</li>
<li>接入层要考虑的问题<br>负载均衡冗余<br>负载均衡的安全问题</li>
</ul>
</li>
<li><p>接入层高可用设计思路</p>
<ul>
<li>2个解决方案<br><img src="/www6vHomeHexo/2022/01/25/tencentTCP3/access-solution.png" class></li>
</ul>
</li>
</ul>
<h5><span id="32-设计接入层的高可用架构">3.2 设计接入层的高可用架构</span><a href="#32-设计接入层的高可用架构" class="header-anchor">#</a></h5><p>接入层设计相关产品	☆☆<br>云解析与CLB构建接入层高可用架构	☆☆☆<br>CLB与Anycast IP构建接入层高可用架构	☆☆<br>使用Anycast IP解决CLB冗余、接入层安全架构和BGP高防架构 ☆☆</p>
<ul>
<li>基于腾讯云产品的接入层设计<ul>
<li>接入层产品清单<br>CLB， Anycast CLB， 云解析， BGP高防</li>
<li>云解析<br>云解析提高CLB的冗余[pic]</li>
<li>Anycast<br>使用Anycast IP提高用户接入可用性[pic]<br>使用Anycast IP解决CLB冗余[pic]</li>
<li>BGP高防<ul>
<li>BGP高防使用建议<ul>
<li>平滑切换线上业务到BGP高防IP<br>技术维度<br>业务维度</li>
<li>源站IP暴露的解决方法<br><strong>防止出现攻击绕过高防直接攻击源站IP的情况，强烈建议参考以下方法</strong></li>
<li>BGP高防IP+源站[pic]</li>
<li>业务系统压力测试</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="4-构建应用层高可用架构">4. 构建应用层高可用架构</span><a href="#4-构建应用层高可用架构" class="header-anchor">#</a></h3><h5><span id="41-应用层高可用架构的设计原则">4.1 应用层高可用架构的设计原则</span><a href="#41-应用层高可用架构的设计原则" class="header-anchor">#</a></h5><p>应用层设计面临的问题	☆<br>应用层高可用设计思路	☆☆☆</p>
<ul>
<li><p>应用层设计面临的问题</p>
<ul>
<li>软件架构如何选择</li>
<li>应用部署容量如何规划</li>
<li>应用部署落地平台如何选择</li>
<li>如何考虑应用层的容灾</li>
<li>应用层的安全如何保障</li>
</ul>
</li>
<li><p>应用层高可用设计思路</p>
<ul>
<li>软件架构选择<br>传统应用和云原生应用</li>
<li>应用扩展和容量规划<br>传统应用: 云主机+负载均衡+弹性伸缩, 配置伸缩策略实现资源池容量的弹性<br>云原生应用可考虑微服务+容器化部署</li>
<li>应用的容灾问题<br>多可用区 多地域容灾部署<br>应用set化部署</li>
<li>安全设计<br>结合应用层安全防护产品， 加固应用层防护</li>
</ul>
</li>
</ul>
<h5><span id="42-设计应用层高可用架构">4.2 设计应用层高可用架构</span><a href="#42-设计应用层高可用架构" class="header-anchor">#</a></h5><p>应用层设计相关产品	☆☆<br>传统应用高可用	☆☆<br>云原生应用部署、应用的容灾设计	☆☆</p>
<ul>
<li><p>应用层设计相关产品</p>
</li>
<li><p>传统应用高可用</p>
<ul>
<li>CLB+CVM+AS  <ul>
<li>架构图 [pic]</li>
<li>应用实践</li>
</ul>
</li>
</ul>
</li>
<li><p>云原生应用部署</p>
<ul>
<li>涉及的产品<ul>
<li>微服务平台 TSF</li>
<li>API网关</li>
<li>TKE容器服务</li>
</ul>
</li>
<li>云原生应用部署方案[pic]</li>
</ul>
</li>
<li><p>应用的容灾设计[pic 要重新看]</p>
<ul>
<li>单区域容灾   </li>
<li>跨地域容灾</li>
<li>跨地域多活<br>业务拆分, 单元化部署</li>
<li>混合云部署<br>云上和IDC各部署一套完整的业务系统</li>
<li>异地多活set化部署<br>Unit由多个set组成<br>建议单写多读的架构<br>set不一定限制在一个机房，可跨机房、跨地域部署</li>
</ul>
</li>
<li><p>应用安全设计  </p>
<ul>
<li>web应用防火墙</li>
<li>漏洞扫描<br>saas服务</li>
<li>移动应用安全</li>
<li>应用级智能网关</li>
</ul>
</li>
</ul>
<h3><span id="5构建中间件层的高可用架构">5.构建中间件层的高可用架构</span><a href="#5构建中间件层的高可用架构" class="header-anchor">#</a></h3><h5><span id="51-中间件层高可用架构的设计原则">5.1 中间件层高可用架构的设计原则</span><a href="#51-中间件层高可用架构的设计原则" class="header-anchor">#</a></h5><p>中间件层面临的问题	☆<br>中间件层高可用设计思路	☆☆☆</p>
<h5><span id="52-设计中间件层高可用架构">5.2 设计中间件层高可用架构</span><a href="#52-设计中间件层高可用架构" class="header-anchor">#</a></h5><p>中间层设计相关产品	☆☆<br>CMQ实现中间层高可用架构、Ckafka实现中间层高可用架构	☆☆</p>
<h3><span id="6构建数据层高可用架构">6.构建数据层高可用架构</span><a href="#6构建数据层高可用架构" class="header-anchor">#</a></h3><h5><span id="61-数据层高可用架构的设计原则">6.1 数据层高可用架构的设计原则</span><a href="#61-数据层高可用架构的设计原则" class="header-anchor">#</a></h5><p>数据层高可用面临的问题	☆☆<br>数据高可用的设计思路	☆☆☆</p>
<h5><span id="62-设计数据层高可用架构">6.2 设计数据层高可用架构</span><a href="#62-设计数据层高可用架构" class="header-anchor">#</a></h5><p>数据层设计相关产品	☆☆<br>CBS实现数据层高可用架构、CFS实现数据层高可用架构、 COS实现数据层高可用架构、CDB实现数据库层高可用架构	☆☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能-学习资源</title>
    <url>/www6vHomeHexo/2022/01/22/aiStudyResouce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="书籍">书籍</span><a href="#书籍" class="header-anchor">#</a></h2><ul>
<li>理论<ul>
<li>《人工智能的数据基础》</li>
<li>《统计学习方法》 v2</li>
<li>The Hundred-Page Machine Learning Book - 入门</li>
<li>西瓜书 ***</li>
<li>花书</li>
</ul>
</li>
<li>框架<ul>
<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, v3 - 入门</li>
<li>Deep Learning with PyTorch - 入门</li>
<li>Deep Learning with Python - v2 - keras库</li>
</ul>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>浙大 - 吴浩基   ***</li>
<li>周志华 《机器学习初步》 *** </li>
<li>Coursera吴恩达- 《machine learning》+ 笔记  入门  *** </li>
<li><a href="https://www.bilibili.com/video/av79340208/">Machine Learning A-Z Hands-On Python &amp; R In Data Science</a>  Udemy 入门  ***</li>
<li><a href="https://www.bilibili.com/video/BV1KB4y1E73v">【人工智能系列】【中文】机器学习A-Z Machine Learning in Chinese(前7部分)</a></li>
<li><a href="https://www.bilibili.com/video/BV1jF411A7VF/">[Coursera公开课] [机器学习专项课程1&#x2F;4] 机器学习基础：案例研究</a>  ***</li>
<li><a href="https://www.bilibili.com/video/BV1Bg411Z77N">聚类算法：层次聚类、k-means 聚类、k-medoids 聚类、密度聚类</a>  ***</li>
</ul>
<h2><span id="深度学习">深度学习</span><a href="#深度学习" class="header-anchor">#</a></h2><ul>
<li>《动手学深度学习- 第二版 -pyTorch》  ***<br>b站有视频课<br><a href="http://zh.d2l.ai/index.html">《动手学深度学习》</a> 在线<br>电子书+jupternote代码</li>
<li>《神经网络和深度学习 》 复旦  </li>
<li>Coursera吴恩达《深度学习》 + 笔记  ***</li>
<li>老唐 ***</li>
<li>莫烦Python </li>
<li>北京大学 TensorFlow 2.0</li>
<li>算法可视化  ***</li>
<li>李宏毅 台湾</li>
</ul>
<h2><span id="nlp-amp-大模型">NLP &amp; 大模型</span><a href="#nlp-amp-大模型" class="header-anchor">#</a></h2><ul>
<li><p><a href="https://www.zhihu.com/education/video-course/1546509363711614976">沈向洋带你读论文——CV &amp; NLP 专题</a> V </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1C14y147dp">2022年首发！B站讲的最好的【NLP自然语言处理】保姆级教程！</a>  V  有实践  *** </p>
</li>
<li><p>MLNLP第六期学术研讨会开始报名</p>
</li>
</ul>
<h2><span id="知识图谱">知识图谱</span><a href="#知识图谱" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1VT411G7Y6?p=6">【国家级精品课】浙江大学教授（新全44集）知识图谱公开课分享</a>  ***</li>
</ul>
<h2><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h2><ul>
<li>极客时间<ul>
<li>《AI 技术内参》  洪亮劼   全 ***</li>
<li>《机器学习 40 讲》  王天一 </li>
<li>《人工智能基础课》  王天一<br> 机器学习，深度学习</li>
<li>《成为AI产品经理》  刘海丰 京东   </li>
<li>《零基础实战机器学习》 黄佳  ***</li>
<li>《PyTorch深度学习实战》方远 大厂</li>
<li><a href="https://time.geekbang.org/course/intro/100023001?tab=catalog">TensorFlow 快速入门与实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/315">TensorFlow 2 项目进阶实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/100046401">NLP 实战高手课</a></li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>深度学习在CTR预估的应用   张俊林</li>
<li>深度学习在图像理解中的应用  熊鹏飞</li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>知识图谱技术实践  邵蓥侠</li>
</ul>
</li>
<li>极客训练营<br>-《机器学习训练营1期》  视频课</li>
</ul>
<h2><span id="中国大学mooc">中国大学MOOC</span><a href="#中国大学mooc" class="header-anchor">#</a></h2><ul>
<li>中国大学MOOC <a href="https://www.icourse163.org/learn/HIT-1206320802?tid=1468208513#/learn/announce">深度学习基础</a>   哈尔滨工业大学</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/FUDAN-1205806833">深度学习及其应用</a>   复旦</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/ZUCC-1206146808">深度学习应用开发-TensorFlow实践</a>  浙大城市学院</li>
</ul>
<h2><span id="培训">培训</span><a href="#培训" class="header-anchor">#</a></h2><ul>
<li><a href="http://bit.baidu.com/">百度技术培训中心</a>  *** 认证， 自动驾驶，人工智能培训  </li>
<li><a href="http://bit.baidu.com/courseRouteDetail?id=111">人工智能学习路线</a>  百度技术培训中心</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能 知识点</title>
    <url>/www6vHomeHexo/2022/01/22/aiOverview/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="人工智能引论知识点">人工智能引论知识点</span><a href="#人工智能引论知识点" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/22/aiOverview/ai-overview.png" class>

<blockquote>
<p>62个知识点，9个高阶知识点(研究生课程)</p>
</blockquote>
<h2><span id="美国k12-ai知识点">美国K12 AI知识点</span><a href="#美国k12-ai知识点" class="header-anchor">#</a></h2><ul>
<li>智能感知</li>
<li>表示和推理</li>
<li>机器学习</li>
<li>自然交互能力</li>
<li>对社会的影响</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1Wa41157U4?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">吴飞教授解读：人工智能知识点全景图：迈向智能+时代蓝皮书</a> video<br><a href="https://www.163.com/dy/article/HFAFUJPM051193U6.html">人工智能知识点全景图：迈向智能+时代蓝皮书</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 双流Join</title>
    <url>/www6vHomeHexo/2022/01/22/streamingFlinkJoin/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="双流join">双流Join</span><a href="#双流join" class="header-anchor">#</a></h2><h5><span id="基于窗口的join操作">基于窗口的JOIN操作</span><a href="#基于窗口的join操作" class="header-anchor">#</a></h5><ul>
<li>Window Join<ul>
<li>Tumbling Window Join</li>
<li>Sliding Window Join</li>
<li>Session Widnow Join</li>
</ul>
</li>
<li>Interval Join</li>
</ul>
<blockquote>
<p>Flink是通过State状态来缓存等待join的实时流</p>
</blockquote>
<h5><span id="基于原生state的connect算子操作">基于原生State的Connect算子操作</span><a href="#基于原生state的connect算子操作" class="header-anchor">#</a></h5><h2><span id="总结-4">总结 [4]</span><a href="#总结-4" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>JOIN 类型</th>
<th>触发join</th>
<th>场景</th>
<th>实时性</th>
<th>准确度</th>
<th>内存占用</th>
<th>waterrmark</th>
<th>时间属性</th>
</tr>
</thead>
<tbody><tr>
<td>双流join</td>
<td><strong>双流</strong></td>
<td>每一个数据流有变更都会触发join，并且状态会保存</td>
<td>高</td>
<td>先低后高（逐步更新）</td>
<td>高（需要设置状态生存时间）</td>
<td>否</td>
<td>事件时间、处理时间</td>
</tr>
<tr>
<td>时间区间 JOIN</td>
<td><strong>双流</strong></td>
<td>拥有相同key且 事件时间处于 lowerBoundTime 和 upperBoundTime之间的元素进行join</td>
<td>中</td>
<td>中（取决于区间大小）</td>
<td>中（取决于区间大小）</td>
<td>是（都需要）</td>
<td>事件时间、处理时间</td>
</tr>
<tr>
<td>时态表 JOIN（版本表）</td>
<td>单流</td>
<td>单流和版本表的join，具有历史版本状态管理功能。流表：事件时间，版本表：事件时间和主键</td>
<td>中</td>
<td>高（取决于具体实现）</td>
<td>高（取决于版本表大小 ）</td>
<td>是（都需要）</td>
<td>事件时间</td>
</tr>
<tr>
<td>时态表 JOIN（Join Lookup ）</td>
<td>单流</td>
<td>单流和维表的join，join要求一个表具有处理时间属性（流表），另一个表由查找源连接器支持（维表，实现了LookupableTableSource）</td>
<td>高</td>
<td>高（取决于是否缓存、异步等）</td>
<td>低（取决于是否缓存、异步等）</td>
<td>是（流表）</td>
<td>处理时间</td>
</tr>
<tr>
<td>JOIN LATERAL</td>
<td>单流</td>
<td>单流和UDTF的join。JOIN LATERAL 的右边不是一个物理表，而是一个视图（view）或者Table-valued Funciton。不具备状态管理功能</td>
<td>高</td>
<td>高（取决于是否缓存、异步等）</td>
<td>低（取决于是否缓存、异步等）</td>
<td>否</td>
<td></td>
</tr>
<tr>
<td>窗口 JOIN</td>
<td><strong>双流</strong></td>
<td>相同key且位于相同时间窗口的元素进行 join</td>
<td>低</td>
<td>低（取决于窗口大小和类型）</td>
<td>中（取决于窗口大小）</td>
<td>是（都需要）watermark取双流中较慢的为准</td>
<td>事件时间、处理时间</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/m0_49834705/article/details/119421944">Flink中的双流Join详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/452924664">面试高频|趣味Flink双流JOIN</a></li>
<li><a href="https://www.skyzh.dev/posts/articles/2022-01-15-store-of-streaming-states/">流处理系统中状态的表示和存储</a> *** 未</li>
</ol>
<ul>
<li>Full State - 算子维护自己的完整状态      <ul>
<li>Join State 的存储</li>
<li>Aggregation State 的存储</li>
<li>总结</li>
</ul>
</li>
</ul>
<ol start="4">
<li><a href="https://mp.weixin.qq.com/s/Sc1tgBWPVfGIcVuYr-UrKA">大厂案例|基于Flink实现跨库JOIN的数据同步预研方案</a>  ***</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>API Gateway网关</title>
    <url>/www6vHomeHexo/2022/01/21/apiGateway/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%89%B9%E6%80%A7">特性</a></li>
<li><a href="#%E5%88%86%E7%B1%BB">分类</a></li>
<li><a href="#%E6%A1%86%E6%9E%B6">框架</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0-3">实现 [3]</a></li>
<li><a href="#api-gatewaybff">API Gateway+BFF</a><ul>
<li><a href="#api-gateway-bff-3">API Gateway + BFF [3]</a></li>
<li><a href="#bff-%E8%81%9A%E5%90%88%E7%BD%91%E5%85%B3-2">BFF 聚合网关 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h1><ul>
<li>路由</li>
<li>灰度发布</li>
<li>反向代理,负载均衡</li>
<li>鉴权</li>
<li>限流</li>
<li>监控</li>
<li>缓存</li>
</ul>
<h1><span id="分类">分类</span><a href="#分类" class="header-anchor">#</a></h1><ul>
<li>入口网关 </li>
<li>出口网关</li>
</ul>
<h1><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>产品</th>
<th>技术</th>
</tr>
</thead>
<tbody><tr>
<td><a href="/www6vHomeHexo/2022/03/22/apiGatawayApisix/" title="API 网关-apisix">apisix</a> self</td>
<td>lua + Nginx</td>
</tr>
<tr>
<td>Kong</td>
<td>lua + Nginx</td>
</tr>
<tr>
<td>Zuul</td>
<td>Spring Cloud Netflix</td>
</tr>
<tr>
<td><a href="/www6vHomeHexo/2022/03/22/apiGatawaySpringGateway/" title="API 网关-SpringCloud Gateway">Gateway</a> self</td>
<td>Spring Cloud</td>
</tr>
<tr>
<td>Traefik</td>
<td>Golang</td>
</tr>
</tbody></table>
<h1><span id="实现-3">实现 [3]</span><a href="#实现-3" class="header-anchor">#</a></h1><ul>
<li>扩展性<br> 责任链模式 - Zuul filter, Envoy filter</li>
<li>性能<br>  多路 I&#x2F;O 复用模型  和  线程池</li>
<li>可用性<br>线程池  服务隔离</li>
</ul>
<h1><span id="api-gatewaybff">API Gateway+BFF</span><a href="#api-gatewaybff" class="header-anchor">#</a></h1><h3><span id="api-gateway-bff-3">API Gateway + BFF [3]</span><a href="#api-gateway-bff-3" class="header-anchor">#</a></h3><p>流量网关 + 业务网关</p>
<h3><span id="bff-聚合网关-2">BFF 聚合网关 [2]</span><a href="#bff-聚合网关-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/01/21/apiGateway/apiGateway.JPG" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.infoq.cn/article/construct-micro-service-using-api-gateway/">使用 API 网关构建微服务</a></li>
<li><a href="https://juejin.cn/post/6844903806208049159">微服务架构：BFF和网关是如何演化出来的？</a></li>
<li>《27 | API网关：系统的门面要如何做呢？》</li>
<li><a href="https://tech.meituan.com/2021/05/20/shepherd-api-gateway.html">百亿规模API网关服务Shepherd的设计与实现</a> 点评 未</li>
<li><a href="/www6vHomeHexo/2022/02/10/k8sIngressNginx/" title="Kubernetes Nginx Ingress">Kubernetes Nginx Ingress</a> self</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>API Gateway</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP10-视频行业解决方案</title>
    <url>/www6vHomeHexo/2022/01/19/tencentTCP10/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="十-视频行业解决方案5">十、视频行业解决方案(5%)</span><a href="#十-视频行业解决方案5" class="header-anchor">#</a></h2><h3><span id="1-视频行业概述">1. 视频行业概述</span><a href="#1-视频行业概述" class="header-anchor">#</a></h3><h5><span id="11-视频行业解决方案相关概念">1.1 视频行业解决方案相关概念</span><a href="#11-视频行业解决方案相关概念" class="header-anchor">#</a></h5><p>“直播和云直播的概念，点播和云点播的概念，实施音视频的概念”	☆☆</p>
<ul>
<li>视频行业分类<ul>
<li>直播<br>活动直播， 教育类直播</li>
<li>点播</li>
<li>实时音视频<br>视频会议，视频客服等</li>
</ul>
</li>
</ul>
<h5><span id="12-视频行业解决方案面临的挑战">“1.2 视频行业解决方案面临的挑战”</span><a href="#12-视频行业解决方案面临的挑战" class="header-anchor">#</a></h5><p>视频行业解决方案面临的挑战	☆☆</p>
<ul>
<li>视频行业解决方案面临的挑战<ul>
<li>传统直播 vs 云直播</li>
<li>云点播</li>
<li>实时音视频系统技术诉求<ul>
<li>低延迟<br>端到端延迟400ms以下</li>
<li>流畅性</li>
<li>回声消除</li>
<li>国内外互通</li>
<li>海量并发</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="13-腾讯云视频解决方案产品">1.3 腾讯云视频解决方案产品</span><a href="#13-腾讯云视频解决方案产品" class="header-anchor">#</a></h5><p>根据不同的场景，腾讯云拥有不同的产品满足场景需求	☆☆</p>
<ul>
<li>根据不同的场景，腾讯云拥有不同的产品满足场景需求<img src="/www6vHomeHexo/2022/01/19/tencentTCP10/video.png" class title="腾讯云视频解决方案产品"></li>
</ul>
<h3><span id="2-直播场景方案设计">“2. 直播场景方案设计”</span><a href="#2-直播场景方案设计" class="header-anchor">#</a></h3><h5><span id="21-直播场景的设计思路">2.1 直播场景的设计思路</span><a href="#21-直播场景的设计思路" class="header-anchor">#</a></h5><p>直播场景下思考的问题和直播场景问题解决思路	☆</p>
<ul>
<li>直播场景下思考的问题和直播场景问题解决思路<img src="/www6vHomeHexo/2022/01/19/tencentTCP10/live-broadcast-issue.png" class></li>
</ul>
<img src="/www6vHomeHexo/2022/01/19/tencentTCP10/live-broadcast-solution.png" class>

<h5><span id="22-腾讯云直播架构设计">2.2 腾讯云直播架构设计</span><a href="#22-腾讯云直播架构设计" class="header-anchor">#</a></h5><p>腾讯云直播产品：云直播CSS、移动直播SDK	☆☆☆<br>云直播平台工作原理和架构	☆☆☆</p>
<ul>
<li><p>腾讯云直播产品：云直播CSS、移动直播SDK</p>
<ul>
<li>云直播CSS</li>
<li>移动直播SDK</li>
</ul>
</li>
<li><p>云直播平台工作原理和架构</p>
<img src="/www6vHomeHexo/2022/01/19/tencentTCP10/live-broadcast-arch.png" class>

<img src="/www6vHomeHexo/2022/01/19/tencentTCP10/live-broadcast-process.png" class></li>
</ul>
<h5><span id="23-腾讯云直播的解决方案">2.3 腾讯云直播的解决方案</span><a href="#23-腾讯云直播的解决方案" class="header-anchor">#</a></h5><p>发布会场景方案、秀场直播场景方案和教育&amp;培训场景方案	☆☆</p>
<ul>
<li>发布会场景 [pic]</li>
<li>秀场直播场景 [pic]</li>
<li>教育&amp;培训场景 [pic]</li>
<li>大型公开课 &amp; 企业培训 [pic]</li>
</ul>
<h3><span id="3-点播场景方案设计">“3. 点播场景方案设计”</span><a href="#3-点播场景方案设计" class="header-anchor">#</a></h3><h5><span id="31-点播场景的设计思路">3.1 点播场景的设计思路</span><a href="#31-点播场景的设计思路" class="header-anchor">#</a></h5><p>点播场景下面临的挑战、点播场景的设计思路	☆</p>
<h5><span id="32-腾讯云点播架构设计">3.2 腾讯云点播架构设计</span><a href="#32-腾讯云点播架构设计" class="header-anchor">#</a></h5><p>腾讯云点播相关产品：云点播、短视频SDK、美颜特效SDK	☆☆☆<br>云点播平台工作原理和架构	☆☆☆<br>音视频存储架构	☆☆<br>源站迁移架构	☆☆</p>
<h5><span id="33-腾讯云点播的解决方案">3.3 腾讯云点播的解决方案</span><a href="#33-腾讯云点播的解决方案" class="header-anchor">#</a></h5><p>“类微视APP场景方案、互动聊天短视频场景方案、短视频新闻场景方案、问答短视频场景方案和游戏短视频场景方案”	☆☆</p>
<h3><span id="4-实时音视频场景方案设计">“4. 实时音视频场景方案设计”</span><a href="#4-实时音视频场景方案设计" class="header-anchor">#</a></h3><h5><span id="41-实时音视频场景设计思路">4.1 实时音视频场景设计思路</span><a href="#41-实时音视频场景设计思路" class="header-anchor">#</a></h5><p>实时音视频场景面临的挑战、实施音视频场景的设计思路	☆</p>
<h5><span id="42-实时音视频架构设计">4.2 实时音视频架构设计</span><a href="#42-实时音视频架构设计" class="header-anchor">#</a></h5><p>实时音视频相关产品：实时音视频TRTC	☆☆☆<br>TRTC的平台架构	☆☆☆</p>
<h5><span id="43-实时音视频的解决方案">4.3 实时音视频的解决方案</span><a href="#43-实时音视频的解决方案" class="header-anchor">#</a></h5><p>“视频客服场景、110视频报警场景、车险理赔场景、互动课堂场景”	☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP9-游戏行业解决方案</title>
    <url>/www6vHomeHexo/2022/01/19/tencentTCP9/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="九-游戏行业解决方案5">九、游戏行业解决方案(5%)</span><a href="#九-游戏行业解决方案5" class="header-anchor">#</a></h2><h3><span id="1-腾讯云游戏生态及能力">“1. 腾讯云游戏生态及能力”</span><a href="#1-腾讯云游戏生态及能力" class="header-anchor">#</a></h3><h5><span id="11-腾讯云持续领跑游戏服务云市场">“1.1 腾讯云持续领跑游戏服务云市场”</span><a href="#11-腾讯云持续领跑游戏服务云市场" class="header-anchor">#</a></h5><p>腾讯云游戏解决方案优势	☆</p>
<h5><span id="12-腾讯云游戏的生态布局">1.2 腾讯云游戏的生态布局</span><a href="#12-腾讯云游戏的生态布局" class="header-anchor">#</a></h5><p>腾讯云游戏的生态	☆☆</p>
<h3><span id="2-游戏行业的特征概述">“2. 游戏行业的特征概述”</span><a href="#2-游戏行业的特征概述" class="header-anchor">#</a></h3><h5><span id="21-游戏行业解决方案相关概念">2.1 游戏行业解决方案相关概念</span><a href="#21-游戏行业解决方案相关概念" class="header-anchor">#</a></h5><p>游戏行业解决方案相关概念	☆</p>
<h5><span id="22-游戏类型及特征">2.2 游戏类型及特征</span><a href="#22-游戏类型及特征" class="header-anchor">#</a></h5><p>MMO类、MOBA类和棋牌类游戏的概念及特征	☆☆</p>
<h5><span id="23-各类游戏面临的行业需求">2.3 各类游戏面临的行业需求</span><a href="#23-各类游戏面临的行业需求" class="header-anchor">#</a></h5><p>各类游戏面临的行业需求	☆☆</p>
<h3><span id="3-设计游戏行业云架构解决方案">“3. 设计游戏行业云架构解决方案”</span><a href="#3-设计游戏行业云架构解决方案" class="header-anchor">#</a></h3><h5><span id="31-弹性伸缩的高性能计算能力">3.1 弹性伸缩的高性能计算能力</span><a href="#31-弹性伸缩的高性能计算能力" class="header-anchor">#</a></h5><p>“腾讯云多种机型的云服务器提供弹性伸缩的高性能计算能力”	☆☆☆</p>
<h5><span id="32-稳定的网络传输能力">3.2 稳定的网络传输能力</span><a href="#32-稳定的网络传输能力" class="header-anchor">#</a></h5><p>“强大的BGP互联、全球化的数据中心和网络覆盖、专线和对等连接等提供高速稳定的网络传输能力”	☆☆☆</p>
<h5><span id="33-专业级的游戏语音能力">3.3 专业级的游戏语音能力</span><a href="#33-专业级的游戏语音能力" class="header-anchor">#</a></h5><p>游戏多媒体引擎GME	☆☆☆</p>
<h5><span id="34-一站式游戏测试能力">3.4 一站式游戏测试能力</span><a href="#34-一站式游戏测试能力" class="header-anchor">#</a></h5><p>标准兼容测试SCT	☆☆☆</p>
<h5><span id="35-自动化运维平台能力">3.5 自动化运维平台能力</span><a href="#35-自动化运维平台能力" class="header-anchor">#</a></h5><p>蓝鲸智云	☆☆☆</p>
<h5><span id="36-全面的安全防护能力">3.6 全面的安全防护能力</span><a href="#36-全面的安全防护能力" class="header-anchor">#</a></h5><p>“BGP 高防大禹、业务防控天御、主机安全云镜、移动安全乐固”	☆☆☆</p>
<h5><span id="37-mmo类-moba类和棋牌类游戏的解决方案">“3.7 MMO类、MOBA类和棋牌类游戏的解决方案”</span><a href="#37-mmo类-moba类和棋牌类游戏的解决方案" class="header-anchor">#</a></h5><p>“MMO类、MOBA类和棋牌类游戏的解决方案的架构和架构的功能模块”	☆☆☆</p>
<h5><span id="38-云游戏解决方案">3.8 云游戏解决方案</span><a href="#38-云游戏解决方案" class="header-anchor">#</a></h5><p>云游戏解决方案的架构和能力	☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis AOF</title>
    <url>/www6vHomeHexo/2022/01/18/redisAOF/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#aof">AOF</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%891">定义[1]</a></li>
<li><a href="#%E4%B8%A4%E4%B8%AA%E6%BD%9C%E5%9C%A8%E7%9A%84%E9%A3%8E%E9%99%A9-1">两个潜在的风险 [1]</a></li>
<li><a href="#%E4%B8%89%E7%A7%8D%E5%86%99%E5%9B%9E%E7%AD%96%E7%95%A5-1">三种写回策略 [1]</a></li>
<li><a href="#%E5%85%B3%E9%97%ADaof%E8%90%BD%E7%9B%98-2">关闭AOF落盘 [2]</a></li>
</ul>
</li>
<li><a href="#aof-rewrite%E8%BF%87%E7%A8%8B">AOF Rewrite过程</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>
                                    
<h1><span id="aof">AOF</span><a href="#aof" class="header-anchor">#</a></h1><h3><span id="定义1">定义[1]</span><a href="#定义1" class="header-anchor">#</a></h3><ul>
<li><p>[AOF不是WAL，是先操作，后记录日志。]</p>
</li>
<li><p>传统数据库的日志，例如 redo log(重做日志)，记录的是修改后的数据，而AOF里记录的是Redis收到的每一条命令，这些命令是以文本形式保存的。</p>
</li>
<li><p><strong>AOF 日志</strong>正好相反，它是<strong>写后日志</strong>，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志。</p>
</li>
<li><p>而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。<br>所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。</p>
</li>
<li><p>AOF 还有一个好处：它是在命令执行后才记录日志，所以<strong>不会阻塞当前的写操作</strong>。</p>
</li>
</ul>
<p><strong>AOF 是 写前日志</strong></p>
<h3><span id="两个潜在的风险-1">两个潜在的风险 [1]</span><a href="#两个潜在的风险-1" class="header-anchor">#</a></h3><ol>
<li>如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数 据就有丢失的风险.</li>
<li>AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就 会导致写盘很慢，进而导致后续的操作也无法执行了.</li>
</ol>
<h3><span id="三种写回策略-1">三种写回策略 [1]</span><a href="#三种写回策略-1" class="header-anchor">#</a></h3><ul>
<li>AOF 配置项  -  appendfsync 的三个可选值<ul>
<li><strong>Always</strong>，同步写回：每个写命令执行完，<strong>立马同步地将日志写回磁盘</strong>；</li>
<li><strong>Everysec</strong>，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，<strong>每隔一秒把缓冲区中的内容写入磁盘</strong>；</li>
<li><strong>No</strong>，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2022/01/18/redisAOF/aof-1.png" class title="三种写回策略">

<h3><span id="关闭aof落盘-2">关闭AOF落盘 [2]</span><a href="#关闭aof落盘-2" class="header-anchor">#</a></h3><p>AOF落盘会带来一定写性能损耗，如果将Redis实例应用于纯缓存场景中，对数据持久化没有需求，您可以按照本章节的说明，修改appendonly参数的值，关闭AOF落盘。</p>
<h1><span id="aof-rewrite过程">AOF Rewrite过程</span><a href="#aof-rewrite过程" class="header-anchor">#</a></h1><ul>
<li><p>功能<br>压缩AOF文件的大小</p>
</li>
<li><p>AOF Rewrite过程 [1]<br>非阻塞的重写</p>
</li>
<li><p>一个拷贝，两处日志 [1]</p>
<img src="/www6vHomeHexo/2022/01/18/redisAOF/redisRewrite.png" class title="AOF非阻塞的重写过程 838 398">
</li>
<li><p>触发机制 [4]</p>
<ul>
<li>手动触发<br>bgrewriteaof 命令</li>
<li>自动触发<br>AOF文件大小</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《04 | AOF日志:宕机了，Redis如何避免数据丢失? 》    蒋德钧</li>
<li><a href="https://help.aliyun.com/knowledge_detail/147408.html?spm=a2c4g.11186623.2.2.7e373f2e7XKCDO">关闭AOF落盘</a></li>
<li><a href="/www6vHomeHexo/2021/11/21/aofRewrite/" title="Redis AOF Rewrite">Redis AOF Rewrite</a></li>
<li>《Redis开发与运维》  第5章</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 事务</title>
    <url>/www6vHomeHexo/2022/01/18/redisTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<ul>
<li><p>事务的 ACID 属性是我们使用事务进行正确操作的基本要求。</p>
</li>
<li><p>Redis 的事务机制可以<strong>保证一致性和隔离性，但是无法保证持久性</strong>。<br>不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，我们更加关注的还是原子性、 一致性和隔离性这三个属性。</p>
</li>
<li><p><strong>原子性</strong>的情况比较复杂，只有当事务中使用的命令<strong>语法有误时，原子性得不到保证</strong>，在其它情况下，<strong>事务都可以原子性</strong>执行。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">事务</th>
<th align="center">是否能保证</th>
</tr>
</thead>
<tbody><tr>
<td align="center">一致性 C</td>
<td align="center">能保证</td>
</tr>
<tr>
<td align="center">隔离性 I</td>
<td align="center">能保证</td>
</tr>
<tr>
<td align="center">原子性 A</td>
<td align="center">能保证(命令语法无误时)</td>
</tr>
<tr>
<td align="center">持久性 D</td>
<td align="center">不能保证</td>
</tr>
</tbody></table>
<p> 【redis  cluster 没有事务】</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>31 | 事务机制:Redis能实现ACID属性吗?<br><a href="https://www.liangzl.com/get-article-detail-129391.html">[redis]redis集群不支持事务multi、exec</a>   失效</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 的IO模型</title>
    <url>/www6vHomeHexo/2022/01/18/redisIO/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h5><span id="关键词-单线程事件-socket-多路复用">关键词： 单线程，事件， socket， 多路复用</span><a href="#关键词-单线程事件-socket-多路复用" class="header-anchor">#</a></h5><h2><span id="redis-的io模型">Redis 的IO模型</span><a href="#redis-的io模型" class="header-anchor">#</a></h2><p><strong>Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。</strong><br>但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执 行的。</p>
<h5><span id="多路复用">多路复用</span><a href="#多路复用" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/18/redisIO/redis-multiplex.png" class title="基于多路复用的Redis高性能IO模型">

<p>​                               </p>
<p>Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select&#x2F;epoll 机制。简单来说，在 Redis 只运行单线程的情况下，<strong>该机制允许内核中，同 时存在多个监听套接字和已连接套接字</strong>。内核会一直监听这些套接字上的连接请求或数据 请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。</p>
<p>为了在请求到达时能通知到 Redis 线程，<strong>select&#x2F;epoll 提供了基于事件的回调机制，即针 对不同事件的发生，调用相应的处理函数</strong>。</p>
<h5><span id="epoll的api">epoll的API</span><a href="#epoll的api" class="header-anchor">#</a></h5><p>int epoll_create(int size);<br>int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);   &#x2F;&#x2F; 事件注册<br>int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);  &#x2F;&#x2F;  事件分配、事件处理</p>
<h2><span id="netty-io模型">Netty IO模型</span><a href="#netty-io模型" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/18/redisIO/netty-io.jpeg" class title="Netty IO模型">

<h5><span id="channel">channel</span><a href="#channel" class="header-anchor">#</a></h5><p>channel（在socket上的通道） -  event<br>channel - channelPipeline（单线程）</p>
<h5><span id="reactor">reactor</span><a href="#reactor" class="header-anchor">#</a></h5><p>accept事件（boss线程）<br>read，write事件（work线程）</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>03 | 高性能IO模型:为什么单线程Redis能那么快?<br><a href="http://ifeve.com/channel-pipeline/">Netty源码解读（三）Channel与Pipeline</a><br><a href="https://www6v.github.io/www6vHomeHexo/2015/08/23/nettySummary/">https://www6v.github.io/www6vHomeHexo/2015/08/23/nettySummary/</a><br><a href="https://www.cnblogs.com/xuewangkai/p/11158576.html">epoll使用详解：epoll_create、epoll_ctl、epoll_wait、close</a></p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 升级upgrade</title>
    <url>/www6vHomeHexo/2022/01/16/k8sUpgrade/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="宏观升级流程">宏观升级流程</span><a href="#宏观升级流程" class="header-anchor">#</a></h2><p>1、升级主master节点</p>
<p>2、升级其他master节点</p>
<p>3、升级node节点</p>
<h2><span id="微观升级步骤">微观升级步骤</span><a href="#微观升级步骤" class="header-anchor">#</a></h2><p>1、先升级kubeadm版本</p>
<p>2、升级第一个主控制平面节点Master组件</p>
<p>3、升级第一个主控制平面节点上的Kubelet和kubectl</p>
<p>4、升级其他控制平面节点</p>
<p>5、升级Node节点</p>
<p>6、验证集群</p>
<h2><span id="升级注意事项">升级注意事项</span><a href="#升级注意事项" class="header-anchor">#</a></h2><ul>
<li>确定升级前的的kubeadm集群版本。</li>
<li>kubeadm upgrade不会影响到工作负载，仅涉及k8s内部的组件，但是备份etcd数据库是最佳实践。</li>
<li>升级后，所有容器都会重启动，因为容器的hash值已更改。</li>
<li>由于版本的兼容性，只能从一个次要版本升级到另外一个次要版本，不能跳跃升级。</li>
<li>集群控制平面应使用静态Pod和etcd pod或外部etcd。</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://zhuanlan.zhihu.com/p/358338665">Kubernetes版本升级实践</a><br><a href="https://developer.aliyun.com/article/888380">Kubernetes升级：自己动手的权威指南</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 安全实践</title>
    <url>/www6vHomeHexo/2022/01/16/k8sSecurityPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="microsoft的kubernetes-attack-matrix-1">Microsoft的Kubernetes attack matrix [1]</span><a href="#microsoft的kubernetes-attack-matrix-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/16/k8sSecurityPractice/k8s-matrix.png" class title="Kubernetes attack matrix-Microsoft">

<h2><span id="kubernetes-attack-matrix-enhancement-2">Kubernetes attack matrix-enhancement [2]</span><a href="#kubernetes-attack-matrix-enhancement-2" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/16/k8sSecurityPractice/k8s-matrix-modified.png" class title="Kubernetes attack matrix-enhancement">

<h5><span id="初始访问">初始访问</span><a href="#初始访问" class="header-anchor">#</a></h5><p>● API Server 未授权访问 [5]</p>
<ul>
<li><p>API Server 作为 K8s 集群的管理入口，通常使用 8080 和 6443 端口，其中 8080 端口无需认证，6443端口需要认证且有 TLS 保护。如果开发者使用 8080 端口，并将其暴露在公网上，攻击者就可以通过该端口的 API，直接对集群下发指令。</p>
</li>
<li><p>另一种场景是运维人员配置不当，将”system:anonymous”用户绑定到”cluster-admin”用户组，从而使6443端口允许匿名用户以管理员权限向集群内部下发指令。</p>
</li>
</ul>
<p>● kubelet 未授权访问 [5]</p>
<p>● Docker Daemon 公网暴露</p>
<p>● K8s configfile 泄露</p>
<ul>
<li><p>K8s configfile 作为 K8s 集群的管理凭证，其中包含有关 K8s 集群的详细信息（API Server、登录凭证）。</p>
</li>
<li><p>如果攻击者能够访问到此文件(如办公网员工机器入侵、泄露到 Github的代码等)，就可以直接通过 API Server 接管 K8s 集群，带来风险隐患。</p>
</li>
<li><p>拿到K8s configfile完整利用流程：<br>K8s configfile –&gt; 创建后门Pod&#x2F;挂载主机路径 –&gt; 通过Kubectl 进入容器 –&gt; 利用挂载目录逃逸。</p>
</li>
</ul>
<h5><span id="执行">执行</span><a href="#执行" class="header-anchor">#</a></h5><p>● 利用Service Account<br>  容器内部默认携带 K8s Service Account的认证凭据,路径为：&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token<br>  如运维配置不当没有设置 RBAC （基于角色的访问控制）,那么攻击者就可以通过 Pod 获取到 Token 进行API Server认证。<br>  …创建特权Pod…</p>
<p>● CURL方式请求</p>
<p>● kubectl方式请求</p>
<h5><span id="持久化">持久化</span><a href="#持久化" class="header-anchor">#</a></h5><p>● DaemonSets、Deployments</p>
<p>● Shadow API</p>
<p>● Rootkit</p>
<p>● cronjob持久化</p>
<h5><span id="权限提升">权限提升</span><a href="#权限提升" class="header-anchor">#</a></h5><p>● 特权容器逃逸 [3]</p>
<ul>
<li><p>「容器逃逸」指这样的一种过程和结果：首先，攻击者通过劫持容器化业务逻辑，或直接控制（CaaS等合法获得容器控制权的场景）等方式，已经获得了容器内某种权限下的命令执行能力；攻击者利用这种命令执行能力，借助一些手段进一步获得该容器所在直接宿主机（经常见到“物理机运行虚拟机，虚拟机再运行容器”的场景，该场景下的直接宿主机指容器外层的虚拟机）上某种权限下的命令执行能力。</p>
</li>
<li><p>当操作者执行docker run –privileged时，Docker将允许容器访问宿主机上的所有设备，同时修改AppArmor或SELinux的配置，使容器拥有与那些直接运行在宿主机上的进程几乎相同的访问权限。</p>
</li>
</ul>
<p>● Docker漏洞</p>
<p>● Linux Capabilities逃逸</p>
<h5><span id="探测">探测</span><a href="#探测" class="header-anchor">#</a></h5><p>● 内网扫描</p>
<p>● K8s常用端口探测</p>
<p>● 集群内部网络</p>
<h5><span id="横向移动">横向移动</span><a href="#横向移动" class="header-anchor">#</a></h5><p>● 污点(Taint)横向渗透</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.microsoft.com/security/blog/2020/04/02/attack-matrix-kubernetes/">Threat matrix for Kubernetes</a>  overview</li>
<li><a href="https://paper.seebug.org/1803/">云原生之 Kubernetes 安全</a> 深信服千里目安全实验室  overview</li>
<li><a href="https://blog.wohin.me/posts/container-escape-overview/">容器逃逸技术概览</a> ***</li>
<li><a href="https://www.bilibili.com/read/cv17136407">红队视角下的容器逃逸利用及分析</a> 未</li>
<li><a href="https://cloud.tencent.com/developer/article/2000490">浅析K8S各种未授权攻击方法</a></li>
<li><a href="https://www.bilibili.com/read/cv15722560/">k8s对外攻击面总结</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>security</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes CKS</title>
    <url>/www6vHomeHexo/2022/01/15/k8sCKS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h3><span id="cluster-setup-10">Cluster Setup - 10%</span><a href="#cluster-setup-10" class="header-anchor">#</a></h3><h5><span id="securing-a-cluster"></span><a href="#securing-a-cluster" class="header-anchor">#</a></h5><ol>
<li>使用网络安全策略来限制集群级别的访问<br><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Use Network security policies to restrict cluster level access</a></li>
<li>使用CIS基准检查Kubernetes组件(etcd, kubelet, kubedns, kubeapi)的安全配置<br><a href="https://www.cisecurity.org/benchmark/kubernetes/">Use CIS benchmark to review the security configuration of Kubernetes components</a>  (etcd, kubelet, kubedns, kubeapi)<ul>
<li><a href="https://github.com/aquasecurity/kube-bench">Kube-bench</a> - Checks whether Kubernetes is deployed securely by running the checks documented ain the CIS Kubernetes Benchmark.</li>
</ul>
</li>
<li>正确设置带有安全控制的Ingress对象<br>Properly set up <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls">Ingress objects with security control</a></li>
<li>保护节点元数据和端点<br><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-cloud-metadata-api-access">Protect node metadata and endpoints</a></li>
</ol>
<details><summary> Using Kubernetes network policy to restrict pods access to cloud metadata </summary>

<ul>
<li>This example assumes AWS cloud, and metadata IP address is 169.254.169.254 should be blocked while all other external addresses are not.</li>
</ul>
 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deny-only-cloud-metadata-access</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">      <span class="attr">cidr:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/0</span></span><br><span class="line">      <span class="attr">except:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">169.254</span><span class="number">.169</span><span class="number">.254</span><span class="string">/32</span></span><br></pre></td></tr></table></figure>

</details>

<ol start="5">
<li>最小化GUI元素的使用和访问<br><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui">Minimize use of, and access to, GUI elements</a></li>
<li>在部署之前验证平台二进制文件<br><a href="https://github.com/kubernetes/kubernetes/releases">Verify platform binaries before deploying</a></li>
</ol>
<details><summary> Kubernetes binaries can be verified by their digest **sha512 hash**  </summary>

<ul>
<li>Checking the Kubernetes release page for the specific release</li>
<li>Checking the change log for the <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#downloads-for-v1191">images and their digests</a></li>
</ul>
</details>

<h3><span id="cluster-hardening-15">Cluster Hardening - 15%</span><a href="#cluster-hardening-15" class="header-anchor">#</a></h3><ol>
<li>限制访问Kubernetes API<br><a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">Restrict access to Kubernetes API</a></li>
</ol>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests">Control anonymous requests to Kube-apiserver</a></li>
<li><a href="https://kubernetes.io/docs/concepts/security/controlling-access/#api-server-ports-and-ips">Non secure access to the kube-apiserver</a></li>
</ul>
<ol start="2">
<li>使用基于角色的访问控制来最小化暴露<br><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Use Role-Based Access Controls to minimize exposure</a><ul>
<li><a href="https://rbac.dev/">Handy site collects together articles, tools and the official documentation all in one place</a></li>
<li><a href="https://docs.bitnami.com/tutorials/simplify-kubernetes-resource-access-rbac-impersonation/">Simplify Kubernetes Resource Access Control using RBAC Impersonation</a></li>
</ul>
</li>
<li>谨慎使用服务帐户，例如禁用默认设置，减少新创建帐户的权限<br>Exercise caution in using service accounts e.g. <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server">disable defaults</a>, minimize permissions on newly created ones</li>
</ol>
<details><summary> Opt out of automounting API credentials for a service account </summary>

<h4><span id="opt-out-at-service-account-scope">Opt out at service account scope</span><a href="#opt-out-at-service-account-scope" class="header-anchor">#</a></h4>   <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">build-robot</span></span><br><span class="line"><span class="attr">automountServiceAccountToken:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h4><span id="opt-out-at-pod-scope">Opt out at pod scope</span><a href="#opt-out-at-pod-scope" class="header-anchor">#</a></h4>   <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cks-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">automountServiceAccountToken:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

</details>

<ol start="4">
<li>经常更新Kubernetes<br><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/">Update Kubernetes frequently</a></li>
</ol>
<h3><span id="system-hardening-15">System Hardening - 15%</span><a href="#system-hardening-15" class="header-anchor">#</a></h3><ol>
<li>最小化主机操作系统的大小(减少攻击面)<br>Minimize host OS footprint (reduce attack surface)</li>
</ol>
<details><summary>Reduce host attack surface </summary>

<ul>
<li><a href="https://kubernetes.io/docs/tutorials/clusters/seccomp/">seccomp which stands for secure computing was originally intended as a means of safely running untrusted compute-bound programs</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/">AppArmor can be configured for any application to reduce its potential host attack surface and provide greater in-depth defense.</a></li>
<li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PSP enforces</a><br>PodSecurityPolicy is deprecated as of Kubernetes v1.21, and will be removed in v1.25. We recommend migrating to Pod Security Admission.</li>
<li>Apply host updates</li>
<li>Install minimal required OS fingerprint</li>
<li>Identify and address open ports</li>
<li>Remove unnecessary packages</li>
<li>Protect access to data with permissions<ul>
<li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">Restirct allowed hostpaths</a></li>
</ul>
</li>
</ul>
</details>

<ol start="2">
<li>最小化IAM角色<br>Minimize IAM roles<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">Access authentication and authorization</a></li>
</ul>
</li>
<li>最小化对网络的外部访问<br>Minimize external access to the network</li>
</ol>
<details><summary>     if it means deny external traffic to outside the cluster?!! </summary>

<ul>
<li>not tested, however, the thinking is that all pods can talk to all pods in all name spaces but not to the outside of the cluster!!!</li>
</ul>
   <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deny-external-egress</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">    <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

</details>

<ol start="4">
<li>适当使用内核强化工具，如AppArmor, seccomp<br>Appropriately use kernel hardening tools such as AppArmor, seccomp<ul>
<li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/">AppArmor</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/clusters/seccomp/">Seccomp</a></li>
</ul>
</li>
</ol>
<h3><span id="minimize-microservice-vulnerabilities-20">Minimize Microservice Vulnerabilities - 20%</span><a href="#minimize-microservice-vulnerabilities-20" class="header-anchor">#</a></h3><ol>
<li>设置适当的OS级安全域，例如使用PSP, OPA，安全上下文<br>Setup appropriate OS-level security domains e.g. using PSP, OPA, security contexts<ul>
<li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a></li>
<li><a href="https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/">Open Policy Agent</a></li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Security Contexts</a></li>
</ul>
</li>
<li>管理Kubernetes机密<br><a href="https://kubernetes.io/docs/concepts/configuration/secret/">Manage kubernetes secrets</a></li>
<li>在多租户环境中使用容器运行时 (例如gvisor, kata容器)<br>Use <a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">container runtime</a> sandboxes in multi-tenant environments (e.g. <a href="https://github.com/kubernetes/enhancements/blob/5dcf841b85f49aa8290529f1957ab8bc33f8b855/keps/sig-node/585-runtime-class/README.md#examples">gvisor, kata containers</a>)</li>
<li>使用mTLS实现Pod对Pod加密<br><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/">Implement pod to pod encryption by use of mTLS</a></li>
</ol>
<ul>
<li><input disabled type="checkbox"> check if service mesh is part of the CKS exam</li>
</ul>
<h3><span id="supply-chain-security-20">Supply Chain Security - 20%</span><a href="#supply-chain-security-20" class="header-anchor">#</a></h3><ol>
<li><p>最小化基本镜像大小<br>Minimize base image footprint</p>
<details><summary>   Minimize base Image </summary>

<ul>
<li>Use distroless, UBI minimal, Alpine, or relavent to your app nodejs, python but the minimal build.</li>
<li>Do not include uncessary software not required for container during runtime e.g build tools and utilities, troubleshooting and debug binaries.<ul>
<li><a href="https://learnk8s.io/blog/smaller-docker-images">Learnk8s: 3 simple tricks for smaller Docker images</a></li>
<li><a href="https://cloud.google.com/blog/products/gcp/7-best-practices-for-building-containers">GKE 7 best practices for building containers</a></li>
</ul>
</li>
</ul>
</details>
</li>
<li><p>保护您的供应链：将允许的注册表列入白名单，对镜像进行签名和验证<br>Secure your supply chain: <a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/#why-do-i-need-admission-controllers">whitelist allowed image registries</a>, sign and validate images</p>
</li>
</ol>
<ul>
<li>Using <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">ImagePolicyWebhook admission Controller</a></li>
</ul>
<ol start="4">
<li>使用用户工作负载的静态分析(例如kubernetes资源，Docker文件)<br>Use static analysis of user workloads (e.g. <a href="https://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#7-statically-analyse-yaml">kubernetes resources</a>, docker files)</li>
<li>扫描镜像，找出已知的漏洞<br><a href="https://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#10-scan-images-and-run-ids">Scan images for known vulnerabilities</a><ul>
<li><a href="https://github.com/aquasecurity/trivy">Aqua security Trivy</a></li>
<li><a href="https://github.com/anchore/anchore-cli#command-line-examples">Anchore command line scans</a></li>
</ul>
</li>
</ol>
<h3><span id="monitoring-logging-and-runtime-security-20">Monitoring, Logging and Runtime Security - 20%</span><a href="#monitoring-logging-and-runtime-security-20" class="header-anchor">#</a></h3><ol>
<li><p>在主机和容器级别执行系统调用进程和文件活动的行为分析，以检测恶意活动<br> Perform behavioural analytics of syscall process and file activities at the host and container level to detect malicious activities</p>
<ul>
<li><a href="https://falco.org/docs/">Falco installation guide</a></li>
<li><a href="https://learn.sysdig.com/falco-101">Sysdig Falco 101</a></li>
<li><a href="https://github.com/falcosecurity/charts/tree/master/falco">Falco Helm Chart</a></li>
<li><a href="https://github.com/falcosecurity/charts">Falco Kubernetes helmchart</a></li>
<li><a href="https://falco.org/blog/detect-cve-2020-8557/">Detect CVE-2020-8557 using Falco</a></li>
</ul>
</li>
<li><p>检测物理基础架构，应用程序，网络，数据，用户和工作负载中的威胁<br>Detect threats within a physical infrastructure, apps, networks, data, users and workloads</p>
</li>
<li><p>检测攻击的所有阶段，无论它发生在哪里，如何扩散<br>Detect all phases of attack regardless where it occurs and how it spreads</p>
<details><summary>    Attack Phases </summary>

<ul>
<li><a href="https://www.microsoft.com/security/blog/2020/04/02/attack-matrix-kubernetes/">Kubernetes attack martix Microsoft blog</a></li>
<li><a href="https://sysdig.com/blog/mitre-attck-framework-for-container-runtime-security-with-sysdig-falco/">MITRE attack framwork using Falco</a></li>
<li><a href>Lightboard video: Kubernetes attack matrix - 3 steps to mitigating the MITRE ATT&amp;CK Techniques</a></li>
<li><a href="https://www.cncf.io/webinars/mitigating-kubernetes-attacks/">CNCF Webinar: Mitigating Kubernetes attacks</a></li>
</ul>
</details>
</li>
<li><p>对环境中的不良行为者进行深入的分析调查和识别<br>Perform deep analytical investigation and identification of bad actors within the environment</p>
<ul>
<li><a href="https://docs.sysdig.com/">Sysdig documentation</a></li>
<li><a href="https://kubernetes.io/blog/2015/11/monitoring-kubernetes-with-sysdig/">Monitoring Kubernetes with sysdig</a></li>
<li><a href="https://youtu.be/VEFaGjfjfyc">CNCF Webinar: Getting started with container runtime security using Falco</a></li>
</ul>
</li>
<li><p>确保容器在运行时不变<br><a href="https://kubernetes.io/blog/2018/03/principles-of-container-app-design/">Ensure immutability of containers at runtime</a></p>
</li>
<li><p>使用审计日志来监视访问<br><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Use Audit Logs to monitor access</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>security</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes ApiServer 代码走读</title>
    <url>/www6vHomeHexo/2022/01/15/k8sCodeOfApiServer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="kubernetes-apiserver-代码走读">Kubernetes ApiServer 代码走读</span><a href="#kubernetes-apiserver-代码走读" class="header-anchor">#</a></h2><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">cmd/kube-apiserver/app/server.<span class="keyword">go</span>:NewAPIServerCommand()--&gt;</span><br><span class="line">completedOptions, err := Complete(s)--&gt;</span><br><span class="line">	s.Etcd.WatchCacheSizes, err = serveroptions.WriteWatchCacheSizes(sizes)</span><br><span class="line">Run(completedOptions, genericapiserver.SetupSignalHandler())--&gt;CreateServerChain()--&gt;</span><br><span class="line">	CreateServerChain()--&gt;</span><br><span class="line">		CreateKubeAPIServerConfig--&gt;</span><br><span class="line">			buildGenericConfig(s.ServerRunOptions, proxyTransport)--&gt;</span><br><span class="line">				genericapiserver.NewConfig(legacyscheme.Codecs) <span class="comment">// create codec factory for encoding/decoding</span></span><br><span class="line">				controlplane.DefaultAPIResourceConfigSource() <span class="comment">// group version: enabled/disabled</span></span><br><span class="line">				storageFactoryConfig.Complete(s.Etcd)</span><br><span class="line">				completedStorageFactoryConfig.New()--&gt; <span class="comment">// register access path in etcd for all k8s objects</span></span><br><span class="line">					storageFactory.AddCohabitatingResources(networking.Resource(<span class="string">&quot;networkpolicies&quot;</span>), extensions.Resource(<span class="string">&quot;networkpolicies&quot;</span>))</span><br><span class="line">				s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig)--&gt;</span><br><span class="line">					c.AddHealthChecks()</span><br><span class="line">					c.RESTOptionsGetter = &amp;StorageFactoryRestOptionsFactory&#123;Options: *s, StorageFactory: factory&#125;</span><br><span class="line"><span class="comment">// 认证</span></span><br><span class="line">				s.Authentication.ApplyTo()--&gt; <span class="comment">// clientcert, serviceaccount, bootstrap token, </span></span><br><span class="line">					authenticatorConfig.New()--&gt;</span><br><span class="line">						newWebhookTokenAuthenticator(config) <span class="comment">// webhook</span></span><br><span class="line"><span class="comment">// 鉴权</span></span><br><span class="line">				BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers)--&gt;</span><br><span class="line">					authorizationConfig.New()--&gt;</span><br><span class="line">						rbacAuthorizer := rbac.New()--&gt; <span class="comment">// if authorizer type is rbac</span></span><br><span class="line"><span class="comment">// 准入</span></span><br><span class="line">				buildServiceResolver(s.EnableAggregatorRouting, genericConfig.LoopbackClientConfig.Host, versionedInformers)</span><br><span class="line">				admissionConfig.New(proxyTransport, genericConfig.EgressSelector, serviceResolver)--&gt;</span><br><span class="line">					admission.PluginInitializer&#123;webhookPluginInitializer, kubePluginInitializer&#125;</span><br><span class="line"></span><br><span class="line">			net.SplitHostPort(s.Etcd.StorageConfig.Transport.ServerList[<span class="number">0</span>])</span><br><span class="line">			utilwait.PollImmediate(etcdRetryInterval, etcdRetryLimit*etcdRetryInterval, preflight.EtcdConnection&#123;ServerList: s.Etcd.StorageConfig.Transport.ServerList&#125;.CheckEtcdServers)</span><br><span class="line">			capabilities.Initialize() <span class="comment">// allow privillage?</span></span><br><span class="line">			config := &amp;controlplane.Config&#123;&#125;</span><br><span class="line">		createAPIExtensionsConfig()</span><br><span class="line">		createAPIExtensionsServer()--&gt;</span><br><span class="line">			apiextensionsConfig.Complete().New(delegateAPIServer)--&gt;</span><br><span class="line">				s.AddHealthChecks(delegateCheck)</span><br><span class="line"><span class="comment">// 注册通用handler</span></span><br><span class="line">				installAPI(s, c.Config) <span class="comment">// register generic api handler e.g. index, profiling, metrics, flow control</span></span><br><span class="line">		CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer)</span><br><span class="line">			kubeAPIServerConfig.Complete().New(delegateAPIServer)</span><br><span class="line">				m.InstallLegacyAPI(&amp;c, c.GenericConfig.RESTOptionsGetter, legacyRESTStorageProvider)--&gt;</span><br><span class="line">					m.GenericAPIServer.AddPostStartHookOrDie(controllerName, bootstrapController.PostStartHook)--&gt;</span><br><span class="line">						controlplane.controller.Start()--&gt;</span><br><span class="line">							async.NewRunner(c.RunKubernetesNamespaces, c.RunKubernetesService, repairClusterIPs.RunUntil, repairNodePorts.RunUntil)</span><br><span class="line">					m.GenericAPIServer.AddPreShutdownHookOrDie(controllerName, bootstrapController.PreShutdownHook)</span><br><span class="line"><span class="comment">// 注册core group API handler</span></span><br><span class="line">					m.GenericAPIServer.InstallLegacyAPIGroup() <span class="comment">// register handler for /api</span></span><br><span class="line">					restStorageProviders := []RESTStorageProvider&#123;appsrest.StorageProvider&#123;&#125;&#125;</span><br><span class="line">				m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...)--&gt;</span><br><span class="line"><span class="comment">// 初始化对应group中对象的watch cache</span></span><br><span class="line">					restStorageBuilder.NewRESTStorage(apiResourceConfigSource, restOptionsGetter)--&gt; <span class="comment">// trigger appsrest.StorageProvider</span></span><br><span class="line">						p.v1Storage(apiResourceConfigSource, restOptionsGetter)--&gt;</span><br><span class="line">							daemonsetstore.NewREST(restOptionsGetter)--&gt;</span><br><span class="line">								store.CompleteWithOptions(options)--&gt;</span><br><span class="line">									opts, err := options.RESTOptions.GetRESTOptions(e.DefaultQualifiedResource)--&gt; <span class="comment">// etcd.go</span></span><br><span class="line">										ret.Decorator = genericregistry.StorageWithCacher()--&gt;</span><br><span class="line">											cacherstorage.NewCacherFromConfig(cacherConfig)--&gt;</span><br><span class="line">												watchCache := newWatchCache()--&gt;</span><br><span class="line"><span class="comment">// 注册API handler</span></span><br><span class="line">					m.GenericAPIServer.InstallAPIGroups(apiGroupsInfo...)--&gt;  <span class="comment">// register handler for /apis</span></span><br><span class="line">						s.installAPIResources(APIGroupPrefix, apiGroupInfo, openAPIModels)--&gt;</span><br><span class="line">							apiGroupVersion.InstallREST(s.Handler.GoRestfulContainer)--&gt;</span><br><span class="line">								discovery.NewAPIVersionHandler(g.Serializer, g.GroupVersion, staticLister&#123;apiResources&#125;)</span><br><span class="line">		createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers)--&gt;</span><br><span class="line">			apiServices := apiServicesToRegister(delegateAPIServer, autoRegistrationController)</span><br><span class="line">	server.PrepareRun()--&gt;</span><br><span class="line">		s.GenericAPIServer.PrepareRun()--&gt;</span><br><span class="line">			s.installHealthz()</span><br><span class="line">			s.installLivez()</span><br><span class="line">			s.installReadyz()</span><br><span class="line">	prepared.Run(stopCh)--&gt;</span><br><span class="line">		s.runnable.Run(stopCh)--&gt; <span class="comment">// preparedGenericAPIServer.Run()</span></span><br><span class="line">			s.NonBlockingRun(delayedStopCh)--&gt;</span><br><span class="line">				s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh)--&gt;</span><br><span class="line">					RunServer(secureServer, s.Listener, shutdownTimeout, stopCh)</span><br></pre></td></tr></table></figure>

<p hidden>
参考
[kube-apiserver](https://cncamp.notion.site/kube-apiserver-10d5695cbbb14387b60c6d622005583d)
</p>]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Informer Framework 代码走读</title>
    <url>/www6vHomeHexo/2022/01/15/k8sCodeOfInformerFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">secretInformer := kubecoreinformers.NewSecretInformer()--&gt;</span><br><span class="line">	NewFilteredSecretInformer()--&gt;</span><br><span class="line">		NewSharedIndexInformer(&amp;cache.ListWatch&#123;&#125;, &amp;corev1.Secret&#123;&#125;, resyncPeriod, indexers)--&gt;</span><br><span class="line">			sharedIndexInformer := &amp;sharedIndexInformer&#123;</span><br><span class="line">				processor:                       &amp;sharedProcessor&#123;clock: realClock&#125;,</span><br><span class="line">				indexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),</span><br><span class="line">				listerWatcher:                   lw,</span><br><span class="line">				objectType:                      exampleObject,</span><br><span class="line">				resyncCheckPeriod:               defaultEventHandlerResyncPeriod,</span><br><span class="line">				defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod,</span><br><span class="line">				cacheMutationDetector:           NewCacheMutationDetector(fmt.Sprintf(<span class="string">&quot;%T&quot;</span>, exampleObject)),</span><br><span class="line">				clock:                           realClock,</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">secretInformer.AddEventHandler()--&gt;</span><br><span class="line">	AddEventHandlerWithResyncPeriod()--&gt;</span><br><span class="line">		listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize)--&gt;</span><br><span class="line">			ret := &amp;processorListener&#123;</span><br><span class="line">				nextCh:                <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;),</span><br><span class="line">				addCh:                 <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;),</span><br><span class="line">				handler:               handler,</span><br><span class="line">				pendingNotifications:  *buffer.NewRingGrowing(bufferSize),</span><br><span class="line">				requestedResyncPeriod: requestedResyncPeriod,</span><br><span class="line">				resyncPeriod:          resyncPeriod,</span><br><span class="line">			&#125;</span><br><span class="line">		s.processor.addListener(listener)</span><br><span class="line">			listener.run()--&gt;</span><br><span class="line">				<span class="keyword">for</span> next := <span class="keyword">range</span> p.nextCh &#123;</span><br><span class="line">					p.handler.OnUpdate(notification.oldObj, notification.newObj)</span><br><span class="line">					p.handler.OnAdd(notification.newObj)</span><br><span class="line">					p.handler.OnDelete(notification.oldObj)</span><br><span class="line">				&#125;</span><br><span class="line">			listener.pop()--&gt;</span><br><span class="line">				<span class="keyword">for</span> &#123;</span><br><span class="line">					<span class="keyword">select</span> &#123;</span><br><span class="line">					<span class="keyword">case</span> nextCh &lt;- notification:</span><br><span class="line">						notification, ok = p.pendingNotifications.ReadOne()</span><br><span class="line">					<span class="keyword">case</span> notificationToAdd, ok := &lt;-p.addCh:</span><br><span class="line">							p.pendingNotifications.WriteOne(notificationToAdd)</span><br><span class="line">				&#125;</span><br><span class="line">		<span class="keyword">for</span> _, item := <span class="keyword">range</span> s.indexer.List() &#123;</span><br><span class="line">			listener.add(addNotification&#123;newObj: item&#125;)--&gt;</span><br><span class="line">				p.addCh &lt;- notification</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> secretInformer.Run(ctx.Stop)</span><br><span class="line">	fifo := NewDeltaFIFOWithOptions()</span><br><span class="line">	cfg := &amp;Config&#123;</span><br><span class="line">			Queue:            fifo,</span><br><span class="line">			ListerWatcher:    s.listerWatcher,</span><br><span class="line">			ObjectType:       s.objectType,</span><br><span class="line">			FullResyncPeriod: s.resyncCheckPeriod,</span><br><span class="line">			RetryOnError:     <span class="literal">false</span>,</span><br><span class="line">			ShouldResync:     s.processor.shouldResync,</span><br><span class="line">	</span><br><span class="line">			Process: s.HandleDeltas,</span><br><span class="line">		&#125;</span><br><span class="line">	wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)</span><br><span class="line">	wg.StartWithChannel(processorStopCh, s.processor.run)</span><br><span class="line">	s.controller = New(cfg)</span><br><span class="line">	s.controller.Run(stopCh)--&gt;</span><br><span class="line">		r := NewReflector(</span><br><span class="line">			c.config.ListerWatcher,</span><br><span class="line">			c.config.ObjectType,</span><br><span class="line">			c.config.Queue,</span><br><span class="line">			c.config.FullResyncPeriod,</span><br><span class="line">		)</span><br><span class="line">		wg.StartWithChannel(stopCh, r.Run)--&gt;</span><br><span class="line">			r.ListAndWatch(stopCh)--&gt;</span><br><span class="line">				list := pager.List(context.Background(), options) (<span class="number">1</span>)</span><br><span class="line">				items, err := meta.ExtractList(list)</span><br><span class="line">				r.syncWith(items, resourceVersion)--&gt;</span><br><span class="line">					r.store.Replace(found, resourceVersion) (<span class="number">2</span>)</span><br><span class="line">				r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh)--&gt;</span><br><span class="line">					r.store.Update(event.Object)</span><br><span class="line">		c.processLoop--&gt;</span><br><span class="line">			c.config.Queue.Pop(PopProcessFunc(c.config.Process))<span class="comment">//HandleDeltas</span></span><br><span class="line">				<span class="keyword">for</span> _, d := <span class="keyword">range</span> obj.(Deltas) &#123;</span><br><span class="line">					s.processor.distribute(updateNotification)</span><br><span class="line">					s.processor.distribute(addNotification)</span><br><span class="line">					s.processor.distribute(deleteNotification)</span><br><span class="line">				&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p hidden>
参考
[informer framework](https://cncamp.notion.site/informer-framework-31ba746049ec472fb405e61482ed762f)
</p>]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-CDN</title>
    <url>/www6vHomeHexo/2022/01/15/aliyunCDN/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A6%82%E5%BF%B5">概念</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD-1">核心功能 [1]</a><br>- <a href="#%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9%E5%8A%A0%E9%80%9F">动态内容加速</a><br>- <a href="#%E9%9D%99%E6%80%81%E5%86%85%E5%AE%B9%E5%8A%A0%E9%80%9F">静态内容加速</a><br>- <a href="#%E5%AE%89%E5%85%A8%E9%98%B2%E6%8A%A4">安全防护</a></li>
<li><a href="#%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6-1">整体框架 [1]</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E5%AD%90%E7%B3%BB%E7%BB%9F-1">核心子系统 [1]</a><br>- <a href="#%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F">调度系统</a><br>- <a href="#%E8%8A%82%E7%82%B9%E7%B3%BB%E7%BB%9F">节点系统</a><br>- <a href="#%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93">网络传输</a><br>- <a href="#%E8%BF%90%E8%90%A5%E6%94%AF%E6%92%91">运营支撑</a></li>
<li><a href="#%E5%8A%A8%E6%80%81%E5%8A%A0%E9%80%9F%E7%B3%BB%E7%BB%9F-1">动态加速系统 [1]</a><br>- <a href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6">核心组件</a><br>- <a href="#%E5%9B%9B%E5%B1%82%E5%8A%A8%E6%80%81%E5%8A%A0%E9%80%9F">四层动态加速</a><br>- <a href="#%E4%B8%83%E5%B1%82%E5%8A%A8%E6%80%81%E5%8A%A0%E9%80%9F">七层动态加速</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="概念">概念</span><a href="#概念" class="header-anchor">#</a></h2><p>边缘服务器<br>降低用户访问时延， 减少公网流量</p>
<h2><span id="核心功能-1">核心功能 [1]</span><a href="#核心功能-1" class="header-anchor">#</a></h2><h5><span id="动态内容加速">动态内容加速</span><a href="#动态内容加速" class="header-anchor">#</a></h5><ul>
<li>对于动态内容(不可缓存的内容), 可以优化路由来加速</li>
<li>核心技术<ul>
<li>应用层路由路径优化</li>
<li>传输层协议栈(eg TCP)优化</li>
</ul>
</li>
</ul>
<h5><span id="静态内容加速">静态内容加速</span><a href="#静态内容加速" class="header-anchor">#</a></h5><h5><span id="安全防护">安全防护</span><a href="#安全防护" class="header-anchor">#</a></h5><h2><span id="整体框架-1">整体框架 [1]</span><a href="#整体框架-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/15/aliyunCDN/arch.JPG" class>


<h2><span id="核心子系统-1">核心子系统 [1]</span><a href="#核心子系统-1" class="header-anchor">#</a></h2><h5><span id="调度系统">调度系统</span><a href="#调度系统" class="header-anchor">#</a></h5><ul>
<li>DNS调度解析服务</li>
<li>节点资源调度服务</li>
<li>调度策略维护</li>
</ul>
<h5><span id="节点系统">节点系统</span><a href="#节点系统" class="header-anchor">#</a></h5><ul>
<li>缓存服务(重要)<br>多级缓存架构 </li>
<li>回源服务(重要)</li>
<li>接入网关</li>
<li>动态配置能力</li>
<li>量度数据(监控和计费)</li>
</ul>
<h5><span id="网络传输">网络传输</span><a href="#网络传输" class="header-anchor">#</a></h5><ul>
<li>网络传输优化</li>
</ul>
<ul>
<li>阻塞控制</li>
<li>传输协议</li>
<li>选路方式</li>
</ul>
<h5><span id="运营支撑">运营支撑</span><a href="#运营支撑" class="header-anchor">#</a></h5><h2><span id="动态加速系统-1">动态加速系统  [1]</span><a href="#动态加速系统-1" class="header-anchor">#</a></h2><h5><span id="核心组件">核心组件</span><a href="#核心组件" class="header-anchor">#</a></h5><ul>
<li>节点探测组件<br> 链路质量探测</li>
<li>路径计算组件<ul>
<li>计算最优的传输路径</li>
<li>选路架构:<br> 中心式<br> 分布式</li>
</ul>
</li>
<li>基础数据管理组件</li>
<li>离线日志组件</li>
</ul>
<h5><span id="四层动态加速">四层动态加速</span><a href="#四层动态加速" class="header-anchor">#</a></h5><h5><span id="七层动态加速">七层动态加速</span><a href="#七层动态加速" class="header-anchor">#</a></h5><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《CDN技术架构》  阿里云  有pdf</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Kubelet 代码走读</title>
    <url>/www6vHomeHexo/2022/01/15/k8sCodeOfKubelet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">server.<span class="keyword">go</span>:NewKubeletCommand()--&gt;&gt;</span><br><span class="line">kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)--&gt;&gt;</span><br><span class="line">	plugins, err := ProbeVolumePlugins(featureGate)</span><br><span class="line">server.<span class="keyword">go</span>:Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh)--&gt;&gt;</span><br><span class="line">	run(s, kubeDeps, featureGate, stopCh)--&gt;&gt;</span><br><span class="line">		kubeDeps.ContainerManager, err = cm.NewContainerManager() <span class="comment">// init runtime service(CRI), -container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///var/containerd/containerd.sock</span></span><br><span class="line">		kubelet.PreInitRuntimeService()--&gt;&gt;</span><br><span class="line">			remote.NewRemoteRuntimeService(remoteRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration)</span><br><span class="line">		RunKubelet(s, kubeDeps, s.RunOnce)--&gt;&gt;</span><br><span class="line">			createAndInitKubelet()--&gt;&gt;</span><br><span class="line">				kubelet.NewMainKubelet()--&gt;&gt;</span><br><span class="line">					makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)--&gt;&gt;</span><br><span class="line">						updatechannel = cfg.Channel(kubetypes.ApiserverSource)</span><br><span class="line">					klet := &amp;Kubelet&#123;&#125;</span><br><span class="line">					<span class="comment">//*******init volume plugins</span></span><br><span class="line">					runtime, err := kuberuntime.NewKubeGenericRuntimeManager()</span><br><span class="line">					NewInitializedVolumePluginMgr()--&gt;&gt;</span><br><span class="line">						kvh.volumePluginMgr.InitPlugins(plugins, prober, kvh)--&gt;&gt;</span><br><span class="line">			startKubelet()--&gt;&gt;</span><br><span class="line">				k.Run(podCfg.Updates())--&gt;&gt;</span><br><span class="line">					<span class="comment">//*******run volume manager, and reconcile function would attach volume for attachable plugin and mount volume</span></span><br><span class="line">					<span class="keyword">go</span> kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)--&gt;&gt;</span><br><span class="line">						vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh)--&gt;&gt;</span><br><span class="line">							populatorLoop--&gt;&gt;</span><br><span class="line">								dswp.findAndAddNewPods()--&gt;&gt;</span><br><span class="line">									dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize)--&gt;&gt;</span><br><span class="line">										mounts, devices := util.GetPodVolumeNames(pod)</span><br><span class="line">										dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mounts, devices)</span><br><span class="line">										dswp.desiredStateOfWorld.AddPodToVolume(uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue)--&gt;&gt;</span><br><span class="line">											dsw.volumesToMount[volumeName] = volumeToMount&#123;&#125;</span><br><span class="line">						vm.reconciler.Run(stopCh)--&gt;&gt;	</span><br><span class="line">							reconciliationLoopFunc() --&gt;&gt; <span class="comment">// reconcile every 100 ms</span></span><br><span class="line">								mountAttachVolumes--&gt;&gt;</span><br><span class="line">									rc.desiredStateOfWorld.GetVolumesToMount()</span><br><span class="line">									rc.operationExecutor.AttachVolume()--&gt;&gt;	<span class="comment">// attachable plugin, e.g. 	CSI plugin</span></span><br><span class="line">										operationGenerator.GenerateAttachVolumeFunc(volumeToAttach, actualStateOfWorld).Run()</span><br><span class="line">									rc.operationExecutor.MountVolume()--&gt;&gt;  <span class="comment">// volume need to mount, like ceph, configmap, emptyDir</span></span><br><span class="line">										oe.operationGenerator.GenerateMapVolumeFunc().Run()--&gt;&gt;</span><br><span class="line">											volumePlugin, err := og.volumePluginMgr.FindPluginBySpec(volumeToMount.VolumeSpec)</span><br><span class="line">											volumePlugin.NewMounter()</span><br><span class="line">											volumeMounter.SetUp()</span><br><span class="line">					kl.syncLoop(updates, kl)--&gt;&gt;</span><br><span class="line">						kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh)--&gt;&gt;</span><br><span class="line">							<span class="comment">//*******handle pod creation event</span></span><br><span class="line">							handler.HandlePodAdditions(u.Pods)--&gt;&gt;</span><br><span class="line">								kl.podManager.AddPod(pod)</span><br><span class="line">								kl.canAdmitPod(activePods, pod) <span class="comment">// check admit, if admit check fail, it will error out</span></span><br><span class="line">								kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)--&gt;&gt;</span><br><span class="line">									kl.podWorkers.UpdatePod()--&gt;&gt;</span><br><span class="line">										p.managePodLoop(podUpdates)--&gt;&gt;</span><br><span class="line">											p.syncPodFn()--&gt;&gt;</span><br><span class="line">												kubelet.<span class="keyword">go</span>:syncPod()--&gt;&gt;</span><br><span class="line">													runnable := kl.canRunPod(pod) <span class="comment">// check soft admin, pod will be pending if check fails</span></span><br><span class="line">													kl.runtimeState.networkErrors() <span class="comment">// check network plugin status</span></span><br><span class="line">													kl.containerManager.UpdateQOSCgroups()</span><br><span class="line">													kl.makePodDataDirs(pod)</span><br><span class="line">													kl.volumeManager.WaitForAttachAndMount(pod)</span><br><span class="line">													(kubeGenericRuntimeManager)kl.containerRuntime.SyncPod()--&gt;&gt;</span><br><span class="line">														m.computePodActions(pod, podStatus) <span class="comment">// create sandbox container?</span></span><br><span class="line">														m.createPodSandbox(pod, podContainerChanges.Attempt)--&gt;&gt;</span><br><span class="line">															m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, <span class="number">0755</span>)</span><br><span class="line">															<span class="comment">//*******calling CRI</span></span><br><span class="line">															m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler)--&gt;&gt;<span class="comment">// k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go</span></span><br><span class="line">																c.cc.Invoke(ctx, <span class="string">&quot;/runtime.v1alpha2.RuntimeService/RunPodSandbox&quot;</span>) <span class="comment">// call remote runtime service which is served by containerd</span></span><br><span class="line">															start(<span class="string">&quot;init container&quot;</span>, containerStartSpec(container))--&gt;&gt;</span><br><span class="line">																startContainer()</span><br><span class="line">															start(<span class="string">&quot;container&quot;</span>, containerStartSpec(&amp;pod.Spec.Containers[idx]))--&gt;&gt;</span><br><span class="line">																startContainer()--&gt;&gt;</span><br><span class="line">																	m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig)--&gt;&gt;</span><br><span class="line">																		c.cc.Invoke(ctx, <span class="string">&quot;/runtime.v1alpha2.ImageService/PullImage&quot;</span></span><br><span class="line">																	m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)--&gt;&gt;</span><br><span class="line">																		c.cc.Invoke(ctx, <span class="string">&quot;/runtime.v1alpha2.RuntimeService/CreateContainer&quot;</span>)</span><br><span class="line">																	m.internalLifecycle.PreStartContainer(pod, container, containerID) <span class="comment">// set cpu set</span></span><br><span class="line">																	m.runtimeService.StartContainer(containerID)--&gt;&gt;</span><br><span class="line">																		c.cc.Invoke(ctx, <span class="string">&quot;/runtime.v1alpha2.RuntimeService/StartContainer&quot;</span>)</span><br><span class="line">															</span><br><span class="line">containerd</span><br><span class="line">server.<span class="keyword">go</span>:service.Register(grpcServer)--&gt;&gt;</span><br><span class="line">	runtime.RegisterRuntimeServiceServer(s, instrumented)--&gt;&gt;</span><br><span class="line">			s.RegisterService(&amp;_RuntimeService_serviceDesc, srv)--&gt;&gt;</span><br><span class="line">				Handler:    _RuntimeService_RunPodSandbox_Handler,	<span class="comment">// api.pb.go</span></span><br><span class="line">				srv.(RuntimeServiceServer).RunPodSandbox(ctx, in)--&gt;&gt; <span class="comment">//&quot;/runtime.v1alpha2.RuntimeService/RunPodSandbox&quot;</span></span><br><span class="line">					RunPodSandbox()--&gt;&gt; <span class="comment">// pkg/server/sandbox_run.go</span></span><br><span class="line">						sandboxstore.NewSandbox()</span><br><span class="line">						c.ensureImageExists()</span><br><span class="line">						c.getSandboxRuntime(config, r.GetRuntimeHandler())</span><br><span class="line">						netns.NewNetNS()</span><br><span class="line">						c.setupPodNetwork(ctx, &amp;sandbox)--&gt;&gt;</span><br><span class="line">							c.netPlugin.Setup(ctx, id, path, opts...)--&gt;&gt;</span><br><span class="line">								network.Attach(ctx, ns)--&gt;&gt;</span><br><span class="line">									n.cni.AddNetworkList(ctx, n.config, ns.config(n.ifName))--&gt;&gt;</span><br><span class="line">										c.addNetwork(ctx, list.Name, list.CNIVersion, net, result, rt)--&gt;&gt;<span class="comment">// for each network plugin</span></span><br><span class="line">											c.exec.FindInPath(net.Network.Type, c.Path)</span><br><span class="line">											buildOneConfig(name, cniVersion, net, prevResult, rt)</span><br><span class="line">											invoke.ExecPluginWithResult(ctx, pluginPath, newConf.Bytes, c.args(<span class="string">&quot;ADD&quot;</span>, rt), c.exec)</span><br><span class="line">						c.client.NewContainer(ctx, id, opts...)</span><br><span class="line">						c.os.MkdirAll(sandboxRootDir, <span class="number">0755</span>)</span><br><span class="line">						c.os.MkdirAll(volatileSandboxRootDir, <span class="number">0755</span>)</span><br><span class="line">						c.setupSandboxFiles(id, config)</span><br><span class="line">						container.NewTask(ctx, containerdio.NullIO, taskOpts...)</span><br><span class="line">						task.Start(ctx)--&gt;&gt;</span><br><span class="line">							c.client.TaskService().Create(ctx, request)--&gt;&gt;</span><br><span class="line">								s.local.Create(ctx, r)--&gt;&gt;</span><br><span class="line">									l.getRuntime(container.Runtime.Name)</span><br><span class="line">									rtime.Create(ctx, r.ContainerID, opts)--&gt;&gt;</span><br><span class="line">										b := shimBinary(ctx, bundle, opts.Runtime, m.containerdAddress, m.containerdTTRPCAddress, m.events, m.tasks)</span><br><span class="line">										b.Start()</span><br><span class="line">										shim.Create(ctx, opts)--&gt;&gt;</span><br><span class="line">											c.client.Call(ctx, <span class="string">&quot;containerd.task.v2.Task&quot;</span>, <span class="string">&quot;Create&quot;</span>, req, &amp;resp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	runtime.RegisterImageServiceServer(s, instrumented)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p hidden>
参考
[kubelet](https://cncamp.notion.site/kubelet-go-c3b5cf9bbf4b4e3098720f61efb15e0e)
</p>]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes KubeScheduler 代码走读</title>
    <url>/www6vHomeHexo/2022/01/15/k8sCodeOfKubeScheduler/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// Framework manages the set of plugins in use by the scheduling framework.</span></span><br><span class="line"><span class="comment">// Configured plugins are called at specified points in a scheduling context.</span></span><br><span class="line"><span class="keyword">type</span> Framework <span class="keyword">interface</span> &#123;</span><br><span class="line">	Handle</span><br><span class="line">	QueueSortFunc() LessFunc</span><br><span class="line">	RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) *Status</span><br><span class="line">	RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)</span><br><span class="line">	RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>) *Status</span><br><span class="line">	RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>)</span><br><span class="line">	RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>) *Status</span><br><span class="line">	RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>)</span><br><span class="line">	RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>) *Status</span><br><span class="line">	WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status</span><br><span class="line">	RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName <span class="type">string</span>) *Status</span><br><span class="line">	HasFilterPlugins() <span class="type">bool</span></span><br><span class="line">	HasPostFilterPlugins() <span class="type">bool</span></span><br><span class="line">	HasScorePlugins() <span class="type">bool</span></span><br><span class="line">	ListPlugins() *config.Plugins</span><br><span class="line">	ProfileName() <span class="type">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Schedule()--&gt;</span><br><span class="line">	<span class="comment">// filter</span></span><br><span class="line">	g.findNodesThatFitPod(ctx, extenders, fwk, state, pod)--&gt;</span><br><span class="line">		<span class="comment">// 1.filter预处理阶段：遍历pod的所有initcontainer和主container，计算pod的总资源需求</span></span><br><span class="line">		s := fwk.RunPreFilterPlugins(ctx, state, pod) <span class="comment">// e.g. computePodResourceRequest</span></span><br><span class="line">		<span class="comment">// 2. filter阶段，遍历所有节点，过滤掉不符合资源需求的节点</span></span><br><span class="line">		g.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, allNodes)--&gt;</span><br><span class="line">			fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)--&gt;</span><br><span class="line">				s, err := getPreFilterState(cycleState)</span><br><span class="line">				insufficientResources := fitsRequest(s, nodeInfo, f.ignoredResources, f.ignoredResourceGroups)</span><br><span class="line">		<span class="comment">// 3. 处理扩展plugin</span></span><br><span class="line">		findNodesThatPassExtenders(extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)</span><br><span class="line">		<span class="comment">// score</span></span><br><span class="line">	prioritizeNodes(ctx, extenders, fwk, state, pod, feasibleNodes)--&gt;</span><br><span class="line">		<span class="comment">// 4. score，比如处理弱亲和性，将preferredAffinity语法进行解析</span></span><br><span class="line">		fwk.RunPreScorePlugins(ctx, state, pod, nodes) <span class="comment">// e.g. nodeAffinity</span></span><br><span class="line">		fwk.RunScorePlugins(ctx, state, pod, nodes)--&gt;</span><br><span class="line">		<span class="comment">// 5. 为节点打分</span></span><br><span class="line">			f.runScorePlugin(ctx, pl, state, pod, nodeName) <span class="comment">// e.g. noderesource fit</span></span><br><span class="line">		<span class="comment">// 6. 处理扩展plugin</span></span><br><span class="line">		extenders[extIndex].Prioritize(pod, nodes)</span><br><span class="line">		<span class="comment">// 7.选择节点</span></span><br><span class="line">		g.selectHost(priorityList)</span><br><span class="line">sched.assume(assumedPod, scheduleResult.SuggestedHost)--&gt;</span><br><span class="line">	  <span class="comment">// 8.假定选中pod</span></span><br><span class="line">	sched.SchedulerCache.AssumePod(assumed)--&gt;</span><br><span class="line">fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)--&gt;</span><br><span class="line">	f.runReservePluginReserve(ctx, pl, state, pod, nodeName) <span class="comment">// e.g. bindVolume。其实还没大用</span></span><br><span class="line">runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)--&gt;</span><br><span class="line">	f.runPermitPlugin(ctx, pl, state, pod, nodeName) <span class="comment">// empty hook</span></span><br><span class="line">fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) <span class="comment">// 同 runReservePluginReserve</span></span><br><span class="line">		<span class="comment">// bind</span></span><br><span class="line">		<span class="comment">// 9.绑定pod</span></span><br><span class="line">sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)--&gt;</span><br><span class="line">	f.runBindPlugin(ctx, bp, state, pod, nodeName)--&gt;</span><br><span class="line">		b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions&#123;&#125;)--&gt;</span><br><span class="line">			<span class="keyword">return</span> c.client.Post().Namespace(c.ns).Resource(<span class="string">&quot;pods&quot;</span>).Name(binding.Name).VersionedParams(&amp;opts, scheme.ParameterCodec).SubResource(<span class="string">&quot;binding&quot;</span>).Body(binding).Do(ctx).Error()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p hidden>
参考
[kube-scheduler](https://cncamp.notion.site/kube-scheduler-0d45b37a5c9a46008aaf9f9e2088b3ce)
</p>]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>istio 控制面ControlPanel</title>
    <url>/www6vHomeHexo/2022/01/14/istioControlPanel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="istio控制面11-14">istio控制面(1.1-1.4)</span><a href="#istio控制面11-14" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/14/istioControlPanel/istio-control.jpg" class title="istio控制面(1.1-1.4)">

<ul>
<li><p>mixer的三大功能：</p>
<ul>
<li>前置条件检查 </li>
<li>配额管理</li>
<li>遥测报告</li>
</ul>
</li>
<li><p>优化 </p>
<ul>
<li>默认关闭mixer</li>
<li>1.5之后mixer被移除</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://mp.weixin.qq.com/s/BMVCeiA2aqASbLqyhPomWA">Istio 庖丁解牛三：galley</a>  腾讯云 钟华<br><a href="http://www.servicemesher.com/blog/istio-service-visibility/">Istio1.1新特性之限制服务可见性</a>  敖小剑 引<br><a href="https://istio.io/docs/reference/config/policy-and-telemetry/adapters/">官方内置adapters</a>  大多数是metrics and logs系统</p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP8-AI解决方案</title>
    <url>/www6vHomeHexo/2022/01/13/tencentTCP8/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="八-ai解决方案10">八、AI解决方案(10%)</span><a href="#八-ai解决方案10" class="header-anchor">#</a></h2><h3><span id="1-ai的发展和应用">1. AI的发展和应用</span><a href="#1-ai的发展和应用" class="header-anchor">#</a></h3><h5><span id="11-人工智能基本概念">1.1 人工智能基本概念</span><a href="#11-人工智能基本概念" class="header-anchor">#</a></h5><p>人工智能、机器学习和深度学习	☆☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-conception.png" class>

<h5><span id="12-机器学习的流程">1.2 机器学习的流程</span><a href="#12-机器学习的流程" class="header-anchor">#</a></h5><p>机器学习的流程	☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-process.png" class>
<ul>
<li>定义问题<br>转换成数学问题<br>分类，回归，聚类</li>
<li>数据采集</li>
<li>数据预处理<br>原始数据集 转换为 特征数据集，<br>拆分成训练数据和测试数据<ul>
<li>数据清洗</li>
<li>特征工程</li>
</ul>
</li>
<li>模型训练 优化<br>诊断模型是否过拟合</li>
<li>模型验证</li>
<li>模型融合</li>
<li>上线运行</li>
</ul>
<h5><span id="13-机器学习的常见算法">1.3 机器学习的常见算法</span><a href="#13-机器学习的常见算法" class="header-anchor">#</a></h5><p>机器学习的常见算法	☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-algo.jpg" class>

<h5><span id="14-人工智能的发展">1.4 人工智能的发展</span><a href="#14-人工智能的发展" class="header-anchor">#</a></h5><p>人工智能的发展	☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-history.png" class>

<h5><span id="15-人工智能的产业链">1.5 人工智能的产业链</span><a href="#15-人工智能的产业链" class="header-anchor">#</a></h5><p>具体产业链及技术应用	☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-chain.png" class>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-apply.png" class>

<h3><span id="2-腾讯云ai产品体系">2. 腾讯云AI产品体系</span><a href="#2-腾讯云ai产品体系" class="header-anchor">#</a></h3><h5><span id="21-各行业的痛点">2.1 各行业的痛点</span><a href="#21-各行业的痛点" class="header-anchor">#</a></h5><p>各行业的痛点，人工智能如何解决	☆</p>
<ul>
<li>各行业的痛点，人工智能如何解决<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-solution1.png" class>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/1/ai-solution2.png" class></li>
</ul>
<h5><span id="22-腾讯云ai产品体系">2.2 腾讯云AI产品体系</span><a href="#22-腾讯云ai产品体系" class="header-anchor">#</a></h5><p>企业打造AI能力的困难	☆☆<br>腾讯云AI产品体系如何帮助企业打造AI能力	☆☆☆</p>
<ul>
<li><p>企业打造AI能力的困难</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-problem.jpg" class>

</li>
<li><p>腾讯云AI产品体系如何帮助企业打造AI能力</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-products.jpg" class></li>
</ul>
<h5><span id="22-计算机视觉">2.2 计算机视觉</span><a href="#22-计算机视觉" class="header-anchor">#</a></h5><p>计算机视觉概述	☆<br>“计算机视觉相关的腾讯云产品及各个产品的功能和应用场景”	☆☆☆</p>
<ul>
<li><p>计算机视觉概述</p>
<ul>
<li>基本任务 <ul>
<li>图像分类</li>
<li>目标检查</li>
<li>目标跟踪</li>
<li>图像分割</li>
</ul>
</li>
<li>应用方向<ul>
<li>人脸识别</li>
<li>视频结构化</li>
<li>姿势识别</li>
</ul>
</li>
</ul>
</li>
<li><p>计算机视觉相关的腾讯云产品及各个产品的功能和应用场景</p>
<ul>
<li>计算机视觉技术挑战</li>
<li>腾讯云计算机视觉产品<ul>
<li>图像识别<ul>
<li>图像分析</li>
<li>智能鉴黄</li>
<li>文字识别</li>
</ul>
</li>
<li>人体识别<ul>
<li>手势识别</li>
</ul>
</li>
<li>人脸识别<ul>
<li>人脸融合</li>
<li>人脸核身</li>
<li>人脸支付      <img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-view-product1.jpg" class>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-view-product2.jpg" class></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="23-自然语言处理">2.3 自然语言处理</span><a href="#23-自然语言处理" class="header-anchor">#</a></h5><p>自然语言处理的概念、产品介绍、功能及应用场景	☆☆</p>
<ul>
<li>自然语言处理的概念、产品介绍、功能及应用场景<ul>
<li>自然语言处理的概念<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-npl.jpg" class>  </li>
<li>自然语言处理技术挑战</li>
<li>腾讯云自然语言处理产品<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-npl-products.jpg" class></li>
</ul>
</li>
</ul>
<h5><span id="24-语音技术">2.4 语音技术</span><a href="#24-语音技术" class="header-anchor">#</a></h5><p>语音技术的概念、产品介绍、功能及应用场景	☆☆</p>
<ul>
<li>语音技术的概念、产品介绍、功能及应用场景<ul>
<li>语音技术的概念<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-voice.jpg" class></li>
<li>语音技术-技术挑战</li>
<li>腾讯云语音技术产品<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ai-voice-products.jpg" class></li>
</ul>
</li>
</ul>
<h5><span id="25-人工智能平台">2.5 人工智能平台</span><a href="#25-人工智能平台" class="header-anchor">#</a></h5><p>人工智能平台概述	☆<br>“三大人工智能平台：智能钛机器学习TI-one、云智天枢平台TI-Matrix、腾讯智能对话平台”	☆☆</p>
<ul>
<li>人工智能平台概述<ul>
<li>智能钛机器学习TI-one<br>3个子产品<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ti-one.jpg" class>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ti-one-lifecycle.jpg" class>    </li>
<li>云智天枢平台TI-Matrix    <img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ti-matrix.jpg" class>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/ti-matrix-layer.jpg" class>    </li>
<li>腾讯智能对话平台<img src="/www6vHomeHexo/2022/01/13/tencentTCP8/2/dialog.jpg" class></li>
</ul>
</li>
</ul>
<h3><span id="3-腾讯云ai解决方案">3. 腾讯云AI解决方案</span><a href="#3-腾讯云ai解决方案" class="header-anchor">#</a></h3><h5><span id="26-人脸核身解决方案">2.6 人脸核身解决方案</span><a href="#26-人脸核身解决方案" class="header-anchor">#</a></h5><p>人脸核身解决方案的适用场景及人脸核身的实现方案	☆☆</p>
<h5><span id="27-智能客服解决方案">2.7 智能客服解决方案</span><a href="#27-智能客服解决方案" class="header-anchor">#</a></h5><p>智能客服解决方案相关产品及智能客服技术架构	☆☆</p>
<h5><span id="28-优图天眼解决方案">2.8 优图天眼解决方案</span><a href="#28-优图天眼解决方案" class="header-anchor">#</a></h5><p>优图天眼解决方案的架构及应用场景	☆☆</p>
<h5><span id="29-ar云服务解决方案">2.9 AR云服务解决方案</span><a href="#29-ar云服务解决方案" class="header-anchor">#</a></h5><p>AR云服务解决方案架构、AR云服务解决方案的应用场景	☆☆</p>
<h5><span id="210-智慧会场解决方案">2.10 智慧会场解决方案</span><a href="#210-智慧会场解决方案" class="header-anchor">#</a></h5><p>“智慧法庭解决方案、大型现场会议解决方案、远程会议解决方案”	☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP7-构建混合云</title>
    <url>/www6vHomeHexo/2022/01/13/tencentTCP7/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="七-构建混合云10">七、构建混合云(10%)</span><a href="#七-构建混合云10" class="header-anchor">#</a></h2><h3><span id="1构建混合云的相关概念及背景">1.构建混合云的相关概念及背景</span><a href="#1构建混合云的相关概念及背景" class="header-anchor">#</a></h3><h5><span id="11-构建混合云的相关概念">1.1 构建混合云的相关概念</span><a href="#11-构建混合云的相关概念" class="header-anchor">#</a></h5><p>“混合云含义，公有云、私有云、混合云三者的区别，云管理平台概念”	☆☆</p>
<ul>
<li>混合云含义，公有云、私有云、混合云三者的区别，云管理平台概念 [pic]</li>
</ul>
<h5><span id="12-构建混合云的原因">1.2 构建混合云的原因</span><a href="#12-构建混合云的原因" class="header-anchor">#</a></h5><p>降低成本，提高可靠性和安全性	☆☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/reasons.png" class title="企业应用混合云的原因">

<h5><span id="13-构建混合云的目的场景">1.3 构建混合云的目的（场景）</span><a href="#13-构建混合云的目的场景" class="header-anchor">#</a></h5><p>“包括：负载扩容、灾难恢复、数据备份、应用部署和开发测试生产部署”	☆☆☆</p>
<ul>
<li>包括：负载扩容、灾难恢复、数据备份、应用部署和开发测试生产部署 [todo 34.4]<ul>
<li>负载扩容       <img src="/www6vHomeHexo/2022/01/13/tencentTCP7/load-scaling.png" class></li>
<li>灾难恢复<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/disaster-recovery.png" class>      </li>
<li>数据备份<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/data-restore.png" class></li>
<li>应用部署<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/app-deploy.png" class>       </li>
<li>开发测试生产部署<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/product-deploy.png" class></li>
</ul>
</li>
</ul>
<h3><span id="2-混合云的设计原则">2. 混合云的设计原则</span><a href="#2-混合云的设计原则" class="header-anchor">#</a></h3><h5><span id="21-混合云的挑战">2.1 混合云的挑战</span><a href="#21-混合云的挑战" class="header-anchor">#</a></h5><p>混合云的挑战：环境复杂、IT环境分散等	☆☆</p>
<ul>
<li>混合云的挑战：环境复杂、IT环境分散等<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/hybrid-challage.png" class></li>
</ul>
<h5><span id="22-混合云架构设计原则">2.2 混合云架构设计原则</span><a href="#22-混合云架构设计原则" class="header-anchor">#</a></h5><p>“网络、存储和应用架构设计需要考虑的因素、设计的方法等”	☆☆</p>
<h3><span id="3-设计混合云网络架构">3. 设计混合云网络架构</span><a href="#3-设计混合云网络架构" class="header-anchor">#</a></h3><h5><span id="31-混合云网络架构">3.1 混合云网络架构</span><a href="#31-混合云网络架构" class="header-anchor">#</a></h5><p>“专线、VPN、对等连接等产品的概念和功能；如何使用专线、VPN、对等连接等产品进行混合云网络架构设计”	☆☆☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/hybrid-net-arch.png" class>
<blockquote>
<p>测试的时候用vpn, 上生产时用专线</p>
</blockquote>
<h5><span id="32-多分支机构的网络连接">3.2 多分支机构的网络连接</span><a href="#32-多分支机构的网络连接" class="header-anchor">#</a></h5><p>“云联网的概念和功能；对等连接&#x2F;专线接入与云联网的差异；如何使用云联网构建混合云网络架构”	☆☆☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/hybrid-net-branchs-arch.png" class>

<h5><span id="33-多云网络的互通">3.3 多云网络的互通</span><a href="#33-多云网络的互通" class="header-anchor">#</a></h5><p>“涉及技术：SD-WAN软件定义广域网。为什么需要SD-WAN？SD-WAN如何实现多云网络互通？”	☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP7/hybrid-net-clouds-arch.png" class>

<h3><span id="4-设计混合云存储架构">4. 设计混合云存储架构</span><a href="#4-设计混合云存储架构" class="header-anchor">#</a></h3><h5><span id="41-云存储网关">4.1 云存储网关</span><a href="#41-云存储网关" class="header-anchor">#</a></h5><p>“存储网关的类型：卷网关、文件网关、磁带网关。几种网关的差异和应用场景、原理、架构图”	☆☆☆</p>
<h5><span id="42-混合云的数据库同步dts">4.2 混合云的数据库同步：DTS</span><a href="#42-混合云的数据库同步dts" class="header-anchor">#</a></h5><p>腾讯云 DTS的功能原理	☆☆</p>
<h3><span id="5-设计混合云的应用架构">5. 设计混合云的应用架构</span><a href="#5-设计混合云的应用架构" class="header-anchor">#</a></h3><h5><span id="51-分割应用层级">5.1 分割应用层级</span><a href="#51-分割应用层级" class="header-anchor">#</a></h5><p>“在混合云架构中，应用的层级是如何分布在公有云和私有云中的？”	☆☆</p>
<h3><span id="6构建混合云的解决方案">6.构建混合云的解决方案</span><a href="#6构建混合云的解决方案" class="header-anchor">#</a></h3><h5><span id="61-基于腾讯云的私有云选型">6.1 基于腾讯云的私有云选型</span><a href="#61-基于腾讯云的私有云选型" class="header-anchor">#</a></h5><p>如何选择不同的私有云方案	☆☆<br>黑石混合云架构、腾讯云专有云TCE、腾讯云专有云Tstack	☆☆</p>
<h5><span id="62-混合云管理的解决方案">6.2 混合云管理的解决方案</span><a href="#62-混合云管理的解决方案" class="header-anchor">#</a></h5><p>混合云管理的挑战、如何解决混合云管理的难题	☆<br>TStack云管平台、AWCloud云管平台 、EasyOps云管平台	☆☆<br>混合云行业解决方案	☆☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP6-腾讯云大数据产品及服务</title>
    <url>/www6vHomeHexo/2022/01/13/tencentTCP6/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="六-腾讯云大数据产品及服务10">六、腾讯云大数据产品及服务(10%)</span><a href="#六-腾讯云大数据产品及服务10" class="header-anchor">#</a></h2><h3><span id="1-大数据概述">1. 大数据概述</span><a href="#1-大数据概述" class="header-anchor">#</a></h3><h5><span id="11-大数据应用的相关概念">1.1 大数据应用的相关概念</span><a href="#11-大数据应用的相关概念" class="header-anchor">#</a></h5><p>大数据的基本概念	☆<br>离线计算的概念和原理、实时计算的概念和原理	☆☆</p>
<ul>
<li>大数据的基本概念<ul>
<li>大数据定义<ul>
<li>对大规模数据进行分析</li>
<li>帮助企业决策</li>
<li>与企业业务融合，产生新价值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="12-大数据的背景">1.2 大数据的背景</span><a href="#12-大数据的背景" class="header-anchor">#</a></h5><p>大数据产生的背景、特征和核心产业	☆</p>
<ul>
<li><p>大数据产生的背景</p>
</li>
<li><p>大数据特征</p>
<ul>
<li>4V<ul>
<li>数据规模大 volume</li>
<li>数据流转快 velocity</li>
<li>数据类型多 </li>
<li>价值密度低 valueless</li>
</ul>
</li>
</ul>
</li>
<li><p>核心产业 [pic]</p>
<ul>
<li>数据</li>
<li>产品</li>
<li>服务</li>
</ul>
</li>
</ul>
<h5><span id="13-大数据的技术框架">1.3 大数据的技术框架</span><a href="#13-大数据的技术框架" class="header-anchor">#</a></h5><p>大数据的技术框架	☆☆</p>
<ul>
<li>大数据的技术框架</li>
</ul>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/tech-arch.png" class title="大数据的技术框架">

<h5><span id="14-云环境下的大数据基础平台">1.4 云环境下的大数据基础平台</span><a href="#14-云环境下的大数据基础平台" class="header-anchor">#</a></h5><p>大数据解决方案与云环境下的大数据解决方案差异	☆☆</p>
<ul>
<li><p>大数据解决方案</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/solution.png" class title="大数据的技术框架">

</li>
<li><p>云环境下的大数据解决方案</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/solution-cloud.png" class title="云环境下的大数据解决方案"></li>
</ul>
<h3><span id="2-设计原则">2. 设计原则</span><a href="#2-设计原则" class="header-anchor">#</a></h3><h5><span id="21-传统数据分析的痛点">2.1 传统数据分析的痛点</span><a href="#21-传统数据分析的痛点" class="header-anchor">#</a></h5><p>传统数据分析面临的问题，大数据如何解决这些问题？	☆</p>
<h5><span id="22-腾讯云大数据产品体系">2.2 腾讯云大数据产品体系</span><a href="#22-腾讯云大数据产品体系" class="header-anchor">#</a></h5><p>腾讯云大数据产品体系	☆☆☆</p>
<ul>
<li><p>在发展演进中的腾讯大数据能力</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/capability.png" class title="在发展演进中的腾讯大数据能力">
</li>
<li><p>大数据产品线</p>
  <img src="/www6vHomeHexo/2022/01/13/tencentTCP6/products.png" class title="大数据产品线">
</li>
<li><p>大数据基础设施的解决方案 </p>
<ul>
<li>数仓领域解决方案 <img src="/www6vHomeHexo/2022/01/13/tencentTCP6/warehouse-arch.png" class title="数仓领域解决方案">     
<ul>
<li>海量日志分析[pic]</li>
<li>经营分析[pic]</li>
</ul>
</li>
<li>流计算领域解决方案<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/streamComputing-arch.png" class title="流计算域解决方案"> 
<ul>
<li>网站点击流分析[pic]</li>
<li>金融实时风控[pic]</li>
</ul>
</li>
<li>搜索领域解决方案<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/search-arch.png" class title="搜索领域解决方案">
<ul>
<li>小程序搜索支持[pic]</li>
<li>日志实时分析监控[pic]</li>
<li>BI分析[pic]</li>
</ul>
</li>
</ul>
</li>
<li><p>大数据应用服务的解决方案</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/solution-app.png" class title="搜索领域解决方案">
<ul>
<li>企业画像</li>
<li>智能选址</li>
</ul>
</li>
</ul>
<h3><span id="3-架构设计">3. 架构设计</span><a href="#3-架构设计" class="header-anchor">#</a></h3><h5><span id="31-大数据处理套件tbds-没有相关的视频">3.1 大数据处理套件TBDS  [没有相关的视频]</span><a href="#31-大数据处理套件tbds-没有相关的视频" class="header-anchor">#</a></h5><p>TBDS的概述：基本概念、功能原理、应用场景	☆☆</p>
<h5><span id="32-使用大数据处理套件tbds构建数据仓库-没有相关的视频">3.2 使用大数据处理套件TBDS构建数据仓库 [没有相关的视频]</span><a href="#32-使用大数据处理套件tbds构建数据仓库-没有相关的视频" class="header-anchor">#</a></h5><p>使用大数据处理套件TBDS构建数据仓库	☆☆</p>
<h5><span id="33-云数据仓库套件sparking-没有相关的视频">3.3 云数据仓库套件Sparking [没有相关的视频]</span><a href="#33-云数据仓库套件sparking-没有相关的视频" class="header-anchor">#</a></h5><p>Sparking的概述：基本概念、功能原理、应用场景	☆☆</p>
<ul>
<li>云数据仓库PostgreSQL产品概述 [只有这个]</li>
</ul>
<h5><span id="34-使用云数据仓库套件-sparking实现企业全域数据资产管理-没有相关的视频">3.4 使用云数据仓库套件 Sparking实现企业全域数据资产管理 [没有相关的视频]</span><a href="#34-使用云数据仓库套件-sparking实现企业全域数据资产管理-没有相关的视频" class="header-anchor">#</a></h5><p>使用云数据仓库套件Sparking实现企业全域数据资产管理	☆☆</p>
<ul>
<li>腾讯云数据仓库PostgreSQL案例 [只有这个]</li>
</ul>
<h5><span id="35-弹性mapreduce">3.5 弹性MapReduce</span><a href="#35-弹性mapreduce" class="header-anchor">#</a></h5><p>MapReduce的概述：基本概念、功能原理、应用场景	☆☆</p>
<ul>
<li>MapReduce的概述：基本概念、功能原理、应用场景<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/mr-base.png" class title="概念和优势">
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/mr-senario.png" class></li>
</ul>
<h5><span id="36-使用弹性mapreduce实现离线数据分析">3.6 使用弹性MapReduce实现离线数据分析</span><a href="#36-使用弹性mapreduce实现离线数据分析" class="header-anchor">#</a></h5><p>使用弹性MapReduce实现离线数据分析	☆☆☆</p>
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/mr-offline-sulution.png" class>


<h5><span id="37-流计算oceanus">3.7 流计算Oceanus</span><a href="#37-流计算oceanus" class="header-anchor">#</a></h5><p>Oceanus的概述：基本概念、功能原理、应用场景	☆☆</p>
<ul>
<li>Oceanus的概述：基本概念、功能原理、应用场景<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/oceanus.png" class title="概念和优势">
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/oceanus-senario.png" class></li>
</ul>
<h5><span id="38-使用流计算oceanus实现互联网点击流分析">3.8 使用流计算Oceanus实现互联网点击流分析</span><a href="#38-使用流计算oceanus实现互联网点击流分析" class="header-anchor">#</a></h5><p>使用流计算Oceanus实现互联网点击流分析	☆☆</p>
<h5><span id="39-商业智能分析bi">3.9 商业智能分析BI</span><a href="#39-商业智能分析bi" class="header-anchor">#</a></h5><p>商业智能分析BI的概述：基本概念、功能原理、应用场景	☆☆</p>
<ul>
<li>商业智能分析BI的概述：基本概念、功能原理、应用场景<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/bi-base.png" class title="概念和优势">
<img src="/www6vHomeHexo/2022/01/13/tencentTCP6/bi-senario.png" class></li>
</ul>
<h5><span id="310-使用商业智能分析bi实现数据即时分析与决策">3.10 使用商业智能分析BI实现数据即时分析与决策</span><a href="#310-使用商业智能分析bi实现数据即时分析与决策" class="header-anchor">#</a></h5><p>使用商业智能分析BI实现数据即时分析与决策	☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Rook</title>
    <url>/www6vHomeHexo/2022/01/12/k8sRook/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="rook-for-ceph">Rook for ceph</span><a href="#rook-for-ceph" class="header-anchor">#</a></h2><h5><span id="rook建立ceph集群流程1">Rook建立Ceph集群流程[1]</span><a href="#rook建立ceph集群流程1" class="header-anchor">#</a></h5><blockquote>
<p>rook的crd建立好之后，<br>Rook的operator会把rook的控制面建立出来<br>然后把storageclass和csi driver都注册好。<br>用户只要建立pvc，pvc里指定用哪个storageclass，然后csi driver会去工作。</p>
</blockquote>
<h5><span id="rook-cluster-installation-23-i">Rook cluster installation [2][3] [I]</span><a href="#rook-cluster-installation-23-i" class="header-anchor">#</a></h5><h5><span id="ceph存储类型">Ceph存储类型</span><a href="#ceph存储类型" class="header-anchor">#</a></h5><ul>
<li>Block </li>
<li>FileSystem</li>
<li>Object</li>
</ul>
<h5><span id="block-storage-45-ii">Block Storage [4][5] [II]</span><a href="#block-storage-45-ii" class="header-anchor">#</a></h5><ul>
<li><p>install</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephBlockPool</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">replicapool</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">failureDomain:</span> <span class="string">host</span></span><br><span class="line">  <span class="attr">replicated:</span></span><br><span class="line">    <span class="attr">size:</span> <span class="number">3</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">rook-ceph-block</span></span><br><span class="line"><span class="comment"># Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rook-ceph.rbd.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">    <span class="comment"># clusterID is the namespace where the rook cluster is running</span></span><br><span class="line">    <span class="attr">clusterID:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="comment"># Ceph pool into which the RBD image shall be created</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">replicapool</span></span><br><span class="line"></span><br><span class="line">    <span class="string">...</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Delete the rbd volume when a PVC is deleted</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Consume Block[II]<br>eg 1.   <a href="/www6vHomeHexo/2019/11/11/k8sStatefulSet/" title="Kubernetes StatefulSet原理和源码">StatefulSet</a>通过StorageClass动态申请pv  </p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">volumeClaimTemplates:</span></span><br><span class="line"><span class="string">-metadata:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">    <span class="attr">storageClassName:</span> <span class="string">rook-ceph-block</span></span><br><span class="line">    <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure>
<p>eg 2. pvc动态申请pv， deployment申请pvc</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">volumes:</span> <span class="string">...</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pvc-test</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">rook-ceph-test-pvc</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5><span id="filesystem67-iii">FileSystem[6][7] [III]</span><a href="#filesystem67-iii" class="header-anchor">#</a></h5><h5><span id="pvc在线扩容pvc快照和回滚">PVC在线扩容，PVC快照和回滚</span><a href="#pvc在线扩容pvc快照和回滚" class="header-anchor">#</a></h5><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p>云原生训练营 第0期-模块七</p>
</li>
<li><p><a href="https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html">Ceph Cluster CRD</a> ***</p>
</li>
<li><p><a href="https://rook.io/docs/rook/v1.2/ceph-quickstart.html">Ceph Storage Quickstart</a></p>
</li>
<li><p><a href="https://rook.github.io/docs/rook/v1.2/ceph-block.html">Block Storage</a></p>
</li>
<li><p><a href="https://rook.github.io/docs/rook/v1.2/ceph-pool-crd.html#spec">Ceph Block Pool CRD</a></p>
</li>
<li><p><a href="https://rook.github.io/docs/rook/v1.2/ceph-filesystem.html">Shared Filesystem</a></p>
</li>
<li><p><a href="https://rook.github.io/docs/rook/v1.2/ceph-filesystem-crd.html">Ceph Shared Filesystem CRD</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV16t4y1w7r6?p=111">kubernetes架构师课程</a> 杜宽 ***<br>[I]P112-P114 installation, [II]P115-P117 block, [III]  P118 FileSystem</p>
</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes CoreDNS</title>
    <url>/www6vHomeHexo/2022/01/12/k8sDNS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="coredns插件架构">CoreDNS插件架构</span><a href="#coredns插件架构" class="header-anchor">#</a></h2><p>github中</p>
<h2><span id="kubernetes-dns">Kubernetes DNS</span><a href="#kubernetes-dns" class="header-anchor">#</a></h2><ul>
<li>Kubernetes服务发现 <img src="/www6vHomeHexo/2022/01/12/k8sDNS/k8sDNS.png" class title="k8s DNS"></li>
</ul>
<h2><span id="自定义域名解析">自定义域名解析</span><a href="#自定义域名解析" class="header-anchor">#</a></h2><ul>
<li>hosts: 存量代码域名调用</li>
<li>rewrite: 集群外服务调用</li>
<li>forward： 配置存根域</li>
</ul>
<img src="/www6vHomeHexo/2022/01/12/k8sDNS/customizeDNS.png" class title="自定义域名解析">

<h2><span id="coredns-部署架构">CoreDNS 部署架构</span><a href="#coredns-部署架构" class="header-anchor">#</a></h2><ul>
<li>kubesphere.local</li>
<li>prod.kubesphere.local</li>
<li>dev.kubesphere.local</li>
<li>qa.kubesphere.local</li>
<li>test.kubesphere.local</li>
</ul>
<img src="/www6vHomeHexo/2022/01/12/k8sDNS/coreDNS-arch.png" class title="CoreDNS部署架构">


<h2><span id="reference">reference:</span><a href="#reference" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV1HY4y1n7PV">【Meetup 分享】使用 KubeSphere &amp; CoreDNS 搭建云原生 DNS</a></li>
<li><a href="https://draveness.me/dns-coredns/">详解 DNS 与 CoreDNS 的实现原理</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes PaaS平台</title>
    <url>/www6vHomeHexo/2022/01/12/k8sPaaS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">OpenShift</th>
<th align="center">KubeSphere</th>
</tr>
</thead>
<tbody><tr>
<td align="center">网络</td>
<td align="center">ovs(openshift SDN), flannel, calico</td>
<td align="center">calico, flannel, QinCloud CNI, Porter</td>
</tr>
<tr>
<td align="center">存储</td>
<td align="center">Ceph,NFS</td>
<td align="center">NeonSAN， QingCloud-csi，Ceph-csi</td>
</tr>
<tr>
<td align="center">CI&#x2F;CD</td>
<td align="center">OCP Pipeline&#x2F;Tekton<br>s2i</td>
<td align="center">Pipeline, Jenkins, sonarqube<br>s2i</td>
</tr>
<tr>
<td align="center">微服务</td>
<td align="center">istio</td>
<td align="center">istio</td>
</tr>
<tr>
<td align="center">ingress</td>
<td align="center">HA Proxy</td>
<td align="center">openELB</td>
</tr>
<tr>
<td align="center">infa(多云)</td>
<td align="center">kvm,openstack,aws,GCP,azure<br> marketplace</td>
<td align="center">OpenPitrix(aws, aliyun, openstack)</td>
</tr>
<tr>
<td align="center">Service Catelog</td>
<td align="center">operatorhub</td>
<td align="center">无</td>
</tr>
<tr>
<td align="center">serverless</td>
<td align="center">knative</td>
<td align="center">openfunction(基于knative)</td>
</tr>
<tr>
<td align="center">多集群</td>
<td align="center"></td>
<td align="center">solo、 kubefed(二开)</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>青云官网</p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>HAProxy抓包</title>
    <url>/www6vHomeHexo/2022/01/11/haproxyTcpdump/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>


<h2><span id="环境">环境</span><a href="#环境" class="header-anchor">#</a></h2><p>haproxy的机器 -  work-arch-renault-redis-6，10.100.140.234<br>codis机器  -  st-arch-renault-redis-1，172.16.22.139 </p>
<h2><span id="一-在codis-proxy机器上抓包">一. 在codis-proxy机器上抓包</span><a href="#一-在codis-proxy机器上抓包" class="header-anchor">#</a></h2><p>在codis-proxy机器上抓包（codis-proxy端口 19000）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tcpdump -n tcp port 19000 -w eth1.cap</span><br></pre></td></tr></table></figure>

<h5><span id="haproxy-流向-codis-proxy的数据">haproxy 流向 codis-proxy的数据</span><a href="#haproxy-流向-codis-proxy的数据" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/11/haproxyTcpdump/h1.jpg" class title="haproxy流向codis-proxy的数据">

<h5><span id="jedis连到haproxy写入数据转发到codis">jedis连到haproxy，写入数据转发到codis</span><a href="#jedis连到haproxy写入数据转发到codis" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/11/haproxyTcpdump/h2.png" class title="jedis连到haproxy，写入数据转发到codis">


<h5><span id="resp协议-看到的setampget长连接">RESP协议 看到的set&amp;get(长连接)</span><a href="#resp协议-看到的setampget长连接" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/11/haproxyTcpdump/h3.png" class title="RESP协议 看到的set&amp;get(长连接)">


<h2><span id="二-在haproxy的机器上抓包">二.  在haproxy的机器上抓包</span><a href="#二-在haproxy的机器上抓包" class="header-anchor">#</a></h2><p>在haproxy的机器上抓包（haproxy 11111端口）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tcpdump -n tcp port 11111 -w eth1.cap</span><br></pre></td></tr></table></figure>

<h5><span id="得到客户端ip">得到客户端ip</span><a href="#得到客户端ip" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/11/haproxyTcpdump/h5.png" class title="得到客户端ip">


<h5><span id="实际的客户端ip和抓包分析的一致">实际的客户端IP，和抓包分析的一致</span><a href="#实际的客户端ip和抓包分析的一致" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/11/haproxyTcpdump/h6.png" class title="实际的客户端IP，和抓包分析的一致">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://zhuanlan.zhihu.com/p/417275080">https://zhuanlan.zhihu.com/p/417275080</a><br><a href="https://blog.csdn.net/weixin_40748006/article/details/106429957">https://blog.csdn.net/weixin_40748006/article/details/106429957</a></p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>tcpdump</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP5-云上信息安全</title>
    <url>/www6vHomeHexo/2022/01/11/tencentTCP5/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="五-云上信息安全10">五、云上信息安全(10%)</span><a href="#五-云上信息安全10" class="header-anchor">#</a></h2><h3><span id="1-云安全体系与标准">1. 云安全体系与标准</span><a href="#1-云安全体系与标准" class="header-anchor">#</a></h3><h5><span id="11-云上信息安全相关概念">1.1 云上信息安全相关概念</span><a href="#11-云上信息安全相关概念" class="header-anchor">#</a></h5><p>加密和证书	☆</p>
<h5><span id="12-云上信息安全面临的问题">1.2 云上信息安全面临的问题</span><a href="#12-云上信息安全面临的问题" class="header-anchor">#</a></h5><p>云安全的痛点与措施	☆☆</p>
<h5><span id="13-云安全的体系与标准">1.3 云安全的体系与标准</span><a href="#13-云安全的体系与标准" class="header-anchor">#</a></h5><p>“包括：CSA STAR、ISO 27001、可信云服务认证、等级保护标准2.0和安全责任共担模型”	☆</p>
<h5><span id="14-腾讯云安全体系">1.4 腾讯云安全体系</span><a href="#14-腾讯云安全体系" class="header-anchor">#</a></h5><p>腾讯云安全体系	☆☆</p>
<h3><span id="2-在腾讯云上保障数据安全">2. 在腾讯云上保障数据安全</span><a href="#2-在腾讯云上保障数据安全" class="header-anchor">#</a></h3><h5><span id="21-数据安全需要遵从的基本原则">2.1 数据安全需要遵从的基本原则</span><a href="#21-数据安全需要遵从的基本原则" class="header-anchor">#</a></h5><p>数据保密性、完整性和可用性	☆☆</p>
<ul>
<li>数据保密性、完整性和可用性<br>[pic]</li>
</ul>
<h5><span id="22-数据安全的责任划分">2.2 数据安全的责任划分</span><a href="#22-数据安全的责任划分" class="header-anchor">#</a></h5><p>腾讯云的数据保护原则	☆☆<br>数据生命周期	☆☆</p>
<ul>
<li><p>数据安全责任公担</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/responsibility.jpg" class> 
</li>
<li><p>腾讯云的数据保护原则</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/principle.jpg" class>
</li>
<li><p>数据生命周期</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/lifecycle.jpg" class></li>
</ul>
<h5><span id="23-用腾讯云产品解决数据安全问题">2.3 用腾讯云产品解决数据安全问题</span><a href="#23-用腾讯云产品解决数据安全问题" class="header-anchor">#</a></h5><p>用腾讯云产品解决数据安全问题	☆☆☆</p>
<ul>
<li>用腾讯云产品解决数据安全问题<ul>
<li>数盾<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/solution.jpg" class title="数盾"></li>
</ul>
</li>
</ul>
<h5><span id="24-敏感数据发现">2.4 敏感数据发现</span><a href="#24-敏感数据发现" class="header-anchor">#</a></h5><p>敏感数据处理CDSM	☆☆<br>使用敏感数据处理实现敏感数据发现	☆☆☆</p>
<ul>
<li>敏感数据处理CDSM</li>
<li>使用敏感数据处理实现敏感数据发现<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/discover.jpg" class></li>
</ul>
<h5><span id="25-数据分类分级">2.5 数据分类分级</span><a href="#25-数据分类分级" class="header-anchor">#</a></h5><p>数据安全治理中心DSGC	☆☆<br>使用数据安全治理中心实现数据分类分级	☆☆☆</p>
<ul>
<li>数据安全治理中心DSGC</li>
<li>使用数据安全治理中心实现数据分类分级<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/classify.jpg" class></li>
</ul>
<h5><span id="26-数据存储安全">2.6 数据存储安全</span><a href="#26-数据存储安全" class="header-anchor">#</a></h5><p>数据存储安全面临的问题	☆<br>制定备份策略	☆☆☆<br>数据存储加密	☆<br>使用腾讯云产品实现数据加密	☆☆☆</p>
<ul>
<li><p>数据存储安全面临的问题</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/store-problem.jpg" class>
</li>
<li><p>制定备份策略</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/store-backup.jpg" class>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/store-backup1.jpg" class>
</li>
<li><p>数据存储加密</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/store-decode.jpg" class>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/store-decode1.jpg" class>
</li>
<li><p>使用腾讯云产品实现数据加密</p>
<ul>
<li>KMS使用HSM<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/decode-products.jpg" class></li>
</ul>
</li>
</ul>
<h5><span id="27-数据传输安全">2.7 数据传输安全</span><a href="#27-数据传输安全" class="header-anchor">#</a></h5><p>数据传输的风险	☆<br>网络层和应用层可信传输	☆☆☆</p>
<ul>
<li><p>数据传输的风险</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/transfer-risk.jpg" class>
</li>
<li><p>网络层和应用层可信传输</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/network-layer-transfer.jpg" class title="网络层可信传输">
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/app-layer-transfer.jpg" class title="应用层可信传输"></li>
</ul>
<h5><span id="28-数据访问安全">2.8 数据访问安全</span><a href="#28-数据访问安全" class="header-anchor">#</a></h5><p>数据访问安全概述	☆☆<br>用户&#x2F;业务鉴权和安全审计	☆<br>腾讯云审计工具	☆☆</p>
<ul>
<li><p>数据访问安全概述</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/access-security.jpg" class>
</li>
<li><p>用户&#x2F;业务鉴权和安全审计</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/authorization.jpg" class title="用户&#x2F;业务鉴权">
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/audit.jpg" class title="安全审计">
</li>
<li><p>腾讯云审计工具 </p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/audit-cloud.jpg" class></li>
</ul>
<h5><span id="29-数据使用安全">2.9 数据使用安全</span><a href="#29-数据使用安全" class="header-anchor">#</a></h5><p>敏感数据脱敏	☆☆<br>泄露溯源水印	☆☆</p>
<ul>
<li><p>敏感数据脱敏</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/seceret-sentive.jpg" class>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/seceret-sentive1.jpg" class>
</li>
<li><p>泄露溯源水印</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/watermark.jpg" class></li>
</ul>
<h5><span id="210-数据销毁">2.10 数据销毁</span><a href="#210-数据销毁" class="header-anchor">#</a></h5><p>云上数据销毁的流程和方法	☆☆</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/destroy.jpg" class>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP5/2/destroy1.jpg" class>

<h3><span id="3-在腾讯云上进行访问控制管理">3. 在腾讯云上进行访问控制管理</span><a href="#3-在腾讯云上进行访问控制管理" class="header-anchor">#</a></h3><h5><span id="31-在腾讯云上实现访问控制">3.1 在腾讯云上实现访问控制</span><a href="#31-在腾讯云上实现访问控制" class="header-anchor">#</a></h5><p>账号权限管理	☆☆<br>腾讯云账号体系、在腾讯云中使用多主账号和子账号	☆☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP4-业务流量高峰处理架构设计</title>
    <url>/www6vHomeHexo/2022/01/11/tencentTCP4/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="四-业务流量高峰处理架构设计15">四、业务流量高峰处理架构设计(15%)</span><a href="#四-业务流量高峰处理架构设计15" class="header-anchor">#</a></h2><h3><span id="1-业务高峰流量处理概述">1. 业务高峰流量处理概述</span><a href="#1-业务高峰流量处理概述" class="header-anchor">#</a></h3><h5><span id="11-业务流量高峰处理相关概念">1.1 业务流量高峰处理相关概念</span><a href="#11-业务流量高峰处理相关概念" class="header-anchor">#</a></h5><p>消息队列、缓存数据库	☆</p>
<h5><span id="12-高峰流量可能带来的问题">1.2 高峰流量可能带来的问题</span><a href="#12-高峰流量可能带来的问题" class="header-anchor">#</a></h5><p>访问延迟、性能下降、安全威胁等等	☆☆</p>
<ul>
<li>服务能力下降, 访问延迟,  用户体验差</li>
<li>整体系统性能下降</li>
<li>服务宕机，系统雪崩，导致服务不可用</li>
<li>异常流量攻击, 安全受威胁</li>
</ul>
<h5><span id="13-高峰流量的应对思路">1.3 高峰流量的应对思路</span><a href="#13-高峰流量的应对思路" class="header-anchor">#</a></h5><p>从8个维度进行规划	☆☆</p>
<ul>
<li>从8个维度进行规划<img src="/www6vHomeHexo/2022/01/11/tencentTCP4/solution.png" class title="高峰流量的应对思路"></li>
</ul>
<h5><span id="14-应对高峰流量的架构">1.4 应对高峰流量的架构</span><a href="#14-应对高峰流量的架构" class="header-anchor">#</a></h5><p>应对高峰流量的分层架构，以及每一层所需要使用到的云产品	☆☆☆</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP4/layer.png" class title="应对高峰流量的分层架构">
<img src="/www6vHomeHexo/2022/01/11/tencentTCP4/product.png" class title="每一层所需要使用到的云产品">

<h3><span id="2-用户及应用接入高峰处理">2. 用户及应用接入高峰处理</span><a href="#2-用户及应用接入高峰处理" class="header-anchor">#</a></h3><h5><span id="21-用户及应用高并发接入面临的问题">2.1 用户及应用高并发接入面临的问题</span><a href="#21-用户及应用高并发接入面临的问题" class="header-anchor">#</a></h5><p>用户及应用层压力来源：目标服务器和接入链路	☆</p>
<ul>
<li>用户及应用层压力来源：目标服务器和接入链路</li>
<li>压力分为: 目标服务器 + 接入链路 两个层级<ul>
<li>目标服务器:<ul>
<li>对tps&#x2F;qps&#x2F;最大并发连接数&#x2F;吞吐量的挑战</li>
<li>服务器节点单点故障</li>
</ul>
</li>
<li>接入链路<ul>
<li>链路规划: 局部最优解 不等于 全局最优解, 用户之间争抢优势链路资源(造成全局压力过高)</li>
<li>重复请求: 不同用户请求的内容大量重复, 多余的传输成本</li>
<li>跨运营商网络接入（导致更高的用户访问延迟）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="22-用户及应用接入高并发应对思路">2.2 用户及应用接入高并发应对思路</span><a href="#22-用户及应用接入高并发应对思路" class="header-anchor">#</a></h5><p>目标服务器和接入链路的优化措施	☆☆</p>
<ul>
<li><p>目标服务器</p>
<ul>
<li>集群+负载均衡</li>
<li>将业务进行拆分(在集群内多个节点部署)</li>
</ul>
</li>
<li><p>接入链路的优化</p>
<ul>
<li>综合考虑距离， 流量负载、网络质量，实现全网级别的链路选择优化(尽可能避免用户较高的访问延时)</li>
<li>读写分离， 动静分离<ul>
<li>静态&#x2F;只读内容由缓存服务器提供</li>
<li>多使用静态页面, 减少对数据库的访问</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="23-设计用户及应用接入高并发架构-要重新看">2.3 设计用户及应用接入高并发架构 [要重新看]</span><a href="#23-设计用户及应用接入高并发架构-要重新看" class="header-anchor">#</a></h5><p>涉及产品包括：CDN、云解析、HttpDNS、负载均衡	☆☆☆<br>CDN-GLSB全局调度系统、CDN缓存处理架构	☆☆<br>多地域流量分流架构	☆☆☆<br>通过CLB横向扩展分担高峰流量	☆☆☆<br>最佳实践	☆☆☆</p>
<ul>
<li><p>CDN-GLSB全局调度系统 [pic]</p>
<ul>
<li>最优接入</li>
<li>最优回源</li>
<li>动态加速</li>
</ul>
</li>
<li><p>CDN缓存处理架构 [pic 3个模块]</p>
<ul>
<li>CDN全局调度系统</li>
<li>CDN加速节点集群 </li>
<li>COS托管源</li>
</ul>
</li>
<li><p>CDN加速的场景 [pic]</p>
<ul>
<li>静态加速</li>
<li>动态加速<br>DSA</li>
<li>海外加速</li>
<li>安全加速</li>
</ul>
</li>
<li><p>云解析、HttpDNS [pic]</p>
<ul>
<li>云解析</li>
<li>HttpDNS<br>移动互联网中的域名解析的异常问题</li>
</ul>
</li>
<li><p>多地域流量分流架构 [pic]</p>
<ul>
<li>DNS + 负载均衡</li>
</ul>
</li>
<li><p>通过CLB横向扩展分担高峰流量</p>
<ul>
<li>调度算法<ul>
<li>加权轮询算法<br>看权重</li>
<li>加权最小连接数算法<br>权重相同的时候，看连接数</li>
<li>源地址散列调度算法<br>ip_hash</li>
</ul>
</li>
<li>高峰流量分担和消除单点</li>
<li>横向扩展分担高峰流量<br>CVM + 弹性伸缩</li>
</ul>
</li>
<li><p>最佳实践</p>
<img src="/www6vHomeHexo/2022/01/11/tencentTCP4/best-practice.png" class title="最佳实践"></li>
</ul>
<h3><span id="3-web及服务层高峰处理">3. web及服务层高峰处理</span><a href="#3-web及服务层高峰处理" class="header-anchor">#</a></h3><h5><span id="31-web及服务层流量高峰面临的问题">3.1 web及服务层流量高峰面临的问题</span><a href="#31-web及服务层流量高峰面临的问题" class="header-anchor">#</a></h5><p>web及服务层压力来源：资源瓶颈和资源分配	☆</p>
<h5><span id="32-web及服务层流量高峰的应对思路">3.2 web及服务层流量高峰的应对思路</span><a href="#32-web及服务层流量高峰的应对思路" class="header-anchor">#</a></h5><p>web及服务层流量高峰的应对思路	☆☆</p>
<h5><span id="33-设计web及服务层高并发架构">3.3 设计web及服务层高并发架构</span><a href="#33-设计web及服务层高并发架构" class="header-anchor">#</a></h5><p>Web应用的高峰流量处理架构	☆☆<br>高性能计算集群流量处理架构	☆☆<br>请求类服务器高峰流量处理架构	☆☆<br>弹性伸缩构建高并发架构的最佳实践	☆☆☆<br>实现分层解耦收益	☆☆☆</p>
<h3><span id="4-数据缓存架构设计">4. 数据缓存架构设计</span><a href="#4-数据缓存架构设计" class="header-anchor">#</a></h3><h5><span id="41-数据缓存的应用架构和处理逻辑">4.1 数据缓存的应用架构和处理逻辑</span><a href="#41-数据缓存的应用架构和处理逻辑" class="header-anchor">#</a></h5><p>数据缓存的应用架构和处理逻辑	☆☆☆</p>
<h5><span id="42-缓存雪崩">4.2 缓存雪崩</span><a href="#42-缓存雪崩" class="header-anchor">#</a></h5><p>缓存雪崩的概念；如何预防缓存雪崩	☆☆</p>
<h5><span id="43-设计数据缓存架构">4.3 设计数据缓存架构</span><a href="#43-设计数据缓存架构" class="header-anchor">#</a></h5><p>Redis概念和功能原理	☆☆<br>使用Redis设计不同场景下的高并发架构	☆☆</p>
<h3><span id="5-异步消息队列架构设计">5. 异步消息队列架构设计</span><a href="#5-异步消息队列架构设计" class="header-anchor">#</a></h3><h5><span id="51-异步消息队列架构设计原则">5.1 异步消息队列架构设计原则</span><a href="#51-异步消息队列架构设计原则" class="header-anchor">#</a></h5><p>消息队列的概念、特点、功能	☆☆<br>“消息中间件的通讯过程、高峰流量异步通信过程、流量削峰处理过程和异步通知应对高峰流量”	☆☆</p>
<h5><span id="52-设计异步消息队列架构">5.2 设计异步消息队列架构</span><a href="#52-设计异步消息队列架构" class="header-anchor">#</a></h5><p>腾讯云的消息中间件服务	☆☆<br>“基于CMQ消息队列对系统解耦应对高峰流量、基于CMQ的异步消息队列架构设计”	☆☆☆<br>“Ckafka应对高并发大数据处理场景架构、Ckafka日志聚合场景架构”	☆☆</p>
<h3><span id="6-数据层高峰流量处理">6. 数据层高峰流量处理</span><a href="#6-数据层高峰流量处理" class="header-anchor">#</a></h3><h5><span id="61-传统架构面临的问题">6.1 传统架构面临的问题</span><a href="#61-传统架构面临的问题" class="header-anchor">#</a></h5><p>传统数据库OLAP+OLTP+分库分表方案面临的问题	☆</p>
<h5><span id="62-数据层高峰流量处理设计思路">6.2 数据层高峰流量处理设计思路</span><a href="#62-数据层高峰流量处理设计思路" class="header-anchor">#</a></h5><p>“分库分表、主备同步+读写分离、动态静态数据分离、数据库分布式架构”	☆☆☆</p>
<h5><span id="63-设计数据层的高峰流量处理架构">6.3 设计数据层的高峰流量处理架构</span><a href="#63-设计数据层的高峰流量处理架构" class="header-anchor">#</a></h5><p>基于COS的数据层架构	☆☆☆</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch 数据模型Model</title>
    <url>/www6vHomeHexo/2022/01/08/elasticsearchModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="数据模型处理关联关系">数据模型(处理关联关系)</span><a href="#数据模型处理关联关系" class="header-anchor">#</a></h2><ul>
<li>ES考虑Denormalize数据<ul>
<li>对象关系(Object)</li>
<li>嵌套关系(Nested Object)</li>
<li>父子关联关系(Parent&#x2F;Child)</li>
<li>应用端关联</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>49丨对象及Nested对象</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph 总结</title>
    <url>/www6vHomeHexo/2022/01/08/ceph/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#ceph%E6%9E%B6%E6%9E%84%E5%9B%BE">Ceph架构图</a><br>- <a href="#%E6%9E%B6%E6%9E%842">架构[2]</a><br>- <a href="#ceph%E4%B8%BB%E8%A6%81%E6%9C%89%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E8%BF%9B%E7%A8%8B25">Ceph主要有三个基本进程[2][5]</a></li>
<li><a href="#ceph%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%8F%8A%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D26">Ceph核心组件及概念介绍[2][6]</a></li>
<li><a href="#ceph%E6%94%AF%E6%8C%81%E7%9A%84%E5%AD%98%E5%82%A8%E6%8E%A5%E5%8F%A37">ceph支持的存储接口[7]</a></li>
<li><a href="#%E9%83%A8%E7%BD%B2-%E5%B7%A5%E5%85%B7">部署 &amp; 工具</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF7">场景[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="ceph架构图">Ceph架构图</span><a href="#ceph架构图" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/08/ceph/ceph-arch.png" class title="ceph架构图">

<img src="/www6vHomeHexo/2022/01/08/ceph/ceph-arch1.png" class title="ceph架构图">
<h5><span id="架构2">架构[2]</span><a href="#架构2" class="header-anchor">#</a></h5><p>基础存储系统</p>
<ul>
<li>rados:基础存现系统RADOS(Reliable,Autonomic,Distribuuted object store，既可靠的、自动化的、分布式的对象存储).所有存储在Ceph系统中的用户数据事实上最终都是由这一层来存储的。Ceph的高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。</li>
<li>RADOS -<br>  RADOS全称Reliable Autonomic Distrubuted object Store,是Ceph集群的精华，用户实现数据分配，Failover等集群操作</li>
</ul>
<p>基础库librados:</p>
<ul>
<li>librodos:这一层的功能是对RADOS进行抽象和封装，并向上层提供API，以便直接基于RADOS进行应用开发。特别要注意的是，RADOS是一个对象存储系统，因此，librodos实现的API也只是针对对象存储功能的。</li>
<li>Libradio -<br>  Librados是RODOS提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。</li>
</ul>
<p>高层应用接口</p>
<ul>
<li>radosgw:对象网关接口</li>
<li>rbd:块存储</li>
<li>cephfs：文件系统存储，其作用是在librodos库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。</li>
<li>RBD -<br>  RBD全称RADOS Block Device，是ceph对外提供服务的块设备服务。</li>
<li>RGW -<br>  RGW全称RADOS gateway,是ceph对外提供的对象存储服务，接口与S3和Swift兼容</li>
<li>CephFS -<br>  CephFS全称Ceph File System，是ceph对外提供的文件系统服务。</li>
</ul>
<h5><span id="ceph主要有三个基本进程25">Ceph主要有三个基本进程[2][5]</span><a href="#ceph主要有三个基本进程25" class="header-anchor">#</a></h5><ul>
<li><p>osd:<br>  用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再负载。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。<br>  当Ceph存储集群设定数据有两个副本时，则至少需要两个OSD守护进程即OSD节点，集群才能达到active+clean状态。</p>
</li>
<li><p>MDS(可选)<br>  为Ceph文件系统提供元数据计算、缓存与同步(也就是说，ceph块设备和ceph对象存储不使用MDS)。在ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。MDS进程并不是必须的进程，只有需要使用CEPHFS时，才需要配置MDS节点。</p>
</li>
<li><p>Monitor<br>  监控整个集群的状态，维护集群的cluster MA二进制表，保证集群数据的一致性。ClusterMAP描述了对象存储的物理位置，以及一个将设备聚合到物理位置的桶列表。</p>
</li>
<li><p>Manager(ceph-mgr)<br>  用于收集ceph集群状态，运行指标，比如存储利用率、当前性能指标和系统负载。对外提供ceph dashboard(ceph-ui)和resetful api,manger组件开启高可用时，至少2个</p>
</li>
<li><p>MDS -<br>  MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务    </p>
</li>
<li><p>Monitor -<br>  监控整个集群的状态，维护集群的cluster MA二进制表，保证集群数据的一致性</p>
</li>
<li><p>OSD -<br>  OSD全程Object storage Device,也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD</p>
</li>
</ul>
<hr>
<ul>
<li>rbd: 不需要部署独立的守护进程</li>
<li>Cephfs  需要部署独立的守护进程 MDS</li>
<li>对象存储 需要部署独立的守护进程  radosgw</li>
</ul>
<hr>
<p>生产环境</p>
<ul>
<li>Monitor 需要至少3个</li>
<li>Manager 需要至少2个</li>
</ul>
<h2><span id="ceph核心组件及概念介绍26">Ceph核心组件及概念介绍[2][6]</span><a href="#ceph核心组件及概念介绍26" class="header-anchor">#</a></h2><ul>
<li>Object<br>  Ceph最底层的存储单元是Object对象，每个object包含数据和原始数据。<br>  [自带元数据]<br>  [id + binary data + metadate(key+value)]</li>
<li>PG<br>  PG全称Placement Grouops,是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。</li>
<li>CRUSH<br>  CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。</li>
<li>Ceph纠删码<br> [数据保护, 数据恢复]</li>
<li>客户端的数据条带化<br> 存储内容进行顺序分片, 分布式存储每个分片</li>
</ul>
<h2><span id="ceph支持的存储接口7">ceph支持的存储接口[7]</span><a href="#ceph支持的存储接口7" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/08/ceph/ceph-interface.png" class title="ceph支持的存储接口">


<h2><span id="部署-amp-工具">部署 &amp; 工具</span><a href="#部署-amp-工具" class="header-anchor">#</a></h2><ul>
<li>ceph ansible<br>主流</li>
<li>rook operator<ul>
<li><strong>超融合模式</strong><br>混合部署 资源预留</li>
<li><strong>存算分离模式</strong><br>ceph独立部署, 打label</li>
</ul>
</li>
<li>工具<br>RedHat OCS sizing tool</li>
</ul>
<h2><span id="场景7">场景[7]</span><a href="#场景7" class="header-anchor">#</a></h2><ul>
<li>openstack<br>静态化</li>
<li>云原生 rook<ul>
<li><strong>动态化</strong><br>动态扩缩容</li>
<li><strong>支持混合云，多云场景</strong><br>通过CSI接口，提供混合云环境下的一致性</li>
<li>拓扑感知<br>副本分到到3个rack中<br>跨AZ的多副本</li>
<li>multus管理网络， 提升性能[pic]<br>管理网 - ovs<br>ceph private network - vlan10<br>ceph public network - vlan20</li>
</ul>
</li>
<li>占比<br>Ceph RBD - 48%<br>LVM - 15%<br>NFS - 8%</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/0voice/kernel_awsome_feature/blob/main/%E3%80%8C%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E3%80%8DCEPH%20%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D.md">「基础理论」CEPH 基础介绍</a> good</li>
<li><a href="https://github.com/0voice/kernel_awsome_feature/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8Ceph.md">分布式存储Ceph</a></li>
<li><a href="https://github.com/0voice/kernel_awsome_feature/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9Fceph%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86.md">分布式文件系统ceph知识整理</a></li>
<li><a href="https://github.com/0voice/kernel_awsome_feature/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%20Ceph%20%E7%9A%84%E6%BC%94%E8%BF%9B%E7%BB%8F%E9%AA%8C%20%C2%B7%20SOSP%20'19.md">分布式存储 Ceph 的演进经验 · SOSP ‘19</a></li>
<li><a href="https://www.bilibili.com/video/BV17p4y1a7Em?p=4&vd_source=f6e8c1128f9f264c5ab8d9411a644036">马哥教育2021-Ceph分布式存储系统快速入门</a></li>
<li>《Ceph企业级分布式存储》</li>
<li><a href="https://www.bilibili.com/video/BV1D3411873Z?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">【直播回放】ROOK 云原生分布式存储开源项目的介绍及其在企业中的应用未来</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>存储</category>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>LSM-Tree Compaction压缩策略</title>
    <url>/www6vHomeHexo/2022/01/08/lsmTreeCompaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="rum猜想-三选二-5">RUM猜想-三选二 [5]</span><a href="#rum猜想-三选二-5" class="header-anchor">#</a></h2><p>  读取（Read）、更新（Update）和内存（Memory）<br>  <img src="/www6vHomeHexo/2022/01/08/lsmTreeCompaction/RUM.jpg" class width="720" height="576" title="RUM猜想"></p>
<h2><span id="lsm-tree-面临的三个问题">LSM-Tree 面临的三个问题</span><a href="#lsm-tree-面临的三个问题" class="header-anchor">#</a></h2><ul>
<li>三个问题<ul>
<li>读放大 Read amplification ： 为了检索数据, 需要一层层的查找, 造成额外的磁盘IO操作</li>
<li>写放大 Write amplification： 在合并过程中 不断地重写为新的文件， 从而导致写放大</li>
<li>空间放大 Space amplification ：由于重复是允许的, 并且过期的数据不会被马上清除掉</li>
</ul>
</li>
</ul>
<h2><span id="评估和设计-compaction-策略的关键性能指标1">评估和设计 compaction 策略的关键性能指标[1]</span><a href="#评估和设计-compaction-策略的关键性能指标1" class="header-anchor">#</a></h2><ul>
<li>Compaction trigger：什么时候触发</li>
<li>Data layout：数据的物理存放方式是怎样的<ul>
<li>leveling</li>
<li>tiering</li>
</ul>
</li>
<li>Compaction granularity：每次移动多少数据量</li>
<li>Data movement policy：具体移动哪些data block</li>
</ul>
<img src="/www6vHomeHexo/2022/01/08/lsmTreeCompaction/compaction-summary.png" class title="LSM-Tree Compaction Summary">

<h2><span id="10-种-compaction-策略">10 种 compaction 策略</span><a href="#10-种-compaction-策略" class="header-anchor">#</a></h2><p>[todo]</p>
<h2><span id="compaction-on-rocksdb">Compaction on Rocksdb</span><a href="#compaction-on-rocksdb" class="header-anchor">#</a></h2><p>[level0<br>level0+]</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV12U4y177g3">Research_R42.5 Constructing and Analyzing the LSM Compaction Design Space</a>  论文 VLDB2021 bili<br><a href="https://loopjump.com/pr-lsm-compaction-design-space/">构建分析LSM Compaction Design Space</a> ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/413463723">VLDB 2021论文概述</a>   Overview</li>
<li><a href="https://github.com/facebook/rocksdb/wiki/Compaction">rocksdb Compaction</a>  ***</li>
<li>《22｜RUM猜想：想要读写快还是存储省？又是三选二》 王磊 未</li>
<li><a href="https://zhuanlan.zhihu.com/p/404352955">Paper read: Designing Access Methods: The RUM Conjecture</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>存储引擎</category>
      </categories>
      <tags>
        <tag>LSM-Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringCloud迁移到istio</title>
    <url>/www6vHomeHexo/2022/01/06/istioMigrateFromSpringCloud/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#java%E6%96%B9%E6%A1%88-mesh%E5%BC%80%E5%85%B33">Java方案-mesh开关[3]</a><br>- <a href="#java-agent-mesh%E5%BC%80%E5%85%B3">Java Agent + mesh开关</a></li>
<li><a href="#%E9%9D%9Ejava%E6%96%B9%E6%A1%88-%E5%9F%BA%E4%BA%8Eproxy1">非Java方案-基于Proxy[1]</a><br>- <a href="#%E6%B5%81%E9%87%8F%E4%BA%92%E9%80%9A%E6%96%B9%E6%A1%88">流量互通方案</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="java方案-mesh开关3">Java方案-mesh开关[3]</span><a href="#java方案-mesh开关3" class="header-anchor">#</a></h2><h5><span id="java-agent-mesh开关">Java Agent + mesh开关</span><a href="#java-agent-mesh开关" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/06/istioMigrateFromSpringCloud/mesh-switcher.JPG" class title="mesh-switcher">

<ul>
<li>pod不需要重新注入sidecar,   通过 mesh-switcher开关来修改iptables，达到服务直连或者mesh的效果。</li>
</ul>
<h2><span id="非java方案-基于proxy1">非Java方案-基于Proxy[1]</span><a href="#非java方案-基于proxy1" class="header-anchor">#</a></h2><h5><span id="流量互通方案">流量互通方案</span><a href="#流量互通方案" class="header-anchor">#</a></h5><p><strong>当目标服务不在 mesh 内</strong>，Discovery 服务返回的地址是服务的 HAProxy 的地址（也就是将流量代理到 consul）。<br><strong>当目标服务再 mesh 内</strong>，Discovery 服务将返回服务的 ServiceIP，此时应用将通过 Sidecar 触发 ServiceMesh 的路由能力,将请求直接传递到对端 Sidecar。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/436796453">istio 在知乎大规模集群的落地实践</a>  ***<br>迁移-回滚 限流 平台化，优化-性能，坑</li>
<li><a href="https://support.huaweicloud.com/bestpractice-cce/istio_bestpractice_3012.html">SpringCloud微服务Istio迁移指导</a> *</li>
<li><a href="https://www.bilibili.com/video/BV17y4y1e7Pt">百度服务网格在金融行业的大规模落地实践（孙召昌，百度）</a> ***<br> <a href="https://www.aliyundrive.com/s/dXxngxjTkZE">相关ppt</a></li>
<li><a href="https://www.bilibili.com/video/BV1V64y1r7oU?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">基于OpenShift Service Mesh 实现微服务网格化 林斯锐 中国DevOps社区</a></li>
<li><a href="https://time.geekbang.org/course/detail/100053601-274751">51 | Spring Cloud、K8s和Istio该如何集成？</a>  有个图</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes OpenShift</title>
    <url>/www6vHomeHexo/2022/01/05/k8sOpenShift/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/openshift.png" class>


<h2><span id="interface">Interface</span><a href="#interface" class="header-anchor">#</a></h2><ul>
<li>CNI<br>OpenFlow(OVS)(使用的， 更通用，场景适合的更多)<br>Calico</li>
<li>CRI<ul>
<li>OpenShift3.0 用的<br>  Docker（有守护进程，守护进程宕掉后，运行时会有问题）<br>  Red Hat Enterprise Linux - 4G大小</li>
<li>OpenShift4.0 用的<br>  CRI-O（没有守护进程）<br>  Ret Hat CoreOS - 不到800M大小</li>
</ul>
</li>
<li>CSI<br>CEPH(ROOK)<br>NFS</li>
<li>入口 ingress<ul>
<li>openshift 默认是haproxy的ingress（首选建议使用， dns解析， mTLS）</li>
<li>也可以装nginx ingress</li>
<li>也可以装istio ingress gateway（4层选， istio ingress gateway）</li>
</ul>
</li>
</ul>
<h2><span id="paas">PaaS</span><a href="#paas" class="header-anchor">#</a></h2><p> 角色 - RBAC<br> 配置管理 - LimitRanage， Quota<br> 联邦集群 - kubefed </p>
<h2><span id="对k8s的增强">对K8S的增强</span><a href="#对k8s的增强" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/enhancement.png" class>

<ul>
<li>Operator</li>
<li>Multus多个虚拟网卡（用的多）<br>一个pod有多个虚拟网卡， ovs管理网络 + macvlan网络（性能比较好） </li>
<li>kube-virt 虚拟机管理</li>
</ul>
<img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/openshift-vs-k8s1.png" class>

<img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/openshift-vs-k8s2.png" class>


<h2><span id="应用迁移">应用迁移</span><a href="#应用迁移" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/appMigrate.png" class>

<ul>
<li>应用改造的难度比较大</li>
</ul>
<h2><span id="多云">多云</span><a href="#多云" class="header-anchor">#</a></h2><h5><span id="基于多云的cix2fcd">基于多云的CI&#x2F;CD</span><a href="#基于多云的cix2fcd" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2022/01/05/k8sOpenShift/cicd-multiCluster.png" class>
<ul>
<li>基于jenkins<br>容易编写，耗内存</li>
<li>基于Tekton<br>不容易编写， 不耗内存</li>
</ul>
<h5><span id="应用发布到多个集群">应用发布到多个集群</span><a href="#应用发布到多个集群" class="header-anchor">#</a></h5><p>  RHACM 本来是IBM的，后来给了RedHat<br>  RHACM基于CNCF的Open Cluster Management孵化项目 </p>
<h2><span id="竞品分析">竞品分析</span><a href="#竞品分析" class="header-anchor">#</a></h2><ul>
<li>占有率30%， 早2018年，<br> 生态做的好（开源项目operator， marketplace， istio， serverlesss）<br> IBM AI，中间件相关的东西挪到OpenShift</li>
</ul>
<h2><span id="市场">市场</span><a href="#市场" class="header-anchor">#</a></h2><p>银行， 企业级客户<br><a href="https://quay.io/repository/openshift/okd">社区版本 OKD</a> </p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV19p4y1k7yA?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">基于 Red Hat OpenShift 4 构建 Paas、DevOps 平台</a> bilibili<br><a href="https://www.bilibili.com/video/BV1FL4y1c767?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">15分钟搞定 OpenShift 多云管理，并统一部署应用</a><br><a href="https://www.bilibili.com/video/BV1nZ4y1b71F?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">用 Kubernetes Operator 管理 OpenShift 应用生命周期</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-混合云HybridCloud</title>
    <url>/www6vHomeHexo/2022/01/04/aliyunHybridCloud/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%B7%B7%E5%90%88%E4%BA%91">混合云</a></li>
<li><a href="#%E5%88%86%E7%B1%BB1">分类[1]</a><br>- <a href="#%E6%8C%89%E5%BD%A2%E6%80%81%E5%88%86%E7%B1%BB">按形态分类</a><br>- <a href="#%E6%8C%89%E6%8A%80%E6%9C%AF%E5%B9%B3%E5%8F%B0%E5%88%86%E7%B1%BB">按技术平台分类</a><br>- <a href="#%E6%8C%89%E6%9E%B6%E6%9E%84%E5%B1%82%E5%88%86%E7%B1%BB">按架构层分类</a><br>- <a href="#%E6%8C%89%E8%BF%90%E7%BB%B4%E4%B8%BB%E4%BD%93%E5%88%86%E7%B1%BB">按运维主体分类</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF">场景</a></li>
<li><a href="#%E9%98%BF%E9%87%8C%E4%BA%91%E6%B7%B7%E5%90%88%E4%BA%91%E6%95%B4%E4%BD%93%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%882">阿里云混合云整体解决方案[2]</a></li>
<li><a href="#%E6%B7%B7%E5%90%88%E4%BA%91%E4%BA%A7%E5%93%81%E9%98%BF%E9%87%8C">混合云产品(阿里)</a><br>- <a href="#%E7%BD%91%E7%BB%9C%E9%80%9A">网络通</a><br>- <a href="#%E6%95%B0%E6%8D%AE%E9%80%9A">数据通</a><br>- <a href="#%E4%B8%9A%E5%8A%A1%E9%80%9A">业务通</a></li>
<li><a href="#%E6%B7%B7%E5%90%88%E4%BA%91-vs-%E5%A4%9A%E4%BA%91">混合云 vs. 多云</a></li>
<li><a href="#%E9%98%BF%E9%87%8C%E4%BA%91%E6%B7%B7%E5%90%88%E4%BA%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%882">阿里云混合云解决方案[2]</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%9E%B6%E6%9E%841">解决方案架构[1]</a><br>- <a href="#%E5%BC%B9%E6%80%A7%E6%9E%B6%E6%9E%84">弹性架构</a><br>- <a href="#%E5%AE%B9%E7%81%BE%E5%A4%87%E4%BB%BD%E6%9E%B6%E6%9E%84">容灾备份架构</a><br>- <a href="#%E5%AE%89%E5%85%A8%E9%98%B2%E6%8A%A4%E6%9E%B6%E6%9E%84">安全防护架构</a><br>- <a href="#%E5%A4%9A%E4%BA%91%E7%AE%A1%E6%8E%A7%E6%9E%B6%E6%9E%84">多云管控架构</a><br>- <a href="#%E6%B7%B7%E5%90%88%E4%BA%91%E7%BB%84%E7%BD%91%E6%9E%B6%E6%9E%84">混合云组网架构</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="混合云">混合云</span><a href="#混合云" class="header-anchor">#</a></h2><p>定义: 多个云之间互联的IT架构</p>
<h2><span id="分类1">分类[1]</span><a href="#分类1" class="header-anchor">#</a></h2><h5><span id="按形态分类">按形态分类</span><a href="#按形态分类" class="header-anchor">#</a></h5><ul>
<li>公有云之间的混合<ul>
<li>同构公有云混合<br>阿里云上海 + 阿里云北京（专线） </li>
<li>异构公有云混合<br>阿里云上海 + 腾讯云新加坡(专线&#x2F;VNP)</li>
</ul>
</li>
<li>私有云之间的混合</li>
<li>公有云和私有云的混合(主流)<ul>
<li>优点：<br>  私有云的数据安全性 + 公有云的丰富产品</li>
<li>缺点：<br>  性能<br>  复杂性</li>
</ul>
</li>
<li>公有云和传统IT的混合</li>
</ul>
<h5><span id="按技术平台分类">按技术平台分类</span><a href="#按技术平台分类" class="header-anchor">#</a></h5><ul>
<li>同构混合云: 技术架构平台一致(首选)<br>最佳的混合云方案</li>
<li>异构混合云: 技术架构平台不同<ul>
<li>首选同构混合云</li>
<li>如果要异构<br> 兼容方案：<br>   在公有云上部署相同的PasS应用(VMware)<br>   企业容灾</li>
</ul>
</li>
</ul>
<h5><span id="按架构层分类">按架构层分类</span><a href="#按架构层分类" class="header-anchor">#</a></h5><ul>
<li>IaaS层混合<br>IaaS整合在同构混合云里相对容易<br>IaaS整合在异构混合云里相对困难 </li>
<li>PaaS层混合(未来趋势)<br>K8S混合， Redis混合</li>
</ul>
<h5><span id="按运维主体分类">按运维主体分类</span><a href="#按运维主体分类" class="header-anchor">#</a></h5><ul>
<li>专有云主营(主流)</li>
<li>公有云托管</li>
</ul>
<h2><span id="场景">场景</span><a href="#场景" class="header-anchor">#</a></h2><ul>
<li>业务呈峰谷交错， 需要利用云上资源， 进行弹性伸缩  [弹性]</li>
<li>面向互联网的业务在云上， 有更好的访问体验和网络资源， 云下部署企业核心系统或数据库</li>
<li>影音、照片、文件， 云上存储或备份 [大网盘] </li>
<li>多分支机构就近接入阿里云， 网络打通互访， 共享云上资源  [公司分支之间vpn互联]</li>
<li>云下、云上数据融合(物联网、智慧生活), 进行大数据分析  </li>
<li>第三方公有云平台与阿里云混合，能力互补， 联合运营</li>
<li>数据备份， 应用容灾  [容灾备份]</li>
</ul>
<h2><span id="阿里云混合云整体解决方案2">阿里云混合云整体解决方案[2]</span><a href="#阿里云混合云整体解决方案2" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2022/01/04/aliyunHybridCloud/aliyunHydridCloud.png" class title="阿里云混合云整体解决方案">

<h2><span id="混合云产品阿里">混合云产品(阿里)</span><a href="#混合云产品阿里" class="header-anchor">#</a></h2><h5><span id="网络通">网络通</span><a href="#网络通" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">场景</th>
<th align="center">优势</th>
<th align="center">带宽</th>
</tr>
</thead>
<tbody><tr>
<td align="center">高速通道-物理专线</td>
<td align="center">大中型企业, 多地容灾, 高可用, 高弹性</td>
<td align="center">直连，不需要绕行公网<br> 支持BGP路由</td>
<td align="center">100M, 10G - 100G</td>
</tr>
<tr>
<td align="center">VPN网关</td>
<td align="center">本地IDC上云， 移动客户端上云</td>
<td align="center">安全IPSec, SSL-VPN <br> 成本低 <br> 配置简单 <br> 网络质量受限于公网</td>
<td align="center">最高100M</td>
</tr>
</tbody></table>
<h5><span id="数据通">数据通</span><a href="#数据通" class="header-anchor">#</a></h5><ul>
<li>混合云备份(HBR)</li>
<li>数据传输服务(DTS)</li>
</ul>
<h5><span id="业务通">业务通</span><a href="#业务通" class="header-anchor">#</a></h5><ul>
<li>DevOps平台</li>
<li>容器服务</li>
</ul>
<h2><span id="混合云-vs-多云">混合云 vs. 多云</span><a href="#混合云-vs-多云" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">关注点</th>
<th align="center">细节</th>
</tr>
</thead>
<tbody><tr>
<td align="center">混合云(连)</td>
<td align="center">关注资源互联</td>
<td align="center">通过专线或VPN来连接</td>
</tr>
<tr>
<td align="center">多云(管)</td>
<td align="center">关注云资源的管理</td>
<td align="center">通过CMP云管平台、OpenAPI来管理多个云</td>
</tr>
</tbody></table>
<h2><span id="阿里云混合云解决方案2">阿里云混合云解决方案[2]</span><a href="#阿里云混合云解决方案2" class="header-anchor">#</a></h2><ul>
<li>本地存储容量无限扩展<ul>
<li>相关产品<br>-  使用存储网关， 小成本网关<br>-  使用混合云存储阵列， 专业存储</li>
</ul>
</li>
<li>线下数据海量迁移上云   <ul>
<li>相关产品<br>-  闪电立方</li>
</ul>
</li>
<li>云上云下数据库同步、 灾备<ul>
<li>相关产品    <ul>
<li>DTS</li>
</ul>
</li>
</ul>
</li>
<li>数据云下采集, 云上加工<ul>
<li>相关产品<ul>
<li>MaxCompute , RDS, 数据迁移, 数据集成, QuickBI</li>
</ul>
</li>
</ul>
</li>
<li>云上灾备中心<ul>
<li>相关产品<ul>
<li>ECS, OSS, 高速通道，容灾网关(关键)</li>
</ul>
</li>
</ul>
</li>
<li>云上云下双活互备<ul>
<li>相关产品<ul>
<li>云解析DNS, ECS, VPC，高速通道(时延)， DTS</li>
</ul>
</li>
<li>问题<br> 应用要进行改造</li>
</ul>
</li>
<li>云上云下统一的安全管理<ul>
<li>相关产品<br>- 安骑士 ， Web防火墙， DDoS高防</li>
</ul>
</li>
<li>多云平台， 统一管理</li>
</ul>
<h2><span id="解决方案架构1">解决方案架构[1]</span><a href="#解决方案架构1" class="header-anchor">#</a></h2><h5><span id="弹性架构">弹性架构</span><a href="#弹性架构" class="header-anchor">#</a></h5><ol>
<li>弹性架构实现</li>
</ol>
<ul>
<li>ECS弹性架构</li>
<li>容器弹性架构<ul>
<li>k8s弹性伸缩：<br>hpa， vpa， ca</li>
<li>rancher弹性伸缩：<br> webhook微服务的方式 服务和主机的弹性伸缩</li>
<li>ACK</li>
<li>ASK</li>
<li>ECI</li>
</ul>
</li>
<li>数据库弹性架构</li>
<li>网络弹性架构</li>
</ul>
<ol start="2">
<li>架构方案</li>
</ol>
<ul>
<li>弹性组网</li>
<li>弹性架构设计<ul>
<li>单一入口的混合云弹性伸缩</li>
<li>多入口的混合云弹性伸缩</li>
<li>基于K8s实现混合云弹性伸缩<br>负载均衡的方式路由到2个集群<br>virtual kubelet<br>Kubernetes Cluster Federation</li>
</ul>
</li>
</ul>
<h5><span id="容灾备份架构">容灾备份架构</span><a href="#容灾备份架构" class="header-anchor">#</a></h5><p><a href="../../../../2022/06/26/cloudDisasterRecovery/">云计算 容灾恢复DisasterRecovery</a></p>
<h5><span id="安全防护架构">安全防护架构</span><a href="#安全防护架构" class="header-anchor">#</a></h5><h5><span id="多云管控架构">多云管控架构</span><a href="#多云管控架构" class="header-anchor">#</a></h5><h5><span id="混合云组网架构">混合云组网架构</span><a href="#混合云组网架构" class="header-anchor">#</a></h5><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>1.《混合云架构》<br>2. <a href="https://www.bilibili.com/video/BV1uy4y1a7Ba?vd_source=f6e8c1128f9f264c5ab8d9411a644036">阿里云云上常见架构设计及优化-课时9：混合云架构及解决方案</a> bilibili</p>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S高可用-控制面</title>
    <url>/www6vHomeHexo/2022/01/02/k8sHA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="控制面-高可用">控制面 高可用</span><a href="#控制面-高可用" class="header-anchor">#</a></h2><ol>
<li>控制平面组件划分单独节点；</li>
<li>控制平面所在节点，应该确保在不同机架上；</li>
<li>保证控制平面的每个组件有足够的CPU、内存和磁盘资源；</li>
<li>减少或消除外部依赖； Cloud Provider API</li>
<li>核心组件以普通Pod形式加载运行时， 可能会调度到任意工作节点。</li>
</ol>
<h2><span id="控制面-高可用方案">控制面 高可用方案</span><a href="#控制面-高可用方案" class="header-anchor">#</a></h2><p>[1] [2]</p>
<h3><span id="架构图">架构图</span><a href="#架构图" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2022/01/02/k8sHA/k8sHa.png" class title="控制面高可用">

<ul>
<li><p>load balancer<br>   虚ip</p>
</li>
<li><p>controller-manager：<br> 用lease来实现controller-manager和scheduler的leader election</p>
</li>
</ul>
<img src="/www6vHomeHexo/2022/01/02/k8sHA/lease.png" class title="用lease来实现scheduler的leader election">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/">Options for Highly Available topology</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">Creating Highly Available clusters with kubeadm</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Operator-kubebuilder</title>
    <url>/www6vHomeHexo/2021/12/30/k8s-operator/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BD%BF%E7%94%A8kubebuilder%E6%9E%84%E5%BB%BAoperator%E7%9A%84%E8%BF%87%E7%A8%8B">使用kubebuilder构建Operator的过程</a></li>
<li><a href="#step1-init-project">Step1: init project</a><br>- <a href="#create-a-kubebuilder-project-which-requires-an-empty-folder">Create a kubebuilder project, which requires an empty folder</a><br>- <a href="#check-project-layout">Check project layout</a></li>
<li><a href="#step2-create-crd-and-controller">Step2: Create  CRD and Controller</a><br>- <a href="#create-api-create-resourcey-create-controllery">Create API, create resource[Y], create controller[Y]</a><br>- <a href="#edit-apiv1alpha1simplestatefulset_typesgo">edit <code>api/v1alpha1/simplestatefulset_types.go</code></a><br>- <a href="#check-makefile">Check Makefile</a><br>- <a href="#edit-controllersmydaemonset_controllergo-add-permissions-to-the-controller">Edit <code>controllers/mydaemonset_controller.go</code>, add permissions to the controller</a><br>- <a href="#generate-crd">Generate crd</a><br>- <a href="#build-install">Build &amp; install</a></li>
<li><a href="#step3-enable-webhooks">Step3: Enable webhooks</a><br>- <a href="#install-cert-manager">Install cert-manager</a><br>- <a href="#create-webhooks">Create webhooks</a><br>- <a href="#change-code">Change code</a><br>- <a href="#enable-webhook-in-configdefaultkustomizationyaml">Enable webhook in <code>config/default/kustomization.yaml</code></a><br>- <a href="#redeploy">Redeploy</a></li>
<li><a href="#%E9%99%84%E4%BB%B6-operator%E6%BA%90%E4%BB%A3%E7%A0%81">附件: Operator源代码</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="使用kubebuilder构建operator的过程">使用kubebuilder构建Operator的过程</span><a href="#使用kubebuilder构建operator的过程" class="header-anchor">#</a></h2><ul>
<li>自己定义DaemonSet Operator包括：<ul>
<li><a href="#step2-create-crd-and-controller">在每个node上启动一个Pod（CRD + Controller）</a></li>
<li><a href="#step3-enable-webhooks">webhook(证书 + mutation + validation)</a></li>
</ul>
</li>
</ul>
<h2><span id="step1-init-project">Step1:  init project</span><a href="#step1-init-project" class="header-anchor">#</a></h2><h5><span id="create-a-kubebuilder-project-which-requires-an-empty-folder">Create a kubebuilder project, which requires an empty folder</span><a href="#create-a-kubebuilder-project-which-requires-an-empty-folder" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kubebuilder init --domain cncamp.io</span><br></pre></td></tr></table></figure>

<h5><span id="check-project-layout">Check project layout</span><a href="#check-project-layout" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> PROJECT</span><br><span class="line"></span><br><span class="line">domain: cncamp.io</span><br><span class="line">layout:</span><br><span class="line">- go.kubebuilder.io/v3</span><br><span class="line">projectName: mysts</span><br><span class="line">repo: github.com/www6v/demo-operator</span><br><span class="line">version: <span class="string">&quot;3&quot;</span></span><br></pre></td></tr></table></figure>

<h2><span id="step2-create-crd-and-controller">Step2: Create  CRD and Controller</span><a href="#step2-create-crd-and-controller" class="header-anchor">#</a></h2><h5><span id="create-api-create-resourcey-create-controllery">Create API, create resource[Y], create controller[Y]</span><a href="#create-api-create-resourcey-create-controllery" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kubebuilder create api --group apps --version v1beta1 --kind MyDaemonset</span><br></pre></td></tr></table></figure>

<h5><span id="edit-apiv1alpha1simplestatefulset_typesgo">edit <code>api/v1alpha1/simplestatefulset_types.go</code></span><a href="#edit-apiv1alpha1simplestatefulset_typesgo" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">// MyDaemonsetSpec defines the desired state of MyDaemonset</span><br><span class="line"><span class="built_in">type</span> MyDaemonsetSpec struct &#123;</span><br><span class="line">	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster</span><br><span class="line">	// Important: Run <span class="string">&quot;make&quot;</span> to regenerate code after modifying this file</span><br><span class="line"></span><br><span class="line">	// Foo is an example field of MyDaemonset. Edit mydaemonset_types.go to remove/update</span><br><span class="line">	Image string `json:<span class="string">&quot;image,omitempty&quot;</span>`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// MyDaemonsetStatus defines the observed state of MyDaemonset</span><br><span class="line"><span class="built_in">type</span> MyDaemonsetStatus struct &#123;</span><br><span class="line">	AvaiableReplicas int `json:<span class="string">&quot;avaiableReplicas,omitempty&quot;</span>`</span><br><span class="line">	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster</span><br><span class="line">	// Important: Run <span class="string">&quot;make&quot;</span> to regenerate code after modifying this file</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5><span id="check-makefile">Check Makefile</span><a href="#check-makefile" class="header-anchor">#</a></h5><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">Build targets:</span><br><span class="line">    <span class="comment">### create code skeletion</span></span><br><span class="line">    manifests: generate crd</span><br><span class="line">    generate: generate api functions, like deepCopy</span><br><span class="line"></span><br><span class="line">    <span class="comment">### generate crd and install</span></span><br><span class="line">    run: Run a controller from your host.</span><br><span class="line">    install: Install CRDs into the K8s cluster specified in ~/.kube/config.</span><br><span class="line"></span><br><span class="line">    <span class="comment">### docker build and deploy</span></span><br><span class="line">    docker-build: Build docker image with the manager.</span><br><span class="line">    docker-push: Push docker image with the manager.</span><br><span class="line">    deploy: Deploy controller to the K8s cluster specified in ~/.kube/config.</span><br></pre></td></tr></table></figure>

<h5><span id="edit-controllersmydaemonset_controllergo-add-permissions-to-the-controller">Edit <code>controllers/mydaemonset_controller.go</code>, add permissions to the controller</span><a href="#edit-controllersmydaemonset_controllergo-add-permissions-to-the-controller" class="header-anchor">#</a></h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//+kubebuilder:rbac:groups=apps.cncamp.io,resources=mydaemonsets/finalizers,verbs=update</span></span><br><span class="line"><span class="comment">// Add the following</span></span><br><span class="line"><span class="comment">//+kubebuilder:rbac:groups=core,resources=nodes,verbs=get;list;watch</span></span><br><span class="line"><span class="comment">//+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete</span></span><br></pre></td></tr></table></figure>

<h5><span id="generate-crd">Generate crd</span><a href="#generate-crd" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">make manifests</span><br></pre></td></tr></table></figure>

<h5><span id="build-amp-install">Build &amp; install</span><a href="#build-amp-install" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">make build</span><br><span class="line">make docker-build</span><br><span class="line">make docker-push</span><br><span class="line">make deploy</span><br></pre></td></tr></table></figure>

<h2><span id="step3-enable-webhooks">Step3: Enable webhooks</span><a href="#step3-enable-webhooks" class="header-anchor">#</a></h2><h5><span id="install-cert-manager">Install cert-manager</span><a href="#install-cert-manager" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml</span><br></pre></td></tr></table></figure>

<h5><span id="create-webhooks">Create webhooks</span><a href="#create-webhooks" class="header-anchor">#</a></h5><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kubebuilder create webhook --group apps --version v1beta1 --kind MyDaemonset --defaulting --programmatic-validation</span><br></pre></td></tr></table></figure>

<h5><span id="change-code">Change code</span><a href="#change-code" class="header-anchor">#</a></h5><h5><span id="enable-webhook-in-configdefaultkustomizationyaml">Enable webhook in <code>config/default/kustomization.yaml</code></span><a href="#enable-webhook-in-configdefaultkustomizationyaml" class="header-anchor">#</a></h5><h5><span id="redeploy">Redeploy</span><a href="#redeploy" class="header-anchor">#</a></h5><h2><span id="附件-operator源代码">附件: Operator源代码</span><a href="#附件-operator源代码" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/mydaemonset">https://github.com/www6v/mydaemonset</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis RDB源码</title>
    <url>/www6vHomeHexo/2021/12/08/rdb/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>


<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// 对应了 Redis 的 save 命 令，会在 save 命令的实现函数 saveCommand(在 rdb.c 文件中)中被调用</span></span><br><span class="line"><span class="comment">/* Save the DB on disk. Return C_ERR on error, C_OK on success. */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">rdbSave</span><span class="params">(<span class="type">char</span> *filename, rdbSaveInfo *rsi)</span> &#123;</span><br><span class="line">    <span class="type">char</span> tmpfile[<span class="number">256</span>];</span><br><span class="line">    <span class="type">char</span> cwd[MAXPATHLEN]; <span class="comment">/* Current working dir path for error messages. */</span></span><br><span class="line">    FILE *fp;</span><br><span class="line">    rio rdb;</span><br><span class="line">    <span class="type">int</span> error = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">snprintf</span>(tmpfile,<span class="number">256</span>,<span class="string">&quot;temp-%d.rdb&quot;</span>, (<span class="type">int</span>) getpid());</span><br><span class="line">    fp = fopen(tmpfile,<span class="string">&quot;w&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!fp) &#123;</span><br><span class="line">        <span class="type">char</span> *cwdp = getcwd(cwd,MAXPATHLEN);</span><br><span class="line">        serverLog(LL_WARNING,</span><br><span class="line">            <span class="string">&quot;Failed opening the RDB file %s (in server root dir %s) &quot;</span></span><br><span class="line">            <span class="string">&quot;for saving: %s&quot;</span>,</span><br><span class="line">            filename,</span><br><span class="line">            cwdp ? cwdp : <span class="string">&quot;unknown&quot;</span>,</span><br><span class="line">            strerror(errno));</span><br><span class="line">        <span class="keyword">return</span> C_ERR;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rioInitWithFile(&amp;rdb,fp);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (server.rdb_save_incremental_fsync)</span><br><span class="line">        rioSetAutoSync(&amp;rdb,REDIS_AUTOSYNC_BYTES);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (rdbSaveRio(&amp;rdb,&amp;error,RDB_SAVE_NONE,rsi) == C_ERR) &#123;. <span class="comment">/// 调用 rdbSaveRio 函数(在 rdb.c 文件中)来实际创建 RDB 文件</span></span><br><span class="line">        errno = error;</span><br><span class="line">        <span class="keyword">goto</span> werr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Make sure data will not remain on the OS&#x27;s output buffers */</span></span><br><span class="line">    <span class="keyword">if</span> (fflush(fp) == EOF) <span class="keyword">goto</span> werr;</span><br><span class="line">    <span class="keyword">if</span> (fsync(fileno(fp)) == <span class="number">-1</span>) <span class="keyword">goto</span> werr;</span><br><span class="line">    <span class="keyword">if</span> (fclose(fp) == EOF) <span class="keyword">goto</span> werr;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Use RENAME to make sure the DB file is changed atomically only</span></span><br><span class="line"><span class="comment">     * if the generate DB file is ok. */</span></span><br><span class="line">    <span class="keyword">if</span> (rename(tmpfile,filename) == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="type">char</span> *cwdp = getcwd(cwd,MAXPATHLEN);</span><br><span class="line">        serverLog(LL_WARNING,</span><br><span class="line">            <span class="string">&quot;Error moving temp DB file %s on the final &quot;</span></span><br><span class="line">            <span class="string">&quot;destination %s (in server root dir %s): %s&quot;</span>,</span><br><span class="line">            tmpfile,</span><br><span class="line">            filename,</span><br><span class="line">            cwdp ? cwdp : <span class="string">&quot;unknown&quot;</span>,</span><br><span class="line">            strerror(errno));</span><br><span class="line">        unlink(tmpfile);</span><br><span class="line">        <span class="keyword">return</span> C_ERR;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    serverLog(LL_NOTICE,<span class="string">&quot;DB saved on disk&quot;</span>);</span><br><span class="line">    server.dirty = <span class="number">0</span>;</span><br><span class="line">    server.lastsave = time(<span class="literal">NULL</span>);</span><br><span class="line">    server.lastbgsave_status = C_OK;</span><br><span class="line">    <span class="keyword">return</span> C_OK;</span><br><span class="line"></span><br><span class="line">werr:</span><br><span class="line">    serverLog(LL_WARNING,<span class="string">&quot;Write error saving DB on disk: %s&quot;</span>, strerror(errno));</span><br><span class="line">    fclose(fp);</span><br><span class="line">    unlink(tmpfile);</span><br><span class="line">    <span class="keyword">return</span> C_ERR;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// 对应 了 Redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsaveCommand(在 rdb.c 文 件中)中被调用</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">rdbSaveBackground</span><span class="params">(<span class="type">char</span> *filename, rdbSaveInfo *rsi)</span> &#123;</span><br><span class="line">    <span class="type">pid_t</span> childpid;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> start;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (server.aof_child_pid != <span class="number">-1</span> || server.rdb_child_pid != <span class="number">-1</span>) <span class="keyword">return</span> C_ERR;</span><br><span class="line"></span><br><span class="line">    server.dirty_before_bgsave = server.dirty;</span><br><span class="line">    server.lastbgsave_try = time(<span class="literal">NULL</span>);</span><br><span class="line">    openChildInfoPipe();</span><br><span class="line"></span><br><span class="line">    start = ustime();</span><br><span class="line">    <span class="keyword">if</span> ((childpid = fork()) == <span class="number">0</span>) &#123;  <span class="comment">/// 子进程的代码执行分支</span></span><br><span class="line">        <span class="type">int</span> retval;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Child */</span></span><br><span class="line">        closeClildUnusedResourceAfterFork();</span><br><span class="line">        redisSetProcTitle(<span class="string">&quot;redis-rdb-bgsave&quot;</span>);</span><br><span class="line">        retval = rdbSave(filename,rsi);   <span class="comment">///  调用rdbSave函数创建RDB文件</span></span><br><span class="line">        <span class="keyword">if</span> (retval == C_OK) &#123;</span><br><span class="line">            <span class="type">size_t</span> private_dirty = zmalloc_get_private_dirty(<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (private_dirty) &#123;</span><br><span class="line">                serverLog(LL_NOTICE,</span><br><span class="line">                    <span class="string">&quot;RDB: %zu MB of memory used by copy-on-write&quot;</span>,</span><br><span class="line">                    private_dirty/(<span class="number">1024</span>*<span class="number">1024</span>));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            server.child_info_data.cow_size = private_dirty;</span><br><span class="line">            sendChildInfo(CHILD_INFO_TYPE_RDB);</span><br><span class="line">        &#125;</span><br><span class="line">        exitFromChild((retval == C_OK) ? <span class="number">0</span> : <span class="number">1</span>);   <span class="comment">///  子进程退出</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="comment">/* Parent */</span>    <span class="comment">//父进程代码执行分支</span></span><br><span class="line">        server.stat_fork_time = ustime()-start;</span><br><span class="line">        server.stat_fork_rate = (<span class="type">double</span>) zmalloc_used_memory() * <span class="number">1000000</span> / server.stat_fork_time / (<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1024</span>); <span class="comment">/* GB per second. */</span></span><br><span class="line">        latencyAddSampleIfNeeded(<span class="string">&quot;fork&quot;</span>,server.stat_fork_time/<span class="number">1000</span>);</span><br><span class="line">        <span class="keyword">if</span> (childpid == <span class="number">-1</span>) &#123;</span><br><span class="line">            closeChildInfoPipe();</span><br><span class="line">            server.lastbgsave_status = C_ERR;</span><br><span class="line">            serverLog(LL_WARNING,<span class="string">&quot;Can&#x27;t save in background: fork: %s&quot;</span>,</span><br><span class="line">                strerror(errno));</span><br><span class="line">            <span class="keyword">return</span> C_ERR;</span><br><span class="line">        &#125;</span><br><span class="line">        serverLog(LL_NOTICE,<span class="string">&quot;Background saving started by pid %d&quot;</span>,childpid);</span><br><span class="line">        server.rdb_save_time_start = time(<span class="literal">NULL</span>);</span><br><span class="line">        server.rdb_child_pid = childpid;</span><br><span class="line">        server.rdb_child_type = RDB_CHILD_TYPE_DISK;</span><br><span class="line">        updateDictResizePolicy();</span><br><span class="line">        <span class="keyword">return</span> C_OK;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> C_OK; <span class="comment">/* unreached */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// 这是 Redis server 在采用不落盘方式传输 RDB 文件进行主从复制时，创建 RDB 文件的入 口函数</span></span><br><span class="line"><span class="comment">/// rdbSaveToSlavesSockets 函数是通过网络以字节流的形式，直接发送 RDB 文件的二进制 数据给从节点。</span></span><br><span class="line"><span class="comment">/* Spawn an RDB child that writes the RDB to the sockets of the slaves</span></span><br><span class="line"><span class="comment"> * that are currently in SLAVE_STATE_WAIT_BGSAVE_START state. */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">rdbSaveToSlavesSockets</span><span class="params">(rdbSaveInfo *rsi)</span> &#123;</span><br><span class="line">    <span class="type">int</span> *fds;</span><br><span class="line">    <span class="type">uint64_t</span> *clientids;</span><br><span class="line">    <span class="type">int</span> numfds;</span><br><span class="line">    listNode *ln;</span><br><span class="line">    listIter li;</span><br><span class="line">    <span class="type">pid_t</span> childpid;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> start;</span><br><span class="line">    <span class="type">int</span> pipefds[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (server.aof_child_pid != <span class="number">-1</span> || server.rdb_child_pid != <span class="number">-1</span>) <span class="keyword">return</span> C_ERR;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Before to fork, create a pipe that will be used in order to</span></span><br><span class="line"><span class="comment">     * send back to the parent the IDs of the slaves that successfully</span></span><br><span class="line"><span class="comment">     * received all the writes. */</span></span><br><span class="line">    <span class="keyword">if</span> (pipe(pipefds) == <span class="number">-1</span>) <span class="keyword">return</span> C_ERR;</span><br><span class="line">    server.rdb_pipe_read_result_from_child = pipefds[<span class="number">0</span>];</span><br><span class="line">    server.rdb_pipe_write_result_to_parent = pipefds[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Collect the file descriptors of the slaves we want to transfer</span></span><br><span class="line"><span class="comment">     * the RDB to, which are i WAIT_BGSAVE_START state. */</span></span><br><span class="line">    fds = zmalloc(<span class="keyword">sizeof</span>(<span class="type">int</span>)*listLength(server.slaves));</span><br><span class="line">    <span class="comment">/* We also allocate an array of corresponding client IDs. This will</span></span><br><span class="line"><span class="comment">     * be useful for the child process in order to build the report</span></span><br><span class="line"><span class="comment">     * (sent via unix pipe) that will be sent to the parent. */</span></span><br><span class="line">    clientids = zmalloc(<span class="keyword">sizeof</span>(<span class="type">uint64_t</span>)*listLength(server.slaves));</span><br><span class="line">    numfds = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    listRewind(server.slaves,&amp;li);</span><br><span class="line">    <span class="keyword">while</span>((ln = listNext(&amp;li))) &#123;</span><br><span class="line">        client *slave = ln-&gt;value;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) &#123;</span><br><span class="line">            clientids[numfds] = slave-&gt;id;</span><br><span class="line">            fds[numfds++] = slave-&gt;fd;</span><br><span class="line">            replicationSetupSlaveForFullResync(slave,getPsyncInitialOffset());</span><br><span class="line">            <span class="comment">/* Put the socket in blocking mode to simplify RDB transfer.</span></span><br><span class="line"><span class="comment">             * We&#x27;ll restore it when the children returns (since duped socket</span></span><br><span class="line"><span class="comment">             * will share the O_NONBLOCK attribute with the parent). */</span></span><br><span class="line">            anetBlock(<span class="literal">NULL</span>,slave-&gt;fd);</span><br><span class="line">            anetSendTimeout(<span class="literal">NULL</span>,slave-&gt;fd,server.repl_timeout*<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Create the child process. */</span></span><br><span class="line">    openChildInfoPipe();</span><br><span class="line">    start = ustime();</span><br><span class="line">    <span class="keyword">if</span> ((childpid = fork()) == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">/* Child */</span></span><br><span class="line">        <span class="type">int</span> retval;</span><br><span class="line">        rio slave_sockets;</span><br><span class="line"></span><br><span class="line">        rioInitWithFdset(&amp;slave_sockets,fds,numfds);</span><br><span class="line">        zfree(fds);</span><br><span class="line"></span><br><span class="line">        closeClildUnusedResourceAfterFork();</span><br><span class="line">        redisSetProcTitle(<span class="string">&quot;redis-rdb-to-slaves&quot;</span>);</span><br><span class="line"></span><br><span class="line">        retval = rdbSaveRioWithEOFMark(&amp;slave_sockets,<span class="literal">NULL</span>,rsi);</span><br><span class="line">        <span class="keyword">if</span> (retval == C_OK &amp;&amp; rioFlush(&amp;slave_sockets) == <span class="number">0</span>)</span><br><span class="line">            retval = C_ERR;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (retval == C_OK) &#123;</span><br><span class="line">            <span class="type">size_t</span> private_dirty = zmalloc_get_private_dirty(<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (private_dirty) &#123;</span><br><span class="line">                serverLog(LL_NOTICE,</span><br><span class="line">                    <span class="string">&quot;RDB: %zu MB of memory used by copy-on-write&quot;</span>,</span><br><span class="line">                    private_dirty/(<span class="number">1024</span>*<span class="number">1024</span>));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            server.child_info_data.cow_size = private_dirty;</span><br><span class="line">            sendChildInfo(CHILD_INFO_TYPE_RDB);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* If we are returning OK, at least one slave was served</span></span><br><span class="line"><span class="comment">             * with the RDB file as expected, so we need to send a report</span></span><br><span class="line"><span class="comment">             * to the parent via the pipe. The format of the message is:</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * &lt;len&gt; &lt;slave[0].id&gt; &lt;slave[0].error&gt; ...</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * len, slave IDs, and slave errors, are all uint64_t integers,</span></span><br><span class="line"><span class="comment">             * so basically the reply is composed of 64 bits for the len field</span></span><br><span class="line"><span class="comment">             * plus 2 additional 64 bit integers for each entry, for a total</span></span><br><span class="line"><span class="comment">             * of &#x27;len&#x27; entries.</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * The &#x27;id&#x27; represents the slave&#x27;s client ID, so that the master</span></span><br><span class="line"><span class="comment">             * can match the report with a specific slave, and &#x27;error&#x27; is</span></span><br><span class="line"><span class="comment">             * set to 0 if the replication process terminated with a success</span></span><br><span class="line"><span class="comment">             * or the error code if an error occurred. */</span></span><br><span class="line">            <span class="type">void</span> *msg = zmalloc(<span class="keyword">sizeof</span>(<span class="type">uint64_t</span>)*(<span class="number">1</span>+<span class="number">2</span>*numfds));</span><br><span class="line">            <span class="type">uint64_t</span> *len = msg;</span><br><span class="line">            <span class="type">uint64_t</span> *ids = len+<span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> j, msglen;</span><br><span class="line"></span><br><span class="line">            *len = numfds;</span><br><span class="line">            <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; numfds; j++) &#123;</span><br><span class="line">                *ids++ = clientids[j];</span><br><span class="line">                *ids++ = slave_sockets.io.fdset.state[j];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* Write the message to the parent. If we have no good slaves or</span></span><br><span class="line"><span class="comment">             * we are unable to transfer the message to the parent, we exit</span></span><br><span class="line"><span class="comment">             * with an error so that the parent will abort the replication</span></span><br><span class="line"><span class="comment">             * process with all the childre that were waiting. */</span></span><br><span class="line">            msglen = <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>)*(<span class="number">1</span>+<span class="number">2</span>*numfds);</span><br><span class="line">            <span class="keyword">if</span> (*len == <span class="number">0</span> ||</span><br><span class="line">                write(server.rdb_pipe_write_result_to_parent,msg,msglen)</span><br><span class="line">                != msglen)</span><br><span class="line">            &#123;</span><br><span class="line">                retval = C_ERR;</span><br><span class="line">            &#125;</span><br><span class="line">            zfree(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        zfree(clientids);</span><br><span class="line">        rioFreeFdset(&amp;slave_sockets);</span><br><span class="line">        exitFromChild((retval == C_OK) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/* Parent */</span></span><br><span class="line">        <span class="keyword">if</span> (childpid == <span class="number">-1</span>) &#123;</span><br><span class="line">            serverLog(LL_WARNING,<span class="string">&quot;Can&#x27;t save in background: fork: %s&quot;</span>,</span><br><span class="line">                strerror(errno));</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* Undo the state change. The caller will perform cleanup on</span></span><br><span class="line"><span class="comment">             * all the slaves in BGSAVE_START state, but an early call to</span></span><br><span class="line"><span class="comment">             * replicationSetupSlaveForFullResync() turned it into BGSAVE_END */</span></span><br><span class="line">            listRewind(server.slaves,&amp;li);</span><br><span class="line">            <span class="keyword">while</span>((ln = listNext(&amp;li))) &#123;</span><br><span class="line">                client *slave = ln-&gt;value;</span><br><span class="line">                <span class="type">int</span> j;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; numfds; j++) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (slave-&gt;id == clientids[j]) &#123;</span><br><span class="line">                        slave-&gt;replstate = SLAVE_STATE_WAIT_BGSAVE_START;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            close(pipefds[<span class="number">0</span>]);</span><br><span class="line">            close(pipefds[<span class="number">1</span>]);</span><br><span class="line">            closeChildInfoPipe();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            server.stat_fork_time = ustime()-start;</span><br><span class="line">            server.stat_fork_rate = (<span class="type">double</span>) zmalloc_used_memory() * <span class="number">1000000</span> / server.stat_fork_time / (<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1024</span>); <span class="comment">/* GB per second. */</span></span><br><span class="line">            latencyAddSampleIfNeeded(<span class="string">&quot;fork&quot;</span>,server.stat_fork_time/<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">            serverLog(LL_NOTICE,<span class="string">&quot;Background RDB transfer started by pid %d&quot;</span>,</span><br><span class="line">                childpid);</span><br><span class="line">            server.rdb_save_time_start = time(<span class="literal">NULL</span>);</span><br><span class="line">            server.rdb_child_pid = childpid;</span><br><span class="line">            server.rdb_child_type = RDB_CHILD_TYPE_SOCKET;</span><br><span class="line">            updateDictResizePolicy();</span><br><span class="line">        &#125;</span><br><span class="line">        zfree(clientids);</span><br><span class="line">        zfree(fds);</span><br><span class="line">        <span class="keyword">return</span> (childpid == <span class="number">-1</span>) ? C_ERR : C_OK;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> C_OK; <span class="comment">/* Unreached. */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<img src="/www6vHomeHexo/2021/12/08/rdb/rdb.png" class title="rdb流程">



<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>18 | 如何生成和解读RDB文件?<br>《Redis源码剖析与实战》</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis AOF Rewrite</title>
    <url>/www6vHomeHexo/2021/11/21/aofRewrite/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>


<h2><span id="一-aof-重写函数与触发时机">一.   AOF 重写函数与触发时机</span><a href="#一-aof-重写函数与触发时机" class="header-anchor">#</a></h2><p>我们就了解了 AOF 重写的四个触发时机，这里我也给你总结下，方便你回 顾复习。</p>
<h5><span id="时机一bgrewriteaof-命令被执行">时机一:bgrewriteaof 命令被执行。</span><a href="#时机一bgrewriteaof-命令被执行" class="header-anchor">#</a></h5><h5><span id="时机二主从复制完成-rdb-文件解析和加载无论是否成功">时机二:主从复制完成 RDB 文件解析和加载(无论是否成功)。</span><a href="#时机二主从复制完成-rdb-文件解析和加载无论是否成功" class="header-anchor">#</a></h5><pre><code>而对于 restartAOFAfterSYNC 函数来说，它会在主从节点的复制过程中被调用。简单来 说，就是当主从节点在进行复制时，如果从节点的 AOF 选项被打开，那么在加载解析 RDB 文件时，AOF 选项就会被关闭。然后，无论从节点是否成功加载了 RDB 文件， restartAOFAfterSYNC 函数都会被调用，用来恢复被关闭的 AOF 功能。
那么在这个过程中，restartAOFAfterSYNC 函数就会调用 startAppendOnly 函数，并进 一步调用 rewriteAppendOnlyFileBackground 函数，来执行一次 AOF 重写。
</code></pre>
<h5><span id="时机三aof-重写被设置为待调度执行">时机三:AOF 重写被设置为待调度执行。</span><a href="#时机三aof-重写被设置为待调度执行" class="header-anchor">#</a></h5><h5><span id="时机四aof-被启用同时-aof-文件的大小比例超出阈值以及-aof-文件的大小绝对-值超出阈值">时机四:AOF 被启用，同时 AOF 文件的大小比例超出阈值，以及 AOF 文件的大小绝对 值超出阈值。</span><a href="#时机四aof-被启用同时-aof-文件的大小比例超出阈值以及-aof-文件的大小绝对-值超出阈值" class="header-anchor">#</a></h5><p>auto-aof-rewrite-percentage:AOF 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。<br>auto-aof-rewrite-min-size:AOF 文件大小绝对值的最小值，默认为 64MB。</p>
<p>另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 RDB 子进程和 AOF 重写子进程，否则的话，AOF 重写就无法执行了。</p>
<h2><span id="二-aof-重写的基本过程">二.  AOF 重写的基本过程</span><a href="#二-aof-重写的基本过程" class="header-anchor">#</a></h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">rewriteAppendOnlyFileBackground</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">	<span class="keyword">if</span> ((childpid = fork()) == <span class="number">0</span>) &#123; <span class="comment">//创建子进程 </span></span><br><span class="line">         ...</span><br><span class="line">		<span class="comment">//子进程调用rewriteAppendOnlyFile进行AOF重写</span></span><br><span class="line">		<span class="keyword">if</span> (rewriteAppendOnlyFile(tmpfile) == C_OK) &#123;</span><br><span class="line">            <span class="type">size_t</span> private_dirty = zmalloc_get_private_dirty(<span class="number">-1</span>);</span><br><span class="line">            ...</span><br><span class="line">            exitFromChild(<span class="number">0</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            exitFromChild(<span class="number">1</span>);</span><br><span class="line">		&#125; </span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123; <span class="comment">//父进程执行的逻辑 </span></span><br><span class="line">        ...</span><br><span class="line">		server.aof_rewrite_scheduled = <span class="number">0</span>; </span><br><span class="line">		server.aof_rewrite_time_start = time(<span class="literal">NULL</span>); </span><br><span class="line">		server.aof_child_pid = childpid; <span class="comment">//记录重写子进程的进程号 </span></span><br><span class="line">		updateDictResizePolicy(); <span class="comment">//关闭rehash功能</span></span><br><span class="line">        ... </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<img src="/www6vHomeHexo/2021/11/21/aofRewrite/aof-rewrite.png" class title="AOF Rewrite">


<p>不过，AOF 重写和 RDB 文件又有两个不同的地方:<br>一是，AOF 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 RDB 文件记录的是键值对数据本身;<br>二是，在 AOF 重写或是创建 RDB 的过程中，主进程仍然可以接收客户端写请求。不 过，因为 RDB 文件只需要记录某个时刻下数据库的所有数据就行，而 AOF 重写则需要 尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，AOF 重写子进程 就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 多租户</title>
    <url>/www6vHomeHexo/2021/10/18/k8sMultiTenancy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="一-概念和原则">一. 概念和原则</span><a href="#一-概念和原则" class="header-anchor">#</a></h2><ol>
<li>租户是指一组拥有访问特定软件资源权限的用户集合，在多租户环境中，它还包括共享的应用、服务、数据和各项配置等。</li>
<li>多租户集群必须将租户彼此隔离</li>
<li>集群须在租户之间公平的分配集群资源。</li>
</ol>
<h2><span id="二-overview">二. Overview</span><a href="#二-overview" class="header-anchor">#</a></h2><ul>
<li><p>软隔离：在这种情况下，我们有一个企业，不同的团队访问同一个集群，这需要较少的安全开销，因为用户可以相互信任。</p>
</li>
<li><p>硬隔离：当 Kubernetes 暴露给具有独立且完全不受信任的用户的多个企业时，这是必需的。</p>
</li>
</ul>
<img src="/www6vHomeHexo/2021/10/18/k8sMultiTenancy/k8sMultiTenancy1.png" class title="多租户Overview">


<ul>
<li><p>方案B  软隔离 - hierarchical-namespaces<br><a href="https://github.com/kubernetes-sigs/hierarchical-namespaces">https://github.com/kubernetes-sigs/hierarchical-namespaces</a><br><a href="https://icloudnative.io/posts/introducing-hierarchical-namespaces/">Kubernetes 的层级命名空间介绍</a></p>
</li>
<li><p>方案C  硬隔离 - virtualcluster<br><a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster">https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster</a></p>
</li>
</ul>
<h2><span id="三-方案b-解决方案">三. 方案B 解决方案</span><a href="#三-方案b-解决方案" class="header-anchor">#</a></h2><h5><span id="1-认证">1. 认证</span><a href="#1-认证" class="header-anchor">#</a></h5><ul>
<li>识别访问的用户是谁</li>
<li>K8s可管理2类用户   <ul>
<li>ServiceAccount<br> 是存在于K8s中的虚拟账户;<br> 原生支持的动态认证;</li>
<li>用户认证<br> webhook是和企业认证平台集成的重要手段</li>
</ul>
</li>
</ul>
<h5><span id="2-授权">2. 授权</span><a href="#2-授权" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2021/10/18/k8sMultiTenancy/k8sMultiTenancy.png" class title="授权">


<h5><span id="3-隔离">3. 隔离</span><a href="#3-隔离" class="header-anchor">#</a></h5><ul>
<li><p>节点隔离<br>Taints和Toleration机制<br><a href="https://mp.weixin.qq.com/s/rza4euQCLuMLTI5fHdj67Q">Kubernetes 调度 - 污点和容忍度详解</a><br><a href="https://jimmysong.io/kubernetes-handbook/concepts/taint-and-toleration.html">Taint 和 Toleration（污点和容忍）</a></p>
</li>
<li><p>网路隔离<br>NetworkPolicy 网路策略 - 实质上也是iptables规则</p>
<p>网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。</p>
</li>
<li><p>容器隔离<br>  5种隔离机制</p>
<ul>
<li>Mount Namespace 隔离</li>
<li>PID Namespace 隔离</li>
<li>Network Namespace 隔离</li>
<li>IPC Namespace 隔离</li>
<li>UTS Namespace 隔离<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">4个打破隔离的机制 </span><br><span class="line">   - spec.hostNetwork</span><br><span class="line">   - spec.volumes</span><br><span class="line">   - spec.shareProcessNamespace</span><br><span class="line">   - spec.containers[].securityContext.privileged</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h5><span id="4-配额">4. 配额</span><a href="#4-配额" class="header-anchor">#</a></h5><p>   ResourceQuota  </p>
<h5><span id="5-开源方案">5. 开源方案</span><a href="#5-开源方案" class="header-anchor">#</a></h5><p><a href="https://github.com/loft-sh/kiosk">kiosk</a><br><a href="https://aws.amazon.com/de/blogs/containers/set-up-soft-multi-tenancy-with-kiosk-on-amazon-elastic-kubernetes-service/">Set up soft multi-tenancy with Kiosk on Amazon Elastic Kubernetes Service</a></p>
<h3><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://mp.weixin.qq.com/s/YBxsZ5a_K6AWnOISTtiX3g">Kubernetes 多租户简介</a><br> <a href="https://static.sched.com/hosted_files/kccnceu19/74/kubecon-eu-multitenancy-wg-deepdive.pdf">https://static.sched.com/hosted_files/kccnceu19/74/kubecon-eu-multitenancy-wg-deepdive.pdf</a><br> 官方 <a href="https://github.com/kubernetes-sigs/multi-tenancy">https://github.com/kubernetes-sigs/multi-tenancy</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/PobybjwmzOdbLcx53onJZQ">解决 K8s 落地难题的方法论提炼</a></p>
</li>
<li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/network-policy.html">NetworkPolicy</a> jimmysong</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Y-dUtp1yqRxpd40YwjyE0Q">轻量级 Kubernetes 多租户方案的探索与实践</a> 火山引擎云原生<br>本质上来说 KubeZoo 的方案和 Virtual Cluster 有点类似，是一种 Serverless 的 Kubernetes 方案。</p>
</li>
<li><p><a href="https://kubesphere.com.cn/blogs/gitlab-kubesphere/">KubeSphere 多租户与认证鉴权实践：使用 GitLab 账号登陆 KubeSphere</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>AI 应用场景</title>
    <url>/www6vHomeHexo/2021/08/11/ai/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E4%B8%8E%E8%A1%8C%E4%B8%9A-2">应用与行业 [2]</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-4">计算机视觉 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">应用场景</a></li>
<li><a href="#%E8%AF%AD%E9%9F%B3%E6%8A%80%E6%9C%AF-4">语音技术 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1">应用场景</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-4">自然语言处理 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2">应用场景</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/08/11/ai/ai.png" class title="AI">

<ul>
<li>人工智能的三个层面<ul>
<li>计算智能<br>能算能存</li>
<li>感知智能<br>能听会说， 能看会认</li>
<li>认知智能<br>能理解，会思考</li>
</ul>
</li>
</ul>
<h2><span id="应用与行业-2">应用与行业 [2]</span><a href="#应用与行业-2" class="header-anchor">#</a></h2><ul>
<li>健康码<img src="/www6vHomeHexo/2021/08/11/ai/ai-hangye.png" class title="行业"></li>
</ul>
<h2><span id="计算机视觉-4">计算机视觉 [4]</span><a href="#计算机视觉-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>图像分类<ul>
<li>计算机视觉的核心问题<ul>
<li>细粒度图像分类</li>
</ul>
</li>
<li>人脸识别<ul>
<li>身份确认</li>
<li>身份查找</li>
</ul>
</li>
</ul>
</li>
<li>图像重建</li>
<li>目标检测<ul>
<li>物体定位</li>
<li>热门方向，领域<ul>
<li>在无人驾驶领域很重要</li>
<li>机器人导航</li>
<li>智能视频监控</li>
<li>工业检查</li>
</ul>
</li>
<li>关键问题<ul>
<li>小目标 高精度检测</li>
<li>多类别物体检测</li>
</ul>
</li>
</ul>
</li>
<li>图像搜索</li>
<li>图像分割<ul>
<li>核心问题</li>
<li>三类(逐层递进)<ul>
<li>语义分割</li>
<li>实例分割</li>
<li>全景分割</li>
</ul>
</li>
<li>应用场景</li>
</ul>
</li>
<li>目标跟踪</li>
</ul>
<h2><span id="语音技术-4">语音技术 [4]</span><a href="#语音技术-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>语音识别<ul>
<li>语音转文字</li>
<li>应用<ul>
<li>智能音响</li>
<li>语音输入发</li>
</ul>
</li>
</ul>
</li>
<li>语音合成 <ul>
<li>文字转语音 TTS</li>
<li>应用<ul>
<li>人机交互</li>
<li>语音客服</li>
<li>虚拟偶像-腾讯AI主播 艾灵</li>
</ul>
</li>
</ul>
</li>
<li>声纹识别<ul>
<li>微信的声音锁功能</li>
</ul>
</li>
</ul>
<h2><span id="自然语言处理-4">自然语言处理 [4]</span><a href="#自然语言处理-4" class="header-anchor">#</a></h2><ul>
<li>人工智能的最高境界</li>
</ul>
<h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>文本分类<ul>
<li>新闻分类  </li>
<li>邮件自动回复，垃圾邮件</li>
<li>客服聊天情感分析</li>
<li>内容审核</li>
</ul>
</li>
<li>机器翻译<ul>
<li>在线多语言翻译</li>
<li>会议中的语音同传</li>
<li>翻译机</li>
<li>跨语言检索</li>
</ul>
</li>
<li>知识图谱<ul>
<li>认知智能</li>
</ul>
</li>
<li>对话系统<ul>
<li>任务导向 - 问答系统</li>
<li>非任务导向 - 聊天机器人</li>
</ul>
</li>
<li>信息检索</li>
<li>文本生成<ul>
<li>写作机器人</li>
</ul>
</li>
</ul>
<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li>机器可以看   -  计算机视觉 </li>
<li>机器可以听   -  语音技术</li>
<li>机器可以理解 -  自然语言处理</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://cloud.tencent.com/edu/learning/course-3460-61199">腾讯云人工智能从业者认证线上培训课程</a> </p>
<ol>
<li>1.1  </li>
<li>1.2 </li>
<li>1.4 未</li>
<li>1.5</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>应用场景</category>
      </categories>
      <tags>
        <tag>应用场景</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云TCP2-云原生应用设计</title>
    <url>/www6vHomeHexo/2021/07/22/tencentTCP2/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="二-云原生应用设计10">二、云原生应用设计(10%)</span><a href="#二-云原生应用设计10" class="header-anchor">#</a></h2><h3><span id="1-云原生应用介绍">1. 云原生应用介绍</span><a href="#1-云原生应用介绍" class="header-anchor">#</a></h3><h5><span id="11-云原生应用设计相关概念">1.1 云原生应用设计相关概念</span><a href="#11-云原生应用设计相关概念" class="header-anchor">#</a></h5><p>包括：应用架构的发展、云原生的概念、云原生的关键技术、微服务概念、容器化概念、DevOps概念、持续交付概念和Serverless	☆</p>
<ul>
<li><p>应用架构的发展</p>
<ul>
<li>系统资源<br>操作系统 -&gt; 虚拟化 -&gt; 云计算 -&gt; 容器</li>
<li>应用架构<br>单体架构 -&gt; SOA -&gt; 微服务，serverless架构</li>
<li>生命周期管理<br>瀑布开发 -&gt; CI&#x2F;CD -&gt; CI&#x2F;CD&#x2F;CO</li>
</ul>
</li>
<li><p>云原生的概念<br>“适合云的应用” 和 “好用的云架构”</p>
</li>
<li><p>【云原生应用的12-factors】</p>
</li>
<li><p>云原生的关键技术、</p>
<ul>
<li>微服务概念<br>拆服务， 串联服务。 内聚，敏捷</li>
<li>容器化概念<br>轻量级虚拟化， namespace隔离, 每个容器都有唯一的可写文件系统和资源配额<br>两部分: 运行时 + 编排层</li>
<li>DevOps概念<br>计划 -&gt; 需求 -&gt; 设计 -&gt; 开发 -&gt; 部署 -&gt; 运营<br>CI&#x2F;CD<br>DevOps 工具链[有个图]</li>
<li>持续交付概念<br>CI&#x2F;CD pipeline</li>
<li>Serverless</li>
</ul>
</li>
</ul>
<h5><span id="12-为什么需要云原生">1.2 为什么需要云原生</span><a href="#12-为什么需要云原生" class="header-anchor">#</a></h5><p>云原生应用 VS 传统应用	☆</p>
<table>
<thead>
<tr>
<th align="center">类型</th>
<th align="center">部署可预测性</th>
<th align="center">抽象性</th>
<th align="center">弹性能力</th>
<th align="center">开发运维模式</th>
<th align="center">服务架构</th>
<th align="center">恢复能力</th>
</tr>
</thead>
<tbody><tr>
<td align="center">云原生应用</td>
<td align="center">可预测</td>
<td align="center">操作系统抽象</td>
<td align="center">弹性调度</td>
<td align="center">DevOps</td>
<td align="center">微服务解耦架构</td>
<td align="center">自动化运维快速恢复</td>
</tr>
<tr>
<td align="center">传统架构</td>
<td align="center">不可预测</td>
<td align="center">依赖操作系统</td>
<td align="center">资源冗余多 缺乏资源扩展能力</td>
<td align="center">瀑布式开发, 部门孤立</td>
<td align="center">单体耦合架构</td>
<td align="center">手动运维恢复缓慢</td>
</tr>
</tbody></table>
<h3><span id="2-在腾讯云上实现微服务架构">2. 在腾讯云上实现微服务架构</span><a href="#2-在腾讯云上实现微服务架构" class="header-anchor">#</a></h3><h5><span id="21-微服务的常见框架">2.1 微服务的常见框架</span><a href="#21-微服务的常见框架" class="header-anchor">#</a></h5><p>微服务的挑战	☆<br>微服务框架：Spring Cloud、Service Mesh	☆</p>
<ul>
<li><p>微服务的挑战</p>
<ul>
<li>分布式系统的复杂性</li>
<li>服务监控报警</li>
<li>服务治理</li>
<li>集中化配置管理</li>
<li>调用链跟踪</li>
</ul>
</li>
<li><p>微服务框架：Spring Cloud、Service Mesh</p>
<ul>
<li>Spring Cloud</li>
<li>Service Mesh<br>service mesh处理服务间请求&#x2F;响应的可靠传递，并可用于服务治理、遗留系统的零侵入接入<br>以及异构框架开发的微服务。</li>
</ul>
</li>
</ul>
<h5><span id="22-微服务的设计原则">2.2 微服务的设计原则</span><a href="#22-微服务的设计原则" class="header-anchor">#</a></h5><p>“总体原则：领域优先，聚焦对象模型，聚焦面向对象设计，减少复杂性，增加可维护性”	☆☆</p>
<ul>
<li>微服务的设计原则<ul>
<li>业务领域蓝图(DDD)<ul>
<li>领域模型， 系统核心价值所在</li>
<li>设计理念<br> 广泛使用的语言<br> 界限上下文<br> 保持改进: 基于模型驱动<br> 易于重构<br> query&#x2F;command分离</li>
<li>设计模型<br>entity object(有生命周期)&#x2F;value object(描述性)<br>aggregate roots<br>object creation patterns<br>repository<br>specification<br>domain services<br>modules<br>domain events<br>   1 系统中的重要事件<br>   2 cqrs 读写模型之间的数据同步<br>   3 事件驱动架构: 事件源, 重复领域事件<br>state machines</li>
</ul>
</li>
<li>全维度可视化<br> 服务拓扑<br> 服务监控<br> 调用链</li>
<li>全流程自动化</li>
<li>容错设计<br> 熔断器<br> 集群</li>
<li>去中心化设计<br> 提供服务扩展能力</li>
<li>独立部署</li>
</ul>
</li>
</ul>
<h5><span id="23-设计微服务架构">2.3 设计微服务架构</span><a href="#23-设计微服务架构" class="header-anchor">#</a></h5><p>包括：TSF与全维度可视化、TSF与Devops、TSF与微服务容错设计、TSF与微服务去中心化设计和TSF与独立部署	☆☆☆</p>
<ul>
<li><p>TSF与全维度可视化<br>基于调用链跟踪的可视化架构</p>
</li>
<li><p>TSF与Devops<br>全流程自动化[有个图]</p>
</li>
<li><p>TSF与微服务容错设计</p>
<ul>
<li><p>优雅的服务降级</p>
</li>
<li><p>变更管理</p>
</li>
<li><p>健康检查和负载均衡</p>
</li>
<li><p>自我修复</p>
</li>
<li><p>舱壁模式<br>隔离资源</p>
</li>
<li><p>测试故障 chaosmonkey</p>
</li>
<li><p>重试逻辑 </p>
</li>
<li><p>快速失败原则和独立性  </p>
</li>
<li><p>限流器和负载降级 - </p>
</li>
<li><p>故障转移缓存 -<br>帮助应用程序在服务故障时提供必要的数据<br>[有个图] </p>
</li>
<li><p>断路器<br>[有个图]</p>
</li>
</ul>
</li>
<li><p>TSF平台的容错设计</p>
<ul>
<li>服务注册中心</li>
<li>健康检查与负载均衡</li>
<li>重试逻辑</li>
<li>服务限流</li>
<li>断路器</li>
<li>配置管理，回滚</li>
</ul>
</li>
<li><p>TSF与微服务去中心化设计</p>
<ul>
<li>[三种方式，pic]</li>
<li>微服务工程中, 架构理念由面向系统结构转为面向服务设计</li>
</ul>
</li>
<li><p>TSF与独立部署   </p>
<ul>
<li>微服务TSF平台 在发布部署过程中基于’部署组’维度进行服务路由，完成不停服的灰度发布[pic]</li>
<li>微服务TSF平台中，容器是微服务运行的计算资源，结合使用[pic]<br>[TSF同时支持vm和container]<ol>
<li>微服务-应用层演化<br>演进方式 自上而下</li>
<li>container是资源层演化<br>演进方式 自下而上</li>
</ol>
</li>
</ul>
</li>
</ul>
<h5><span id="24-tsf微服务架构最佳实践">2.4 TSF微服务架构最佳实践</span><a href="#24-tsf微服务架构最佳实践" class="header-anchor">#</a></h5><p>微服务调用中的安全架构、技术架构	☆☆</p>
<ul>
<li><p>微服务调用中的安全架构</p>
<ul>
<li>鉴权包括两部分[pic] <ul>
<li>微服务之间的内部鉴权<br>黑白名单<br>自定义tag</li>
<li>微服务对外通过api鉴权<br>api网关</li>
</ul>
</li>
</ul>
</li>
<li><p>微服务最佳实践-技术架构 </p>
<ul>
<li>CLB, TSF, 消息队列[pic]</li>
</ul>
</li>
<li><p>分层微服务架构-组件最佳实践[pic 要看]</p>
<ul>
<li>公共服务层(稳定可靠)<br>kafka, redis, es+kibana, oauth2</li>
<li>公共基础应用微服务（高内聚，低耦合）<br>user，privilige， registry, metadata,<br>scheduler, dataflow…</li>
<li>聚合服务层(聚焦核心业务)<br>sales，order， member， shopping cart</li>
</ul>
</li>
</ul>
<h3><span id="3-在腾讯云上实现-serverless架构">3. 在腾讯云上实现 Serverless架构</span><a href="#3-在腾讯云上实现-serverless架构" class="header-anchor">#</a></h3><h5><span id="31-serverless架构的设计原则">3.1 Serverless架构的设计原则</span><a href="#31-serverless架构的设计原则" class="header-anchor">#</a></h5><p>Serverless架构组件	☆☆<br>无服务器架构与主流部署形态的对比	☆<br>Serverless面临的困难	☆</p>
<ul>
<li><p>Serverless架构介绍 </p>
<ul>
<li>是什么？</li>
<li>为什么要用？<br>没有用，不需要付费</li>
</ul>
</li>
<li><p>Serverless架构组件[pic]</p>
<ul>
<li>faas + baas</li>
<li>faas(开发者使用)<br>定义业务逻辑<br>代码部署</li>
<li>baas(云服务商提供)<br>对象存储<br>数据库<br>mq<br>身份认证<br>内容分发</li>
</ul>
</li>
<li><p>无服务器架构与主流部署形态的对比[pic 要看]</p>
<ul>
<li>kvm<br>稳态业务的首选<br>隔离环境<br>场景: 视频编码，机器学习</li>
<li>container<br>微服务的最佳载体</li>
<li>serverless<br>应用层隔离<br>事件驱动</li>
</ul>
</li>
<li><p>Serverless面临的困难</p>
<ul>
<li>场景[pic]<ul>
<li>应用后端服务</li>
<li>大规模数据处理和计算类应用</li>
<li>基于事件的内容处理类应用</li>
</ul>
</li>
<li>Serverless 架构的局限<ul>
<li>代码调试复杂</li>
<li>不适用低时延服务</li>
<li>未标准化</li>
<li>厂商绑定</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5><span id="32-在腾讯云上实现serverless架构">3.2 在腾讯云上实现Serverless架构</span><a href="#32-在腾讯云上实现serverless架构" class="header-anchor">#</a></h5><p>腾讯云函数SCF	☆☆☆<br>无服务器云函数的架构设计	☆☆☆</p>
<ul>
<li><p>腾讯云函数SCF</p>
<ul>
<li>SCF 运行方式依赖事件触发</li>
<li>SCF的工作<br>服务器 cpu memory network<br>资源维护 代码部署 弹性伸缩 负载均衡<br>安全升级 资源运行监控等</li>
<li>容错性<br>函数在多地域部署</li>
</ul>
</li>
<li><p>腾讯云函数原理[pic]</p>
<ul>
<li>分四层<ul>
<li>调用层<br>sdk web cos cmq</li>
<li>接入层<br>api接口</li>
<li>控制层<br>函数调用<br>函数配置 + 函数调度</li>
<li>执行层<br>函数实例</li>
</ul>
</li>
</ul>
</li>
<li><p>腾讯云函数主要参数[pic]<br>内存， cpu …</p>
</li>
<li><p>无服务器云函数应用案例[pic]<br>腾讯相册</p>
</li>
</ul>
<h2><span id="思考题">思考题</span><a href="#思考题" class="header-anchor">#</a></h2><ul>
<li><p>云原生关键技术<br>microservice ，container， ci&#x2F;cd, devops等</p>
</li>
<li><p>微服务可以怎么拆分？<br>根据ddd拆分</p>
</li>
<li><p>serverless架构的应用场景<br> 应用后端服务<br> 大规模数据处理和计算类应用<br> 基于事件的内容处理类应用</p>
</li>
</ul>
]]></content>
      <categories>
        <category>云计算</category>
        <category>腾讯云</category>
      </categories>
      <tags>
        <tag>认证</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 使用场景UseCase</title>
    <url>/www6vHomeHexo/2021/06/29/redisUseCase/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ul>
<li>缓存</li>
<li>分布式锁</li>
<li>计数器</li>
<li>限流</li>
<li>购物车</li>
<li>用户消息时间线timeline</li>
<li>消息队列</li>
<li>点赞、签到、打卡</li>
<li>商品标签</li>
<li>用户关注、推荐模型</li>
<li>排行榜</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://blog.csdn.net/gp_911014/article/details/124744869">Redis 16 个常见的使用场景</a></p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>重构</title>
    <url>/www6vHomeHexo/2021/06/28/refactor/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="bad-smell">Bad Smell</span><a href="#bad-smell" class="header-anchor">#</a></h2><h2><span id="refactor">Refactor</span><a href="#refactor" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://refactoring.guru/refactoring/smells">Code Smells</a> ***<br><a href="https://refactoring.guru/refactoring/techniques">Refactor</a> ***</p>
]]></content>
      <categories>
        <category>架构</category>
        <category>重构</category>
      </categories>
      <tags>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云-云迁移Migrate</title>
    <url>/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/process.png" class title="迁云流程">

<h2><span id="系统调研">系统调研</span><a href="#系统调研" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/survey.png" class title="系统调研">

<h2><span id="解决方案和选型">解决方案和选型</span><a href="#解决方案和选型" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/choose.png" class>

<img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/solution.png" class>

<img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/details.png" class>

<h2><span id="迁移类型">迁移类型</span><a href="#迁移类型" class="header-anchor">#</a></h2><ul>
<li>服务器迁移<ul>
<li>VM环境</li>
<li>物理机环境</li>
<li>操作系统镜像</li>
</ul>
</li>
<li>存储迁移<ul>
<li>文件系统</li>
<li>对象存储</li>
<li>事务型</li>
<li>归档</li>
</ul>
</li>
<li>数据库迁移<ul>
<li>离线</li>
<li>在线</li>
<li>异构&#x2F;同构</li>
</ul>
</li>
</ul>
<h2><span id="服务器迁移-1">服务器迁移 [1]</span><a href="#服务器迁移-1" class="header-anchor">#</a></h2><ul>
<li>云官方迁移工具<ul>
<li>快捷方便</li>
<li>要公网</li>
</ul>
</li>
<li>镜像导入<ul>
<li>无需公网</li>
<li>自定义配置</li>
</ul>
</li>
<li>镜像工具<ul>
<li>无需公网</li>
<li>步骤简单</li>
<li>模板配置复杂</li>
</ul>
</li>
</ul>
<h2><span id="数据库迁移-2">数据库迁移 [2]</span><a href="#数据库迁移-2" class="header-anchor">#</a></h2><h5><span id="注意事项">注意事项</span><a href="#注意事项" class="header-anchor">#</a></h5><ul>
<li>对业务的影响</li>
<li>RTO&#x2F;RPO</li>
<li>数据库复杂程度</li>
<li>应用程序调用复杂程度</li>
<li>迁移的技能</li>
</ul>
<h5><span id="迁移工具">迁移工具</span><a href="#迁移工具" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2021/06/27/aliyunCloudMigrate/dbMigrate.png" class>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.bilibili.com/video/BV15y4y1a7ds">阿里云云上常见架构设计及优化-课时1：迁移上云架构设计及解决方案</a></li>
<li><a href="/www6vHomeHexo/2022/04/11/dbMigrate/" title="数据库迁移">数据库迁移</a> self</li>
<li><a href="/www6vHomeHexo/2022/06/25/kvm/" title="KVM 虚拟机">KVM 虚拟机</a>  self  KVM动态迁移</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis Rehash机制</title>
    <url>/www6vHomeHexo/2021/06/20/redisRehash/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E8%A7%A6%E5%8F%91-rehash">什么时候触发 rehash？</a></li>
<li><a href="#rehash-%E6%89%A9%E5%AE%B9%E6%89%A9%E5%A4%9A%E5%A4%A7">rehash 扩容扩多大？</a></li>
<li><a href="#rehash-%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C">rehash 如何执行？</a></li>
<li><a href="#dictrehash-%E7%9A%84%E4%B8%BB%E8%A6%81%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B">dictRehash 的主要执行流程</a></li>
</ul>
<!-- tocstop -->

</div>

<h3><span id="什么时候触发-rehash">什么时候触发 rehash？</span><a href="#什么时候触发-rehash" class="header-anchor">#</a></h3><p>，_dictExpandIfNeeded 函数中定义了三个扩容条件。<br>下面的代码就展示了 _dictExpandIfNeeded 函数对这三个条件的定义，你可以看下。<br>条件一：ht[0]的大小为 0。<br>条件二：ht[0]承载的元素个数已经超过了 ht[0]的大小，同时 Hash 表可以进行扩容。<br>条件三：ht[0]承载的元素个数，是 ht[0]的大小的 dict_force_resize_ratio 倍，其中，<br><strong>dict_force_resize_ratio 的默认值是 5</strong>。</p>
<h3><span id="rehash-扩容扩多大">rehash 扩容扩多大？</span><a href="#rehash-扩容扩多大" class="header-anchor">#</a></h3><p>对 Hash表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，<strong>那么就将表扩容到</strong> size*2 <strong>的大小</strong>。</p>
<h3><span id="rehash-如何执行">rehash 如何执行？</span><a href="#rehash-如何执行" class="header-anchor">#</a></h3><p>其实这是因为，Hash 表在执行 rehash 时，由于 Hash 表空间扩大，原本映射到某一位置<br>的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位<br>置。<strong>而在键拷贝时，由于 Redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这<br>样就会产生 rehash 开销。</strong></p>
<p><strong>而为了降低 rehash 开销，Redis 就提出了渐进式 rehash 的方法。</strong></p>
<h3><span id="dictrehash-的主要执行流程">dictRehash 的主要执行流程</span><a href="#dictrehash-的主要执行流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2021/06/20/redisRehash/rehash.png" class title="Rehash">]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>OLAP 在线分析处理</title>
    <url>/www6vHomeHexo/2021/06/15/olap/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="存储">存储</span><a href="#存储" class="header-anchor">#</a></h1><ul>
<li>Apache Kylin - MOLAP， 预计算<br>hadoop体系</li>
<li>Apache Doris </li>
<li>Clickhouse - ROLAP</li>
<li>Apache Kudu</li>
<li>Iceberg<br>tables</li>
</ul>
<h1><span id="查询引擎">查询引擎</span><a href="#查询引擎" class="header-anchor">#</a></h1><ul>
<li>Presto</li>
<li>Impala</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://blog.csdn.net/qq_39945938/article/details/114013486">1.OLAP 技术选型 Apache Kylin、Apache Doris、Clickhouse对比</a><br><a href="https://cloud.tencent.com/developer/article/1477234">Apache Kylin VS Apache Doris全方位对比</a><br><a href="https://www.zhihu.com/question/303991599/answer/1555185339">有人说下kudu,kylin,druid,clickhouse的区别,使用场景么?</a><br><a href="https://blog.csdn.net/u013256816/article/details/108271371">Kylin、Druid、ClickHouse核心技术对比</a><br><a href="https://blog.csdn.net/qq_43376286/article/details/117953122">Kylin基础</a></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>olap</category>
      </categories>
      <tags>
        <tag>olap</tag>
      </tags>
  </entry>
  <entry>
    <title>前端技术</title>
    <url>/www6vHomeHexo/2021/06/13/frontend/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://github.com/liyupi/code-roadmap/blob/main/docs/roadmap/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.md">前端学习路线</a><br><a href="https://objtube.gitee.io/front-end-roadmap/#/">前端学习路线</a></p>
]]></content>
      <categories>
        <category>前端技术</category>
      </categories>
      <tags>
        <tag>前端技术</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase Hotkey-预分区和Rowkey设计</title>
    <url>/www6vHomeHexo/2021/06/07/hbaseHotkey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%9B%AE%E6%A0%87">目标</a></li>
<li><a href="#rowkey%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%8E%9F%E5%88%99">Rowkey设计的原则</a></li>
<li><a href="#row_key%E8%AE%BE%E8%AE%A1-%E9%98%B2%E6%AD%A2%E7%83%AD%E7%82%B9%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95">Row_key设计 防止热点的三种方法</a><ul>
<li><a href="#hash-%E9%80%9A%E7%94%A8">Hash (通用)</a></li>
<li><a href="#reversing">Reversing</a></li>
<li><a href="#salt">Salt</a></li>
</ul>
</li>
<li><a href="#hbase-sdk%E4%B8%8E%E9%A2%84%E5%88%86%E5%8C%BA-%E7%9B%B8%E5%85%B3%E7%9A%84%E5%8F%82%E6%95%B0">HBase SDK与预分区 相关的参数</a></li>
<li><a href="#%E5%88%86%E5%8C%BA%E6%95%B0%E5%BB%BA%E8%AE%AE">分区数建议</a></li>
<li><a href="#%E5%8F%AF%E9%85%8D%E7%BD%AE%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0">可配置的分区数</a></li>
<li><a href="#%E5%B7%B2%E6%9C%89%E8%A1%A8%E8%B4%9F%E8%BD%BD%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%9A%84%E8%A1%A8%E8%BF%81%E7%A7%BB%E5%88%B0%E6%96%B0%E7%9A%84%E8%A1%A8-%E6%9A%82%E6%97%B6%E4%B8%8D%E5%81%9A">已有表（负载不均衡的表）迁移到新的表-暂时不做</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="目标">目标</span><a href="#目标" class="header-anchor">#</a></h1><p>Region自动split有<strong>数据倾斜</strong>问题，所以要<strong>预分区</strong>。</p>
<p>热的Region会有更多的读&#x2F;写请求流入， 造成RegionServer的不稳定和更多的资源消耗（eg. gc 时间长）。</p>
<h1><span id="rowkey设计的原则">Rowkey设计的原则</span><a href="#rowkey设计的原则" class="header-anchor">#</a></h1><ul>
<li>散列性</li>
<li>集中性 </li>
<li>唯一性</li>
</ul>
<h1><span id="row_key设计-防止热点的三种方法">Row_key设计 防止热点的三种方法</span><a href="#row_key设计-防止热点的三种方法" class="header-anchor">#</a></h1><h3><span id="hash-通用">Hash (通用)</span><a href="#hash-通用" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">十进制_年月日小时分秒_订单号</span><br><span class="line">xx_20200101010101_364758  </span><br><span class="line">xx_20200101010101_585775</span><br><span class="line">xx_20200101010101_385748  </span><br><span class="line"></span><br><span class="line">3个分区</span><br><span class="line">xx = hash（年月日小时分秒，订单号）% 分区数</span><br></pre></td></tr></table></figure>

<h3><span id="reversing">Reversing</span><a href="#reversing" class="header-anchor">#</a></h3><p><strong>倒序时间戳</strong></p>
<p>一个数据库处理的通常问题是找到最近版本的值。采用倒序时间戳作为键的一部分可以对此特定情况有很大帮助。该技术包含追加( Long.MAX_VALUE - timestamp ) 到key的后面，如 [key][reverse_timestamp] 。</p>
<p><strong>表内[key]的最近的值可以用[key]进行Scan，找到并获取第一个记录。</strong>由于HBase行键是排序的，该键排在任何比它老的行键的前面，所以是第一个。</p>
<p>该技术可以用于代替版本数，其目的是保存所有版本到“永远”(或一段很长时间) 。<strong>同时，采用同样的Scan技术，可以很快获取其他版本。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Rowkey: reverse(order_id) + (MAX_LONG – timestamp)</span><br><span class="line">Columns: 该订单各种状态</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021010101010101 -&gt; 101010101010101202</span><br><span class="line">20210202020202 -&gt; 2020202020201202</span><br><span class="line">20210303030303 -&gt; 3030303030301202</span><br></pre></td></tr></table></figure>

<h3><span id="salt">Salt</span><a href="#salt" class="header-anchor">#</a></h3><ul>
<li><p>对get、put友好，对scan操作并不友好</p>
</li>
<li><p>有热点就要打散，但打散就难以做范围查询。因此，要同时满足这对相互矛盾的需求，<strong>必须有一种折中的方案：既能在一定程度上打散，又能保证一定程度的有序。</strong></p>
</li>
</ul>
<p>这个解决方案就是加盐，其实叫分桶(salt buckets)更准确。<strong>数据在桶内保序，桶之间随机。</strong></p>
<p>写入时按桶个数取模，数据随机落在某个桶里，保证写请求在桶之间是均衡的。查询时读取所有的桶来保证结果集的有序和完备。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">加盐的过程就是在原来key的基础上增加一个byte作为前缀,计算公式如下：</span><br><span class="line">new_row_key = ((byte) (hash(key) % BUCKETS_NUMBER) + original_key</span><br></pre></td></tr></table></figure>

<img src="/www6vHomeHexo/2021/06/07/hbaseHotkey/h-1.png" class>

<h1><span id="hbase-sdk与预分区-相关的参数">HBase SDK与预分区 相关的参数</span><a href="#hbase-sdk与预分区-相关的参数" class="header-anchor">#</a></h1><ul>
<li>其中 NUMREGIONS 为 <strong>region的个数</strong>，一般按<strong>每个region 6~8GB左右</strong>来计算region数量，集群规模大，region数量可以适当取大一些</li>
<li>SPLITALGO 为 rowkey分割的算法：Hbase自带了三种pre-split的算法，分别是 HexStringSplit、DecimalStringSplit 和 UniformSplit。</li>
</ul>
<p>各种Split算法适用场景：</p>
<ul>
<li><strong>HexStringSplit</strong>: rowkey是<strong>十六进制</strong>的字符串作为前缀的 （MD5编码为16进制的rowkey） </li>
<li><strong>DecimalStringSplit</strong>: rowkey是<strong>10进制</strong>数字字符串作为前缀的</li>
<li>UniformSplit: rowkey前缀完全随机 （hash算法转换为字节数组的rowkey）</li>
</ul>
<h1><span id="分区数建议">分区数建议</span><a href="#分区数建议" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>表</th>
<th>分区数建议，表大小</th>
</tr>
</thead>
<tbody><tr>
<td>小表（&lt;50G）</td>
<td>5～10个分区</td>
</tr>
<tr>
<td>大表（500G～10T）</td>
<td>表的大小&#x2F;10G, 最大120个分区<br> eg. 1T&#x2F;10G &#x3D; 100个分区</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>吞吐</th>
<th>有sacn需求</th>
</tr>
</thead>
<tbody><tr>
<td>吞吐高， 分区数相应多<br>吞吐低， 分区数相应低</td>
<td>分区要控制在一定范围</td>
</tr>
</tbody></table>
<h1><span id="可配置的分区数">可配置的分区数</span><a href="#可配置的分区数" class="header-anchor">#</a></h1><ol>
<li>Portal里可以配置hbase 表的分区数， 配置的值存储在zk中。</li>
<li>在建立表的时候读取预先存储好的分区数， 做到建表时自动分区。</li>
<li>sdk 里读取预先存储好的分区数，保证负载均匀的写入Region。</li>
</ol>
<h1><span id="已有表负载不均衡的表迁移到新的表-暂时不做">已有表（负载不均衡的表）迁移到新的表-暂时不做</span><a href="#已有表负载不均衡的表迁移到新的表-暂时不做" class="header-anchor">#</a></h1><p>  老的表的rowkey清洗一遍存入新的表中。</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th>put，get</th>
<th>scan</th>
<th>通用性</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Salt</strong></td>
<td>友好</td>
<td>不友好<br>多个Re<strong>gion的多路归并算法</strong></td>
<td>通用</td>
<td>SDK（方式2-salt）</td>
</tr>
<tr>
<td><strong>Reversing</strong></td>
<td>友好</td>
<td>友好<br>相同rowkey的会存储在一起</td>
<td>和业务相关</td>
<td>SDK（方式3-自定义）</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/menghuannvxia/article/details/53842320">HBase Rowkey的散列与预分区设计</a></li>
<li><a href="http://www.inter12.org/archives/1192">滴滴hbase常见的rowkey设计</a></li>
<li><a href="https://sematext.com/blog/hbasewd-avoid-regionserver-hotspotting-despite-writing-records-with-sequential-keys/">HBaseWD: Avoid RegionServer Hotspotting Despite Sequential Keys</a> *** 代码</li>
<li><a href="https://help.aliyun.com/document_detail/59035.htm?spm=a2c4g.11186623.2.2.500e620cMI9Ks1">Rowkey设计</a> 阿里云</li>
<li><a href="https://help.aliyun.com/document_detail/71787.html">预分区</a> 阿里云</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringCloud</title>
    <url>/www6vHomeHexo/2021/06/06/springCloud/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/crazymakercircle/p/13900212.html">SpringCloud 面试题 （持续更新、吐血推荐）</a></p>
]]></content>
      <categories>
        <category>中间件</category>
        <category>spring</category>
        <category>springcloud</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes安装-kubeasz</title>
    <url>/www6vHomeHexo/2021/06/02/k8sDeploy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(ansible control machine)  </span><br><span class="line">st-arch-renault-redis-1  </span><br><span class="line"> </span><br><span class="line">-集群节点</span><br><span class="line">work-arch-renault-portal-1    10.100.140.227   master</span><br><span class="line">work-arch-renault-portal-2    10.100.140.228.  master</span><br><span class="line">work-arch-renault-redis-1     10.100.140.229   node</span><br><span class="line">work-arch-renault-redis-2     10.100.140.230.  node</span><br><span class="line">work-arch-renault-redis-3     10.100.140.231.  node</span><br><span class="line">work-arch-renault-redis-4     10.100.140.232.  node</span><br><span class="line">work-arch-kv-redis-1          172.16.28.92     append</span><br><span class="line">work-arch-kv-redis-2          172.16.28.93     append</span><br><span class="line">work-arch-kv-redis-3          172.16.28.94     append	</span><br></pre></td></tr></table></figure>

<h2><span id="新建集群-2">新建集群 [2]</span><a href="#新建集群-2" class="header-anchor">#</a></h2><h5><span id="安装k8s包到本地目录-x2fbin-x2fdown">安装k8s包到本地目录 &#x2F;bin &#x2F;down</span><a href="#安装k8s包到本地目录-x2fbin-x2fdown" class="header-anchor">#</a></h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> release=2.2.4</span>  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -C- -fLO --retry 3 https://github.com/easzlab/kubeasz/releases/download/<span class="variable">$&#123;release&#125;</span>/easzup</span>  </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> +x ./easzup</span>  </span><br><span class="line"><span class="meta prompt_">  </span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./easzup -D</span>  </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./easzup -S</span>	  </span><br></pre></td></tr></table></figure>


<h5><span id="step1">step1</span><a href="#step1" class="header-anchor">#</a></h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CentOS7   </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install git python-pip -y</span>    </span><br><span class="line"></span><br><span class="line">-pip安装ansible(国内如果安装太慢可以直接用pip阿里云加速)   </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install pip --upgrade -i https://mirrors.aliyun.com/pypi/simple/</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install ansible==2.6.18 netaddr==0.7.19 -i https://mirrors.aliyun.com/pypi/simple/</span> </span><br></pre></td></tr></table></figure>


<h5><span id="step2-vim-x2fetcx2fansiblex2fhosts">step2 - vim &#x2F;etc&#x2F;ansible&#x2F;hosts</span><a href="#step2-vim-x2fetcx2fansiblex2fhosts" class="header-anchor">#</a></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[etcd]</span><br><span class="line">10.100.140.229 NODE_NAME=etcd1</span><br><span class="line">10.100.140.230 NODE_NAME=etcd2</span><br><span class="line">10.100.140.231 NODE_NAME=etcd3</span><br><span class="line"></span><br><span class="line">[kube-master]</span><br><span class="line">10.100.140.227</span><br><span class="line">10.100.140.228</span><br><span class="line"></span><br><span class="line">[kube-node]</span><br><span class="line">10.100.140.229</span><br><span class="line">10.100.140.230</span><br><span class="line">10.100.140.231</span><br><span class="line">10.100.140.232</span><br><span class="line">10.100.140.233  ---</span><br><span class="line">10.100.140.234  ---</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5><span id="step3">step3</span><a href="#step3" class="header-anchor">#</a></h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 01.prepare.yml</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 02.etcd.yml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 03.docker.yml</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 04.kube-master.yml</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 05.kube-node.yml</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 06.network.yml</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ansible-playbook 07.cluster-addon.yml</span> </span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2><span id="扩展node-3">扩展node [3]</span><a href="#扩展node-3" class="header-anchor">#</a></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./tools/easzctl add-node 172.16.28.92</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./tools/easzctl add-node 172.16.28.93</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./tools/easzctl add-node 172.16.28.94</span></span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/easzlab/kubeasz/blob/master/docs/setup/00-planning_and_overall_intro.md">https://github.com/easzlab/kubeasz/blob/master/docs/setup/00-planning_and_overall_intro.md</a>   这个已更新，不能参考这个</li>
<li><a href="https://github.com/www6v/kubeasz/blob/tuhu-2.2.4/docs/setup/00-planning_and_overall_intro.md">https://github.com/www6v/kubeasz/blob/tuhu-2.2.4/docs/setup/00-planning_and_overall_intro.md</a>   参考这个</li>
<li><a href="https://github.com/www6v/kubeasz/blob/tuhu-2.2.4/docs/op/op-node.md">https://github.com/www6v/kubeasz/blob/tuhu-2.2.4/docs/op/op-node.md</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 回收策略</title>
    <url>/www6vHomeHexo/2021/06/02/redisDelete/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="回收策略">回收策略</span><a href="#回收策略" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">回收策略</th>
<th align="center">redis</th>
<th align="center">kafka</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于时间</td>
<td align="center">过期删除策略 <br>1. 定时删除(对内存最友好， 对CPU时间最不友好) <br>2. 惰性删除(对CPU时间最友好， 对内存最不友好) <br>3.定期删除(整合和折中)</td>
<td align="center"><img src="https://user-images.githubusercontent.com/5608425/66014513-cab37f80-e501-11e9-9b2c-917838d91a4d.png" alt="kafka-time"></td>
</tr>
<tr>
<td align="center">基于大小</td>
<td align="center">内存淘汰策略 <br>1. noeviction <br>2.lru <br>3. random <br>4. ttl</td>
<td align="center"><img src="https://user-images.githubusercontent.com/5608425/66014512-ca1ae900-e501-11e9-93d7-840409a862c5.png" alt="kafka-size"></td>
</tr>
<tr>
<td align="center">其他</td>
<td align="center">x</td>
<td align="center"><img src="https://user-images.githubusercontent.com/5608425/66014514-cab37f80-e501-11e9-9be8-a247690b5f9f.png" alt="kafka-offset"></td>
</tr>
</tbody></table>
<ul>
<li>近似LRU算法[11]</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="7">
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&mid=2650780240&idx=1&sn=49fb636a97a3c21fec7d2e2b59bea09f">七问Redis，才知道我与技术大牛的差距在哪里 </a> ***</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/gkkjJu04sS2qtRdd-yB5DQ">经典面试题：Redis 内存满了怎么办？</a></p>
</li>
</ol>
<h3><span id="回收策略">回收策略</span><a href="#回收策略" class="header-anchor">#</a></h3><ol start="10">
<li><a href="http://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655826994&idx=2&sn=c7efe2b7cdd350f1b3c6fb72cc8c1cd7">Redis内存回收机制，把我整懵了…</a></li>
<li><a href="https://blog.csdn.net/u013256816/article/details/80418297">Kafka日志清理之Log Deletion</a></li>
<li>《Redis 开发与运维》  8.2.3  未</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S 弃用Docker</title>
    <url>/www6vHomeHexo/2021/06/01/k8sAbandonDocker/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="1">1.</span><a href="#1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2021/06/01/k8sAbandonDocker/img1.png" class title="CRI">


<p>如果以 kubelet 调用 CRI 为起点，OCI 的 runC 调用为终点，三种模式经历的可执行程序分别是：</p>
<ul>
<li>dockershim 模式：dockershim(*)-&gt;dockd-&gt;containerd-&gt;containerd-shim</li>
<li>cri-containerd 模式：cri-containerd(*)-&gt; containerd-&gt;containerd-shim</li>
<li>cri-o 模式：cri-o</li>
</ul>
<p>K8S弃用Docker是指弃用Docker daemon<br>RedHat 推崇的 cri-o  → Podman</p>
<h3><span id="2">2.</span><a href="#2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2021/06/01/k8sAbandonDocker/img2.jpeg" class title="CRI">

<p>容器引擎可选的替代方案大概有以下几种：</p>
<ul>
<li>containerd </li>
<li>CRI-O  → Podman, Red Hat</li>
<li>gVisor  → “guest 内核”层</li>
<li>kata-containers  →  安全</li>
<li>Nabla</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.infoq.cn/article/VasFWOChD6JoL5avhfAz">https://www.infoq.cn/article/VasFWOChD6JoL5avhfAz</a><br><a href="https://www.zhihu.com/question/433184969">https://www.zhihu.com/question/433184969</a></p>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库  关联查询</title>
    <url>/www6vHomeHexo/2021/05/30/distributedDatabaseJoinQuery/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2021/05/30/distributedDatabaseJoinQuery/join.jpg" class>

<h2><span id="tidb-的-join-算法25">TiDB 的 Join 算法[2][5]</span><a href="#tidb-的-join-算法25" class="header-anchor">#</a></h2><ul>
<li><p>TiDB 的 Join 算法包括如下几类：</p>
<ul>
<li>Hash Join</li>
<li>Merge Join</li>
<li>Index Hash Join</li>
<li>Index Merge Join</li>
</ul>
</li>
<li><p>TiDB 目前表 Join 的方式 [3][4]</p>
<ul>
<li>Sort Merge Join</li>
<li>Index Nested Loop Join</li>
<li>Hash Join</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《20 | 关联查询：如何提升多表Join能力？ 》</li>
<li><a href="https://cn.pingcap.com/blog/tidb-query-optimization-and-tuning-2">TiDB 查询优化及调优系列（二）TiDB 查询计划简介</a>  <strong>查看 Join 的执行计划</strong></li>
<li><a href="https://cn.pingcap.com/blog/?tag=TiDB%20%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB">TiDB 源码阅读系列</a></li>
<li><a href="https://cn.pingcap.com/blog/tidb-query-optimization-and-tuning-4">TiDB 查询优化及调优系列（四）查询执行计划的调整及优化原理</a></li>
<li><a href="https://tidb.net/blog/cf459d89">JOIN 查询的执行计划 比较</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-Code</title>
    <url>/www6vHomeHexo/2021/05/28/gptPromptCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E5%85%B7">工具</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%AE%80%E5%8D%95%E4%BB%BB%E5%8A%A1-1">代码相关-简单任务 [1]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%B9%81%E7%90%90%E5%B7%A5%E4%BD%9C-1">代码相关- 繁琐工作 [1]</a></li>
<li><a href="#%E8%BF%90%E7%BB%B4-ops-4">运维 Ops [4]</a><ul>
<li><a href="#%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80-vs-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80">编程语言 vs 自然语言</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="工具">工具</span><a href="#工具" class="header-anchor">#</a></h1><ul>
<li>Copilot *** - 收费</li>
<li>AWS CodeWhispter</li>
<li>Cursor ***</li>
<li>tabnine 免费</li>
<li>Code Llama  - 开源</li>
</ul>
<h1><span id="代码相关-简单任务-1">代码相关-简单任务 [1]</span><a href="#代码相关-简单任务-1" class="header-anchor">#</a></h1><ul>
<li><p><strong>注释</strong><br> 你作为一名程序员，请解释一下下面这段代码</p>
</li>
<li><p><strong>防御性编程</strong><br> 请为这段代码增加防御性编程的功能</p>
</li>
<li><p>写单元测试 </p>
</li>
<li><p><strong>时间复杂度  time complexity</strong><br> 这段代码的时间复杂度是多少</p>
</li>
<li><p>流程图<br> 画出redis master和slave之间同步的流程图</p>
</li>
<li><p>Writing shell script</p>
</li>
<li><p>Writing git commands<br>一个分支中的代码合并到另一个分支中</p>
</li>
<li><p><strong>Improve code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  How do i improve this code?</span><br><span class="line">  fruits = [<span class="string">&quot;apple&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;cherry&quot;</span>]</span><br><span class="line">  newlist = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> fruits:</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&quot;a&quot;</span> <span class="keyword">in</span> x:</span><br><span class="line">    newlist.append(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(newlist)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Translating Code</strong> 代码转换</p>
<ul>
<li>Convert this Python code to Javascript    </li>
<li>请把下面这段python代码转换成Java代码</li>
</ul>
</li>
</ul>
<h1><span id="代码相关-繁琐工作-1">代码相关- 繁琐工作 [1]</span><a href="#代码相关-繁琐工作-1" class="header-anchor">#</a></h1><ul>
<li><p>Building API</p>
<ul>
<li>I need an API built with express.js to return the list of products. Each product should have attributes like ID, title, description, price and imageUrl</li>
<li>modify the code and  retrieve the products from a MongoDB database</li>
<li>use TypeScript in this code</li>
<li>Generate this API using Python and FastAPI</li>
</ul>
</li>
<li><p><strong>Generating Dummy Data</strong></p>
<ul>
<li>Generate dummy data for a table called customers. Each customer should have an ID, first name, last name and city.</li>
<li>I don’t need a Javascript. Just give the data.</li>
<li>Create a Python class for storing these objects.</li>
</ul>
</li>
<li><p><strong>SQL</strong></p>
<ul>
<li>write a SQL query to generate a table called products with these columns：<br>ID（int）<br>title（string）<br>category(int)</li>
<li>write a query to retrieve the top 5 customers in Shanghai</li>
<li>Revise this query and join the customers table with the orders table to find out how much each cumster has spent. Then pick the top 5 who have spent the most.</li>
</ul>
</li>
<li><p>正则  [2]</p>
</li>
<li><p>CronJob [2]</p>
</li>
<li><p>K8s</p>
</li>
</ul>
<h1><span id="运维-ops-4">运维 Ops [4]</span><a href="#运维-ops-4" class="header-anchor">#</a></h1><h2><span id="编程语言-vs-自然语言">编程语言 vs 自然语言</span><a href="#编程语言-vs-自然语言" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>语言类型</th>
<th>执行原理</th>
</tr>
</thead>
<tbody><tr>
<td>C++语言</td>
<td>C++语言 –&gt; 编译器&#x2F;链接器 –&gt; 既定任务</td>
</tr>
<tr>
<td>Java语言</td>
<td>Java语言 –&gt; 编译器&#x2F;虚拟机 –&gt; 既定任务</td>
</tr>
<tr>
<td>Python语言</td>
<td>Python语言 –&gt; 解释器 –&gt; 既定任务</td>
</tr>
<tr>
<td>人类自然语言</td>
<td>人类自然语言 –&gt; LLMs –&gt; 各种后端组件 –&gt; 既定任务</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1Z84y1G7nY/">【ChatGPT】面向程序员的ChatGPT使用教程38种方式来提升生产力</a> V</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a><br>03, 04, 06<br><a href="https://github.com/www6v/AICoder">AICoder</a> git</li>
<li><a href="https://cloud.tencent.com/developer/article/2207540">ChatGPT 帮我跑了一个完整的 DevOps 流水线，离了个大谱…</a><br><a href="https://github.com/www6v/AICoder/tree/master/Cursor/">Gin on K8s</a> git</li>
<li><a href="https://www.promptops.com/">PromptOps</a>    </li>
<li><a href="https://www.geeksforgeeks.org/chatgpt-prompts-for-software-developers/">Top 20 ChatGPT Prompts For Software Developers</a> 未</li>
</ol>
<pre><code>

</code></pre>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-How to use</title>
    <url>/www6vHomeHexo/2021/05/26/gptPrompt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B9%94%E5%93%88%E9%87%8C%E6%B2%9F%E9%80%9A%E8%A7%86%E7%AA%97-4-%E8%B1%A1%E9%99%90">乔哈里沟通视窗-4 象限</a><ul>
<li><a href="#%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93gpt%E7%9F%A5%E9%81%93">你不知道，GPT知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B9%9F%E7%9F%A5%E9%81%93">你知道，GPT也知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B8%8D%E7%9F%A5%E9%81%93">你知道，GPT不知道</a></li>
<li><a href="#%E4%BD%A0%E5%92%8Cgpt%E9%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93">你和GPT都不知道</a></li>
</ul>
</li>
<li><a href="#%E8%BE%BE%E5%85%8B%E6%95%88%E5%BA%94">达克效应</a><ul>
<li><a href="#%E6%A3%80%E9%AA%8C%E8%87%AA%E5%B7%B1%E8%AE%A4%E7%9F%A5%E8%83%BD%E5%8A%9B%E6%B0%B4%E5%B9%B3%E6%8F%90%E9%97%AE%E5%8F%A5%E5%BC%8F">检验自己认知&#x2F;能力水平提问句式</a></li>
</ul>
</li>
<li><a href="#%E7%9F%A5%E9%81%93%E5%81%9A%E5%88%B0">知道做到</a></li>
<li><a href="#%E8%A7%92%E8%89%B2%E5%85%B3%E7%B3%BB">角色关系</a></li>
<li><a href="#%E9%80%9A%E7%94%A8">通用</a><ul>
<li><a href="#%E6%B2%9F%E9%80%9A%E6%A8%A1%E5%BC%8F">沟通模式</a></li>
<li><a href="#%E5%BD%92%E7%BA%B3">归纳</a></li>
<li><a href="#%E6%80%9D%E7%BB%B4%E9%93%BE">思维链</a></li>
<li><a href="#%E6%B2%A1%E5%95%A5%E7%94%A8">没啥用</a></li>
</ul>
</li>
<li><a href="#prompt-how-to-use">Prompt - How to use</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="乔哈里沟通视窗-4-象限">乔哈里沟通视窗-4 象限</span><a href="#乔哈里沟通视窗-4-象限" class="header-anchor">#</a></h1><h3><span id="你不知道gpt知道">你不知道，GPT知道</span><a href="#你不知道gpt知道" class="header-anchor">#</a></h3><p>1、元问题：我想了解xxxx，我应该向你问哪些问题？<br>2、请给我列出xxx领域&#x2F;行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。<br>3、请详细介绍一下elon musk的主要生平事迹。请详细介绍一下tesla这家企业的发展历程。</p>
<h3><span id="你知道gpt也知道">你知道，GPT也知道</span><a href="#你知道gpt也知道" class="header-anchor">#</a></h3><p>检验认知：<br>1、对于xxx主题&#x2F;技能，你认为哪些是我必须理解和掌握的核心要点？<br>2、我理解的xxx是这样的，你觉得我的理解对吗？<br>3、我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？<br>4、我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？</p>
<p>扩充认知：<br>1、我知道xxx的概念，我想知道更多关于xxx的信息。<br>2、我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？<br>3、我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？<br>4、我想要在xxx领域有所创新，你能提供一些启发或想法吗？<br>5、我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？<br>6、我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？<br>7、（背景信息xxx），我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。<br>8、我是一个xx新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？</p>
<h3><span id="你知道gpt不知道">你知道，GPT不知道</span><a href="#你知道gpt不知道" class="header-anchor">#</a></h3><p>介绍背景现象之后可以向gpt发问，你怎么看待这种现象？可能的原因有哪些？这会对xxx产生什么样的影响？你觉得xxx应该怎么做？</p>
<h3><span id="你和gpt都不知道">你和GPT都不知道</span><a href="#你和gpt都不知道" class="header-anchor">#</a></h3><p>如果xxx，这对社会会产生什么影响？</p>
<h1><span id="达克效应">达克效应</span><a href="#达克效应" class="header-anchor">#</a></h1><h3><span id="检验自己认知x2f能力水平提问句式">检验自己认知&#x2F;能力水平提问句式</span><a href="#检验自己认知x2f能力水平提问句式" class="header-anchor">#</a></h3><p>1、为了测试我对xxx的理解程度，你会问我什么问题来检验我的水平，最少10个。<br>2、我是xx领域的专家，你会问我哪些问题来检验我的专业水平？<br>3、追问一句，这些我都懂，还有更专业更细更深的问题吗？<br>4、你问我答的游戏</p>
<p>扩展自己能力边界的提问句式我已经很精通xxx了，我想知道我是否还有需要学习的地方？然后不停的问，还有呢还有呢？</p>
<h1><span id="知道做到">知道做到</span><a href="#知道做到" class="header-anchor">#</a></h1><p>让GPT完成具体任务<br>1、我想做xxx，你能给我提供什么帮助？<br>2、我想要你做xxx，我应该给你输入什么信息？<br>3、直接下指令</p>
<h1><span id="角色关系">角色关系</span><a href="#角色关系" class="header-anchor">#</a></h1><ul>
<li>模拟虚拟人物</li>
<li>模拟名人</li>
<li>模拟一段关系</li>
<li>模拟多个具体的人</li>
<li>模拟多类人</li>
</ul>
<h1><span id="通用">通用</span><a href="#通用" class="header-anchor">#</a></h1><h3><span id="沟通模式">沟通模式</span><a href="#沟通模式" class="header-anchor">#</a></h3><p>prompt &#x3D;  定义角色+背景信息+任务目标+输出要求</p>
<h3><span id="归纳">归纳</span><a href="#归纳" class="header-anchor">#</a></h3><ul>
<li>使用markdown格式写富爸爸穷爸爸的思维导图，以代码格式输出</li>
<li>以脑图的方式归纳上文</li>
<li>请用提纲的方式来归纳上文</li>
<li>请用简练的列提纲的方式来归纳上文</li>
</ul>
<h3><span id="思维链">思维链</span><a href="#思维链" class="header-anchor">#</a></h3><h3><span id="没啥用">没啥用</span><a href="#没啥用" class="header-anchor">#</a></h3><p>请用简单的语句来归纳上文，归纳的语句可以生成脑图<br>请用金字塔思维的方式来简单的归纳上文</p>
<h1><span id="prompt-how-to-use">Prompt - How to use</span><a href="#prompt-how-to-use" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://learnprompting.org/zh-Hans/docs/intro">Learn Prompting</a> *** **</p>
</li>
<li><p><a href="https://www.aishort.top/">Chatgpt ShortCut</a><br><a href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT Shortcut </a></p>
</li>
<li><p><a href="https://prompts.chat/"> Awesome ChatGPT Prompts</a>  </p>
</li>
<li><p><a href="https://snackprompt.com/">snackprompt.com</a> ***</p>
</li>
<li><p><a href="https://flowgpt.com/">flowgpt</a> ***</p>
</li>
<li><p><a href="https://prompthero.com/">prompthero</a></p>
</li>
<li><p><a href="https://publicprompts.art/">publicprompts</a></p>
</li>
<li><p><a href="https://learningprompt.wiki/">https://learningprompt.wiki/</a> prompt 学习教程</p>
</li>
<li><p><a href="https://gpt.candobear.com/prompt">Prompt 大全</a></p>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/ZYkUwbXgzi5eqYkHl0MceNrSnhb">提示指令库</a>  汇总</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://www.bilibili.com/video/BV1Lg4y1c7fk/">学完这个视频，简历加一条：熟练掌握ChatGPT解决复杂问题｜ChatGPT使用教程</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>分库分表</title>
    <url>/www6vHomeHexo/2021/05/26/dbSharding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8">分库分表</a><ul>
<li><a href="#%E5%9E%82%E7%9B%B4%E5%88%87%E5%88%86-%E6%B0%B4%E5%B9%B3%E5%88%87%E5%88%86-4">垂直切分 &amp; 水平切分 [4]</a></li>
<li><a href="#%E5%88%86%E7%89%87%E5%8E%9F%E5%88%99%E7%9A%84%E6%80%BB%E7%BB%93-3">分片原则的总结 [3]</a></li>
</ul>
</li>
<li><a href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">问题&amp;解决方案</a><ul>
<li><a href="#%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98-1">分库分表带来的问题 [1]</a></li>
<li><a href="#shardingsphere-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-2">ShardingSphere-分布式事务 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="分库分表">分库分表</span><a href="#分库分表" class="header-anchor">#</a></h1><h3><span id="垂直切分-amp-水平切分-4">垂直切分 &amp; 水平切分 [4]</span><a href="#垂直切分-amp-水平切分-4" class="header-anchor">#</a></h3><ul>
<li><p>垂直切分</p>
<ul>
<li>垂直分库<br>不同的业务使用不同的数据库.    eg.订单库和促销活动库</li>
<li>垂直分表<br>  根据一张表中的字段，将一张表划分为两张表</li>
</ul>
</li>
<li><p>水平切分</p>
<ul>
<li>水平分表<br>将表中的某一列作为切分的条件，按照某种规则（Range 或 Hash 取模）来<br>切分为更小的表</li>
<li>水平分库分表<br>水平切换的表分布到不同机器的库中</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>切分方式</th>
<th>数据量</th>
<th>访问特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>单库单表</td>
<td>较小</td>
<td>xxx</td>
<td>小规模业务</td>
</tr>
<tr>
<td>表分区</td>
<td>较大</td>
<td>热点数据比较集中</td>
<td>数据量大、历史访问少</td>
</tr>
<tr>
<td>单库多表</td>
<td>较大</td>
<td>访问热点数据分散，所有数据都会访问</td>
<td>数据量较大</td>
</tr>
<tr>
<td>多库多表</td>
<td>极大</td>
<td>并发量高、海量数据</td>
<td>高并发、大数据量</td>
</tr>
</tbody></table>
<h3><span id="分片原则的总结-3">分片原则的总结 [3]</span><a href="#分片原则的总结-3" class="header-anchor">#</a></h3><ol>
<li>尽可能不要使用分片，优先考虑单表优化。[规避]</li>
<li>如果必须使用分片，分片数量应该尽量少，均匀分布在多个数据节点上。</li>
<li>分片规则应提前规划选择，考虑数据增长模式、访问模式、关联性和扩容问题。</li>
<li>避免在一个事务中跨越多个分片。 [规避]</li>
<li>查询条件要优化，避免使用 “SELECT *” 的方式，尽量避免返回大量结果集，并为频繁使用的查询语句建立索引。</li>
<li>通过数据冗余和表分区等方式降低跨库 joins 的可能性。 [规避]</li>
<li>对于有时间特征的数据表，采用时间范围分片，当前活跃数据采用短跨度分片，历史数据采用长跨度分片。</li>
<li>分片的选择应该取决于最频繁的查询 SQL 的条件，避免不带 where 语句的查询 SQL。</li>
</ol>
<h1><span id="问题amp解决方案">问题&amp;解决方案</span><a href="#问题amp解决方案" class="header-anchor">#</a></h1><h3><span id="分库分表带来的问题-1">分库分表带来的问题 [1]</span><a href="#分库分表带来的问题-1" class="header-anchor">#</a></h3><ul>
<li><strong>事务性问题</strong><ul>
<li>方案一：尽可能保证一个事务所操作的表分布在一个库中 [规避]</li>
<li>方案二：业务层<strong>引入分布式事务组件</strong>，如事务性消息、TCC、Seata等</li>
</ul>
</li>
<li><strong>主键唯一性问题</strong><ul>
<li>方案一：设置自增步长，采用等差数列递增</li>
<li>方案二：采用<strong>全局统一ID生成机制</strong>，如UUID、雪花算法、数据库号段等方式</li>
</ul>
</li>
<li><strong>跨库多表join问题</strong><ul>
<li>建议尽可能避免表join操作，可以采用多次查询业务层进行数据组装  [规避]</li>
<li><strong>冗余表或冗余字段</strong>来优化跨库 JOIN 查询 [规避] [4]</li>
</ul>
</li>
<li><strong>跨库聚合查询问题</strong> [5]<ul>
<li>方案一：赛道赛马机制，每次从N个库表中查询出TOP N数据，然后在业务层代码中进行聚合合并操作 [全局视野法]</li>
<li>方案二：将经常使用到groupby、orderby字段存储到一个单一库表中，先到单一表中查询出相应数据，然后根据查询到的主键ID，到分库分表中查询详情进行返回 [二次查询法]</li>
</ul>
</li>
<li><strong>跨节点分页查询问题</strong>  [5]<br>跨节点分页查询问题是指在分库分表的环境下，通过某些非分库分表字段进行查询时，可能需要跨多个库或表进行查询，这会带来性能问题。解决这个问题的一种方法是<strong>使用两套数据</strong>，一套是<strong>基于分库分表的用户查询数据</strong>，另一套是<strong>基于Elasticsearch或Solr的订单数据，主要用于运营人员进行分页查询。</strong> 为了不影响订单业务性能，可以使用<strong>异步消息来实现Elasticsearch或Solr订单数据的新增和修改。</strong>  [gpt 总结] [4]</li>
<li><strong>扩容问题 [4]</strong><br>一旦动态增加表了，就会涉及到<strong>数据迁移问题</strong><br>在最开始设计表数据量时，尽量使用 2 的倍数来设置表数量。当需要扩容时，也<br>同样按照 2 的倍数来扩容，这样可以减少数据的迁移量。</li>
</ul>
<h3><span id="shardingsphere-分布式事务-2">ShardingSphere-分布式事务 [2]</span><a href="#shardingsphere-分布式事务-2" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th></th>
<th><em>LOCAL</em></th>
<th><em>XA</em></th>
<th><em>BASE</em></th>
</tr>
</thead>
<tbody><tr>
<td>业务改造</td>
<td>无</td>
<td>无</td>
<td>需要 seata server</td>
</tr>
<tr>
<td>一致性</td>
<td>不支持</td>
<td>支持</td>
<td>最终一致</td>
</tr>
<tr>
<td>隔离性</td>
<td>不支持</td>
<td>支持</td>
<td>业务方保证</td>
</tr>
<tr>
<td>并发性能</td>
<td>无影响</td>
<td>严重衰退</td>
<td>略微衰退</td>
</tr>
<tr>
<td>适合场景</td>
<td>业务方处理不一致</td>
<td>短事务 &amp; 低并发</td>
<td>长事务 &amp; 高并发</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/535713197">一文读懂数据库分库分表</a></li>
<li><a href="https://shardingsphere.apache.org/document/5.3.2/cn/features/transaction/">ShardingSphere-分布式事务</a></li>
<li><a href="https://mp.weixin.qq.com/s/cVwyE7_oZ0F8Q1Mg7zClmQ">MySQL 大表优化方案（长文）</a></li>
<li>《36 | 什么时候需要分表分库？》</li>
<li><a href="/www6vHomeHexo/2022/08/01/dbShardingPaging/" title="分库分表-分页">分库分表-分页</a> self</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>DAL</category>
        <category>分库分表</category>
      </categories>
      <tags>
        <tag>分库分表</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 架构</title>
    <url>/www6vHomeHexo/2021/05/25/redisArch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#redis%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B1">Redis架构演进[1]</a></li>
<li><a href="#%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84">主从架构</a></li>
<li><a href="#%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4-sentinel">哨兵集群 sentinel</a></li>
<li><a href="#redis-cluster">Redis cluster</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="redis架构演进1">Redis架构演进[1]</span><a href="#redis架构演进1" class="header-anchor">#</a></h1><ul>
<li><strong>数据怕丢失</strong> -&gt; 持久化（RDB&#x2F;AOF）</li>
<li><strong>恢复时间久</strong> -&gt; 主从副本（副本随时可切）</li>
<li><strong>故障手动切换慢</strong> -&gt; 哨兵集群（自动切换）</li>
<li><strong>读存在压力</strong> -&gt; 扩容副本（读写分离）</li>
<li><strong>写存在压力&#x2F;容量瓶颈</strong> -&gt; 分片集群<ul>
<li><strong>分片集群社区方案</strong> -&gt;  Twemproxy、Codis（Redis 节点之间无通信，需要部署哨兵，可横向扩容）</li>
<li><strong>分片集群官方方案</strong> -&gt;  Redis Cluster （Redis 节点之间 Gossip 协议，无需部署哨兵，可横向扩容）</li>
</ul>
</li>
<li><strong>业务侧升级困难</strong> -&gt; Proxy + Redis Cluster（不侵入业务侧）</li>
</ul>
<h1><span id="主从架构">主从架构</span><a href="#主从架构" class="header-anchor">#</a></h1><ul>
<li>主从同步<ul>
<li>异步方式来同步数据<ul>
<li>最终一致性</li>
</ul>
</li>
<li>分类<ul>
<li><ol>
<li>增量同步 （同步的是指令）</li>
</ol>
<ul>
<li>1.指令记录在master本地的内存 buffer</li>
<li>2.异步将 buffer 中的指令同步到从节点</li>
<li>类似 AOF</li>
</ul>
</li>
<li><ol start="2">
<li>快照同步（同步的是内容，全量同步）</li>
</ol>
<ul>
<li>bgsave存磁盘rdb，然后rdb发到slave，slave装载</li>
<li>优化: 2.8.18 版开始支持无盘复制</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="哨兵集群-sentinel">哨兵集群 sentinel</span><a href="#哨兵集群-sentinel" class="header-anchor">#</a></h1><p>   master-slave异步复制, 所以会丢消息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">参数: </span><br><span class="line"># 至少有一个slave复制</span><br><span class="line">min-slaves-to-write 1   </span><br><span class="line"># slave节点最大10s的延迟</span><br><span class="line">min-slaves-max-lag 10   </span><br></pre></td></tr></table></figure>

<h1><span id="redis-cluster">Redis cluster</span><a href="#redis-cluster" class="header-anchor">#</a></h1><p><strong>整体架构</strong></p>
<ol>
<li>去中心化的;</li>
<li>所有数据划分为16384个slots，每个节点负责其中一部分slots;</li>
</ol>
<p><strong>容错</strong></p>
<ol>
<li>主从容错，主升从。</li>
<li>PFail（Possibly Fail） -&gt; 临时不可用<br>Fail -&gt; 确定不可用， PFail Count&gt; 集群的1&#x2F;2</li>
</ol>
<p><strong>Gossip</strong>协议<br>集群节点采用 Gossip 协议来广播自己的状态以及自己对整个集群认知的改变;<br>可能下线 (PFAIL-Possibly Fail) &amp;&amp; 确定下线 (Fail)</p>
<p><strong>slot迁移</strong><br>在迁移过程中，客户端访问的流程会有很大的变化。<br>迁移是会影响服务效率的，同样的指令在正常情况下一个 ttl 就能完成，而在迁移中得 3 个 ttl 才能搞定。</p>
<p><strong>网络抖动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 表示当某个节点持续 timeout 的时间失联时，才可以认定该节点出现故障</span><br><span class="line">cluster-node-timeout </span><br></pre></td></tr></table></figure>

<p><strong>槽位迁移感知</strong><br>2个error指令<br>1.<strong>MOVED错误</strong>：  用来纠正槽位<br>  槽的负责权已经从一个节点转移到了另一节点<br>2. <strong>ASK错误， ASKING命令</strong>： 用来临时纠正槽位<br>    只是两个节点在迁移槽的过程中使用的一种临时措施</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/543953543">一文搞懂 Redis 架构演化之路</a> 腾讯 ***</li>
</ol>
<p>《Redis 深度历险：核心原理与应用实践》 钱文品<br>3. 原理 8：有备无患 —— 主从同步<br><del>4. 原理 3：未雨绸缪 —— 持久化</del><br>5. 集群 1：李代桃僵 —— Sentinel<br>6. 集群 3：众志成城 —— Cluster</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>缓存 一致性</title>
    <url>/www6vHomeHexo/2021/05/25/cacheConsistent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%85%B3%E9%94%AE%E8%AF%8D">关键词</a></li>
<li><a href="#%E5%A4%B1%E6%95%88%E7%AD%96%E7%95%A5%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0-9">失效策略(缓存更新) [9]</a><ul>
<li><a href="#readwrite-through%E6%A8%A1%E5%BC%8F">Read&#x2F;Write Through模式</a></li>
<li><a href="#cache-aside-%E6%A8%A1%E5%BC%8F">Cache Aside 模式</a></li>
<li><a href="#write-behind%E6%A8%A1%E5%BC%8F">Write Behind模式</a></li>
</ul>
</li>
<li><a href="#%E5%A4%B1%E6%95%88%E7%AD%96%E7%95%A5">失效策略</a></li>
<li><a href="#%E7%BC%93%E5%AD%98%E7%9A%84%E5%BA%94%E7%94%A8%E6%A8%A1%E5%BC%8F">缓存的应用模式</a></li>
<li><a href="#cache%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7">Cache与数据库的一致性</a></li>
<li><a href="#%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5-%E4%B8%80%E8%87%B4%E6%80%A7">缓存与数据库的数据同步 一致性</a></li>
<li><a href="#%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5">过期策略</a><ul>
<li><a href="#%E8%A2%AB%E5%8A%A8%E8%BF%87%E6%9C%9F-%E4%B8%80%E8%87%B4%E6%80%A7%E4%BD%8E-%E7%BC%93%E5%AD%98%E8%B6%85%E6%97%B6">被动过期 （一致性低， 缓存超时）</a></li>
<li><a href="#%E4%B8%BB%E5%8A%A8%E8%BF%87%E6%9C%9F%E4%B8%80%E8%87%B4%E6%80%A7%E9%AB%98-%E4%BA%8B%E4%BB%B6%E8%BF%87%E6%9C%9F-%E5%BC%82%E6%AD%A5%E8%BF%87%E6%9C%9F-%E5%8F%98%E5%8C%96%E4%BA%8B%E4%BB%B6">主动过期（一致性高， 事件过期， 异步过期，  变化事件）</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E7%89%88%E6%9C%AC%E7%9A%84%E8%BF%87%E6%9C%9F%E6%96%B9%E5%BC%8F">基于版本的过期方式</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="关键词">关键词</span><a href="#关键词" class="header-anchor">#</a></h1><p> 失效策略(缓存更新)、  缓存与数据库的一致性</p>
<h1><span id="失效策略缓存更新-9">失效策略(缓存更新) [9]</span><a href="#失效策略缓存更新-9" class="header-anchor">#</a></h1><h3><span id="readx2fwrite-through模式">Read&#x2F;Write Through模式</span><a href="#readx2fwrite-through模式" class="header-anchor">#</a></h3>
<p>Read Through : 被动失效<br>Write Through ： 主动失效</p>
<h3><span id="cache-aside-模式">Cache Aside 模式</span><a href="#cache-aside-模式" class="header-anchor">#</a></h3><p>Cache Aside 模式和上面的 Read&#x2F;Write Through 模式非常像，它们处理读请求的逻辑是完全一样的，唯一的一个小差别就是，Cache Aside 模式在更新数据的时候，并不去尝试更新缓存，而是去删除缓存。</p>

<p><strong>普遍使用这种方式</strong></p>
<p>场景：<br>订单服务收到更新数据请求之后，先更新数据库，如果更新成功了，再尝试去删除缓存中订 单，如果缓存中存在这条订单就删除它，如果不存在就什么都不做，然后返回更新成功。这 条更新后的订单数据将在下次被访问的时候加载到缓存中。使用 Cache Aside 模式来更新 缓存，可以非常有效地避免并发读写导致的脏数据问题。</p>
<h3><span id="write-behind模式">Write Behind模式</span><a href="#write-behind模式" class="header-anchor">#</a></h3><p>linux page cache: dirty page刷盘。<br>kafka使用page cache异步刷盘。</p>
<h1><span id="失效策略">失效策略</span><a href="#失效策略" class="header-anchor">#</a></h1><ul>
<li>失效策略<ul>
<li>被动失效<ul>
<li>有空窗期问题</li>
</ul>
</li>
<li>主动更新【1】<ul>
<li>无空窗期问题</li>
<li>有并发更新问题。并发写，数据覆盖一致性问题【2】<ul>
<li>锁控制,少有这样做的<ul>
<li>客户端读写锁</li>
<li>服务端加锁</li>
</ul>
</li>
<li>版本控制<ul>
<li>单版本机制【3】</li>
<li>多版本机制<ul>
<li>类似向量时钟， NoSQL使用</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>消息机制【3,4】<ul>
<li>增量db数据通过消息来异步变更缓存的数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="缓存的应用模式">缓存的应用模式</span><a href="#缓存的应用模式" class="header-anchor">#</a></h1><ul>
<li>缓存的应用模式<ul>
<li>Cache-Aside<ul>
<li>业务代码直接维护缓存</li>
<li>有并发更新问题【2】</li>
</ul>
</li>
<li>Cache-As-SoR<ul>
<li>Read-Through</li>
<li>Write-Through<ul>
<li>同步写</li>
</ul>
</li>
<li>Write-Behind<ul>
<li>异步写</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="cache与数据库的一致性">Cache与数据库的一致性</span><a href="#cache与数据库的一致性" class="header-anchor">#</a></h1><ul>
<li>Cache与数据库的一致性<ul>
<li>读操作<ul>
<li>先读缓存，再读数据库</li>
</ul>
</li>
<li>写操作<ul>
<li>先写数据库，在写缓存</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="缓存与数据库的数据同步-一致性">缓存与数据库的数据同步  一致性</span><a href="#缓存与数据库的数据同步-一致性" class="header-anchor">#</a></h1><p>  在应用层， 可以根据业务场景对一致性的要求不同， 给数据分配不同的队列，即<br>一致性分级队列。 强一致性的场景如自己发布的评论， 自己应该及时看到。 而别人看到我的评论属于会话一致性， 一致性要求比较弱。 这样可以把一致性要求高的业务分配更多资源， 做到快速同步。<br>  缓存与数据库同步的异步化也是提高响应度的方式， 在write-behind的方式中，所有的数据的操作都在缓存中， 更新的数据并不会立即传到数据库。相反，在缓存中一旦进行更新操作，缓存就会跟踪脏记录列表，并定期将当前的脏记录集刷新到数据库中。</p>
<p>  <del>在DAO层的hibernate针对一致性的要求提出了类似DBMS的事务级别的4个配置项。 非严格读写型(nonstrict-read-write)策略提供弱一致性，不保证缓存与数据库中数据的一致性。如果存在两个事务同时访问缓存中相同数据的可能，必须为该数据配置一个很短的数据过期时间，从而尽量避免脏读。对于极少被修改，并且允许偶尔脏读的数据，可以采用这种并发访问策略。 事务策略(transactional )只可用于托管环境，如有必要，它还保证完全的事务隔离级别直到可重复读。事务策略可以用在强一致性的场景中。</del></p>
<h1><span id="过期策略">过期策略</span><a href="#过期策略" class="header-anchor">#</a></h1><h3><span id="被动过期-一致性低-缓存超时">被动过期 （一致性低， 缓存超时）</span><a href="#被动过期-一致性低-缓存超时" class="header-anchor">#</a></h3><p>对一致性要求较低的系统，可以采用常规的缓存超时策略，此类策略属于被动过期。<br>存放数据时，永不过期的数据不要与有过期策略的数据放在一起， 早期的版本memcache曾经有一个这样的bug， 永不过期的数据被有过期策略的数据踢走了。不要把所有的数据的过期时间设为同一个时间， 这样可能造成大规模的数据同时过期，hit rate变小， 对数据库的查询数瞬时变大，造成数据库的压力。 </p>
<h3><span id="主动过期一致性高-事件过期-异步过期-变化事件">主动过期（一致性高， 事件过期， 异步过期，  变化事件）</span><a href="#主动过期一致性高-事件过期-异步过期-变化事件" class="header-anchor">#</a></h3><p>对一致性要求较高的系统，可以采用事件过期策略，此类策略属于主动过期。<br>      某个组件的结构发生变化，或者某个业务对象状态发生变化，把组件id或业务对象id放入过期队列中， 缓存节点异步读取这些数据， 将对应cache的对象移除。亦可把变化封装成事件放入过期队列中， 由代理处理这个事件， 异步的移除相应的缓存。         </p>
<h3><span id="基于版本的过期方式">基于版本的过期方式</span><a href="#基于版本的过期方式" class="header-anchor">#</a></h3><p>在存储空间较大的前提下，借鉴mvcc的概念，每次更改数据时增加一个副本，并带版本号元数据。 然后由一个代理定时的删除低版本的过期的数据。</p>
<p><del>## 服务端缓存</del></p>
<p><del>缓存服务端常用的有memcache。 Nosql的解决方案有Redies，Redies在作为cache时往往配置为无持久化的形式 。两者数据模型都是key-value的。 Redies比老牌的memcache能提供更好的性能， 更快的速度。 Memcache 没有自建的replicaion 机制, 可靠性需要在客户端以双写支持。 Redies可以看成自带持久化机制的Write-back缓存，在write-behind缓存中，数据的读取和更新通过缓存进行，与write-through缓存不同，更新的数据并不会立即持久化。相反，在缓存中一旦进行更新操作，缓存就会跟踪脏记录列表，并定期将当前的脏记录集刷新到外部存储中， 在Redies中这种机制叫做AOF</del></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.geek-share.com/detail/2615401101.html">应用系统数据缓存设计</a> 淘宝技术部 *** 失效</li>
<li>Local Cache的小TIP  阿里 放翁（文初）</li>
<li><a href="https://www.csdn.net/article/1970-01-01/2825234">阿里云分布式缓存OCS与DB之间的数据一致性</a> 杨成虎</li>
<li><a href="https://developer.aliyun.com/article/55842">缓存失效竟然可以这么解？</a> serana_cai</li>
<li>xxx</li>
<li>《亿级流量网站架构核心技术》 第9章 张开涛</li>
<li>xxx</li>
<li><a href="https://coolshell.cn/articles/17416.html">缓存更新的套路</a>  coolshell ***</li>
<li>《后端存储实战课 - MySQL如何应对高并发（一）：使用缓存保护MySQL》  李玥</li>
<li><a href="https://zhuanlan.zhihu.com/p/554879252">浅谈缓存最终一致性的解决方案</a>   腾讯 未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 优化建议</title>
    <url>/www6vHomeHexo/2021/05/24/redisOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="优化建议">优化建议</span><a href="#优化建议" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/05/24/redisOptimize/redis-suggest.jpg" class width="450" height="300">

<h2><span id="优化建议2">优化建议[2]</span><a href="#优化建议2" class="header-anchor">#</a></h2><ul>
<li><p>对象内存优化</p>
</li>
<li><p>客户端缓冲优化</p>
</li>
<li><p>碎片优化</p>
</li>
<li><p>子进程内存优化</p>
</li>
<li><p>序列化 + 压缩<br>Renault（序列化： Json or Hession，压缩： Hession自带压缩 ）</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>加餐（六）| Redis的使用规范小建议</li>
<li><a href="https://zhuanlan.zhihu.com/p/506470564">Redis 内存优化在 vivo 的探索与实践</a> vivo</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 学习资源</title>
    <url>/www6vHomeHexo/2021/05/24/redisStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<ul>
<li><p>client<br><a href="https://zhuanlan.zhihu.com/p/388041321">初探 Redis 客户端 Lettuce：真香！</a></p>
</li>
<li><p>优化<br><a href="https://zhuanlan.zhihu.com/p/382677486">十亿级流量下，我与Redis时延小突刺的战斗史</a><br><a href="https://zhuanlan.zhihu.com/p/422659513">Redis大集群扩容性能优化实践</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 大Key</title>
    <url>/www6vHomeHexo/2021/05/21/redisBigKey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#redis-%E5%A4%A7key">Redis 大Key</a><ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF-bigkey-5">什么是 Bigkey [5]</a></li>
<li><a href="#%E9%97%AE%E9%A2%98">问题</a></li>
<li><a href="#%E5%BB%BA%E8%AE%AE-6">建议 [6]</a></li>
<li><a href="#%E6%9F%A5%E6%89%BE-%E5%A4%A7key">查找 大Key</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-3">解决方案 [3]</a></li>
<li><a href="#scan%E5%91%BD%E4%BB%A4">scan命令</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="redis-大key">Redis 大Key</span><a href="#redis-大key" class="header-anchor">#</a></h1><h3><span id="什么是-bigkey-5">什么是 Bigkey [5]</span><a href="#什么是-bigkey-5" class="header-anchor">#</a></h3><p>【字符串类型】： 单个string类型的value值超过1MB，就可以认为是Bigkey。<br>【非字符串类型】：哈希、列表、集合、有序集合等， 它们的元素个数超过2000个，就可以认为是Bigkey。</p>
<h3><span id="问题">问题</span><a href="#问题" class="header-anchor">#</a></h3><p>问题1 [1] </p>
<ul>
<li>大key在数据迁移[遇到过]、扩容、删除[遇到过]时会有卡顿。</li>
</ul>
<p>问题2 [3]</p>
<ul>
<li>执行大key命令的客户端本身，耗时明显增加，甚至超时</li>
<li>执行大key相关读取或者删除操作时，会严重占用带宽和CPU，影响其他客户端</li>
<li>大key本身的存储带来分布式系统中分片数据不平衡，CPU使用率也不平衡</li>
<li>大key有时候也是热key，读取操作频繁，影响面会很大</li>
<li>执行大key删除时，在低版本redis中可能阻塞线程   [遇到过]</li>
</ul>
<h3><span id="建议-6">建议 [6]</span><a href="#建议-6" class="header-anchor">#</a></h3><ul>
<li><p>bigKey 容量</p>
<ul>
<li>string类型控制在10KB以内</li>
<li>hash、list、set、zset元素个数不要超过5000</li>
</ul>
</li>
<li><p><strong>非字符串的bigkey</strong>，不要使用del删除，<strong>使用hscan、sscan、zscan方式渐进式删除</strong></p>
</li>
<li><p><strong>防止bigkey过期时间自动删除问题</strong>(例如一个200万的zset设置1小时过期，会触发del操作，造成阻塞，而且该操作不会不出现在慢查询中(latency可查))，查找方法和删除方法</p>
</li>
</ul>
<h3><span id="查找-大key">查找 大Key</span><a href="#查找-大key" class="header-anchor">#</a></h3><ul>
<li>–bigkeys参数,   用scan扫描大key  [1][2][5]<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">redis-cli --bigkeys</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Scanning the entire keyspace to find biggest keys as well as</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">average sizes per key <span class="built_in">type</span>.  You can use -i 0.1 to <span class="built_in">sleep</span> 0.1 sec</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">per 100 SCAN commands (not usually needed).</span></span><br><span class="line">[00.00%] Biggest zset   found so far &#x27;testzset&#x27; with 129 members</span><br><span class="line">[00.00%] Biggest hash   found so far &#x27;h2&#x27; with 513 fields</span><br><span class="line">[00.00%] Biggest set    found so far &#x27;si1&#x27; with 5 members</span><br><span class="line">[00.00%] Biggest hash   found so far &#x27;h4&#x27; with 514 fields</span><br><span class="line">[00.00%] Biggest string found so far &#x27;key&#x27; with 9 bytes</span><br><span class="line">-------- summary -------</span><br><span class="line">Sampled 9 keys in the keyspace!</span><br><span class="line">Total key length in bytes is 27 (avg len 3.00)</span><br><span class="line">Biggest string found &#x27;key&#x27; has 9 bytes</span><br><span class="line">Biggest    set found &#x27;si1&#x27; has 5 members</span><br><span class="line">Biggest   hash found &#x27;h4&#x27; has 514 fields</span><br><span class="line">Biggest   zset found &#x27;testzset&#x27; has 129 members</span><br><span class="line">1 strings with 9 bytes (11.11% of keys, avg size 9.00)</span><br><span class="line">0 lists with 0 items (00.00% of keys, avg size 0.00)</span><br><span class="line">2 sets with 8 members (22.22% of keys, avg size 4.00)</span><br><span class="line">4 hashs with 1541 fields (44.44% of keys, avg size 385.25)</span><br><span class="line">2 zsets with 132 members (22.22% of keys, avg size 66.00)</span><br><span class="line">0 streams with 0 entries (00.00% of keys, avg size 0.00)</span><br></pre></td></tr></table></figure>
<ul>
<li>注意点<ul>
<li><strong>建议在slave节点执行</strong>，因为–Bigkeys也是通过scan完成的，可能会对节点造成阻塞。</li>
<li><strong>建议在节点本机执行</strong>，这样可以减少网络开销。</li>
<li>如果没有从节点，<strong>可以使用–i参数</strong>，例如(–i 0.1 代表100毫秒执行一次)。</li>
<li>–Bigkeys只能计算每种数据结构的top1，如果有些数据结构有比较多的Bigkey，是查找不出来的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>–bigkeys的不足</strong> [7]<br>  想查询大于10kb的所有key， –bigkeys参数就无能为力了。需要用到memory usage命令来计算每个键值的字节数</p>
</blockquote>
<ul>
<li><p>MEMORY USAGE key</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SET cento 01234567890123456789012345678901234567890123</span></span><br><span class="line">45678901234567890123456789012345678901234567890123456789</span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">MEMORY USAGE cento</span></span><br><span class="line">(integer) 153</span><br></pre></td></tr></table></figure>
</li>
<li><p>离线分析RDB [3]<br>使用redis-rdb-tools离线分析工具来扫描RDB持久化文件</p>
<p><a href="https://github.com/sripathikrishnan/redis-rdb-tools">redis-rdb-tools</a></p>
</li>
</ul>
<h3><span id="解决方案-3">解决方案 [3]</span><a href="#解决方案-3" class="header-anchor">#</a></h3><ul>
<li><p>可删除</p>
<ul>
<li>渐进式删除&lt;4.0版本<br><a href="#scan%E5%91%BD%E4%BB%A4">scan命令</a>  <strong>游标式迭代扫描</strong><br>使用 hscan、sscan、zscan 方式渐进式删除 [4]</li>
<li>惰性删除&gt;4.0版本  [7][8]<br><strong>unlink命令  异步惰性非阻塞删除</strong> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unlink &lt;keyName&gt; </span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lazyfree-lazy-server-del：Yes(default no)</span><br><span class="line">replica-lazy-flush：Yes(default no)</span><br><span class="line"></span><br><span class="line">lazyfree-lazy-user-del: Yes(default no) （6.0 新增）</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>不可删除</p>
<ul>
<li>拆分<ul>
<li>字符串类型<br>比如商品信息，根据的类别拆分 key。</li>
<li>集合类元素<br>按照日期拆分，key20220101、key20220102</li>
</ul>
</li>
<li>压缩 Value 数据<br>使用序列化、压缩算法将 Key 的大小控制在合理范围内，但是需要注意，序列化、反序列化都会带来一定的消耗</li>
</ul>
</li>
</ul>
<h3><span id="scan命令">scan命令</span><a href="#scan命令" class="header-anchor">#</a></h3><ul>
<li>scan命令[1]<ul>
<li>zset -&gt; zscan<br>ZSCAN：命令用于迭代  zset 中的元素（包括元素成员和元素分值）。</li>
<li>hash -&gt; hscan<br>HSCAN：命令用于迭代 hash 中的键值对。</li>
<li>set -&gt; sscan<br>SSCAN：命令用于迭代 set 中的元素。</li>
<li>key of hash -&gt; scan<br>SCAN：命令用于迭代 数据库键。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;db_number&quot;</span><br><span class="line">2) &quot;key1&quot;</span><br><span class="line">3) &quot;myKey&quot;</span><br><span class="line">127.0.0.1:6379&gt; scan 0 MATCH * COUNT 1</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) 1) &quot;db_number&quot;</span><br><span class="line">127.0.0.1:6379&gt; scan 2 MATCH * COUNT 1</span><br><span class="line">1) &quot;1&quot;</span><br><span class="line">2) 1) &quot;myKey&quot;</span><br><span class="line">127.0.0.1:6379&gt; scan 1 MATCH * COUNT 1</span><br><span class="line">1) &quot;3&quot;</span><br><span class="line">2) 1) &quot;key1&quot;</span><br><span class="line">127.0.0.1:6379&gt; scan 3 MATCH * COUNT 1</span><br><span class="line">1) &quot;0&quot;</span><br><span class="line">2) (empty list or set)</span><br></pre></td></tr></table></figure>

<ul>
<li>Hash删除: hscan + hdel [6]<br>先删除hash里的每个field，最后再删除hash<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delBigHash</span><span class="params">(String host, <span class="type">int</span> port, String password, String bigHashKey)</span> &#123;</span><br><span class="line">    <span class="type">Jedis</span> <span class="variable">jedis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Jedis</span>(host, port);</span><br><span class="line">    <span class="keyword">if</span> (password != <span class="literal">null</span> &amp;&amp; !<span class="string">&quot;&quot;</span>.equals(password)) &#123;</span><br><span class="line">        jedis.auth(password);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">ScanParams</span> <span class="variable">scanParams</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ScanParams</span>().count(<span class="number">100</span>);</span><br><span class="line">    <span class="type">String</span> <span class="variable">cursor</span> <span class="operator">=</span> <span class="string">&quot;0&quot;</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        ScanResult&lt;Entry&lt;String, String&gt;&gt; scanResult = jedis.hscan(bigHashKey, cursor, scanParams);</span><br><span class="line">        List&lt;Entry&lt;String, String&gt;&gt; entryList = scanResult.getResult();</span><br><span class="line">        <span class="keyword">if</span> (entryList != <span class="literal">null</span> &amp;&amp; !entryList.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Entry&lt;String, String&gt; entry : entryList) &#123;</span><br><span class="line">                jedis.hdel(bigHashKey, entry.getKey());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cursor = scanResult.getStringCursor();</span><br><span class="line">    &#125; <span class="keyword">while</span> (!<span class="string">&quot;0&quot;</span>.equals(cursor));</span><br><span class="line">    <span class="comment">//删除bigkey</span></span><br><span class="line">    jedis.del(bigHashKey);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《Redis 深度历险：核心原理与应用实践》 钱文品<br>大海捞针—scan  </li>
<li><a href="https://segmentfault.com/a/1190000018193214?utm_source=tag-newest">Redis中查找大key</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/473930220">解决了Redis大key问题</a>  ***<br><a href="https://mp.weixin.qq.com/s/0WS7_9EIQqpYlUNWgZZJog">解决了Redis大key问题</a></li>
<li><a href="https://blog.csdn.net/qq_34827674/article/details/126225192">Redis 大 key 要如何处理？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/584594738">Bigkey问题的解决思路与方式探索</a> vivo team *** </li>
<li><a href="https://developer.aliyun.com/article/846851">一份完整的阿里云 Redis 开发规范，值得收藏！</a>  ***</li>
<li><a href="https://www.bilibili.com/video/BV13R4y1v7sP/?p=106">尚硅谷Redis零基础到进阶，最强redis7教程，阳哥亲自带练（附redis面试题）</a> V</li>
<li><a href="/www6vHomeHexo/2022/06/01/redisLazyFree/" title="Redis LazyFree">Redis LazyFree</a> self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>消息系统 顺序消息</title>
    <url>/www6vHomeHexo/2021/05/19/mqOrdering/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF">顺序消息</a><ul>
<li><a href="#%E4%B8%A5%E6%A0%BC%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF%E5%9C%BA%E6%99%AF%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%8D%E5%88%B6%E5%8D%95%E4%B8%AA%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85jmqrocketmq">严格顺序消息(场景:数据库复制),单个生产者&#x2F;消费者[jmq,Rocketmq]</a></li>
<li><a href="#partition-%E5%86%85%E9%83%A8%E6%9C%89%E5%BA%8F-%E5%B1%80%E9%83%A8%E6%9C%89%E5%BA%8Fkafka">partition 内部有序, 局部有序[kafka]</a></li>
<li><a href="#pull%E6%A8%A1%E5%9E%8B%E5%8D%95%E7%BA%BF%E7%A8%8B%E7%94%9F%E4%BA%A7%E6%B6%88%E8%B4%B9metaq">pull模型+单线程生产&#x2F;消费[metaq]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#jmq%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF">jmq顺序消息</a></li>
<li><a href="#rocketmq%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF">rocketmq顺序消息</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="顺序消息">顺序消息</span><a href="#顺序消息" class="header-anchor">#</a></h1><h3><span id="严格顺序消息场景数据库复制单个生产者x2f消费者jmqrocketmq">严格顺序消息(场景:数据库复制),单个生产者&#x2F;消费者[jmq,Rocketmq]</span><a href="#严格顺序消息场景数据库复制单个生产者x2f消费者jmqrocketmq" class="header-anchor">#</a></h3><ul>
<li>jmq方案，任意时刻都有两个消息副本 [3]<ul>
<li>单个生产者生产，其余生产者会抛出异常[jmq]</li>
<li>单个消费者消费，其余消费者会抛出异常[jmq]</li>
<li>正常流程:消息队列分配在两组以上的BROKER组，每个BROKER组由MASTER-SLAVE组成。消息同步写入单个broker的MASTER。</li>
<li>异常处理流程：Master宕机后，slave只能消费读。生产failover到下一个可用的Broker组，等宕机的broker组的slave消费完，然后消费被挑选到的可用的Broker组的slave</li>
<li>协调者 controller：BROKER组的集群信息在协调者上保存为一个单向的链表，消费者和发送者各有一份独立的链表数据。</li>
</ul>
</li>
</ul>
<ul>
<li>rocketmq方案   顺序消息   rocketmq 顺序消息   [6][9][10]<ul>
<li>从业务层面来保证消息的顺序而不仅仅是依赖于消息系统.比如，订单号相同的消息会被先后发送到同一个队列中</li>
<li>从业务层面来保证消息的顺序, 而不仅仅是依赖于消息系统  Eg. 订单号相同的消息会被先后发送到同一个队列中</li>
</ul>
</li>
</ul>
<h3><span id="partition-内部有序-局部有序kafka">partition 内部有序, 局部有序[kafka]</span><a href="#partition-内部有序-局部有序kafka" class="header-anchor">#</a></h3><h3><span id="pull模型单线程生产x2f消费metaq">pull模型+单线程生产&#x2F;消费[metaq]</span><a href="#pull模型单线程生产x2f消费metaq" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="jmq顺序消息">jmq顺序消息</span><a href="#jmq顺序消息" class="header-anchor">#</a></h3><ol start="3">
<li>高可用保证消息绝对顺序消费的BROKER设计方案 丁俊</li>
</ol>
<h3><span id="rocketmq顺序消息">rocketmq顺序消息</span><a href="#rocketmq顺序消息" class="header-anchor">#</a></h3><ol start="6">
<li><a href="https://zhuanlan.zhihu.com/p/396726719">分布式开放消息系统(RocketMQ)的原理与实践</a>   CHEN川  ***  消息的顺序问题  消息的重复问题</li>
<li><a href="https://www.cnblogs.com/hzmark/p/orderly_message.html">聊一聊顺序消息（RocketMQ顺序消息的实现机制）</a>  杭州.Mark<br>producer: selector, 相同orderid的发送到同一个topic;<br>consumer: 多消费者加锁竞争消息</li>
<li><a href="https://help.aliyun.com/document_detail/49323.html">收发顺序消息</a>  阿里云文档</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>顺序消息</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>MQ 学习资料&amp;案例</title>
    <url>/www6vHomeHexo/2021/05/19/mqStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kafka">Kafka</span><a href="#kafka" class="header-anchor">#</a></h1><h3><span id="个人">个人</span><a href="#个人" class="header-anchor">#</a></h3><ul>
<li>朱忠华 ***</li>
<li>胡夕  kafka contributor  ***</li>
<li><a href="http://www.jasongj.com/">郭俊 Jason</a>   kafka spark  停更</li>
<li><a href="https://www.szzdzhp.com/">石臻臻的杂货铺</a> 石臻臻 kafka contributor  ***<br><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzg4ODY1NTcxNg==&action=getalbum&album_id=1966026980307304450">石臻臻的杂货铺</a><br><a href="https://www.szzdzhp.com/kafka/">Kafka大全</a> 石臻臻<br><a href="https://www.processon.com/view/link/618b2f20e0b34d73f7ed3fd5">Kafka 思维导图</a>  ***</li>
</ul>
<h3><span id="问题">问题</span><a href="#问题" class="header-anchor">#</a></h3><ul>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247493695&idx=1&sn=88d97fa5463f1d18a42d40c7981c2ba0">生产者客户端常见异常Case及其解决方案集锦(强烈建议收藏) </a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247490197&idx=1&sn=5853a44b227b75b33c34f2cfccbf630e">【读者答疑】为啥我创建的topic分区分配不均匀？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247492616&idx=1&sn=baebd69c8e6959ab77b1176a244d7b7b">实战 | Kafka生产环境磁盘坏掉了之后的正确处理姿势,(建议收藏,以备不时之需) </a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247495054&idx=1&sn=a5f474e225f4cb564e36a69d2b25e158">新增消费组出现异常排查流程 </a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247495087&idx=1&sn=767251dd7eab83dba313f623b4472306">Kafka扩分区之后 消费组会不会重新平衡呢？ </a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/523447452">Kafka 负载均衡在 vivo 的落地实践</a><br>服务端负载不均衡, 负载迁移</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/517244997">Kafka 万亿级消息实践之资源组流量掉零故障排查分析</a><br> 若发送消息的时候指定了key，并使用的是 Kafka producer默认的分区分配器请款下会出现 Kafka producer 客户端缓冲区资源被耗尽而出现topic所有分区雪崩效应。</p>
</li>
</ul>
<h1><span id="rocketmq">RocketMQ</span><a href="#rocketmq" class="header-anchor">#</a></h1><p><a href="https://time.geekbang.org/opencourse/videointro/100546401">深度剖析 RocketMQ5.0
</a>  林清山</p>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>学习资料</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka-ZeroCopy</title>
    <url>/www6vHomeHexo/2021/05/16/kafkaZeroCopy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#index-%E4%B8%AD%E7%9A%84mmap">Index 中的mmap</a><ul>
<li><a href="#mmapwrite-45">mmap+write [4][5]</a></li>
</ul>
</li>
<li><a href="#%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%AD%E7%9A%84sendfile-6">消费者中的sendfile() [6]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="index-中的mmap">Index 中的mmap</span><a href="#index-中的mmap" class="header-anchor">#</a></h2><p>在 AbstractIndex 中，这个 MappedByteBuffer 就是名为 <strong>mmap</strong> 的变量。<br>接下来，我用注释的方式，带你深入了解下这个 mmap 的主要流程。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@volatile</span> <span class="keyword">protected</span> <span class="keyword">var</span> mmap: <span class="type">MappedByteBuffer</span> = &#123; </span><br><span class="line"><span class="comment">// 第1步：创建索引文件</span></span><br><span class="line"><span class="keyword">val</span> newlyCreated = file.createNewFile() </span><br><span class="line"><span class="comment">// 第2步：以writable指定的方式（读写方式或只读方式）打开索引文件</span></span><br><span class="line"><span class="keyword">val</span> raf = <span class="keyword">if</span> (writable) <span class="keyword">new</span> <span class="type">RandomAccessFile</span>(file, <span class="string">&quot;rw&quot;</span>) <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">Rando</span></span><br><span class="line"><span class="keyword">try</span> &#123;    </span><br><span class="line">  <span class="keyword">if</span>(newlyCreated) &#123;       </span><br><span class="line">    <span class="keyword">if</span>(maxIndexSize &lt; entrySize) <span class="comment">// 预设的索引文件大小不能太小，如果连一个索引</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">&quot;Invalid max index size: &quot;</span> + m</span><br><span class="line">    <span class="comment">// 第3步：设置索引文件长度，roundDownToExactMultiple计算的是不超过maxInde</span></span><br><span class="line">    <span class="comment">// 比如maxIndexSize=1234567，entrySize=8，那么调整后的文件长度为1234560</span></span><br><span class="line">    raf.setLength(roundDownToExactMultipl</span><br></pre></td></tr></table></figure>

<p>这些代码最主要的作用就是创建 mmap 对象。AbstractIndex 其他大部分的操作都是和 mmap 相关。</p>
<p>AbstractIndex：这是 Kafka 所有类型索引的抽象父类，里面的 mmap 变量是实现索引机制的核心。</p>
<h3><span id="mmapwrite-45">mmap+write [4][5]</span><a href="#mmapwrite-45" class="header-anchor">#</a></h3><ul>
<li>4次context切换</li>
</ul>
<img src="/www6vHomeHexo/2021/05/16/kafkaZeroCopy/mmap+write.JPG" class>


<h2><span id="消费者中的sendfile-6">消费者中的sendfile()  [6]</span><a href="#消费者中的sendfile-6" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2021/05/16/kafkaZeroCopy/sendfile.JPG" class>





<ul>
<li><em>kafka的mmap是对写的优化，还是读的优化？</em></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="4">
<li><p><a href="https://www.bilibili.com/video/BV1Sy4y1o7Ej/">动画讲解：Kafka为什么快之零拷贝技术 – mmap</a></p>
</li>
<li><a href="/www6vHomeHexo/2019/09/14/zeroCopy/" title="Linux zero copy-零拷贝">Linux zero copy-零拷贝</a> self</li>
<li><p><a href="https://www.bilibili.com/video/BV1NX4y1f7e2/">动画讲解：Kafka为什么快之零拷贝技术-sendfile()函数</a> *** </p>
</li>
<li><p><a href="https://blog.csdn.net/allwefantasy/article/details/50663533">Kafka Zero-Copy 使用分析</a>  transferTo()  未</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/news/333695">linux零拷贝原理，RocketMQ＆Kafka使用对比</a> *  未</p>
</li>
<li><p><a href="http://www.uml.org.cn/zjjs/201504011.asp">RocketMQ入门（上）</a>  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Controller-控制器</title>
    <url>/www6vHomeHexo/2021/05/16/kafkaController/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="选举控制器的规则">选举控制器的规则:</span><a href="#选举控制器的规则" class="header-anchor">#</a></h2><p>第一个成功创建 &#x2F;controller 节点的 Broker 会被指定为控制器。</p>
<h2><span id="作用">作用</span><a href="#作用" class="header-anchor">#</a></h2><ul>
<li><p>主题管理(创建、删除、增加分区)</p>
</li>
<li><p>分区重分配<br>kafka-reassign-partitions</p>
</li>
<li><p>Preferred 领导者选举<br>Preferred 领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。</p>
</li>
<li><p>集群成员管理(新增 Broker、Broker 主动关闭、Broker 宕机)<br>包括自动检测新增 Broker、Broker 主动关闭及被动宕机。<br>这种自动检测是依赖于前面提到的 Watch 功能和 ZooKeeper 临时节点组合实现的。</p>
</li>
<li><p>数据服务<br>控制器上保存了最全的集群 元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存 中的缓存数据。</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://hiddenpps.blog.csdn.net/article/details/80865540">直击Kafka的心脏——控制器</a><br>&lt;&lt;26 | 你一定不能错过的Kafka控制器&gt;&gt;  胡夕</p>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Replication-副本机制</title>
    <url>/www6vHomeHexo/2021/05/16/kafkaReplica/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h3><span id="partition">Partition</span><a href="#partition" class="header-anchor">#</a></h3><ol>
<li>首领 leader ， 首领副本</li>
<li>跟随者 follower， 跟随者副本</li>
<li>首选首领</li>
</ol>
<h3><span id="kafka的副本机制">Kafka的副本机制</span><a href="#kafka的副本机制" class="header-anchor">#</a></h3><ol>
<li>AR &#x3D; ISR + OSR。<br>ISR 不只是追随者副本集合，它必然包括 Leader 副本。</li>
<li>ISR中副本有主从之分，但是<strong>读写都是主副本</strong>， 从副本只负责<strong>拉取</strong>主副本的数据。<br>好处（一致性方面）</li>
</ol>
<ul>
<li>方便实现<strong>“Read-your-writes”</strong></li>
<li>方便实现<strong>单调读（Monotonic Reads）</strong></li>
</ul>
<ol start="3">
<li>Kafka 判断 Follower 是否与 Leader 同步的标准,<strong>不是看”相差的消息数”，而是看”落后的时间”。</strong><br><strong>落后的时间</strong>就是 Broker 端参数 replica.lag.time.max.ms 参数值。<br>这个参数的含义是Follower 副本能够落后 Leader 副本的最长时间间隔，当前默认值是 10 秒。</li>
<li>Unclean 领导者选举（Unclean Leader Election）<br><a href="../../../../2016/07/05/kafkaReliability/">Kafka 可靠性总结</a> self</li>
</ol>
<h2><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h2><ul>
<li>失效副本是指什么？有那些应对措施？</li>
</ul>
<p>怎么样判定一个分区是否有副本是处于同步失效状态的呢？从Kafka 0.9.x版本开始通过唯一的一个参数replica.lag.time.max.ms（默认大小为10,000）来控制，当ISR中的一个follower副本滞后leader副本的时间超过参数replica.lag.time.max.ms指定的值时即判定为副本失效，需要将此follower副本剔出除ISR之外。<br><a href="https://honeypps.com/mq/kafka-analysis-of-under-replicated-partitions/">Kafka解析之失效副本</a></p>
<ul>
<li>优先副本是什么？它有什么特殊的作用？  </li>
<li>多副本下，各个副本中的HW和LEO的演变过程</li>
<li>为什么Kafka不支持读写分离？</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol start="7">
<li>Kafka核心技术与实战 - 23丨Kafka副本机制详解  胡夕</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 中的选主</title>
    <url>/www6vHomeHexo/2021/05/16/kafkaElection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="kafka中的选主1">Kafka中的选主[1]</span><a href="#kafka中的选主1" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">组件</th>
<th align="center">详细</th>
</tr>
</thead>
<tbody><tr>
<td align="center">kafka</td>
<td align="center">Controller leader</td>
<td align="center">依赖zk选主, kafka只有一个Controller</td>
</tr>
<tr>
<td align="center">Partition leader[2]</td>
<td align="center">leader在ISR中</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Consumer leader的选举</td>
<td align="center">消费组内的消费者选举出一个消费组的leader</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">zookeeper</td>
<td align="center">zk自身的选主</td>
<td align="center">Zab协议(原子广播+奔溃恢复)</td>
</tr>
<tr>
<td align="center">其他系统依赖zk选主</td>
<td align="center">使用zk的临时节点， session结束， 临时节点消失</td>
<td align="center"></td>
</tr>
</tbody></table>
<h3><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h3><ul>
<li><del>Kafka中有那些地方需要选举？</del>这些地方的选举策略又有哪些？</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0MzQ5MDA0Mw==&mid=2247485365&idx=1&sn=f55d8d2e1d6e82d23b6f60b847382c25">Kafka科普系列 | 原来Kafka中的选举有这么多？</a>  朱小厮<br><a href="https://honeypps.com/mq/kafka-basic-knowledge-of-selection/">Kafka科普系列 | 原来Kafka中的选举有这么多</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247491541&idx=1&sn=d4dbd0840fd3f61b7e86d6169caf2994&">你想知道的所有关于Kafka Leader选举流程和选举策略都在这(内含12张高清大图,建议收藏) </a>   石臻臻  kafka contributor  ***  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 索引</title>
    <url>/www6vHomeHexo/2021/05/15/kafkaIndex/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#kafka%E7%B4%A2%E5%BC%95">Kafka索引</a><ul>
<li><a href="#logsegment-%E6%9E%84%E6%88%90">LogSegment 构成</a></li>
<li><a href="#index%E7%B1%BB%E5%9E%8B">Index类型</a></li>
<li><a href="#%E6%94%B9%E8%BF%9B%E7%89%88%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95-1">改进版二分查找算法 [1]</a></li>
</ul>
</li>
<li><a href="#%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97-qa">日志模块 Q&amp;A</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="kafka索引">Kafka索引</span><a href="#kafka索引" class="header-anchor">#</a></h2><ul>
<li>稀疏索引</li>
</ul>
<h3><span id="logsegment-构成">LogSegment 构成</span><a href="#logsegment-构成" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tree /tmp/kafka-logs/t1-1/</span></span><br><span class="line">/tmp/kafka-logs/t1-1/</span><br><span class="line">├── 00000000000000000000.index</span><br><span class="line">├── 00000000000000000000.log  ## 位移索引</span><br><span class="line">├── 00000000000000000000.timeindex  ## 时间戳索引</span><br><span class="line">└── leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>

<h3><span id="index类型">Index类型</span><a href="#index类型" class="header-anchor">#</a></h3><ul>
<li><p>位移索引 [3]</p>
<img src="/www6vHomeHexo/2021/05/15/kafkaIndex/offsetIndex.jpg" class>  
<ul>
<li>假设要查找偏移量为230的消息，查找过程如下：<ul>
<li>首先找到baseOffset&#x3D;217的日志段文件（这里使用了跳跃表的结构来加速查找）</li>
<li>计算相对偏移量relativeOffset&#x3D;230-217&#x3D;13</li>
<li>在索引文件中查找不大于13的最大相对偏移量对应的索引项，即[12,456]</li>
<li>根据12对应的物理地址456，在日志文件.log中定位到准确位置</li>
<li>从日志文件物理位置456继续向后查找找到相对偏移量为13，即绝对偏移量为230，物理地址为468的消息</li>
</ul>
</li>
</ul>
</li>
<li><p>时间戳索引 [3]</p>
<img src="/www6vHomeHexo/2021/05/15/kafkaIndex/timeIndex.jpg" class>
<ul>
<li>假设要查找时间戳为1540的消息，查找过程如下（这里时间戳只是一个示意值）：<ul>
<li>将要查找的时间戳1540和每个日志段的最大时间戳逐一对比，直到找到最大时间戳不小于1540的日志段。（日志段的最大时间戳：获取时间戳索引文件最后一个索引项的时间戳，如果大于0，取该值；否则取日志段的最近修改时间）</li>
<li>找到对应的日志段后，在时间戳索引文件中使用二分查找找到不大于目标时间戳1540的最大索引项，即图中的[1530,12]，获取对应的相对偏移量12</li>
<li>在该日志段的偏移量索引文件中找到相对偏移量不大于12的索引项，即图中的[12，456]</li>
<li>在日志文件中从物理位置456开始查找时间戳不小于1540的消息</li>
</ul>
</li>
</ul>
</li>
<li><p>位移索引 vs 时间戳索引 [2]</p>
<img src="/www6vHomeHexo/2021/05/15/kafkaIndex/kafkaIndex.jpg" class></li>
</ul>
<h3><span id="改进版二分查找算法-1">改进版二分查找算法 [1]</span><a href="#改进版二分查找算法-1" class="header-anchor">#</a></h3><ul>
<li><p>在位移索引 和 时间戳索引中都使用二分查找算法</p>
</li>
<li><p>示例<br>现在，最新索引项保存在 Page #13 中。如果要查找最新索引项，原版二分查找算法将会<br>依次访问 Page #0、7、10、12 和 13。此时，问题来了：Page 7 和 10 已经很久没有被<br>访问过了，它们大概率不在页缓存中，因此，一旦索引开始征用 Page #13，就会发生<br><strong>Page Fault，</strong>等待那些<strong>冷页数据从磁盘中加载到页缓存</strong>。根据国外用户的测试，这种加载<br>过程可能长达 1 秒。</p>
</li>
</ul>
<h2><span id="日志模块-qampa">日志模块 Q&amp;A</span><a href="#日志模块-qampa" class="header-anchor">#</a></h2><ul>
<li><del>简述Kafka的日志目录结构</del></li>
<li><del>Kafka中有那些索引文件？</del></li>
<li><del>如果我指定了一个offset，Kafka怎么查找到对应的消息？</del></li>
<li><del>如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</del></li>
<li>聊一聊你对Kafka的Log Retention的理解</li>
<li>聊一聊你对Kafka的Log Compaction的理解</li>
<li>聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《04 | 索引（上）：改进的二分查找算法在Kafka索引的应用》</li>
<li>《05丨索引（下）：位移索引和时间戳索引的区别是什么？》</li>
<li><a href="https://blog.csdn.net/qq_32907195/article/details/127635622">深入理解Kafka服务端之索引文件及mmap内存映射</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper-分布式锁</title>
    <url>/www6vHomeHexo/2021/05/12/zookeeperDistributedLock/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8Ezk%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81">基于ZK的分布式锁</a><ul>
<li><a href="#%E5%8F%AF%E9%87%8D%E5%85%A5%E6%80%A723-%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F">可重入性[2][3]-实现方式</a></li>
<li><a href="#%E5%8F%AF%E9%87%8D%E5%85%A5%E6%80%A72-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">可重入性[2]-代码实现</a></li>
<li><a href="#%E7%BE%8A%E7%BE%A4%E6%95%88%E5%BA%94-%E4%BC%98%E5%8C%96-1">羊群效应 优化 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于zk的分布式锁">基于ZK的分布式锁</span><a href="#基于zk的分布式锁" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2021/05/12/zookeeperDistributedLock/distributedLock-zk.jpg" class title="图3.zk分布式锁">

<h3><span id="可重入性23-实现方式">可重入性[2][3]-实现方式</span><a href="#可重入性23-实现方式" class="header-anchor">#</a></h3><p>[chat]</p>
<p>在ZooKeeper中，分布式锁的可重入性是通过在锁节点的数据中保存一个拥有者ID和计数器来实现的。具体来说，当一个客户端第一次获取锁时，它会在锁节点的数据中设置一个拥有者ID，并将计数器设置为1。当这个客户端再次请求获取锁时，它会检查锁节点的数据中是否已经设置了拥有者ID，并且该ID是否与自己相同。如果相同，则将计数器加1，表示该客户端已经多次获取了锁；如果不同，则该客户端需要等待锁的释放。</p>
<p>通过保存拥有者ID和计数器，ZooKeeper可以轻松地实现分布式锁的可重入性。当一个客户端多次获取锁时，它只需要检查锁节点的数据中保存的拥有者ID是否与自己相同，并且计数器是否大于等于1即可。如果满足条件，则说明该客户端已经拥有了锁，并且可以继续执行操作；否则，该客户端需要等待锁的释放。</p>
<p>需要注意的是，为了避免死锁，ZooKeeper分布式锁的可重入性实现中还需要考虑锁的释放顺序。具体来说，当一个客户端释放锁时，需要按照获取锁的顺序依次释放所有的锁，从而避免死锁的发生。</p>
<p>总之，ZooKeeper分布式锁的可重入性是通过在锁节点的数据中保存拥有者ID和计数器来实现的。这种实现方式简单、高效，并且可以避免死锁的发生。</p>
<p>[endchat]</p>
<p>[chat]  总结: 在ZooKeeper中，分布式锁的可重入性是如何实现的   </p>
<p>具体来说，它通过在锁节点的数据中保存<strong>拥有者ID和计数器</strong>来判断一个客户端是否已经持有锁，并且在客户端多次获取锁时，可以通过增加计数器的方式来实现可重入性。为了避免死锁，ZooKeeper分布式锁的实现中还需要考虑锁的释放顺序，即按照获取锁的顺序依次释放所有的锁。这种实现方式简单、高效，并且可以避免死锁的发生。</p>
<p>[endchat]</p>
<h3><span id="可重入性2-代码实现">可重入性[2]-代码实现</span><a href="#可重入性2-代码实现" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line"></span><br><span class="line">public boolean lock() &#123;</span><br><span class="line"></span><br><span class="line">    //可重入的判断</span><br><span class="line"></span><br><span class="line">    synchronized (this) &#123;</span><br><span class="line"></span><br><span class="line">        if (lockCount.get() == 0) &#123;</span><br><span class="line"></span><br><span class="line">            thread = Thread.currentThread();</span><br><span class="line"></span><br><span class="line">            lockCount.incrementAndGet();</span><br><span class="line"></span><br><span class="line">        &#125; else &#123;</span><br><span class="line"></span><br><span class="line">            if (!thread.equals(Thread.currentThread())) &#123; ## 本线程是否已经占有锁</span><br><span class="line"></span><br><span class="line">            return false;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        lockCount.incrementAndGet();</span><br><span class="line"></span><br><span class="line">        return true;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   //....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="羊群效应-优化-1">羊群效应 优化 [1]</span><a href="#羊群效应-优化-1" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《从Paxos到Zookeeper分布式一致性原理与实践》 倪超 6.1.7节</li>
<li><a href="https://www.cnblogs.com/crazymakercircle/p/14504520.html">Zookeeper 分布式锁 （图解+秒懂+史上最全）</a> *** </li>
<li><a href="https://blog.csdn.net/u013278314/article/details/82715716">分布式锁（一）基于Zookeeper实现可重入分布式锁</a>  *</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Reactive-Actor</title>
    <url>/www6vHomeHexo/2021/05/10/reactive/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="reactive">Reactive</span><a href="#reactive" class="header-anchor">#</a></h2><h5><span id="akka-ampamp-actor">Akka &amp;&amp; Actor</span><a href="#akka-ampamp-actor" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">Responsive</th>
<th align="center">Resilient</th>
<th align="center">Elastic</th>
<th align="center">Message Driven</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Akka</td>
<td align="center">Actor and Stream</td>
<td align="center">Supervisor策略</td>
<td align="center">Load balance and adptive routing（dispatcher）</td>
<td align="center">mail-box, private State， backpressure</td>
</tr>
</tbody></table>
<ul>
<li><p>Actor 允许的操作</p>
<ul>
<li>create Actor</li>
<li>Send message</li>
<li>desigate how to handle the next message</li>
</ul>
</li>
<li><p>Actor Message：<br>Simple，Immutable data structures</p>
</li>
<li><p>Actor<br>Actors process only one message at a time</p>
</li>
<li><p>Actor 类型：<br>Local actor<br>Remote actor</p>
</li>
<li><p>哪些系统在用Akka<br>Flink，<br>spark 2.0从Akka迁移到Netty<br>play framework（web）</p>
</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV13e411W7kX?spm_id_from=333.1007.top_right_bar_window_history.content.click">5分钟理解分布式架构Actor模型</a><br><a href="https://www.reactivemanifesto.org/">The Reactive Manifesto</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>异步化</category>
      </categories>
      <tags>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis数据结构</title>
    <url>/www6vHomeHexo/2021/05/05/redisDataStructure/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="数据结构和实现">数据结构和实现</span><a href="#数据结构和实现" class="header-anchor">#</a></h2><p><img src="/www6vHomeHexo/redisDataStructure.png" alt="redis 数据结构"></p>
<hr>
<table>
<thead>
<tr>
<th align="center">数据结构</th>
<th align="center">基础</th>
<th align="center">优化</th>
<th align="center">特点</th>
</tr>
</thead>
<tbody><tr>
<td align="center">String</td>
<td align="center">SDS</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">List</td>
<td align="center">双向列表</td>
<td align="center">压缩列表</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Hash</td>
<td align="center">hash表</td>
<td align="center">压缩列表</td>
<td align="center">渐进式rehash</td>
</tr>
<tr>
<td align="center">SortedSet</td>
<td align="center">跳表</td>
<td align="center">压缩列表</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Set</td>
<td align="center">整数数组</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<hr>
<p><img src="/www6vHomeHexo/Complex.png" alt="数据结构的时间复杂度"></p>
<h2><span id="全局hash表-ampamp-渐进式-rehash">全局hash表 &amp;&amp; 渐进式 rehash</span><a href="#全局hash表-ampamp-渐进式-rehash" class="header-anchor">#</a></h2><p><img src="/www6vHomeHexo/globalHash.png" alt="全局Hash表"></p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p>02 | 数据结构:快速的Redis有哪些慢操作?</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis RDB</title>
    <url>/www6vHomeHexo/2021/01/02/redisRDB/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rdb">RDB</a><ul>
<li><a href="#%E5%8A%9F%E8%83%BD-1">功能 [1]</a></li>
<li><a href="#%E8%A7%A6%E5%8F%91%E6%9C%BA%E5%88%B6-2">触发机制 [2]</a></li>
<li><a href="#%E4%BC%98%E7%82%B9%E7%BC%BA%E7%82%B92">优点&#x2F;缺点[2]</a></li>
<li><a href="#%E5%BB%BA%E8%AE%AE-1">建议 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="rdb">RDB</span><a href="#rdb" class="header-anchor">#</a></h1><h3><span id="功能-1">功能 [1]</span><a href="#功能-1" class="header-anchor">#</a></h3><ul>
<li><p>只需要把 <strong>RDB 文件</strong>直接读入内存，可以快速恢复数据库。这就避免了 AOF 需要顺序、逐一重新执行操作命令带来的低效性能问题。</p>
</li>
<li><p>Redis 设计了 <strong>bgsave</strong> 和<strong>写时复制</strong>方式，尽可能减少了内存快照对正常读写的影响，但是，频繁快照仍然是不太能接受的。</p>
</li>
</ul>
<p>【 总结: <strong>bgsave 异步 + 写时复制（copy-on-write）</strong>】</p>
<h3><span id="触发机制-2">触发机制 [2]</span><a href="#触发机制-2" class="header-anchor">#</a></h3><ul>
<li>手动触发<ul>
<li>bgsave [3]</li>
<li><del>save 废弃</del>  [3]</li>
</ul>
</li>
<li>自动触发<ul>
<li>save的相关配置</li>
<li>从节点全量复制，主节点执行bgsave生成RDB并发送给从节点</li>
<li>其他</li>
</ul>
</li>
</ul>
<h3><span id="优点x2f缺点2">优点&#x2F;缺点[2]</span><a href="#优点x2f缺点2" class="header-anchor">#</a></h3><ul>
<li>优点<ul>
<li>加载RDB恢复数据远远快于AOF的方式</li>
</ul>
</li>
<li>缺点<ul>
<li>没办法做到实时持久化</li>
</ul>
</li>
</ul>
<h3><span id="建议-1">建议 [1]</span><a href="#建议-1" class="header-anchor">#</a></h3><ul>
<li><strong>数据不能丢失时</strong>，<strong>内存快照和 AOF 的混合使用</strong>是一个很好的选择；</li>
<li>如果允许<strong>分钟级别的数据丢失</strong>，可以只使用 <strong>RDB</strong>；</li>
<li>如果只用 <strong>AOF</strong>，优先使用 <strong>everysec</strong> 的配置选项，因为它在可靠性和性能之间取了一个平衡。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《05丨内存快照：宕机后，Redis如何实现快速恢复？》 </li>
<li>《redis开发与运维》 第5章</li>
<li><a href="/www6vHomeHexo/2021/12/08/rdb/" title="Redis RDB源码">Redis RDB源码</a> self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>centos7 内核升级到4.9  &amp;&amp;  bbr配置</title>
    <url>/www6vHomeHexo/2020/11/29/linuxKernelBBR/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>首先，让我们添加 ELRepo GPG key：</p>
<p>rpm –import <a href="https://www.elrepo.org/RPM-GPG-KEY-elrepo.org">https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</a></p>
<p>为 RHEL-6，SL-7，CentOS-7 源：</p>
<p>rpm -Uvh <a href="http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm">http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm</a></p>
<p>当然，别忘了 fastestmirror 还是需要的</p>
<p>yum install yum-plugin-fastestmirror</p>
<p>最后，安装 kernel 4.9</p>
<p>yum –enablerepo&#x3D;elrepo-kernel install kernel-ml</p>
<p>当然，将 kernel-ml 选为第一启动，首先查看系统的内核以及顺序</p>
<p>awk -F&#39; ‘$1&#x3D;&#x3D;”menuentry “ {print i++ “ : “ $2}’ &#x2F;etc&#x2F;grub2.cfg</p>
<p>看下你当前默认启动项</p>
<p>grub2-editenv list</p>
<p>将 kernel-ml 版本的内核设置为默认启动内核</p>
<p>grub2-set-default N</p>
<p>以后升级内核默认启用 kernel-ml，编辑文件 &#x2F;etc&#x2F;sysconfig&#x2F;kernel</p>
<p>DEFAULTKERNEL&#x3D;kernel-ml</p>
<p>同时编辑文件 &#x2F;etc&#x2F;yum.repos.d&#x2F;elrepo.repo，在 [elrepo-kernel] 下   </p>
<p>enabled&#x3D;1</p>
<p>开启 BBR TCP<br>echo “net.core.default_qdisc&#x3D;fq” &gt;&gt; &#x2F;etc&#x2F;sysctl.conf<br>echo “net.ipv4.tcp_congestion_control&#x3D;bbr” &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</p>
<p>重启后，首先 uname -a 看下内核是否切换到 4.9，然后执行下面明亮查看内核是否开启 TCP BBR：</p>
<p>sysctl net.ipv4.tcp_available_congestion_control<br>sysctl net.ipv4.tcp_congestion_control</p>
<p>查看 tcp_bbr 模块是否加载：</p>
<p>lsmod | grep tcp_bbr</p>
<p>参考：<br>升级 CentOS 内核至 4.9 good<br><a href="https://havee.me/linux/2017-01/upgrade-kernel-4-9-for-centos.html">https://havee.me/linux/2017-01/upgrade-kernel-4-9-for-centos.html</a></p>
<p>Centos内核升级的三种方法<br><a href="https://www.cnblogs.com/zengkefu/p/5667145.html">https://www.cnblogs.com/zengkefu/p/5667145.html</a></p>
<p><a href="https://blog.51cto.com/7794482/2462466">https://blog.51cto.com/7794482/2462466</a><br><a href="http://www.d0z.net/archives/7804">http://www.d0z.net/archives/7804</a><br><a href="https://zhuanlan.zhihu.com/p/47372096">https://zhuanlan.zhihu.com/p/47372096</a><br><a href="https://www.cnblogs.com/cnwangshijun/p/7405153.html">https://www.cnblogs.com/cnwangshijun/p/7405153.html</a></p>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>升级</category>
      </categories>
      <tags>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>范式Paradigm-语言</title>
    <url>/www6vHomeHexo/2020/11/08/paradigm/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="语言范式">语言范式</span><a href="#语言范式" class="header-anchor">#</a></h2><h5><span id="面向对象">面向对象</span><a href="#面向对象" class="header-anchor">#</a></h5><h5><span id="函数式">函数式</span><a href="#函数式" class="header-anchor">#</a></h5><ul>
<li><p>成熟度<br>Clojure<br>Scala，Java， Javascript</p>
</li>
<li><p>Function is First Class<br>闭包;<br>高阶函数;<br>柯里化： Scala的柯里化<br>部分施用 partial：Scala的部分施用<br>【更加接近人的语言】</p>
</li>
<li><p>并行<br>Java Lamda，Java Stream<br>Spark算子（基本构造单元）</p>
</li>
<li><p>效率<br>Scala的效率生产率是Java的2-3倍，<br>Scala 10行代码Java要20-30行</p>
</li>
</ul>
<h5><span id="面向过程">面向过程</span><a href="#面向过程" class="header-anchor">#</a></h5><p>【用了面向对象语言写出的不一定是面向对象的代码】</p>
<h2><span id="语言编程范式">语言编程范式</span><a href="#语言编程范式" class="header-anchor">#</a></h2><h5><span id="传引用">传引用</span><a href="#传引用" class="header-anchor">#</a></h5><p>JAVA（native）, C++指针</p>
<h5><span id="传值">传值</span><a href="#传值" class="header-anchor">#</a></h5><p>Java的clone浅拷贝 vs 深拷贝 ，<br>C++拷贝构造</p>
<h5><span id="所有权">所有权</span><a href="#所有权" class="header-anchor">#</a></h5><p>Rust，<br>C++ move所有权</p>
<h2><span id="语言编程范式">语言编程范式</span><a href="#语言编程范式" class="header-anchor">#</a></h2><h5><span id="命令式">命令式</span><a href="#命令式" class="header-anchor">#</a></h5><h5><span id="申明式">申明式</span><a href="#申明式" class="header-anchor">#</a></h5><p>Spring Annotation 申明式<br>K8S yaml 申明式</p>
<h2><span id="并发范式">并发范式</span><a href="#并发范式" class="header-anchor">#</a></h2><h5><span id="多线程-锁">多线程 +锁</span><a href="#多线程-锁" class="header-anchor">#</a></h5><p>Java</p>
<h5><span id="csp-协程">CSP 协程</span><a href="#csp-协程" class="header-anchor">#</a></h5><p>Golang，Rust，<br>C++， Python， Lua<br>Java协程框架</p>
<h5><span id="actor">Actor</span><a href="#actor" class="header-anchor">#</a></h5><p>Scala<br>Akka</p>
<h5><span id="函数式">函数式</span><a href="#函数式" class="header-anchor">#</a></h5><h5><span id="分离标识与状态">分离标识与状态</span><a href="#分离标识与状态" class="header-anchor">#</a></h5><p>Clojure</p>
<h5><span id="future-promise">Future + Promise</span><a href="#future-promise" class="header-anchor">#</a></h5><hr>
]]></content>
      <categories>
        <category>语言</category>
        <category>范式</category>
      </categories>
      <tags>
        <tag>范式</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase总结</title>
    <url>/www6vHomeHexo/2020/09/04/hbase/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B">数据模型</a><ul>
<li><a href="#%E9%80%BB%E8%BE%91%E8%A7%86%E5%9B%BE">逻辑视图</a></li>
<li><a href="#%E7%89%A9%E7%90%86%E8%A7%86%E5%9B%BE">物理视图</a></li>
</ul>
</li>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a><ul>
<li><a href="#regionserver">RegionServer</a></li>
</ul>
</li>
<li><a href="#%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%92%8C%E8%AF%BB%E4%BC%98%E5%8C%96">读写流程和读优化</a></li>
<li><a href="#region%E5%88%86%E8%A3%82split">Region分裂（split）</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">优化和最佳实践</a><ul>
<li><a href="#%E9%A2%84%E5%88%86%E5%8C%BA"><strong>预分区</strong></a></li>
<li><a href="#row_key%E8%AE%BE%E8%AE%A1-%E9%98%B2%E6%AD%A2%E7%83%AD%E7%82%B9%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%953">Row_key设计 防止热点的三种方法[3]</a></li>
</ul>
</li>
<li><a href="#%E6%80%A7%E8%83%BD%E5%92%8C%E7%89%88%E6%9C%AC">性能和版本</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD">性能</a></li>
<li><a href="#%E7%89%88%E6%9C%AC">版本</a></li>
<li><a href="#%E5%B7%A5%E4%BD%9C%E4%B8%AD%E6%8E%A5%E8%A7%A6%E7%9A%84">工作中接触的</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据模型">数据模型</span><a href="#数据模型" class="header-anchor">#</a></h1><h3><span id="逻辑视图">逻辑视图</span><a href="#逻辑视图" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2020/09/04/hbase/model-logic.jpg" class title="hbase逻辑视图">
<p><strong>row</strong>: <code>rowkey + (column + columnValue)*n</code><br><strong>column</strong>: <code>column family:qualifier</code>   column family固定, qualifier可增加<br><strong>cell</strong>： (row, column, timestamp, type, value) </p>
<h3><span id="物理视图">物理视图</span><a href="#物理视图" class="header-anchor">#</a></h3><p>HBase中的数据是按照列簇存储的。<br>KV存储结构 K( (row, column, timestamp, type) ) -&gt; V( value )<br><code><br>&#123;"com.cnn.www","anchor","cnnsi.com","put","t9"&#125; -> "CNN"<br>&#123;"com.cnn.www","anchor","my.look.ca","put","t8"&#125; -> "CNN.com"<br>&#123;"com.cnn.www","contents","html","put","t7"&#125; -> "..."<br>&#123;"com.cnn.www","contents","html","put","t6"&#125; -> "..."<br>&#123;"com.cnn.www","contents","html","put","t5"&#125; -> "..."<br>&#123;"com.example.www","people","author","put","t5"&#125; -> "John Doe"<br></code></p>
<h1><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2020/09/04/hbase/arch.JPG" class title="hbase体系结构">

<h3><span id="regionserver">RegionServer</span><a href="#regionserver" class="header-anchor">#</a></h3><ul>
<li>HLog （WAL）<br>一个RegionServer有一个或者多个HLog；<br><strong>每个HLog是多个Region共享的</strong>；<br>生命周期： 构建，滚动， 失效（MemStore落盘后）， 删除；</li>
<li>BlockCache<br>LRUBlockCache(时间局部性)；<br>BucketCache(空间局部性)；</li>
<li>Region<br><strong>分片， 水平切分（split）， 负载均衡的基本单位</strong><br>Region包含<code>n个Store</code>   <strong>Store&#x3D;&#x3D;column family</strong><br>Store包含<code>1个MemStore</code>和<code>n个HFile</code><br>MemStore： 写缓存<br>HFile的Compact操作， 小文件合并成大文件  </li>
<li>MemStore [1]</li>
<li>HFile [1]</li>
</ul>
<h1><span id="读写流程和读优化">读写流程和读优化</span><a href="#读写流程和读优化" class="header-anchor">#</a></h1><ul>
<li><p>读写流程</p>
<ul>
<li>写流程<br>客户端写入MemStore和HFile就表示写入成功。<br>Flush的时机： 三个flush的参数</li>
<li>读流程<br><strong>BlockCache没有数据的前提下， MemStore和StoreFile（HFile）都会读取数据。</strong></li>
<li>总结<br>Master不参与读写流程，但是master宕机了，集群会处于不健康状态， Region分裂后改不了元数据。<br>客户端只要配置Zookeeper地址，Master的切换对客户端是透明的。<br>客户端缓存Meta数据， RegionServer的BlockCache缓存StoreFile（HFile）的数据。</li>
</ul>
</li>
<li><p>HFile Compaction  读优化</p>
<ul>
<li>类型<br> Minor Compaction: 小的相邻HFile合并成更大的HFiile。<br> Major Compaction: 一个store中所有的HFile合并成一个HFile。<strong>线上建议关闭自动触发，改为在低峰期手动或者自动触发</strong>。<br> <strong>Minor Compaction不会删除数据，Major Compaction会删除数据。</strong><br> <strong>Minor Compaction 合并后，旧的数据不会马上删除， 会对客户端不可见。</strong> </li>
<li>Compaction作用<br>减少文件数， <strong>稳定随机读延迟</strong>; 用短时间的IO消耗以及带宽消耗换取后序读操作的低延迟。（空间换时间）<br>清除无效数据，较少数据存储量。</li>
</ul>
</li>
<li><p>数据删除时机<br>flush和Major Compaction的时候会删除冗余的数据。<br>flush时只删除内存的冗余数据，不删除”Delete标记”,因为在Major Compaction删冗余数据的时候会用到这个”Delete标记”。</p>
</li>
</ul>
<h1><span id="region分裂split">Region分裂（split）</span><a href="#region分裂split" class="header-anchor">#</a></h1><ul>
<li>寻找分裂点</li>
<li>Region迁移的状态存在meta表， Master内存， zookeeper的region-in-transition节点<br><strong>RIT状态</strong>： Region在三个地方不能保持一致</li>
<li>整个分裂过程包装成了一个事务， 保证分裂事务的原子性。<br><strong>迁移的中间状态都只存储在内存中， 一旦在分裂过程中出现RegionServer宕机，有可能出现RIT状态， 需要HBCK工具分析并解决。</strong></li>
<li><strong>Region分裂过程没有涉及数据的移动， 分裂后的子Region的文件没有任何用户数据。 [通过reference文件来查找数据，像游标offset]</strong><br><strong>真正数据迁移的迁移发生在子Region执行Major Compaction时。</strong></li>
</ul>
<h1><span id="优化和最佳实践">优化和最佳实践</span><a href="#优化和最佳实践" class="header-anchor">#</a></h1><h3><span id="预分区"><strong>预分区</strong></span><a href="#预分区" class="header-anchor">#</a></h3><p>自动split有数据倾斜问题，所以要预分区。<br>建议: 生产中列簇不要太多, 列簇数据要有同步增长的趋势（不要一个列簇有很多数据， 其他列簇数据很少），<br>      防止Region分裂时产生多个小文件。</p>
<h3><span id="row_key设计-防止热点的三种方法3">Row_key设计  防止热点的三种方法[3]</span><a href="#row_key设计-防止热点的三种方法3" class="header-anchor">#</a></h3><ol>
<li>Salt</li>
<li>Hash</li>
<li>Reversing</li>
</ol>
<h1><span id="性能和版本">性能和版本</span><a href="#性能和版本" class="header-anchor">#</a></h1><h3><span id="性能">性能</span><a href="#性能" class="header-anchor">#</a></h3><p>单表 千亿行， 百万列  容量TB甚至PB级别</p>
<h3><span id="版本">版本</span><a href="#版本" class="header-anchor">#</a></h3><p><strong>v0.98</strong>    目前生产线上使用最广泛的版本之一<br><strong>v1.4.10</strong>   HBase社区推荐使用的稳定版本<br><strong>v2.x</strong>    最受期待的一个版本</p>
<h3><span id="工作中接触的">工作中接触的</span><a href="#工作中接触的" class="header-anchor">#</a></h3><ul>
<li>MVCC</li>
<li>读写队列 - 4个核心参数</li>
<li>TTL - TTL失效问题</li>
<li>多租户方案<br>rsgroup</li>
<li>二级索引  </li>
<li>超时 Timeout</li>
<li>RIT</li>
<li>高可用<br>双向同步</li>
<li>优化</li>
<li>bulkload</li>
<li>CAS<br>HBase提供基于单 行 数据操作的原子性保证<br>HBase基于行锁来保证单行操作的原子性<br>checkAndPut&#x2F;checkAndDelete&#x2F;increment&#x2F;append</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="/www6vHomeHexo/2023/04/02/hbaselsmTree/" title="HBase - LSM-Tree">HBase - LSM-Tree</a></li>
<li>《Hbase原理和实践》 胡争  范欣欣   第1,2,5,7，8章</li>
<li><a href="/www6vHomeHexo/2021/06/07/hbaseHotkey/" title="HBase Hotkey-预分区和Rowkey设计">HBase Hotkey-预分区和Rowkey设计</a></li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>事件 Event</title>
    <url>/www6vHomeHexo/2020/08/23/event/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>

<h2><span id="事件">事件</span><a href="#事件" class="header-anchor">#</a></h2><ul>
<li><p>K8s<br>Kubernetes Events<br><a href="https://github.com/AliyunContainerService/kube-eventer">AliyunContainerService&#x2F;kube-eventer</a><br>Informer + Queue</p>
</li>
<li><p>Flink<br>CEP 复杂事件处理<br>EventTime， IngestTime，ProcessingTime</p>
</li>
<li><p>Redis<br>文件事件<br>时间事件</p>
</li>
<li><p>Servlet规范<br>Listener 时间监听器</p>
</li>
<li><p>DDD<br><a href="https://martinfowler.com/eaaDev/DomainEvent.html">领域事件</a> Martin Fowler<br>事件驱动架构（EDA）<br>Event Source</p>
</li>
<li><p>EIP 企业集成模式<br>apache camel</p>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>事件</category>
      </categories>
      <tags>
        <tag>事件</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux性能优化 之 cpu优化</title>
    <url>/www6vHomeHexo/2020/08/16/linuxPerformance-cpu/</url>
    <content><![CDATA[<p hidden></p>

<span id="more"></span>

<h2><span id="一-工具与指标">一. 工具与指标</span><a href="#一-工具与指标" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2020/08/16/linuxPerformance-cpu/metric2tool.JPG" class title="从指标找工具">

<img src="/www6vHomeHexo/2020/08/16/linuxPerformance-cpu/tool2metric.JPG" class title="从工具找指标">

<h2><span id="二-最常用的cpu工具">二. 最常用的cpu工具</span><a href="#二-最常用的cpu工具" class="header-anchor">#</a></h2><div style="width:60%; height:60%; text-align: center;">
    
<p><img src="https://user-images.githubusercontent.com/5608425/65083664-b6d52d00-d9db-11e9-918c-2a99c5708189.jpg" alt="最常用的cpu工具"><br>最常用的cpu工具 工具中指标之间的关系</p>
</div>

<h2><span id="三-cpu优化案例">三. cpu优化案例</span><a href="#三-cpu优化案例" class="header-anchor">#</a></h2><h3><span id="1-上下文切换的案例">1. 上下文切换的案例。</span><a href="#1-上下文切换的案例" class="header-anchor">#</a></h3><p><strong>自愿上下文切换</strong>，是指<strong>进程无法获取所需资源</strong>，导致的上下文切换。比如说， I&#x2F;O、内存等系统资源不足时，就会发生自愿上下文切换。<br><strong>非自愿上下文切换</strong>，则是指进程由于<strong>时间片已到等原因</strong>，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。</p>
<blockquote>
<p>先用 vmstat ，查看了系统的上下文切换次数和中断次数；然后通过pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过 pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源，也就是我们的基准测试工具 sysbench。</p>
</blockquote>
<h3><span id="2-进程-cpu-使用率升高的案例">2. 进程 CPU 使用率升高的案例</span><a href="#2-进程-cpu-使用率升高的案例" class="header-anchor">#</a></h3><p>可能代码里有死循环</p>
<h3><span id="3-不可中断进程和僵尸进程的案例">3. 不可中断进程和僵尸进程的案例</span><a href="#3-不可中断进程和僵尸进程的案例" class="header-anchor">#</a></h3><blockquote>
<p>先用 top 观察到了 <strong>iowait 升高</strong>的问题，并发现了大量的不可中断进程和僵尸进程；接着用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但我们用 strace 查看进程系统调用却失败了，最终还是用 perf 分析进程调用链，才发现根源在于<strong>磁盘直接 I&#x2F;O</strong> 。<br>僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，要解决掉它们，就要找到它们的根儿，也就是找出父进程，然后在父进程里解决。</p>
</blockquote>
<h3><span id="4软中断的案例">4.软中断的案例。</span><a href="#4软中断的案例" class="header-anchor">#</a></h3><blockquote>
<p>通过 top 观察到，系统的软中断 CPU 使用率升高；接着查看&#x2F;proc&#x2F;softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用 tcpdump ，找出网络帧的类型和来源，确定是一个<strong>SYN FLOOD 攻击</strong>导致的。</p>
</blockquote>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>&lt;&lt;Linux性能优化实战 - 11 - 套路篇：如何迅速分析出系统CPU的瓶颈在哪里？&gt;&gt; 倪朋飞</li>
<li>&lt;&lt;Linux性能优化实战 - 04讲基础篇：经常说的CPU上下文切换是什么意思（下）&gt;&gt; 倪朋飞</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>性能优化</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>虚拟机和容器中的内核参数 kernel</title>
    <url>/www6vHomeHexo/2020/08/16/linuxKernelParam/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="重要的内核参数">重要的内核参数</span><a href="#重要的内核参数" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">##内存与交换分区</span><br><span class="line">vm.swappiness</span><br><span class="line"></span><br><span class="line"># 内存分配  </span><br><span class="line">## OOM</span><br><span class="line">vm.overcommit_memory=2:  过量使用.   0, 1, 2</span><br><span class="line">  RAM, swap</span><br><span class="line">vm.overcommit_ratio=50</span><br><span class="line">比如：</span><br><span class="line">swap: 2G ，RAM: 8G</span><br><span class="line">则 memory = swap + RAM * ratio =  2G + 8G * 50% = 6G</span><br><span class="line">        </span><br><span class="line">如何充分使用内存：</span><br><span class="line">  1. swap跟RAM一样大； swappiness=0</span><br><span class="line">  2. vm.overcommit_memory=2，  vm.overcommit_ratio=100， swappiness=0 (生产中)</span><br><span class="line">     memory = swap + RAM * ratio        </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 网络</span><br><span class="line">net.ipv4.ip_forward  # ipv4的路由转发功能</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse = 1              #开启重用，允许将TIME_WAIT socket用于新的TCP连接。默认为0，表示关闭。</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1            #开启TCP连接中TIME_WAIT socket的快速回收。默认值为0，表示关闭</span><br><span class="line">。</span><br><span class="line">net.ipv4.tcp_syncookies = 1            #开启SYN cookie，出现SYN等待队列溢出时启用cookie处理，防范少量的SYN攻击。默认为0，表示关闭。</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_keepalive_time = 600      #keepalived启用时TCP发送keepalived消息的拼度。默认位2小时。</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 5      #TCP发送keepalive探测以确定该连接已经断开的次数。根据情形也可以适当地缩短此值。</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 15      #探测消息发送的频率，乘以tcp_keepalive_probes就得到对于从开始探测以来没有响应的连接杀除的时间。</span><br><span class="line">                                        默认值为75秒，也就是没有活动的连接将在大约11分钟以后将被丢弃。</span><br><span class="line">                                        对于普通应用来说,这个值有一些偏大,可以根据需要改小.特别是web类服务器需要改小该值。</span><br><span class="line">net.ipv4.ip_local_port_range = 1024 65000 #指定外部连接的端口范围。默认值为32768 61000。</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 262144  #表示SYN队列的长度，预设为1024，这里设置队列长度为262 144，以容纳更多的等待连接。</span><br><span class="line">net.core.somaxconn = 16384             #定义了系统中每一个端口最大的监听队列的长度, 对于一个经常处理新连接的高负载 web服务环境来说，默认值为128，偏小。</span><br><span class="line"></span><br><span class="line">## 缓冲区大小</span><br><span class="line">net.ipv4.tcp_mem</span><br><span class="line">net.ipv4.tcp_rmem</span><br><span class="line">net.ipv4.tcp_wmem</span><br><span class="line"></span><br><span class="line">## core</span><br><span class="line">wmem_max</span><br><span class="line">wmem_default</span><br><span class="line">rmem_max</span><br><span class="line">rmem_default</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># IPC - 进程通讯</span><br><span class="line">  message</span><br><span class="line">    msgmni</span><br><span class="line">    msgmax</span><br><span class="line">    msgmnb</span><br><span class="line">  shm - sharememory</span><br><span class="line">    shmall</span><br><span class="line">    shmmax</span><br><span class="line">    shmmni</span><br><span class="line">  semaphore  </span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ls /proc/sys</span><br><span class="line">abi  crypto  debug  dev  fs  kernel  net  vm</span><br><span class="line"></span><br><span class="line"># 多线程</span><br><span class="line">vm.max_map_count </span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 进程有若干操作系统资源的限制</span><br><span class="line">➜  ~ cat /proc/1/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units</span><br><span class="line">Max cpu time              unlimited            unlimited            seconds</span><br><span class="line">Max file size             unlimited            unlimited            bytes</span><br><span class="line">Max data size             unlimited            unlimited            bytes</span><br><span class="line">Max stack size            8388608              unlimited            bytes</span><br><span class="line">Max core file size        0                    unlimited            bytes</span><br><span class="line">Max resident set          unlimited            unlimited            bytes</span><br><span class="line">Max processes             15188                15188                processes</span><br><span class="line">Max open files            65536                65536                files</span><br><span class="line">Max locked memory         65536                65536                bytes</span><br><span class="line">Max address space         unlimited            unlimited            bytes</span><br><span class="line">Max file locks            unlimited            unlimited            locks</span><br><span class="line">Max pending signals       15188                15188                signals</span><br><span class="line">Max msgqueue size         819200               819200               bytes</span><br><span class="line">Max nice priority         0                    0</span><br><span class="line">Max realtime priority     0                    0</span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br></pre></td></tr></table></figure>

<h2><span id="容器中的内核参数">容器中的内核参数</span><a href="#容器中的内核参数" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 已经namespace化的内核参数：</span><br><span class="line">kernel.shm*,</span><br><span class="line">kernel.msg*,</span><br><span class="line">kernel.sem,</span><br><span class="line">fs.mqueue.*,</span><br><span class="line">net.*.</span><br><span class="line"></span><br><span class="line"># 没有namespace化</span><br><span class="line">vm.*</span><br><span class="line"></span><br><span class="line"># k8s把syctl参数分为safe和unsafe</span><br><span class="line"># 只有三个参数被认为是safe的</span><br><span class="line">kernel.shm_rmid_forced,</span><br><span class="line">net.ipv4.ip_local_port_range,</span><br><span class="line">net.ipv4.tcp_syncookies</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><h3><span id="内核参数">内核参数</span><a href="#内核参数" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.jianshu.com/p/3096a8e6a36f">Linux内核常见参数的优化</a></li>
<li><a href="https://blog.csdn.net/vic_qxz/article/details/82853447">单个JVM下支撑100w线程数vm.max_map_count</a></li>
<li><a href="http://www.firefoxbug.com/index.php/archives/2800/">Linux的overcommit配置</a></li>
<li><a href="https://tencentcloudcontainerteam.github.io/2018/11/19/kernel-parameters-and-container/">给容器设置内核参数</a> good</li>
<li><a href="../../../../2015/04/25/tcp/">TCP总结</a> self</li>
<li><a href="https://www.bilibili.com/video/BV1W7411J7DP?p=5&vd_source=f6e8c1128f9f264c5ab8d9411a644036">06_虚拟化技术基础原理详解</a> V</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>参数</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 事务-隔离性</title>
    <url>/www6vHomeHexo/2020/08/14/mysqlTransactionAndLock/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#myisam-vs-innodb">MyISAM vs. InnoDB</a></li>
<li><a href="#%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-4">事务隔离级别 [4]</a><ul>
<li><a href="#%E6%96%B0%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB">新的隔离级别</a></li>
<li><a href="#rc%E5%92%8Crr%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-chat">RC和RR隔离级别 [chat]</a></li>
</ul>
</li>
<li><a href="#mvcc">MVCC</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86-23">原理 [2][3]</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0">实现</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="myisam-vs-innodb">MyISAM vs. InnoDB</span><a href="#myisam-vs-innodb" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th align="center">描述</th>
<th align="center">MyISAM</th>
<th align="center">InnoDB</th>
</tr>
</thead>
<tbody><tr>
<td align="center">行锁(并发高，会死锁)</td>
<td align="center">×</td>
<td align="center">√ (默认支持)<br>Record lock: 锁记录<br>Gap lock: 锁范围，不锁记录<br>Next-key lock： 锁范围+锁记录</td>
</tr>
<tr>
<td align="center">表锁(并发低，不会死锁)</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">事务和崩溃恢复</td>
<td align="center">×</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">外键</td>
<td align="center">×</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">MVCC</td>
<td align="center">×</td>
<td align="center">√ <br> 在READ COMMITTED 和 REPEATABLE READ时有效</td>
</tr>
</tbody></table>
<h1><span id="事务隔离级别-4">事务隔离级别  [4]</span><a href="#事务隔离级别-4" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th align="center">隔离级别(从高到低)</th>
<th align="center">脏读</th>
<th align="center">不可重复读<br>（重点是修改）</th>
<th align="center">幻影读<br>（重点是新增或者删除）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SERIALIZABLE</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr>
<td align="center">REPEATABLE-READ<br>（<strong>InnoDB默认隔离级别</strong>）</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">READ-COMMITTED</td>
<td align="center">×</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">READ-UNCOMMITTED</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
</tbody></table>
<blockquote>
<p>innodb对于行的查询使用next-key lock<br>  <strong>Next-locking keying、Gap锁为了解决Phantom Problem幻读问题</strong><br>  当查询的索引含有唯一属性时(单条记录)，将next-key lock降级为record key</p>
</blockquote>
<h3><span id="新的隔离级别">新的隔离级别</span><a href="#新的隔离级别" class="header-anchor">#</a></h3><ul>
<li>SI[Snapshot Isolation]<br>Oracle 可串行化,  PG和MySQL称为RR</li>
<li>SSI[Serializable Snapshot Isolation]<br>PostgreSQL 和 CockroachDB 已经支持 SSI</li>
</ul>
<h3><span id="rc和rr隔离级别-chat">RC和RR隔离级别 [chat]</span><a href="#rc和rr隔离级别-chat" class="header-anchor">#</a></h3><p>下面是一个表格，归纳了以上文字中RC和RR隔离级别的特点：</p>
<table>
<thead>
<tr>
<th>隔离级别</th>
<th>快照读 #1</th>
<th>当前读 #2</th>
<th>幻读</th>
</tr>
</thead>
<tbody><tr>
<td>RC</td>
<td>不加锁</td>
<td>加锁[记录锁 是,间隙锁 否]</td>
<td>存在</td>
</tr>
<tr>
<td>RR</td>
<td>不加锁</td>
<td>加锁[记录锁 是, 间隙锁 是]</td>
<td>不存在</td>
</tr>
</tbody></table>
<p>在RC隔离级别下，快照读和当前读都不会对记录加锁，因此不会阻塞其他事务的读操作。但是，由于RC隔离级别只对读取到的记录加锁，而不对读取的范围加锁，因此可能会出现幻读现象。幻读指的是，在一个事务中先后进行两次相同的查询操作，第二次查询会发现有新增的记录，这种现象是由于其他事务在事务中新增了这些记录所导致的。</p>
<p>在RR隔离级别下，快照读和当前读都不会对记录加锁，但是会对读取的范围加锁，防止其他事务在该范围内插入新的记录。因此，在RR隔离级别下不存在幻读现象。</p>
<p>需要注意的是，虽然RR隔离级别可以避免幻读现象，但是由于对读取范围加锁可能会导致性能问题，因此在实际应用中需要根据具体情况选择合适的隔离级别。<br>[ 当前读   加锁，快照读  不加锁 ]</p>
<h1><span id="mvcc">MVCC</span><a href="#mvcc" class="header-anchor">#</a></h1><h3><span id="原理-23">原理  [2][3]</span><a href="#原理-23" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2020/08/14/mysqlTransactionAndLock/mvcc.JPG" class title="MVCC（一致性读视图）">

<p>  InnoDB 中的 <strong>RC(READ COMMITTED) 和 RR(REPEATABLE READ) 隔离事务</strong>是基于<strong>多版本并发控制（MVVC）</strong>实现高性能事务。<br>  <strong>MVCC 对普通的 Select 不加锁</strong>，如果读取的数据正在执行 Delete 或 Update 操作，这时读取操作不会等待排它锁的释放，而是<strong>直接利用 MVCC 读取该行的数据快照</strong>（数据快照是指在该行的之前版本的数据，而数据快照的版本是基于 undo 实现的，undo 是用来做事务回滚的，记录了回滚的不同版本的行记录）。</p>
<p>  <strong>MySQL默认的事务隔离级别是RR(REPEATABLE READ)</strong>, InnoDB引擎的Select操作使用一致性非锁定读（MVCC）。 对于一致性非锁定读， 即使读取的行已经被执行了select…for update,也是可以读取的。</p>
<h3><span id="实现">实现</span><a href="#实现" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>当前读&#x2F;快照读</th>
<th>含义</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>当前读 #2</td>
<td>读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。</td>
<td><code>select ... lock in share mode</code>(共享锁)，<br><code>select ...for update</code>、<br><code>update</code>、<code>insert</code>、<code>delete</code>(排他锁)</td>
</tr>
<tr>
<td>快照读 #1</td>
<td>简单的select（不加锁）就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。</td>
<td>Read Committed：每次select，都生成一个快照读。<br> Repeatable Read：开启事务后第一个select语句才是快照读的地方。<br> Serializable：快照读会退化为当前读。</td>
</tr>
</tbody></table>
<ul>
<li>MVCC实现 [0]<ul>
<li>隐藏字段<br>DB_TRX_ID:  最近修改事务ID<br>DB_ROLL_PTR: 回滚指针<br>DB_ROW_ID: 隐藏主键    </li>
<li>undolog版本链<br>链表的头部是最新的旧记录，链表尾部是最早的旧记录</li>
<li>readview<ul>
<li>ReadView（读视图）是 <strong>快照读</strong> SQL执行时MVCC提取数据的依据，记录并维护系统当前活跃的事务（未提交的）id。</li>
<li>不同的隔离级别，生成ReadView的时机不同：<ul>
<li>READ COMMITTED ：在事务中每一次执行快照读时生成ReadView。</li>
<li>REPEATABLE READ：仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?p=78">黑马程序员 MySQL数据库入门到精通</a>  P141-P144<br><a href="https://github.com/www6v/mysql_note">mysql_note</a> 笔记1<br><a href="https://frxcat.fun/database/MySQL/MySQL_Advanced_index/">MySQL 索引</a> 笔记2 ***</p>
</li>
<li><p>《深入浅出MySQL：数据库开发、优化与管理维护》 </p>
</li>
<li><p>《03 | 事务隔离：为什么你改了我还看不见？ 》MySQL实战45讲  丁奇</p>
</li>
<li><p>《33 | MySQL调优之事务：高并发场景下的数据库事务调优》 Java性能调优实战    刘超 deleted</p>
</li>
<li><p><a href="https://segmentfault.com/a/1190000019619667">可能是全网最好的MySQL重要知识点</a>  </p>
</li>
<li><p>《07 | 行锁功过：怎么减少行锁对性能的影响？》 MySQL实战45讲  丁奇  deleted</p>
</li>
<li><p>《18 | 为什么这些SQL语句逻辑相同性能却差异巨大？》MySQL实战45讲  丁奇</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1V3411z7Hj/">MYSQL死锁的检测与预防</a>  deleted</p>
</li>
<li><a href="/www6vHomeHexo/2015/02/21/mysqlTransaction/" title="MySQL事务-总结">MySQL事务-总结</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Rocketmq中的事务</title>
    <url>/www6vHomeHexo/2020/08/12/mqRocketmqTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rocketmq-%E4%BA%8B%E5%8A%A1">RocketMQ 事务</a></li>
<li><a href="#%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93-2">事务总结 [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><br>- <a href="#%E4%BA%8B%E5%8A%A1">事务</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="rocketmq-事务">RocketMQ 事务</span><a href="#rocketmq-事务" class="header-anchor">#</a></h2><ul>
<li><p>事务消息【6，7】</p>
<ul>
<li>ebay模式，half消息<ul>
<li>1.提交流程（正向流程）<br>  1.发消息（half消息）<ol start="2">
<li>执行本地事务</li>
<li>更新事务消息的最终状态</li>
</ol>
</li>
<li>2.回查流程（逆向流程）<br>  找到尚未提交的超时事务</li>
<li>大事务 &#x3D; 小事务 + 异步（阿里中台战略）【8】</li>
</ul>
</li>
</ul>
</li>
<li><p>半消息<br><strong>半消息</strong>保存在特殊的内部主题RMQ_SYS_TRANS_HALF_TOPIC中，使用的队列号固定为0。这个主题和队列对消费者是不可见的。</p>
</li>
</ul>
<h2><span id="事务总结-2">事务总结 [2]</span><a href="#事务总结-2" class="header-anchor">#</a></h2><p>RocketMQ 和 Kafka 的事务都是基于<strong>两阶段提交</strong>来实现的事务，都利用了<strong>特殊的主题中的队列和分区</strong>来记录<strong>事务日志</strong>.<br>RocketMQ 和 Kafka 的事务，它们的适用场景是不一样的，<strong>RocketMQ的事务适用于解决本地事务和发消息的数据一致性问题</strong>，而 <strong>Kafka 的事务则是用于实现它的 Exactly Once 机制，应用于实时计算的场景中</strong>。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651008627&idx=1&sn=a308010e080e1aa7784abb4a1bcaadb7&chksm=bdbed6208ac95f3614f4055821e870882ea207e8a58af48f043e78cb4391e6f1206b41f86a88&scene=27#wechat_redirect">RocketMQ 4.3正式发布，支持分布式事务</a></li>
<li>《消息队列高手课 - 25 | RocketMQ与Kafka中如何实现事务？》 李玥</li>
</ol>
<h5><span id="事务">事务</span><a href="#事务" class="header-anchor">#</a></h5><ol start="6">
<li><a href="https://zhuanlan.zhihu.com/p/396726719">分布式开放消息系统(RocketMQ)的原理与实践</a>   CHEN川  ***<br>消息的顺序问题  消息的重复问题</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651008627&idx=1&sn=a308010e080e1aa7784abb4a1bcaadb7">RocketMQ 4.3正式发布，支持分布式事务</a></li>
<li><a href="/www6vHomeHexo/2019/05/02/middleStage/" title="中台战略">中台战略</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>TIME_WAIT和优化</title>
    <url>/www6vHomeHexo/2020/08/09/tcpTimewait/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%85%E9%9A%9C%E7%8E%B0%E8%B1%A1">故障现象</a></li>
<li><a href="#time_wait%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%9C%E7%94%A8">TIME_WAIT概念和作用</a><ul>
<li><a href="#%E6%A6%82%E5%BF%B5">概念</a></li>
<li><a href="#%E4%BD%9C%E7%94%A8">作用</a></li>
</ul>
</li>
<li><a href="#time_wait%E7%9A%84%E5%89%AF%E4%BD%9C%E7%94%A8%E5%92%8C%E4%BC%98%E5%8C%96">TIME_WAIT的副作用和优化</a><ul>
<li><a href="#tcp%E5%92%8Ctime_wait">TCP和TIME_WAIT</a></li>
<li><a href="#%E5%89%AF%E4%BD%9C%E7%94%A8%E5%92%8C%E4%BC%98%E5%8C%96">副作用和优化</a></li>
</ul>
</li>
<li><a href="#%E5%A4%A7%E9%87%8Ftime_wait%E7%9A%84%E9%97%AE%E9%A2%98gpt">大量TIME_WAIT的问题[gpt]</a><ul>
<li><a href="#%E9%97%AE%E9%A2%98">问题</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="故障现象">故障现象</span><a href="#故障现象" class="header-anchor">#</a></h2><p>让我们先从一例线上故障说起。在一次升级线上应用服务之后，我们发现该服务的可用性变<br>得时好时坏，<strong>一段时间可以对外提供服务，一段时间突然又不可以，</strong> 大家都百思不得其解。<br>运维同学登录到服务所在的主机上，使用 netstat 命令查看后才发现，主机上有成千上万处<br>于 TIME_WAIT 状态的连接。</p>
<p>经过层层剖析后，我们发现罪魁祸首就是 TIME_WAIT。为什么呢？我们这个应用服务需要<br>通过发起 TCP 连接对外提供服务。<strong>每个连接会占用一个本地端口，当在高并发的情况下，<br>TIME_WAIT 状态的连接过多，多到把本机可用的端口耗尽，应用服务对外表现的症状，就<br>是不能正常工作了。当过了一段时间之后，处于 TIME_WAIT 的连接被系统回收并关闭<br>后，释放出本地端口可供使用，应用服务对外表现为，可以正常工作。这样周而复始，便会<br>出现了一会儿不可以，过一两分钟又可以正常工作的现象。</strong></p>
<h2><span id="time_wait概念和作用">TIME_WAIT概念和作用</span><a href="#time_wait概念和作用" class="header-anchor">#</a></h2><h3><span id="概念">概念</span><a href="#概念" class="header-anchor">#</a></h3><ul>
<li><strong>只有发起连接终止的一方会进入 TIME_WAIT 状态</strong>。</li>
<li><strong>TIME_WAIT</strong>停留持续时间是固定的，是最长分节生命期 MSL（maximumsegment lifetime）的两倍（<strong>2MSL</strong>）。Linux系统里有一个硬编码的字段，名称为TCP_IMEWAIT_LEN，其值为 60 秒。也就是说，<strong>Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒</strong>。</li>
</ul>
<h3><span id="作用">作用</span><a href="#作用" class="header-anchor">#</a></h3><ul>
<li>这样做是为了<strong>确保最后的 ACK 能让被动关闭方接收</strong>（可能丢失ACK后<strong>重传ACK</strong>），从而帮助其<strong>正常关闭</strong>。</li>
<li>第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。</li>
</ul>
<h2><span id="time_wait的副作用和优化">TIME_WAIT的副作用和优化</span><a href="#time_wait的副作用和优化" class="header-anchor">#</a></h2><h3><span id="tcp和time_wait">TCP和TIME_WAIT</span><a href="#tcp和time_wait" class="header-anchor">#</a></h3><p>当连接的一方主动关闭连接，在接收到对端的 FIN 报文之后，<strong>主动关闭连接的一方</strong>会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。</p>
<p><strong>主动关闭连接的一方可以是客户端，也可以是服务端；</strong><br><strong>副作用1 主动关闭连接的一方 是客户端；</strong><br><strong>副作用2 主动关闭连接的一方 是服务端；</strong></p>
<h3><span id="副作用和优化">副作用和优化</span><a href="#副作用和优化" class="header-anchor">#</a></h3><ul>
<li><p>副作用1 <strong>对端口资源的占用，一个 TCP 连接至少消耗一个本地端口</strong></p>
</li>
<li><p>副作用1 优化<br> <strong>net.ipv4.tcp_tw_reuse（推荐）</strong><br>  从协议角度理解如果是安全可控的，可以<strong>复用</strong>处于 TIME_WAIT 的套接字为新的<strong>连接</strong>所用.</p>
</li>
</ul>
<ol>
<li>只适用于连接<strong>发起方</strong>（C&#x2F;S 模型中的客户端）；</li>
<li>对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。</li>
</ol>
<p> <strong>net.ipv4.tcp_max_tw_buckets（不推荐）</strong><br>  这个值默认为 18000，当系统中处于TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置.<br>  这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。</p>
<p> <strong>SO_LINGER（不推荐）</strong><br>  不要试图使用SO_LINGER设置套接字选项，跳过 TIME_WAIT；</p>
<ul>
<li><p>副作用2  <strong>服务器端程序重启时，地址被占用 -&gt; Address already in use 异常</strong><br>通过服务器端发起的关闭连接操作，引起了一个已有的 TCP 连接处于 <strong>TME_WAIT 状态</strong>，正是这个 TIME_WAIT 的连接，使得服务器重启时，<strong>继续绑定在127.0.0.1地址和xxxx端口上的操作</strong>，返回了Address already in use的错误。</p>
</li>
<li><p>副作用2  优化<br>服务器端程序，都应该设置 <strong>SO_REUSEADDR 套接字选项</strong>，以便服务端程序可以在极短时间内<strong>复用同一个端口启动</strong>。</p>
</li>
</ul>
<h2><span id="大量time_wait的问题gpt">大量TIME_WAIT的问题[gpt]</span><a href="#大量time_wait的问题gpt" class="header-anchor">#</a></h2><h3><span id="问题">问题</span><a href="#问题" class="header-anchor">#</a></h3><p>  RPC客户端A调用服务端C有很多time-wait，客户端B调用服务端C会有什么问题 </p>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><p>在RPC客户端A调用服务端C时出现多个TIME_WAIT状态的情况下，如果RPC客户端B也调用相同的服务端C，可能会面临以下<strong>问题</strong>：<br>    1. <strong>端口耗尽</strong>：每个TCP连接会占用一个本地端口号，如果RPC客户端A和B同时发起大量的请求，由于TIME_WAIT状态的存在，可能会导致本地端口耗尽，无法建立新的连接。<br>    2. <strong>连接超时</strong>：由于服务端C处于TIME_WAIT状态，即使RPC客户端B发起请求，也需要等待TIME_WAIT状态消失后才能建立连接，这会导致连接的延迟增加，可能会触发连接超时，导致B无法正常与服务端C建立连接。<br>    3. <strong>并发性能下降</strong>：当RPC客户端A和B同时发起大量请求时，如果每个请求都需要等待TIME_WAIT状态消失后才能建立连接，会导致连接的建立速度变慢，从而降低系统的并发性能。</p>
<p>为了避免这些问题，可以采取以下<strong>措施</strong>：<br>    1. 避免端口耗尽：可以通过增加可用端口数、增加端口重用的时间等方式来缓解端口耗尽的问题。在操作系统级别上，可以通过调整端口范围或增加可用端口的数量来解决。<br>    2. 优化连接复用：可以使用连接池来管理TCP连接，以便复用连接，避免频繁地创建和关闭连接。这样可以减少TIME_WAIT状态的产生，并提高连接的复用率和性能。<br>    3. 调整TIME_WAIT时间：可以在服务端C上调整TIME_WAIT状态的时间，在合理的范围内降低TIME_WAIT状态的持续时间，从而缩短等待时间，提高连接的可用性和性能。</p>
<p>  总结: 最佳的解决方案取决于具体的应用场景和系统配置，需要综合考虑性能、资源消耗和可维护性等因素来做出合理的决策。</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>网络编程实战 - 10 | TIME_WAIT：隐藏在细节下的魔鬼</a> 盛延敏  </li>
<li><a href>网络编程实战 - 15 | 怎么老是出现“地址已经被使用”？</a> 盛延敏</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>网络</category>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP故障模式</title>
    <url>/www6vHomeHexo/2020/08/08/tcpFault/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>

<h2><span id="一-故障模式">一. 故障模式</span><a href="#一-故障模式" class="header-anchor">#</a></h2><h3><span id="对端无fin包发送">对端无FIN包发送</span><a href="#对端无fin包发送" class="header-anchor">#</a></h3><ul>
<li>网络中断<br>read <strong>TIMEOUT</strong>（setsockopt）<br>先write再read， 持续重传直至<strong>TIMEOUT</strong>。 之后再<strong>write()<strong>， 返回</strong>SIGPIPE信号（Broken Pipe）</strong>。<br>write Unreachable</li>
<li>系统奔溃（如断电）<br>read()或者write()持续重传直至<strong>TIMEOUT</strong><br>对端重启，read()或者write()返回RST。 <strong>read（）</strong>调用返回<strong>Connection Reset</strong>。<strong>write()<strong>返回</strong>SIGPIPE信号（Broken Pipe）</strong>。</li>
</ul>
<h3><span id="对端有fin包发送如程序奔溃">对端有FIN包发送（如程序奔溃）</span><a href="#对端有fin包发送如程序奔溃" class="header-anchor">#</a></h3><ul>
<li><p>read直接感知FIN  </p>
</li>
<li><p>没有read<br>如果不read没办法得到TCP对端的响应.</p>
<p>通过write()产生RST， <strong>read（）</strong>感知<strong>RST（Connection reset by peer）</strong>。 </p>
<img src="/www6vHomeHexo/2020/08/08/tcpFault/tcp-write-read.JPG" class>

<p><strong>向一个关闭连接连续写导致SIGPIPE信号（Broken Pipe）</strong>。</p>
<img src="/www6vHomeHexo/2020/08/08/tcpFault/tcp-write.JPG" class></li>
</ul>
<h2><span id="二-java对应的错误">二. Java对应的错误</span><a href="#二-java对应的错误" class="header-anchor">#</a></h2><h3><span id="javanetsockettimeoutexception">java.net.SocketTimeoutException</span><a href="#javanetsockettimeoutexception" class="header-anchor">#</a></h3><h3><span id="javanetsocketexception-connection-resetx2fconnect-reset-by-peer-socket-write-error">java.net.SocketException: Connection reset&#x2F;Connect reset by peer: Socket write error</span><a href="#javanetsocketexception-connection-resetx2fconnect-reset-by-peer-socket-write-error" class="header-anchor">#</a></h3><p>指连接被重置。这里有两种情况，分别对应两种错误：第一种情况是通信的一方已经将Socket 关闭，可能是主动关闭或者是因为异常退出，<br>这时如果通信的另一方还在<strong>写数据</strong>，就会触发这个异常<strong>（Connect reset by peer）</strong>；<br>如果对方还在尝试从 TCP 连接中<strong>读数据</strong>，则会抛出 <strong>Connection reset</strong> 异常。</p>
<blockquote>
<p>为了避免这些异常发生，在编写网络通信程序时要确保：<br>程序退出前要主动关闭所有的网络连接。<br>检测通信的另一方的关闭连接操作，当发现另一方关闭连接后自己也要关闭该连接。</p>
</blockquote>
<h3><span id="javanetsocketexception-broken-pipe">java.net.SocketException: Broken pipe</span><a href="#javanetsocketexception-broken-pipe" class="header-anchor">#</a></h3><p>指通信管道已坏。发生这个异常的场景是，通信的一方在<strong>收到“Connect reset by peer:Socket write error”后</strong>，如果再继续<strong>写数据</strong>则会抛出 <strong>Broken pipe</strong> 异常，解决方法同上。</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>网络编程实战 - 17 | TCP并不总是“可靠”的？</a> 盛延敏   操作系统的视角</li>
<li><a href>深入拆解Tomcat &amp; Jetty -  38 | Tomcat拒绝连接原因分析及网络优化</a> 李号双  Java的视角</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障模型</category>
        <category>TCP故障</category>
      </categories>
      <tags>
        <tag>稳定性</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat总结</title>
    <url>/www6vHomeHexo/2020/08/08/tomcat/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="tomcat-架构">Tomcat 架构</span><a href="#tomcat-架构" class="header-anchor">#</a></h2><div style="text-align: center; width: 60%">

<p><img src="https://user-images.githubusercontent.com/5608425/64508350-cedbeb00-d30f-11e9-8c01-9f7e5abffcba.png" alt="tomcat组件"><br>tomcat组件</p>
</div>

<img src="/www6vHomeHexo/2020/08/08/tomcat/container.JPG" class title="tomcat系统架构-多层容器">
<img src="/www6vHomeHexo/2020/08/08/tomcat/valve.JPG" class title="Valve是Tomcat的私有机制">


<h2><span id="servlet规范">Servlet规范</span><a href="#servlet规范" class="header-anchor">#</a></h2><ul>
<li>Servlet</li>
<li>Filter<br>每个请求生成一个Filter链</li>
<li>Listener<br>定制自己的监听器来监听 <strong>Tomcat 内部发生的各种事件</strong>：包括 Web 应用级别的、Session 级别的和请求级别的</li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.iteye.com/blog/onlyor-1689344">tomcat 组件 Top level view</a><br><a href>深入拆解Tomcat &amp; Jetty - 06 | Tomcat系统架构（下）：聊聊多层容器的设计</a> 李号双<br><a href>深入拆解Tomcat &amp; Jetty - 26 | Context容器（下）：Tomcat如何实现Servlet规范？</a> 李号双</p>
]]></content>
      <categories>
        <category>中间件</category>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算中的虚拟机vm</title>
    <url>/www6vHomeHexo/2020/07/29/vm/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="az和region">AZ和Region</span><a href="#az和region" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2020/07/29/vm/az.jpg" class title="可用区">

<img src="/www6vHomeHexo/2020/07/29/vm/region.jpg" class title="region">

<h2><span id="计算资源高可用-gt-虚拟机">计算资源高可用 -&gt; 虚拟机</span><a href="#计算资源高可用-gt-虚拟机" class="header-anchor">#</a></h2><h3><span id="1-idc内高可用az内部">1. IDC内高可用（AZ内部）</span><a href="#1-idc内高可用az内部" class="header-anchor">#</a></h3><p>   <strong>虚拟机打散分布</strong><br>   AWS 称为置放群组（Placement Group）；<br>   Azure 称为可用性集（Availability Set）；<br>   阿里云对应的服务则是部署集；</p>
<h3><span id="2-idc数据中心之间的高可用az之间">2.  IDC数据中心之间的高可用(AZ之间)</span><a href="#2-idc数据中心之间的高可用az之间" class="header-anchor">#</a></h3><p>   多可用区的实例部署；<br>   vpc打通可用区；</p>
<h3><span id="3-整个区域级别的事故region">3. 整个区域级别的事故(region)</span><a href="#3-整个区域级别的事故region" class="header-anchor">#</a></h3><ul>
<li><strong>多区域架构</strong>层面相关的预案；<br>   DNS 层面进行导流，把域名解析到另外的一个区域的备用服务上，底层的数据则需要我们日常进行着跨区域的实时同步。</li>
<li><strong>多云</strong>的方案<br>   避免厂商锁定</li>
</ul>
<h2><span id="虚拟机弹性伸缩-负载均衡器">虚拟机(弹性伸缩 + 负载均衡器)</span><a href="#虚拟机弹性伸缩-负载均衡器" class="header-anchor">#</a></h2><p>AWS 中相关的产品命名是 EC2 自动伸缩（Auto Scaling）;<br>Azure 中是虚拟机规模集（VM Scale Set）;<br>阿里云则叫做弹性伸缩;</p>
<h2><span id="虚拟机">虚拟机</span><a href="#虚拟机" class="header-anchor">#</a></h2><p>KVM, quem, libvirt</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>深入浅出云计算-07 | 云端架构最佳实践：与故障同舞，与伸缩共生</a>  何恺铎</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>服务治理-分布式配置</title>
    <url>/www6vHomeHexo/2020/07/27/config/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2020/07/27/config/config.JPG" class title="分布式配置">

<h2><span id="需求">需求</span><a href="#需求" class="header-anchor">#</a></h2><ul>
<li>对实时性要求不高</li>
<li>对可用性要求高</li>
</ul>
<h2><span id="产品">产品</span><a href="#产品" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">产品</th>
<th align="center">存储</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Disconf 百度</td>
<td align="center">mysql</td>
</tr>
<tr>
<td align="center">Apollo 携程</td>
<td align="center">mysql</td>
</tr>
<tr>
<td align="center">QConf 360</td>
<td align="center">zookeeper</td>
</tr>
<tr>
<td align="center">微博</td>
<td align="center">redis</td>
</tr>
<tr>
<td align="center">美图</td>
<td align="center">etcd</td>
</tr>
<tr>
<td align="center">spring cloud config</td>
<td align="center">git</td>
</tr>
</tbody></table>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>Spring Boot与Kubernetes云原生微服务实践</a> 杨波</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>分布式配置</category>
      </categories>
      <tags>
        <tag>分布式配置</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL中的SQL更新语句</title>
    <url>/www6vHomeHexo/2020/06/26/mysqlUpdate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="redo-log-ampamp-bin-log">redo log &amp;&amp; bin log</span><a href="#redo-log-ampamp-bin-log" class="header-anchor">#</a></h2><p>有了<strong>redo log</strong>，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个<br>能力称为<strong>crash-safe</strong>。</p>
<table>
<thead>
<tr>
<th align="center">\</th>
<th align="center">redo log</th>
<th align="center">bin log</th>
</tr>
</thead>
<tbody><tr>
<td align="center">where</td>
<td align="center">InnoDB引擎特有的</td>
<td align="center">MySQL的Server层实现的</td>
</tr>
<tr>
<td align="center">what</td>
<td align="center">物理日志，记录的是“在某个数据页上做了什么修改”</td>
<td align="center">逻辑日志，记录的是这个语句的原始逻辑</td>
</tr>
<tr>
<td align="center">how</td>
<td align="center">循环写的</td>
<td align="center">追加写入的</td>
</tr>
</tbody></table>
<h2><span id="update-in-mysql">update in Mysql</span><a href="#update-in-mysql" class="header-anchor">#</a></h2><p>MySQL里的WAL(Write-Ahead Logging)技术，它的关键点就是<strong>先写日志，再写磁盘.</strong></p>
<p>更新流程还涉及两个重要的日志模块，<strong>redo log（重做日志）和 binlog（归档日志）</strong>。</p>
<div style="width:35%;margin:auto">
<img src="/www6vHomeHexo/2020/06/26/mysqlUpdate/update.PNG" class title="update语句执行流程">
</div>
图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。 


<p>redo log的写入拆成了两个步骤：<strong>prepare和commit，这就是”两阶段提交”, 让redo log和bin log之间的逻辑一致</strong>。</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《MySQL实战45讲 - 日志系统：一条SQL更新语句是如何执行的？》    丁奇</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL的主从 高可用 容灾</title>
    <url>/www6vHomeHexo/2020/06/21/mysqlReliability/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86">MySQL主从复制原理</a><br>- <a href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-%E6%B5%81%E7%A8%8B">主从复制-流程</a><br>- <a href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-%E7%B1%BB%E5%9E%8B-6">主从复制-类型 [6]</a><br>- <a href="#%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2-1">主备切换 [1]</a><br>- <a href="#master-master%E5%8F%8Cm%E5%BE%AA%E7%8E%AF%E5%A4%8D%E5%88%B6%E9%97%AE%E9%A2%98-1">Master-Master(双M)循环复制问题 [1]</a><br>- <a href="#%E4%B8%BB%E5%A4%87%E5%BB%B6%E8%BF%9F-2">主备延迟 [2]</a><br>- <a href="#%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2%E7%9A%84%E7%AD%96%E7%95%A5-2">主备切换的策略 [2]</a></li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88-master%E9%AB%98%E5%8F%AF%E7%94%A85">高可用方案-Master高可用[5]</a><br>- <a href="#mmm">MMM</a><br>- <a href="#mha-%E5%8D%95%E4%B8%BB">MHA - 单主 +</a><br>- <a href="#mysql-group-replicatoinmgr-%E5%8D%95%E4%B8%BB-6">MySQL Group Replicatoin(MGR) - 单主 [6]+</a><br>- <a href="#mysql-cluster-%E5%A4%9A%E4%B8%BB">MySQL Cluster - 多主</a><br>- <a href="#galera-cluster-%E5%A4%9A%E4%B8%BB">Galera Cluster - 多主 +</a><br>- <a href="#percona-xtradbpxc-%E5%A4%9A%E4%B8%BB">Percona XtraDB(PXC) -多主</a></li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88-%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A75">高可用方案 - 数据可靠性[5]</a><br>- <a href="#raid10-raid-10">Raid10( Raid 1+0 )</a><br>- <a href="#san%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8-%E8%B4%B5">SAN共享存储- 贵</a><br>- <a href="#drbd%E7%A3%81%E7%9B%98%E5%A4%8D%E5%88%B6-%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%B8%A6">DRBD磁盘复制-系统自带 +</a></li>
<li><a href="#mysql-log%E5%92%8C%E5%8F%AF%E9%9D%A0%E6%80%A7">MySQL Log和可靠性</a></li>
<li><a href="#%E5%AE%B9%E7%81%BE-7">容灾 [7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="mysql主从复制原理">MySQL主从复制原理</span><a href="#mysql主从复制原理" class="header-anchor">#</a></h2><h5><span id="主从复制-流程">主从复制-流程</span><a href="#主从复制-流程" class="header-anchor">#</a></h5><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/66110430-58be6180-e5f9-11e9-9272-da2f69e51b1c.jpg" alt="master-slave"><br>MySQL主从复制</p>
</div>

<ul>
<li>MySQL master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看)</li>
<li>MySQL slave 将 master 的 binary log events 拷贝到它的中继日志(relay log)</li>
<li>MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据</li>
</ul>
<h5><span id="主从复制-类型-6">主从复制-类型 [6]</span><a href="#主从复制-类型-6" class="header-anchor">#</a></h5><ul>
<li>异步复制</li>
<li>半同步复制<br>MHA + 半同步复制</li>
<li>全同步复制<br>MGR + 全同步</li>
</ul>
<h5><span id="主备切换-1">主备切换 [1]</span><a href="#主备切换-1" class="header-anchor">#</a></h5><div style="width:70%;margin:auto">
<img src="/www6vHomeHexo/2020/06/21/mysqlReliability/mysqlMasterSlave1.PNG" class title="MySQL主备切换流程">

<p>因为readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。</p>
</div>

<div style="width:70%;margin:auto">
<img src="/www6vHomeHexo/2020/06/21/mysqlReliability/mysqlMasterSlave2.PNG" class title="主备流程图">
</div>

<ul>
<li>一个事务日志同步的完整过程是这样的：</li>
</ul>
<ol>
<li>在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个<br>位置开始请求binlog，这个位置包含文件名和日志偏移量。</li>
<li>在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和<br>sql_thread。其中io_thread负责与主库建立连接。</li>
<li>主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。</li>
<li>备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。</li>
<li>sql_thread读取中转日志，解析出日志里的命令，并执行。</li>
</ol>
<h5><span id="master-master双m循环复制问题-1">Master-Master(双M)循环复制问题  [1]</span><a href="#master-master双m循环复制问题-1" class="header-anchor">#</a></h5><ul>
<li>如果设置了双M结构，日志的执行流就会变成这样：</li>
</ul>
<ol>
<li>从节点A更新的事务，binlog里面记的都是A的server id；</li>
<li>传到节点B执行一次以后，节点B生成的binlog 的server id也是A的server id；</li>
<li>再传回给节点A，A判断到这个server id与自己的相同，就不会再处理这个日志。所以，死循<br>环在这里就断掉了。</li>
</ol>
<h5><span id="主备延迟-2">主备延迟 [2]</span><a href="#主备延迟-2" class="header-anchor">#</a></h5><ul>
<li>主备延迟原因<ul>
<li>备库所在机器的性能要比主库所在的<strong>机器性能差</strong></li>
<li><strong>备库的压力大</strong><br>解决方案:<br>I. <strong>一主多从</strong>。除了备库外，可以多接几个从库，让这些从库来分担读的压力。<br>II. <strong>通过binlog输出到外部系统</strong>，比如Hadoop这类系统，让外部系统提供统计类查询的能力。</li>
<li><strong>大事务</strong><br>解决方案:<br>I. 不要一次性地用delete语句删除太多数据<br>II. 大表DDL场景, 处理方案就是，计划内的DDL，建议使用gh-ost方案.</li>
<li>备库的并行复制能力 [3]</li>
</ul>
</li>
</ul>
<h5><span id="主备切换的策略-2">主备切换的策略 [2]</span><a href="#主备切换的策略-2" class="header-anchor">#</a></h5><p>由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。</p>
<ul>
<li>可靠性优先策略 - 数据不丢、安全可靠</li>
</ul>
<div style="width:70%;margin:auto">
<img src="/www6vHomeHexo/2020/06/21/mysqlReliability/mysqlMasterSlave-reliable.PNG" class title="可靠性优先主备切换流程">
</div>

<div style="width:70%;margin:auto">
<img src="/www6vHomeHexo/2020/06/21/mysqlReliability/mysqlMasterSlave-reliable-fault.PNG" class title="可靠性优先策略，主库不可用">
</div>

<ul>
<li>可用性优先策略 - 服务可用</li>
</ul>
<p><strong>小结：<br>实际的应用中，我更建议使用可靠性优先的策略。<br>在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。</strong></p>
<h2><span id="高可用方案-master高可用5">高可用方案-Master高可用[5]</span><a href="#高可用方案-master高可用5" class="header-anchor">#</a></h2><h5><span id="mmm">MMM</span><a href="#mmm" class="header-anchor">#</a></h5><p>  早期，不建议使用</p>
<h5><span id="mha-单主">MHA - 单主 +</span><a href="#mha-单主" class="header-anchor">#</a></h5><ul>
<li>MHA-manager 管理Master<br>  使用<strong>半同步复制</strong></li>
<li>缺陷： 只关注到master，对slave关注不够</li>
</ul>
<h5><span id="mysql-group-replicatoinmgr-单主-6">MySQL Group Replicatoin(MGR) - 单主 [6]+</span><a href="#mysql-group-replicatoinmgr-单主-6" class="header-anchor">#</a></h5><ul>
<li><strong>单主(荐)  多主(不推荐)</strong> </li>
<li>5.7之后支持</li>
<li>raft协议算法，自动选主节点， 自动故障转移</li>
<li><strong>全同步复制</strong>, 稳定性高, 强一致性</li>
</ul>
<ul>
<li>缺陷：     <ul>
<li>不支持gap lock(间隙锁)， 隔离级别需设置为read_commited</li>
<li>只能在GTID模式下， 并且日志格式未row格式</li>
<li>不支持对表进行锁操作(lock&#x2F;unlock table)</li>
<li>DDL语句不支持原子性</li>
<li>最多支持9个节点</li>
</ul>
</li>
</ul>
<h5><span id="mysql-cluster-多主">MySQL Cluster - 多主</span><a href="#mysql-cluster-多主" class="header-anchor">#</a></h5><ul>
<li>官方亲儿子</li>
<li>NDB engine， <strong>存算分离</strong></li>
<li>实现数据的强一致</li>
</ul>
<ul>
<li>缺陷：国内使用少， 配置复杂</li>
</ul>
<h5><span id="galera-cluster-多主">Galera Cluster - 多主 +</span><a href="#galera-cluster-多主" class="header-anchor">#</a></h5><ul>
<li>三方提供</li>
<li>Master和数据Node部署在一起</li>
<li>WSREP协议来做数据同步</li>
</ul>
<h5><span id="percona-xtradbpxc-多主">Percona XtraDB(PXC) -多主</span><a href="#percona-xtradbpxc-多主" class="header-anchor">#</a></h5><ul>
<li>早期</li>
</ul>
<h2><span id="高可用方案-数据可靠性5">高可用方案 - 数据可靠性[5]</span><a href="#高可用方案-数据可靠性5" class="header-anchor">#</a></h2><h5><span id="raid10-raid-10">Raid10( Raid 1+0 )</span><a href="#raid10-raid-10" class="header-anchor">#</a></h5><h5><span id="san共享存储-贵">SAN共享存储- 贵</span><a href="#san共享存储-贵" class="header-anchor">#</a></h5><h5><span id="drbd磁盘复制-系统自带">DRBD磁盘复制-系统自带 +</span><a href="#drbd磁盘复制-系统自带" class="header-anchor">#</a></h5><h2><span id="mysql-log和可靠性">MySQL Log和可靠性</span><a href="#mysql-log和可靠性" class="header-anchor">#</a></h2><a href="/www6vHomeHexo/2022/02/27/mysqlLog/" title="MySQL Logs">MySQL Logs</a>  



<h2><span id="容灾-7">容灾 [7]</span><a href="#容灾-7" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《MySQL是怎么保证主备一致的？》 MySQL实战45讲  丁奇</li>
<li>《MySQL是怎么保证高可用的？》 MySQL实战45讲 丁奇</li>
<li>《备库为什么会延迟好几个小时？》MySQL实战45讲  丁奇</li>
<li>xxx</li>
<li><a href="https://www.bilibili.com/video/BV1m44y1Q7ZF/">【IT老齐245】综合对比九种MySQL高可用方案</a></li>
<li><a href="https://www.bilibili.com/video/BV1HL411T7C8/">【IT老齐099】哎，MySQL高可用架构选型要慎重啊！</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/451693431">腾讯云原生数据库 TDSQL-C异地容灾核心能力构建</a>  </li>
<li><a href="https://zhuanlan.zhihu.com/p/620697440">MySQL主从复制原理剖析与应用实践</a> vivo team 未</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>使用MySQL的性能问题和紧急处理手段</title>
    <url>/www6vHomeHexo/2020/06/21/mysqlBestPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="短连接风暴问题">短连接风暴问题</span><a href="#短连接风暴问题" class="header-anchor">#</a></h2><h5><span id="解决方案">解决方案：</span><a href="#解决方案" class="header-anchor">#</a></h5><ul>
<li><strong>调高max_connections</strong><br>让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑<br>上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。</li>
<li>先处理掉那些占着连接但是不工作的线程<ul>
<li><strong>设置wait_timeout参数</strong>表示的是，一个线程空闲wait_timeout这么多秒之后，就会被MySQL直接断开连接。</li>
<li>从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是<br>直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL一直没恢复”。</li>
</ul>
</li>
<li>减少连接过程的消耗<br>  让数据库<strong>跳过权限验证阶段</strong>。<br>  这种方法风险极高，是我特别不建议使用的方案。尤其你的库外网可访问的话，就更不能这么做了。</li>
</ul>
<h2><span id="慢查询性能问题">慢查询性能问题</span><a href="#慢查询性能问题" class="header-anchor">#</a></h2><h5><span id="索引没有设计好">索引没有设计好</span><a href="#索引没有设计好" class="header-anchor">#</a></h5><ul>
<li>解决方案：通过<strong>紧急创建索引</strong>来解决.<br>假设你现在的服务是一主一备，主库A、备库B，这个方案的大致流程是这样的：<br>I. 在备库B上执行 set sql_log_bin&#x3D;off，也就是不写binlog，然后执行alter table 语句加上索<br>引；<br>II. 执行主备切换；<br>III. 这时候主库是B，备库是A。在A上执行 set sql_log_bin&#x3D;off，然后执行alter table 语句加上<br>索引。</li>
</ul>
<h5><span id="sql语句没写好">SQL语句没写好</span><a href="#sql语句没写好" class="header-anchor">#</a></h5><ul>
<li>解决方案：MySQL 5.7提供了query_rewrite功能，可以把输入的一种<strong>语句改写</strong>成另外一种模式。</li>
</ul>
<h5><span id="mysql选错了索引">MySQL选错了索引</span><a href="#mysql选错了索引" class="header-anchor">#</a></h5><ul>
<li>解决方案：给这个语句加上<strong>force index</strong>。</li>
</ul>
<h5><span id="避免1和2两个问题的预案">避免1和2两个问题的预案</span><a href="#避免1和2两个问题的预案" class="header-anchor">#</a></h5><ol>
<li>上线前，在测试环境，把慢查询日志（slow log）打开，并且把long_query_time设置成0，<br>确保每个语句都会被记录入慢查询日志；</li>
<li>在测试表里插入模拟线上的数据，做一遍回归测试；</li>
<li>观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致。</li>
</ol>
<h2><span id="qps突增问题">QPS突增问题</span><a href="#qps突增问题" class="header-anchor">#</a></h2><ul>
<li>解决的方案<ul>
<li>如果能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以<strong>从数据库端直接把白名单去掉</strong>。</li>
<li><strong>可以用管理员账号把这个用户删掉，然后断开现有连接</strong>。 这样，这个新功能的连接不成功，由它引发的QPS就会变成0。</li>
<li>把压力最大的SQL语句直接重写成”select 1”返回。<br> 这个操作的风险很高， 所有选项里优先级最低的一个方案。</li>
</ul>
</li>
</ul>
<h2><span id="总结">总结：</span><a href="#总结" class="header-anchor">#</a></h2><p>要避免一些低效的方法，比如<strong>避免大量地使用短连接</strong>。<br>连接异常断开是常有的事，代码里要有正确地<strong>重连并重试</strong>的机制。</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《MySQL实战45讲》 MySQL有哪些“饮鸩止渴”提高性能的方法？  丁奇</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生-学习资源</title>
    <url>/www6vHomeHexo/2020/06/14/cloudNativeResource/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<ul>
<li><p><a href="https://yq.aliyun.com/live/2831">Alibaba Cloud Native Day 杭州站 2020-05-30 2020年第二期</a> seen  网盘里有ppt</p>
</li>
<li><p><a href="https://edu.csdn.net/course/play/29034/407143?spm=1002.2009.3001.4024">阿里云核心技术竞争力第二期</a> 未</p>
</li>
<li><p><a href="https://jimmysong.io/guide-to-cloud-native-app/">云原生应用白皮书 - 定义云原生应用标准</a>  **</p>
</li>
</ul>
<h3><span id="serverless">Serverless</span><a href="#serverless" class="header-anchor">#</a></h3><p><a href="https://yq.aliyun.com/articles/744370">Serverless 工程化落地与实践</a>  video 未<br><a href="https://www.bilibili.com/video/av925967133">云原生之Serverless互联网实践专场</a> video 未  3个分享 钉钉2020.05.26有相同的视频<br><a href="https://developer.aliyun.com/learning/roadmap/serverless">Serverless 技术公开课</a>   *** 阿里</p>
<h3><span id="devops">DevOps</span><a href="#devops" class="header-anchor">#</a></h3><p><a href="https://developer.aliyun.com/live/2730">2020阿里巴巴研发效能峰会——云原生应用与架构专场</a> video seen<br><a href="https://yq.aliyun.com/live/2499">阿里巴巴DevOps文化浅谈</a> video  未<br><a href="https://yq.aliyun.com/articles/744373">云原生时代 Devops 实操笔记</a> video  10期 未</p>
<h3><span id="容器">容器</span><a href="#容器" class="header-anchor">#</a></h3><ul>
<li><p><a href="https://www.bilibili.com/video/BV1qt4y1C74P">基于阿里云容器服务&amp;ECI的在线业务弹性伸缩最佳实践</a> video seen 钉钉2020.6.12有相同的视频<br>ECI, kata, vitual kubelet</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1mK4y1t7WS/">如何为云原生应用带来稳定高效的部署能力？</a> video seen 钉钉2020.05.28有相同的视频<br><a href="https://yq.aliyun.com/articles/762949">如何为云原生应用带来稳定高效的部署能力？</a> text seen<br>CloneSet, Advanced StatefulSet, OpenKuise</p>
</li>
</ul>
<h3><span id="中间件">中间件</span><a href="#中间件" class="header-anchor">#</a></h3><ul>
<li><a href="https://blog.csdn.net/zl1zl2zl3/article/details/106129314">云原生时代消息中间件的演进路线</a> text 未 钉钉2020.05.28有相同的视频</li>
</ul>
]]></content>
      <categories>
        <category>云原生</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>Mybatis Plugin Inteceptor</title>
    <url>/www6vHomeHexo/2020/06/11/mybatisInteceptor/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="三类mybatis-plugin-inteceptor">三类Mybatis Plugin Inteceptor</span><a href="#三类mybatis-plugin-inteceptor" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/www6v/jDemo/blob/master/src/main/java/javacore/interceptor/sqlinterceptor/SqlInterceptor.java">动态增强sql的（推荐）</a></li>
<li><a href="https://github.com/www6v/jDemo/blob/master/src/main/java/javacore/interceptor/ResultTypeInterceptor.java">改ReturnType的 （推荐）</a></li>
<li><a href="https://github.com/www6v/jDemo/blob/master/src/main/java/javacore/interceptor/DataAuthorityInterceptor.java">改整个sql语句的</a></li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 命令</title>
    <url>/www6vHomeHexo/2020/05/27/dockerCommand/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># docker 加速</span><br><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://8c39031b.m.daocloud.io</span><br><span class="line"></span><br><span class="line"># docker registry</span><br><span class="line">docker login repo.shai.cloud:7443</span><br><span class="line"></span><br><span class="line">docker tag  repo.shai.cloud:7443/mysql:5.7.17  shai.cloud/mysql:5.7.17</span><br><span class="line">docker push shai.cloud/mysql:5.7.17 </span><br><span class="line"></span><br><span class="line">docker pull shai.cloud/linux/centos:ssh-7</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t cup/train  .</span><br><span class="line">docker run -d -p 7001:8081 -p 7002:9001 --name nexus -v /home/admin/nexus-data:/nexus-data  sonatype/nexus3</span><br><span class="line"></span><br><span class="line">docker stop mysql</span><br><span class="line">docker rm mysql</span><br><span class="line">docker rmi  22be5748ecbe</span><br><span class="line">docker restart mysql </span><br><span class="line"></span><br><span class="line">docker  cp   java.policy.ubuntu   els-master1:/etc/java-8-openjdk/security/java.policy</span><br><span class="line">docker inspect -f &#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; container_name_or_id</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># GPU docker</span><br><span class="line">nvidia-docker build -t dprecogpu:1.0   .</span><br><span class="line">nvidia-docker run -itd  --name  dpRecoGpu -v /home/wangwei/jniTest/jniReco:/home/wangwei/jniTest/jniReco  dprecogpu:1.0 </span><br><span class="line">nvidia-docker exec -it dpRecoGpu bash</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># iptables</span><br><span class="line">service iptables save # docker restart之后要save。</span><br><span class="line"></span><br><span class="line"># remove</span><br><span class="line">iptables -t nat -L -n --line-numbers</span><br><span class="line">iptables -t nat -D PREROUTING 1</span><br><span class="line"></span><br><span class="line"># 运行时暴露端口 -- broker 的端口的暴露</span><br><span class="line">iptables -t nat -A PREROUTING  -p tcp -m tcp --dport 10911 -j DNAT --to-destination  172.3.0.4:10911 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/sbin/iptables -I INPUT -p tcp --dport 18080 -j ACCEPT </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 阿里的开源的小工具  derrick</span><br><span class="line">derrick init</span><br><span class="line">derrick up</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>云原生</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s声明式应用管理</title>
    <url>/www6vHomeHexo/2020/05/26/k8sDeclarativeManage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>Kubernetes 声明式应用管理 &#x3D;  声明式风格的 API （行为） + Infrastructure as Data（数据 ）<br>声明式风格的 API： 控制器（Controller）做reconcile状态协调<br>Infrastructure as Data：   API 对象 ( 运行时 希望状态， 实际状态 )</p>
<p>声明式应用管理的本质：Infrastructure as Data</p>
<p>API对象的Schema： CRD 就是一个专门用来定义 Schema 的一个特殊的 API 对象(元数据)</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s/gcCmnB2mlqPeSOawgwV98A">CNCF 官方大使张磊：Kubernetes 是一个“数据库”吗？</a> 张磊</li>
<li><a href="../../../../2019/08/29/k8sDeclarativeAPI/">Kubernetes声明式API</a> self</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>DDD  领域驱动设计</title>
    <url>/www6vHomeHexo/2020/05/22/ddd/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%9B%AE%E6%A0%87">目标</a></li>
<li><a href="#%E4%B8%89%E4%B8%AA%E6%A0%B8%E5%BF%83">三个核心</a><ul>
<li><a href="#%E7%BB%9F%E4%B8%80%E8%AF%AD%E8%A8%80">统一语言</a></li>
<li><a href="#%E9%A2%86%E5%9F%9F%E5%88%92%E5%88%86">领域划分</a></li>
<li><a href="#%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B">领域模型</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%BC%8F-3">模式 [3]</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a><ul>
<li><a href="#%E5%85%AD%E8%BE%B9%E5%BD%A2%E6%9E%B6%E6%9E%84">六边形架构</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E8%B5%84%E6%BA%90">资源</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="目标">目标</span><a href="#目标" class="header-anchor">#</a></h1><ul>
<li>DDD的精髓是降低<strong>系统复杂度</strong></li>
<li>规则</li>
</ul>
<h1><span id="三个核心">三个核心</span><a href="#三个核心" class="header-anchor">#</a></h1><h3><span id="统一语言">统一语言</span><a href="#统一语言" class="header-anchor">#</a></h3><ul>
<li>核心领域词汇表</li>
<li>统一语言重构迭代： 模型-》实现-》 重构-》 隐喻 -》 再到模型</li>
<li>命名规范</li>
<li>DSL - Domain Specific Language</li>
</ul>
<h3><span id="领域划分">领域划分</span><a href="#领域划分" class="header-anchor">#</a></h3><ul>
<li>领域</li>
<li>子域</li>
<li>边界上下文（Bounded Context）</li>
<li>上下文映射（Context Mapping)<br>共享内核（Shared Kernel）<br>防腐层（Anti-Corruption）： 类似adaptor、facade， 对内部领域模型的隔离和屏蔽。</li>
</ul>
<h3><span id="领域模型">领域模型</span><a href="#领域模型" class="header-anchor">#</a></h3><ul>
<li><p>抽象：<br>是从具体事物抽取、概括出它们共同的方面、本质属性与关系等。</p>
</li>
<li><p>领域建模方法论：<br>UML用例分析、 UML用例分析法<br>四色建模法<br><a href="https://www.eventstorming.com/">事件风暴</a></p>
</li>
</ul>
<h1><span id="模式-3">模式 [3]</span><a href="#模式-3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2020/05/22/ddd/ddd.png" class>


<ul>
<li><p><strong>实体</strong> [4]<br><strong>可变性</strong>是实体的特点</p>
</li>
<li><p><strong>值对象</strong>  [4]<br><strong>不变性</strong>是值对象的本质</p>
</li>
<li><p>service<br> 领域服务是多个实体组合出来的一段业务逻辑</p>
</li>
<li><p>聚合[5]<br>真实世界中<strong>整体与部分</strong>的关系<br>正是因为有这样的关系，在操作整体的时候，整体就封装了对部分的操作。<br>所谓的整体与部分的关系，就是当整体不存在时，部分就变得没有了意义。</p>
<ul>
<li><strong>每个聚合对应一个Repo interface</strong> [7]</li>
<li>对聚合内的<strong>数据一致性</strong>负责[7]</li>
</ul>
</li>
<li><p>聚合根<br> 外部访问的唯一入口</p>
</li>
</ul>
<h1><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h1><h3><span id="六边形架构">六边形架构</span><a href="#六边形架构" class="header-anchor">#</a></h3><p>又被称之为<strong>Ports and Adapters（端口和适配器架构）</strong></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《DDD（Domain Driven Design)的精髓》  直播+ppt  阿里张建飞  钉钉2020.05.21视频</li>
<li>xxx</li>
<li><a href="https://www.cnblogs.com/netfocus/archive/2011/10/10/2204949.html">领域驱动设计之领域模型</a> *** </li>
<li>《04  领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏</li>
<li>《05  聚合、仓库与工厂：傻傻分不清楚》  DDD 微服务落地实战-拉钩专栏</li>
<li><a href="/www6vHomeHexo/2018/03/17/DomainLogicAndSQL/" title="领域逻辑和SQL">领域逻辑和SQL</a>  self</li>
</ol>
<h3><span id="资源">资源</span><a href="#资源" class="header-anchor">#</a></h3><ol>
<li>《实现领域驱动设计》 B  *** </li>
<li>《领域专用语言实战》 B  没纸质</li>
<li>《领域驱动设计精粹》B  没纸质 </li>
<li>《中台架构与实现 : 基于DDD和微服务》 B  没纸质</li>
<li><a href="https://developer.aliyun.com/live/2874">事件风暴和领域建模在阿里巴巴的落地实践</a>  未</li>
<li>《DDD实战课》 </li>
<li>《mksz541-DDD（领域驱动设计）思想解读及优秀实践~4》 V  </li>
<li>《lg2061-DDD 微服务落地实战-拉钩专栏》 V  ***   有代码</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>DDD</tag>
      </tags>
  </entry>
  <entry>
    <title>Java和Go的并发</title>
    <url>/www6vHomeHexo/2020/04/30/javaAndGoConcurrent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="java和go的并发">Java和Go的并发</span><a href="#java和go的并发" class="header-anchor">#</a></h2><h5><span id="java和go的并发">Java和Go的并发</span><a href="#java和go的并发" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2020/04/30/javaAndGoConcurrent/javaAndgo.JPG" class title="Java和Go的并发">


<h5><span id="用户线程和系统线程之间的关系mn11">用户线程和系统线程之间的关系（M:N，1:1）</span><a href="#用户线程和系统线程之间的关系mn11" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2020/04/30/javaAndGoConcurrent/threadRelationship.JPG" class title="用户线程和系统线程之间的关系（M:N，1:1）">

]]></content>
      <categories>
        <category>语言</category>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx优化</title>
    <url>/www6vHomeHexo/2020/03/26/nginxOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#nginx-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96">Nginx 参数优化</a><ul>
<li><a href="#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86">反向代理</a></li>
<li><a href="#tls-ssl">tls ssl</a></li>
<li><a href="#%E9%9D%9E%E5%AE%89%E5%85%A8%E8%AF%B7%E6%B1%82%E9%87%8D%E5%AE%9A%E5%90%91">非安全请求重定向</a></li>
<li><a href="#gzip">gzip</a></li>
<li><a href="#worker">worker</a></li>
<li><a href="#%E5%87%8F%E5%B0%91%E8%BF%9B%E7%A8%8B%E9%97%B4%E5%88%87%E6%8D%A2">减少进程间切换</a></li>
<li><a href="#%E6%8F%90%E5%8D%87cpu%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD%E7%8E%87">提升CPU缓存命中率</a></li>
<li><a href="#lua-%E5%88%86%E9%85%8D%E7%9A%84%E5%86%85%E5%AD%98%E6%9A%82%E6%97%B6%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8">lua 分配的内存（暂时没有使用）</a></li>
<li><a href="#http%E7%9A%84keeplive-%E9%95%BF%E9%93%BE%E6%8E%A5%E4%B8%80%E4%B8%AAtcp%E7%9A%84%E9%93%BE%E6%8E%A5%E4%B8%8A%E9%9D%A2%E6%9C%89%E5%A4%9A%E4%B8%AAhttp%E7%9A%84%E8%AF%B7%E6%B1%82">http的keeplive 长链接（一个tcp的链接，上面有多个http的请求）</a></li>
</ul>
</li>
<li><a href="#%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B">测试用例</a><ul>
<li><a href="#case1">case1</a></li>
<li><a href="#case2">case2</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="nginx-参数优化">Nginx 参数优化</span><a href="#nginx-参数优化" class="header-anchor">#</a></h2><h3><span id="反向代理">反向代理</span><a href="#反向代理" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">proxy_cache: 10M(重要)</span><br><span class="line"><span class="attribute">proxy_cache_path</span> /data/nginx_cache/ levels=<span class="number">1</span>:<span class="number">2</span> keys_zone=my_zone:<span class="number">10m</span> inactive=<span class="number">300s</span> max_size=<span class="number">5g</span>;</span><br></pre></td></tr></table></figure>


<h3><span id="tls-ssl">tls ssl</span><a href="#tls-ssl" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">ssl_session_cache</span> builtin:<span class="number">1000</span> shared:SSL:<span class="number">10m</span>;  /// 一天内连接上的用户， 不需要再协商秘钥</span><br><span class="line"><span class="attribute">ssl_session_cache</span> builtin:<span class="number">1000</span> shared:SSL:<span class="number">10m</span>;  /// 1<span class="attribute">m</span> -&gt; <span class="number">4000</span>个https连接</span><br><span class="line">ssl_session_timeout <span class="number">10m</span>; /// 10分钟</span><br><span class="line"><span class="attribute">ssl_protocols</span> TLSv1.<span class="number">2</span>;  /// 版本号</span><br></pre></td></tr></table></figure>

<h3><span id="非安全请求重定向">非安全请求重定向</span><a href="#非安全请求重定向" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">No</span> <span class="literal">redirect</span>:  无重定向 </span><br><span class="line">Redirect： <span class="number">301</span>-<span class="number">302</span> -》 转到https站点 </span><br></pre></td></tr></table></figure>

<h3><span id="gzip">gzip</span><a href="#gzip" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">gzip_comp_level</span> <span class="number">5</span>;</span><br><span class="line"><span class="attribute">gzip_http_version</span> <span class="number">1</span>.<span class="number">1</span>;   /// 注意：<span class="attribute">gzip</span> 在<span class="number">1</span>.<span class="number">1</span>上有效， http2.<span class="number">0</span>上是无效的</span><br><span class="line">gzip_min_length <span class="number">256</span>;</span><br><span class="line"><span class="attribute">gzip_types</span> application/atom+xml ... </span><br><span class="line">gzip_proxied any;</span><br><span class="line"><span class="attribute">gzip_vary</span> <span class="literal">on</span>;</span><br></pre></td></tr></table></figure>


<h3><span id="worker">worker</span><a href="#worker" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">worker_connections</span>  <span class="number">16384</span>;  ///    一个worker有 16384/2=8192 ‬个链接 .</span><br><span class="line">                                   两个事件， 一个读事件， 一个写事件。 </span><br><span class="line">                                   越多的connections对应更多的内存消耗。</span><br><span class="line">Default: <span class="attribute">worker_connections</span> <span class="number">512</span>;                                 </span><br></pre></td></tr></table></figure>

<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">高级选项：</span><br><span class="line"><span class="attribute">worker</span> connections的内存池（pools）， 更少的的内存碎片。一般是nginx自动分配的， 不用分配。</span><br><span class="line">Default: 	connection_pool_size <span class="number">256</span>|<span class="number">512</span></span><br><span class="line">Default: 	request_pool_size <span class="number">4k</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">worker_processes</span>  设置worker进程的数量</span><br></pre></td></tr></table></figure>


<h3><span id="减少进程间切换">减少进程间切换</span><a href="#减少进程间切换" class="header-anchor">#</a></h3><p>程间切换： cpu从一个进程或线程切换到另一个进程或线程。</p>
<ul>
<li>主动切换和被动切换</li>
<li>减少主动切换</li>
<li>被动切换：时间片耗尽。<br>减少被动切换： <strong>增大进程优先级</strong><br>Nice 静态优先级: -20 – 19<br>Priority 动态优先级： 0-139</li>
</ul>
<h3><span id="提升cpu缓存命中率">提升CPU缓存命中率</span><a href="#提升cpu缓存命中率" class="header-anchor">#</a></h3><ul>
<li><strong>绑定worker到指定cpu</strong><br>L1,L2(cpu独享)<br>L3（共享的）<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">worker_cpu_affinity</span> cpumask ...;</span><br><span class="line"><span class="attribute">worker_cpu_affinity</span> auto [cpumask];</span><br></pre></td></tr></table></figure></li>
</ul>
<h3><span id="lua-分配的内存暂时没有使用">lua 分配的内存（暂时没有使用）</span><a href="#lua-分配的内存暂时没有使用" class="header-anchor">#</a></h3><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">lua_shared_dict</span> configuration_data <span class="number">5M</span>;</span><br><span class="line"><span class="attribute">lua_shared_dict</span> certificate_data <span class="number">16M</span>;</span><br><span class="line">// 应用场景: 集群流控,  多个worker之间的内存的共享。</span><br></pre></td></tr></table></figure>


<h3><span id="http的keeplive-长链接一个tcp的链接上面有多个http的请求">http的keeplive  长链接（一个tcp的链接，上面有多个http的请求）</span><a href="#http的keeplive-长链接一个tcp的链接上面有多个http的请求" class="header-anchor">#</a></h3><p>注意: 非tcp keeplive </p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">keepalive_disable;  /// 没有设置</span><br><span class="line"><span class="attribute">keepalive_timeout</span>  <span class="number">75s</span>; // 默认值</span><br><span class="line"><span class="attribute">keepalive_requests</span> <span class="number">100</span>; // 默认值  一个tcp请求中可发100个http请求</span><br></pre></td></tr></table></figure>

<h2><span id="测试用例">测试用例</span><a href="#测试用例" class="header-anchor">#</a></h2><p>URL：logsearch.sh.pre.urtc.com.cn</p>
<h3><span id="case1">case1</span><a href="#case1" class="header-anchor">#</a></h3><p>input：<br>1000用户并发, 短连接， 非keepalive的   </p>
<p>result：<br>链接数  40000+<br>tps 4000+<br>avg 百ms   </p>
<h3><span id="case2">case2</span><a href="#case2" class="header-anchor">#</a></h3><p>input：<br>1500用户并发，短连接， 非keepalive的    </p>
<p>result：<br>链接数 30000+，<br>tps 4000+ ，<br>avg 200ms+ </p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="100">
<li><a href="https://www.cnblogs.com/xfeiyun/p/15877678.html">Nginx全面配置</a>  *** 未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>安全-OAuth2</title>
    <url>/www6vHomeHexo/2020/03/20/securityOAuth2/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#oauth-20-%E6%8E%88%E6%9D%83%E7%B1%BB%E5%9E%8B-14">OAuth 2.0  授权类型 [1][4]</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Eoauth2-%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84-3">基于OAuth2 的微服务 参考架构 [3]</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#oauth2-%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90">OAuth2 与微服务进行集成</a></li>
<li><a href="#%E5%90%84%E5%A4%A7%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-oauth-20-%E7%9A%84-2">各大开放平台是如何使用 OAuth 2.0 的 [2]</a></li>
<li><a href="#%E7%BD%91%E5%85%B3%E9%9B%86%E6%88%90oauth20-5">网关集成OAuth2.0 [5]</a></li>
</ul>
</li>
<li><a href="#oidc-2">OIDC [2]</a><ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF-oidc">什么是 OIDC</a></li>
<li><a href="#oidc-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5">OIDC 核心概念</a></li>
<li><a href="#oidc-%E5%BC%8F%E4%BE%8B">OIDC 式例</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="oauth-20-授权类型-14">OAuth 2.0  授权类型 [1][4]</span><a href="#oauth-20-授权类型-14" class="header-anchor">#</a></h1><ul>
<li><p>授权码模式-用的多<br> <a href="https://oauth.net/2/grant-types/authorization-code/">Authorization Code</a> 授权码 ***</p>
</li>
<li><p>客户端模式<br><a href="https://oauth.net/2/grant-types/client-credentials/">Client Credentials</a></p>
<p>The Client Credentials grant is used when applications request an access token to access their own resources, <strong>not on behalf of a user</strong>.</p>
</li>
<li><p>xxx<br><a href="https://oauth.net/2/grant-types/refresh-token/">Refresh Token</a>  ***<br>动态token</p>
</li>
<li><p>密码模式-Legacy<br><a href="https://oauth.net/2/grant-types/password/">Password Grant</a></p>
</li>
</ul>
<h1><span id="基于oauth2-的微服务-参考架构-3">基于OAuth2 的微服务 参考架构 [3]</span><a href="#基于oauth2-的微服务-参考架构-3" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2020/03/20/securityOAuth2/microservice-oauth2.jpg" class title="基于OAuth 2.0的微服务参考架构">
<ul>
<li>网关<br>令牌的校验和转换，将前端传递过来的 OAuth 2.0 访问令牌，通过调用 IDP 进<br>行校验，并转换为包含用户和权限信息的 JWT 令牌，再将 JWT 令牌向后台微服务传<br>递。</li>
<li>IDP 服务<br>IDP 是 Identity Provider 的简称，主要负责 OAuth 2.0 授权协议处理，OAuth 2.0 和<br>JWT 令牌颁发和管理，以及用户认证等功能。IDP 使用后台的 Login-Service 进行用户认<br>证。<br>选型:   Spring Security OAuth  or KeyCloak(RedHat)</li>
</ul>
<h3><span id="oauth2-与微服务进行集成">OAuth2 与微服务进行集成</span><a href="#oauth2-与微服务进行集成" class="header-anchor">#</a></h3><ul>
<li>第三方 Web 应用 + 授权码模式<img src="/www6vHomeHexo/2020/03/20/securityOAuth2/microservice-oauth2-pattern.jpg" class title="第三方 Web 应用 + 授权码模式"></li>
</ul>
<h3><span id="各大开放平台是如何使用-oauth-20-的-2">各大开放平台是如何使用 OAuth 2.0 的 [2]</span><a href="#各大开放平台是如何使用-oauth-20-的-2" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2020/03/20/securityOAuth2/wechat-oauth2.jpg" class title="微信">

<h3><span id="网关集成oauth20-5">网关集成OAuth2.0 [5]</span><a href="#网关集成oauth20-5" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2020/03/20/securityOAuth2/gateway-oauth2.png" class>


<h1><span id="oidc-2">OIDC [2]</span><a href="#oidc-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2020/03/20/securityOAuth2/OIDC.jpg" class>

<h3><span id="什么是-oidc">什么是 OIDC</span><a href="#什么是-oidc" class="header-anchor">#</a></h3><ul>
<li>什么是 OIDC？<ul>
<li>EU：End User</li>
<li>RP：Relying Party</li>
<li>OP：OpenID Provider</li>
<li>ID Token</li>
<li>UserInfo Endpoint</li>
</ul>
</li>
</ul>
<p>OIDC 是 OpenID Connect 的简称，OIDC&#x3D;(Identity, Authentication) + OAuth 2.0。它在 OAuth2 上构建了一个身份层，是一个基于 OAuth2 协议的身份认证标准协议。OAuth2 是一个授权协议，它无法提供完善的身份认证功能，OIDC 使用 OAuth2 的授权服务器来为第三方客户端提供用户的身份认证，并把对应的身份认证信息传递给客户端.</p>
<p>OAuth2 提供了<strong>Access Token</strong>来解决授权第三方客户端访问受保护资源的问题；OIDC 在这个基础上提供了<strong>ID Token</strong> 来解决第三方客户端标识用户身份认证的问题。</p>
<h3><span id="oidc-核心概念">OIDC 核心概念</span><a href="#oidc-核心概念" class="header-anchor">#</a></h3><ul>
<li>OIDC 核心概念<ul>
<li>主要术语</li>
<li>OIDC 工作流程</li>
<li>ID Token</li>
<li>认证<br>  基于 Authorization Code 的认证请求<br>  获取 ID Token<br>  Implicit Flow 和 Hybrid Flow</li>
<li>UserInfo Endpoint</li>
</ul>
</li>
</ul>
<h3><span id="oidc-式例">OIDC 式例</span><a href="#oidc-式例" class="header-anchor">#</a></h3><details><summary>OIDC 式例</summary><ul>
<li><p>请求示例：<br>POST &#x2F;auth&#x2F;realms&#x2F;ccm&#x2F;protocol&#x2F;openid-connect&#x2F;token HTTP&#x2F;1.1<br>Host: server.example.com<br>Content-Type: application&#x2F;x-www-form-urlencoded<br>Authorization: Basic d2ViX2FwcDp3ZWJfYXBw</p>
<p>grant_type&#x3D;<strong>authorization_code</strong>&amp;code&#x3D;7138b4b3-8c2b-4016-ad98-01c4938750c6.c110ddc8-c6c1-4a95-bd9e-cd8d84b4dd70.1eabef67-6473-4ba8-b07c-14bdbae4aaed&amp;redirect_uri&#x3D;https%3A%2F%2Fclient.example.org%2Fcb</p>
</li>
<li><p>响应示例：<br> HTTP&#x2F;1.1 200 OK<br>Content-Type: application&#x2F;json<br>Cache-Control: no-store<br>Pragma: no-cache</p>
<p>{<br> <strong>“access_token”</strong>: “SlAV32hkKG”,<br> “token_type”: “Bearer”,<br> “refresh_token”: “8xLOxBtZp8”,<br> “expires_in”: 3600,<br> <strong>“id_token”</strong>: “eyJhbGciOiJSUzI1NiIsImtpZCI6IjFlOWdkazcifQ.ewogImlzc<br>   yI6ICJodHRwOi8vc2VydmVyLmV4YW1wbGUuY29tIiwKICJzdWIiOiAiMjQ4Mjg5<br>   NzYxMDAxIiwKICJhdWQiOiAiczZCaGRSa3F0MyIsCiAibm9uY2UiOiAibi0wUzZ<br>   fV3pBMk1qIiwKICJleHAiOiAxMzExMjgxOTcwLAogImlhdCI6IDEzMTEyODA5Nz<br>   AKfQ.ggW8hZ1EuVLuxNuuIJKX_V8a_OMXzR0EHR9R6jgdqrOOF4daGU96Sr_P6q<br>   Jp6IcmD3HP99Obi1PRs-cwh3LO-p146waJ8IhehcwL7F09JdijmBqkvPeB2T9CJ<br>   NqeGpe-gccMg4vfKjkM8FcGvnzZUN4_KSP0aAp1tOJ1zZwgjxqGByKHiOtX7Tpd<br>   QyHE5lcMiKPXfEIQILVq0pc_E2DzL7emopWoaoZTF_m0_N0YzFC6g6EJbOEoRoS<br>   K5hoDalrcvRYLSrQAZZKflyuVCyixEoV9GfNQC3_osjzw2PAithfubEEBLuVVk4<br>   XUVrWOLrLl0nx7RkKU8NXNHq-rvKMzqg”<br>}</p>
</li>
</ul>
</details>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://deepzz.com/post/what-is-oauth2-protocol.html">10 分钟理解什么是 OAuth 2.0 协议</a> ***</li>
<li><a href="http://koca.szkingdom.com/forum/t/topic/139">OAuth2.0 + OIDC 技术规范及应用场景</a> ***</li>
<li>&lt;&lt;12 | 架构案例：基于OAuth 2.0&#x2F;JWT的微服务参考架构&gt;&gt;  杨波 ***</li>
<li><a href="https://java-family.cn/#/OAuth2.0/04-%E5%9B%9B%E7%A7%8D%E6%8E%88%E6%9D%83%E6%A8%A1%E5%BC%8F%E6%BC%94%E7%A4%BA?id=oauth20%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F%E6%B5%8B%E8%AF%95">OAuth2.0的四种模式测试</a></li>
<li><a href="https://java-family.cn/#/OAuth2.0/07-Spring-Cloud-Gateway%E9%9B%86%E6%88%90OAuth2.0">07  网关集成OAuth2.0实现统一认证鉴权</a>  代码</li>
</ol>
]]></content>
      <categories>
        <category>安全</category>
        <category>OAuth2</category>
      </categories>
      <tags>
        <tag>OAuth2</tag>
      </tags>
  </entry>
  <entry>
    <title>istio数据平面-sidecar</title>
    <url>/www6vHomeHexo/2019/11/21/istioDataplane/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#sidecar%E6%B3%A8%E5%85%A5%E5%92%8C%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91">sidecar注入和路由转发</a><br>- <a href="#sidecar%E6%B3%A8%E5%85%A5">sidecar注入</a><br>- <a href="#envoy-sidecar-%E4%BB%A3%E7%90%86%E7%9A%84%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91-78">Envoy Sidecar 代理的路由转发 [7][8]</a></li>
<li><a href="#envoy-proxy%E7%9A%84%E6%9E%B6%E6%9E%84-18">Envoy proxy的架构 [18]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9D%A2%E6%A0%87%E5%87%86apixds%E5%8D%8F%E8%AE%AE">数据面标准API&#x2F;xDS协议</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9D%A2">数据面</a><br>- <a href="#istio-%E6%B3%A8%E5%85%A5sidecar%E5%AE%9E%E7%8E%B0">Istio 注入sidecar实现</a><br>- <a href="#%E6%B3%A8%E5%85%A5pod%E5%86%85%E5%AE%B9">注入Pod内容</a></li>
<li><a href="#envoy%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%92%8C%E9%85%8D%E7%BD%AE">Envoy启动过程和配置</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-2">性能优化 [2]</a><br>- <a href="#proxyless-%E6%A8%A1%E5%BC%8F-v111">Proxyless 模式  (v1.11)</a><br>- <a href="#%E4%BD%BF%E7%94%A8-ebpf-%E4%BC%98%E5%8C%96%E6%B5%81%E9%87%8F%E5%8A%AB%E6%8C%81">使用 eBPF 优化流量劫持</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="sidecar注入和路由转发">sidecar注入和路由转发</span><a href="#sidecar注入和路由转发" class="header-anchor">#</a></h2><h5><span id="sidecar注入">sidecar注入</span><a href="#sidecar注入" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/11/21/istioDataplane/istio-data.jpg" class title="istio数据面-sidecar注入">

<h5><span id="envoy-sidecar-代理的路由转发-78">Envoy Sidecar 代理的路由转发 [7][8]</span><a href="#envoy-sidecar-代理的路由转发-78" class="header-anchor">#</a></h5><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64623499-a50fea80-d41b-11e9-9524-6d834fd45d88.jpg" alt="Envoy Sidecar 代理的路由转发">  Envoy Sidecar 代理的路由转发</p>
</div>

<p>参考:<br>7. <a href="https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/">理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持</a>  宋净超<br>8. <a href="http://www.servicemesher.com/blog/envoy-sidecar-routing-of-istio-service-mesh-deep-dive/">理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发</a>  宋净超  引<br>21. <a href="https://item.jd.com/12538407.html">《云原生服务网格Istio：原理、实践、架构与源码解析》</a> 第6章<br>22. <a href="https://preliminary.istio.io/zh/blog/2019/data-plane-setup/">Istio Sidecar 注入过程解密</a> istio官方</p>
<h2><span id="envoy-proxy的架构-18">Envoy proxy的架构 [18]</span><a href="#envoy-proxy的架构-18" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64623492-a3462700-d41b-11e9-8e2b-6fc0b05d8c5d.jpg" alt="Envoy代理的架构"><br>Envoy proxy的架构</p>
</div>

<p>参考:<br>18. <a href="https://www.servicemesher.com/blog/envoy-proxy-config-deep-dive/">Istio 的数据平面 Envoy Proxy 配置详解</a>  宋净超 引</p>
<ol>
<li><a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">Istio流量管理实现机制深度解析</a>  赵化冰</li>
</ol>
<h2><span id="数据面标准apix2fxds协议">数据面标准API&#x2F;xDS协议</span><a href="#数据面标准apix2fxds协议" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/11/21/istioDataplane/istioXDS.jpg" class title="istio-xDS">

<ul>
<li>pilot和envoy之间的接口</li>
<li>xDS是一类发现服务的总称，包含LDS，RDS，CDS，EDS以及 SDS。</li>
<li>XDS服务接口的最终一致性: 遵循 make before break 模型</li>
</ul>
<div style="text-align: center; width: 50%; height: 50%">

<p><img src="https://user-images.githubusercontent.com/5608425/69417029-dded0980-0d52-11ea-96a1-4c14e08aadf8.jpg" alt="xds"></p>
</div>

<p>参考:<br>17. <a href="https://jimmysong.io/istio-handbook/concepts/envoy-xds-protocol.html">xDS 协议解析</a>  宋净超  引</p>
<ol>
<li><a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">Istio流量管理实现机制深度解析</a>  赵化冰</li>
<li><a href="https://www.servicemesher.com/blog/service-mesh-the-microservices-in-post-kubernetes-era/">Service Mesh——后 Kubernetes 时代的微服务</a></li>
</ol>
<hr>
<h2><span id="数据面">数据面</span><a href="#数据面" class="header-anchor">#</a></h2><h5><span id="istio-注入sidecar实现">Istio 注入sidecar实现</span><a href="#istio-注入sidecar实现" class="header-anchor">#</a></h5><ul>
<li>自动注入: 利用 Kubernetes Dynamic Admission Webhooks 对 新建的pod 进行注入: init container + sidecar<br><a href="https://cloudnative.to/blog/istio-sidecar-injection-method/">一种灵活注入 Istio Sidecar 的方案探索</a></li>
<li>手动注入: 使用 istioctl kube-inject</li>
</ul>
<h5><span id="注入pod内容">注入Pod内容</span><a href="#注入pod内容" class="header-anchor">#</a></h5><ul>
<li>istio-init:<br> 通过配置iptables来劫持Pod中的流量。<br> Init 容器初始化完毕后就会自动终止，但是 Init 容器初始化结果(iptables)会保留到应用容器和 Sidecar 容器中.</li>
<li>istio-proxy:<br> 两个进程pilot-agent和envoy, pilot-agent 进行初始化并启动envoy. 【3.2节】</li>
</ul>
<h2><span id="envoy启动过程和配置">Envoy启动过程和配置</span><a href="#envoy启动过程和配置" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/69950671-b81feb80-152e-11ea-96ba-6261b7f4c09f.png" alt="envoy-config-init"><br>Envoy启动过程和配置</p>
</div>

<blockquote>
<p>Envoy 配置热更新: 配置的动态变更，而不需要重启 Envoy.</p>
</blockquote>
<p>参考:<br><a href="https://mp.weixin.qq.com/s/VwqxrZsVmn4a5PcVckaLxA">Istio 庖丁解牛1：组件概览</a>  腾讯云 钟华<br><a href="../../../../2019/11/21/istioDataplane/">istio数据面</a> self</p>
<hr>
<h2><span id="性能优化-2">性能优化 [2]</span><a href="#性能优化-2" class="header-anchor">#</a></h2><h5><span id="proxyless-模式-v111">Proxyless 模式  (v1.11)</span><a href="#proxyless-模式-v111" class="header-anchor">#</a></h5><h5><span id="使用-ebpf-优化流量劫持">使用 eBPF 优化流量劫持</span><a href="#使用-ebpf-优化流量劫持" class="header-anchor">#</a></h5><p>参考</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/437194208">基于 gRPC 和 Istio 的无 Sidecar 代理的服务网格</a></li>
<li><a href="https://jimmysong.io/blog/beyond-istio-oss/#performance-optimizing">Beyond Istio OSS —— Istio 服务网格的现状与未来</a> ***<br><a href="https://docs.qq.com/pdf/DRWxETHNDZmRsS0l5">Beyond Istio OSS</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio流量管理</title>
    <url>/www6vHomeHexo/2019/11/21/istioTrafficManagement/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="流量管理">流量管理</span><a href="#流量管理" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/11/21/istioTrafficManagement/istio-traffic-manage.jpg" class title="istio流量管理">

<h5><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h5><div style="text-align: center;">
    
<p><img src="https://user-images.githubusercontent.com/5608425/64623501-a5a88100-d41b-11e9-9262-8414adb5831a.png" alt="Pilot流量管理"><br>Pilot Design Overview (来自 <a href="https://github.com/istio/old_pilot_repo/blob/master/doc/design.md">Istio old_pilot_repo</a>)<br>图例说明：图中红色的线表示控制流，黑色的线表示数据流。蓝色部分为和Pilot相关的组件。</p>
</div>

<ul>
<li><p>控制面组件</p>
<ul>
<li>Discovery Services</li>
<li>istio crd</li>
</ul>
</li>
<li><p>数据面组件</p>
<ul>
<li>Pilot-agent</li>
<li>Envoy</li>
</ul>
</li>
</ul>
<p><a href="https://mp.weixin.qq.com/s/VwqxrZsVmn4a5PcVckaLxA">Istio 庖丁解牛1：组件概览</a>  腾讯云 钟华</p>
<h5><span id="流量管理模型">流量管理模型</span><a href="#流量管理模型" class="header-anchor">#</a></h5><ul>
<li><p>VirtualService<br>定义了一系列针对指定服务的流量<strong>路由规则</strong>;<br>将流量路由到给定目标地址;<br>通常和目标规则（DestinationRule）成对出现;</p>
</li>
<li><p>DestinationRule<br>定义虚拟服务路由<strong>目标地址</strong>的<strong>真实地址</strong>，即<strong>子集（subset）</strong>；<br>设置<strong>负载均衡方式</strong>： round robin（默认），随机，权重，最少请求数；</p>
</li>
<li><p>ServiceEntry </p>
<ul>
<li>外部服务定义: 把外部服务注册到网格中</li>
<li>功能：<ul>
<li>为外部服务转发请求</li>
<li>添加超时重试等策略</li>
<li>扩展网格</li>
</ul>
</li>
</ul>
</li>
<li><p>Gateway<br>描述了一个负载均衡器，用于承载网格边缘的进入和发出连接。</p>
</li>
<li><p>EnvoyFilter<br>描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置.</p>
</li>
<li><p>Sidecar<br>调整Envoy代理接管的端口和协议；<br>限制Envoy代理可访问的服务；</p>
</li>
<li><p>总结：<br>管理内部流量：VirtualService + DestinationRule<br>管理外部流量：Gateway<br>管理服务：ServiceEntry</p>
</li>
</ul>
<h5><span id="灰度发布">灰度发布</span><a href="#灰度发布" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/11/21/istioTrafficManagement/istio-gray.jpg" class title="istio灰度发布">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">Istio流量管理实现机制深度解析</a>  赵化冰</li>
<li><a href="https://mp.weixin.qq.com/s/NjMncH84uEl_PywOFFMlFA">腾讯云容器团队内部Istio专题分享</a> 腾讯云 钟华</li>
<li><a href="https://time.geekbang.org/course/intro/100049401">极客时间 《Service Mesh实战 - 核心功能之流量控制：Istio是如何实现流量控制功能的？》</a> 马若飞</li>
</ol>
<hr>
<p>官方Examples 0-5  G</p>
<ol start="0">
<li><a href="https://preliminary.istio.io/zh/docs/concepts/traffic-management">流量管理</a>  istio官网 引</li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/request-routing/">配置请求路由</a> done</li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/traffic-shifting/">流量转移</a> done</li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/tcp-traffic-shifting/">TCP 流量转移</a> done ,验证有点问题</li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/request-timeouts/">设置请求超时</a> done</li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/circuit-breaking/">熔断</a> done</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Runtime</title>
    <url>/www6vHomeHexo/2019/11/19/k8sRuntime/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/11/19/k8sRuntime/k8sRuntime.jpg" class title="Kubernetes Runtime">


<h2><span id="一-虚拟化技术">一. 虚拟化技术</span><a href="#一-虚拟化技术" class="header-anchor">#</a></h2><div style="text-align: center;">

<img src="/www6vHomeHexo/2019/11/19/k8sRuntime/k8sRuntime1.jpg" class title="虚拟化技术">

</div>

<p>runc： OSContainerRuntime（基于进程隔离技术）<br>kvm:   HyperRuntime（基于Hypervisor技术）<br>runv： UnikernelRuntime（基于unikernel）</p>
<h2><span id="二-cri架构">二. CRI架构</span><a href="#二-cri架构" class="header-anchor">#</a></h2><div style="text-align: center;">
<img src="/www6vHomeHexo/2019/11/19/k8sRuntime/k8sRuntime2.jpg" class title="CRI架构">

<p><img src="https://user-images.githubusercontent.com/5608425/69022893-c67eeb00-09f7-11ea-9203-fd96b90dfbef.jpg" alt="cni-arch"><br>CRI架构</p>
</div>


<p>docker调用的链路：dockershim &#x3D;&gt; dockerd &#x3D;&gt; Containerd &#x3D;&gt; runc<br>Containerd调用的链路：Containerd –&gt; shim v2 –&gt; runtimes</p>
<ul>
<li><p>Docker 作为 K8S 容器运行时，调用关系如下：<br>kubelet –&gt; docker shim （在 kubelet 进程中） –&gt; dockerd –&gt; containerd</p>
</li>
<li><p>Containerd 作为 K8S 容器运行时，调用关系如下：<br>kubelet –&gt; cri plugin（在 containerd 进程中） –&gt; containerd</p>
</li>
<li><p>high level运行时： Dockershim， containerd， CRI-O</p>
</li>
<li><p>low level运行时：<br>  runc， kata， gVisor<br>  runc：运行可信容器（弱隔离但性能好）;<br>  runv: 运行不可信容器（强隔离安全性好）;</p>
</li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/cri.html">CRI - Container Runtime Interface（容器运行时接口）</a></li>
<li><a href="https://mp.weixin.qq.com/s/sshrTSsUfqjja6g4-Lb42g">为Kubernetes选择合适的容器运行时</a></li>
<li>模块七 </li>
<li><a href="https://cloud.tencent.com/document/product/457/35747"></a> 腾讯云</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Operator-Etcd</title>
    <url>/www6vHomeHexo/2019/11/19/k8sOperator/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/11/19/k8sOperator/operator.jpg" class title="Operator"> 

<p><strong>Operator: “有状态应用”, 自动化运维工作。</strong></p>
<h2><span id="etcd-operator部署">Etcd Operator部署</span><a href="#etcd-operator部署" class="header-anchor">#</a></h2><p>Controller + CRD<br><a href="https://github.com/coreos/etcd-operator/blob/master/example/deployment.yaml">etcd Controller</a>  Deployment<br><a href="https://github.com/coreos/etcd-operator/blob/master/example/example-etcd-cluster.yaml">etcd CRD</a>  API资源类型 kind: “EtcdCluster”</p>
<p>etcd高可靠： backup + restore<br><a href="https://github.com/coreos/etcd-operator/tree/master/example/etcd-backup-operator">etcd backup</a>  etcd备份<br><a href="https://github.com/coreos/etcd-operator/tree/master/example/etcd-restore-operator">etcd restore</a> etcd恢复: 恢复备份的数据</p>
<h2><span id="普通运维方式etcd运维步骤">普通运维方式etcd运维步骤:</span><a href="#普通运维方式etcd运维步骤" class="header-anchor">#</a></h2><ol>
<li>创建种子节点（集群）的阶段称为：Bootstrap</li>
<li>通过 Etcd 命令行添加一个新成员：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ etcdctl member add infra1 http://10.0.1.11:2380</span><br></pre></td></tr></table></figure></li>
<li>为这个成员节点生成对应的启动参数，并启动它：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">etcd</span><br><span class="line">--data-dir=/var/etcd/data</span><br><span class="line">--name=infra1</span><br><span class="line">--initial-advertise-peer-urls=http://10.0.1.11:2380</span><br><span class="line">--listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">--listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">--advertise-client-urls=http://10.0.1.11:2379</span><br><span class="line">--initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380</span><br><span class="line">--initial-cluster-state=existing</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="etcd-operator">etcd Operator</span><a href="#etcd-operator" class="header-anchor">#</a></h2><div>
<img src="/www6vHomeHexo/2019/11/19/k8sOperator/operator1.jpg" class title="etcd Operator">
</div>

<p>注: 一个etcd集群，一个controller。</p>
<p>当这个 YAML 文件第一次被提交到 Kubernetes 之后，Etcd Operator 的 Informer，就会立<br>刻“感知”到一个新的 EtcdCluster 对象被创建了出来。所以，EventHandler 里的“添加”事<br>件会被触发。</p>
<p>而这个 Handler 要做的操作也很简单，即：在 Etcd Operator 内部创建一个对应的 Cluster 对<br>象（cluster.New），比如流程图里的 Cluster1。</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p><a href>深入剖析Kubernetes - 27  聪明的微创新：Operator工作原理解读</a> 张磊<br><a href="https://mp.weixin.qq.com/s/E5-agHtMvW_X7znVJDkTKA">面向 Kubernetes 编程： Kubernetes 是下一代操作系统</a><br><a href="https://github.com/www6v/awesome-operators">awesome-operators</a>    obsolete<br><a href="https://github.com/operator-framework/operator-sdk">operator-sdk</a>   工具</p>
<details><summary>附件-有用的 Operator</summary><p><a href="https://github.com/elastic/cloud-on-k8s">elastic&#x2F;cloud-on-k8s</a><br><a href="https://github.com/solo-io/envoy-operator">solo-io&#x2F;envoy-operator</a><br><a href="https://github.com/coreos/etcd-operator">coreos&#x2F;etcd-operator</a><br><a href="https://github.com/lyft/flinkk8soperator">lyft&#x2F;flinkk8soperator</a> ***<br><a href="https://github.com/banzaicloud/hpa-operator">banzaicloud&#x2F;hpa-operator</a><br><a href="https://github.com/gianarb/influxdb-operator">gianarb&#x2F;influxdb-operator</a><br><a href="https://github.com/banzaicloud/istio-operator">banzaicloud&#x2F;istio-operator</a><br><a href="https://github.com/jaegertracing/jaeger-operator">jaegertracing&#x2F;jaeger-operator</a><br><a href="https://github.com/banzaicloud/kafka-operator">banzaicloud&#x2F;kafka-operator</a> ***<br><a href="https://github.com/oracle/mysql-operator">oracle&#x2F;mysql-operator</a><br><a href="https://github.com/Percona-Lab/percona-xtradb-cluster-operator">Percona-Lab&#x2F;percona-xtradb-cluster-operator</a><br><a href="https://github.com/coreos/prometheus-operator">coreos&#x2F;prometheus-operator</a> ***<br><a href="https://github.com/ucloud/redis-cluster-operator">ucloud&#x2F;redis-cluster-operator</a> ***<br><a href="https://github.com/apache/rocketmq-operator">apache&#x2F;rocketmq-operator</a>***<br><a href="https://github.com/rook/rook/tree/master/cluster/examples/kubernetes">rook&#x2F;rook</a>***<br><a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator">GoogleCloudPlatform&#x2F;spark-on-k8s-operator</a> ***<br><a href="https://github.com/pingcap/tidb-operator">pingcap&#x2F;tidb-operator</a>***</p>
</details>

]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio知识图谱</title>
    <url>/www6vHomeHexo/2019/11/18/istioKnowledgeMap/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<div style="text-align: center;">
<img width="1722" alt="istio-knowledge-map" src="https://user-images.githubusercontent.com/5608425/64623497-a4775400-d41b-11e9-84a5-6e75d708ad0d.png">  Istio知识图谱
</div>


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/servicemesher/istio-knowledge-map">Istio 知识图谱</a></li>
<li><a href="https://developer.ibm.com/cn/os-academy-istio/">Istio V1.0 系列-开源技术 * IBM 微讲堂</a>  video 未<br><a href="https://istio.cn/t/topic/19">IBM Istio系列开源技术微讲堂</a> video 未</li>
<li><a href="https://developer.ibm.com/cn/os-academy-istio-2020/">Istio v1.6 系列-开源技术 * IBM 微讲堂</a> video 未</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes自动伸缩和HPA</title>
    <url>/www6vHomeHexo/2019/11/16/k8sAutoScale/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2019/11/16/k8sAutoScale/k8sAutoScale.jpg" class title="Kubernetes自动伸缩">

<h2><span id="一-core-metrics核心指标">一. Core metrics(核心指标)</span><a href="#一-core-metrics核心指标" class="header-anchor">#</a></h2><p><img src="https://user-images.githubusercontent.com/5608425/68987952-a3730080-086a-11ea-9a69-6b2c41de98ed.JPG" alt="hpa3"></p>
<p>kubernetes的新监控体系中，metrics-server属于**Core metrics(核心指标)**，提供API metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。而其他Custom Metrics(自定义指标)由Prometheus等组件来完成.<br>图14-3 14-5 14-6 基于Core metrics(核心指标)</p>
<p><img src="https://user-images.githubusercontent.com/5608425/68987950-a3730080-086a-11ea-83b8-1b6e06e3c659.jpg" alt="hpa1"></p>
<h5><span id="kube-aggregator">kube-aggregator</span><a href="#kube-aggregator" class="header-anchor">#</a></h5><p>有了Metrics Server组件，也采集到了该有的数据，也暴露了api，但因为api要统一，如何将请求到api-server的&#x2F;apis&#x2F;metrics请求转发给Metrics Server呢，解决方案就是：kube-aggregator,在k8s的1.7中已经完成，之前Metrics Server一直没有面世，就是耽误在了<strong>kube-aggregator</strong>这一步。<br>kube-aggregator（聚合api）主要提供：<br>Provide an API for registering API servers.<br>Summarize discovery information from all the servers.<br>Proxy client requests to individual servers.</p>
<h5><span id="metrics-server">Metrics Server</span><a href="#metrics-server" class="header-anchor">#</a></h5><p><strong>metric-server是扩展的apiserver.</strong><br><strong>Metrics Server</strong>: metric-aggregator, 聚合metric。<br><strong>Metrics Server</strong> 是集群级别的资源利用率数据的<strong>聚合器（ aggregator ）</strong>。Metrics Server 通过Kubernetes 聚合器（ <strong>kube-aggregator</strong>）<strong>注册</strong>到主API Server 之上，而后基于kubelet 的Summary API 收集每个节点上的指标数据，并将它们存储于内存中然后以指标API 格式提供。</p>
<h5><span id="metric-api">metric api</span><a href="#metric-api" class="header-anchor">#</a></h5><p>metric api的使用：<br>Metrics API 只可以查询当前的度量数据，并不保存历史数据<br>Metrics API URI 为 &#x2F;apis&#x2F;metrics.k8s.io&#x2F;，在 k8s.io&#x2F;metrics 维护<br>必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</p>
<h2><span id="二-自定义指标custom-metrics与prometheus">二.  自定义指标（Custom Metrics）与Prometheus</span><a href="#二-自定义指标custom-metrics与prometheus" class="header-anchor">#</a></h2><p><img src="https://user-images.githubusercontent.com/5608425/68987954-a40b9700-086a-11ea-985a-10d423d2cd15.JPG" alt="hpa5"></p>
<p>Kubernetes 里的 Custom Metrics 机制，是借助 <strong>Aggregator APIServer 扩展机</strong>制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。<strong>而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor</strong>。</p>
<p>目前Kubernetes中自定义指标一般由Prometheus来提供，再利用<strong>k8s-prometheus-adapter</strong>聚合到apiserver，实现和核心指标（metric-server)同样的效果</p>
<p>Prometheus可以采集其它各种指标，但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，因此还需要另外一个组件(<strong>kube-state-metrics</strong>)，将prometheus的metrics数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主API服务器中注册，以便直接通过&#x2F;apis&#x2F;来访问。<br><strong>文件清单</strong>：<br>node-exporter：prometheus的export，收集Node级别的监控数据<br>prometheus：监控服务端，从node-exporter拉数据并存储为时序数据。<br>kube-state-metrics：将prometheus中可以用PromQL查询到的指标数据转换成k8s对应的数<br>k8s-prometheus-adapter：聚合进apiserver，即一种custom-metrics-apiserver实现<br>开启Kubernetes aggregator功能（参考上文metric-server）</p>
<h2><span id="三-hpa">三. HPA</span><a href="#三-hpa" class="header-anchor">#</a></h2><p><img src="https://user-images.githubusercontent.com/5608425/68987951-a3730080-086a-11ea-82d6-78ba5efcdfa5.jpg" alt="hpa2"></p>
<h2><span id="四-实战">四. 实战</span><a href="#四-实战" class="header-anchor">#</a></h2><ul>
<li><a href="https://github.com/rootsongjc/kubernetes-handbook/tree/master/manifests/HPA">HPA</a> 基于http_requests</li>
<li><a href="https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-1-zhang-cai-ji/custom-metrics">custom metrics</a><br><a href="http://www.xuyasong.com/?p=1520">容器监控实践—Custom Metrics</a></li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>《Kubernetes进阶实战》</a> 马永亮</li>
<li><a href>深入剖析Kubernetes - 49  Custom Metrics 让Auto Scaling不再“食之无味”</a> 张磊</li>
<li><a href="https://github.com/www6v/container-monitor">container-monitor</a> git</li>
<li><a href="https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-1-zhang-cai-ji/metrics-server">metrics-server</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubenetes RBAC</title>
    <url>/www6vHomeHexo/2019/11/14/k8sRBAC/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/11/14/k8sRBAC/k8sRBAC.jpg" class title="Kubenetes-RBAC">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《深入剖析Kubernetes - 26  基于角色的权限控制：RBAC》    张磊</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubenetes资源模型</title>
    <url>/www6vHomeHexo/2019/11/14/k8sResouceModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%85%B3%E9%94%AE%E8%AF%8D">关键词</a></li>
<li><a href="#%E8%B5%84%E6%BA%90%E6%A8%A1%E5%9E%8B">资源模型</a><ul>
<li><a href="#requests-limits">requests limits</a></li>
<li><a href="#qos">QoS</a></li>
<li><a href="#cpu-cgroup%E9%85%8D%E7%BD%AE">CPU CGroup配置</a><ul>
<li><a href="#requestscpu">requests.cpu</a></li>
<li><a href="#limitscpu">limits.cpu</a></li>
</ul>
</li>
<li><a href="#cgroup-v2">CGroup v2</a></li>
</ul>
</li>
<li><a href="#%E9%A9%B1%E9%80%90%E7%AD%96%E7%95%A5-oom-killer">驱逐策略 &amp;&amp; OOM Killer</a><ul>
<li><a href="#%E9%A9%B1%E9%80%90">驱逐</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E7%9A%84%E9%A9%B1%E9%80%90">基于内存压力的驱逐</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E7%9A%84%E9%A9%B1%E9%80%90">基于磁盘压力的驱逐</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%AD%98oom-killer%E8%A1%8C%E4%B8%BA">内存OOM Killer行为</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h2><span id="关键词">关键词</span><a href="#关键词" class="header-anchor">#</a></h2><p>资源模型, 资源管理</p>
<img src="/www6vHomeHexo/2019/11/14/k8sResouceModel/k8sResouceModel.jpg" class title="Kubenetes资源模型">

<h2><span id="资源模型">资源模型</span><a href="#资源模型" class="header-anchor">#</a></h2><h3><span id="requests-limits">requests limits</span><a href="#requests-limits" class="header-anchor">#</a></h3><p><strong>CPU资源</strong>被称作 “可压缩资源”: Pod 只会“饥饿”，但不会退出.<br><strong>内存资源</strong>被称作“不可压缩资源”: Pod 会因为 OOM（Out-Of-Memory）被内核杀掉。</p>
<p>Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对<strong>“动态资源边<br>界”</strong>的定义，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守<br>的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。</p>
<p>而 Kubernetes 的 requests+limits 的做法，其实就是上述思路的一个简化版：用户在提交<br>Pod 时，可以声明一个相对<strong>较小的 requests</strong> 值供调度器使用，而 Kubernetes 真正设置给容器<br>Cgroups 的，则是相对<strong>较大的 limits 值</strong>。不难看到，这跟 Borg 的思路相通的。</p>
<p><strong>limits</strong>: 而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置<br><strong>requests</strong>: 在调度的时候，kube-scheduler 只会按照 requests 的值进行计算</p>
<h3><span id="qos">QoS</span><a href="#qos" class="header-anchor">#</a></h3><p><strong>Guaranteed</strong>:  requests &#x3D;&#x3D; limits<br><strong>Burstable</strong>:  至少有一个 Container 设置了 requests<br><strong>BestEffort</strong>:  没有设置 requests，也没有设置 limits</p>
<p><strong>cpuset</strong>: 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力.</p>
<ul>
<li><strong>cpuset配置</strong>: </li>
<li>Guaranteed</li>
<li>CPU 资源的 requests &#x3D;&#x3D; limits</li>
</ul>
<h3><span id="cpu-cgroup配置">CPU CGroup配置</span><a href="#cpu-cgroup配置" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">CGroup类型</th>
<th align="center">参数</th>
<th align="center">QoS</th>
<th align="center">值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">容器的CGroup</td>
<td align="center">cpu.shares</td>
<td align="center">BestEffort <br> Burstable <br> Guaranteed</td>
<td align="center">2  <br> requests.cpu * 1024 <br> requests.cpu * 1024</td>
</tr>
<tr>
<td align="center">容器的CGroup</td>
<td align="center">cpu.cfs_quota_us</td>
<td align="center">BestEffort <br> Burstable <br> Guaranteed</td>
<td align="center">-1 <br> limits.cpu * 100 <br> limits.cpu * 100</td>
</tr>
<tr>
<td align="center">Pod的CGroup</td>
<td align="center">cpu.shares</td>
<td align="center">BestEffort <br> Burstable <br> Guaranteed</td>
<td align="center">2 <br> Pod所有容器(requests.cpu * 1024)之和; <br> Pod所有容器(requests.cpu * 1024)之和;</td>
</tr>
<tr>
<td align="center">Pod的CGroup</td>
<td align="center">cpu.cfs_quota_us</td>
<td align="center">BestEffort <br> Burstable <br> Guaranteed</td>
<td align="center">-1 <br> Pod所有容器(limits.cpu * 100)之和; <br> Pod所有容器(limits.cpu * 100)之和;</td>
</tr>
</tbody></table>
<p>在cgroup中的设置</p>
<h5><span id="requestscpu">requests.cpu</span><a href="#requestscpu" class="header-anchor">#</a></h5><p>requests.cpu&#x3D;250m<br>cpu.shares &#x3D; (250&#x2F;1000)&#x2F;1024<br>cpu.shares 默认 则是 1024</p>
<h5><span id="limitscpu">limits.cpu</span><a href="#limitscpu" class="header-anchor">#</a></h5><p>limits.cpu&#x3D;500m<br>cpu.cfs_quota_us &#x3D;  (500&#x2F;1000)* 100ms<br>cpu.cfs_period_us 的值始终是 100ms</p>
<h3><span id="cgroup-v2">CGroup v2</span><a href="#cgroup-v2" class="header-anchor">#</a></h3><h2><span id="驱逐策略-ampamp-oom-killer">驱逐策略 &amp;&amp; OOM Killer</span><a href="#驱逐策略-ampamp-oom-killer" class="header-anchor">#</a></h2><h3><span id="驱逐">驱逐</span><a href="#驱逐" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">kubelet参数</th>
<th align="center">分类</th>
<th align="center">驱逐方式</th>
</tr>
</thead>
<tbody><tr>
<td align="center">evictionSoft</td>
<td align="center">软驱逐</td>
<td align="center">有宽限期， pod优雅终止， 不会影响应用。</td>
</tr>
<tr>
<td align="center">evictionHard</td>
<td align="center">硬驱逐</td>
<td align="center">没有宽限期， 可能影响应用。</td>
</tr>
</tbody></table>
<h5><span id="基于内存压力的驱逐">基于内存压力的驱逐</span><a href="#基于内存压力的驱逐" class="header-anchor">#</a></h5><pre><code>MemoryPressure = true
驱逐规则
</code></pre>
<h5><span id="基于磁盘压力的驱逐">基于磁盘压力的驱逐</span><a href="#基于磁盘压力的驱逐" class="header-anchor">#</a></h5><pre><code>DiskPressure = true
驱逐规则
</code></pre>
<p><strong>Eviction</strong> 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式。<br><strong>Soft Eviction模式</strong>： 允许你为 Eviction 过程设置一段“优雅时间”；<br><strong>Hard Eviction模式</strong>： Eviction 过程就会在阈值达到之后立刻开始；</p>
<p><strong>Eviction时机</strong>：<br>BestEffort &gt; Burstable &gt; Guaranteed</p>
<h3><span id="内存oom-killer行为">内存OOM Killer行为</span><a href="#内存oom-killer行为" class="header-anchor">#</a></h3><p>按进程的oom_score来进行优先级排序，选择待终止的进程，oom_score越高， 越容器被终止。</p>
<p>oom_score &#x3D; (内存占总内存的比例值) * 10 + oom_score_adj</p>
<table>
<thead>
<tr>
<th align="center">Pod QoS类型</th>
<th align="center">oom_score_adj</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Guaranteed</td>
<td align="center">-998 (不会被kill)</td>
</tr>
<tr>
<td align="center">BestEffort</td>
<td align="center">1000 (优先被kill)</td>
</tr>
<tr>
<td align="center">Burstable</td>
<td align="center">min(max(2,1000 - (1000 * memoryRequestsBytes)&#x2F;machineMemoryCapacityBytes), 999)</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》</a>  七牛容器云团队</li>
<li><a href>深入剖析Kubernetes - 40  Kubernetes的资源模型与资源管理</a> 张磊</li>
<li>&lt;&lt;模块九-生产化集群的管理&gt;&gt; 孟凡杰</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes模式</title>
    <url>/www6vHomeHexo/2019/11/13/k8sPattern/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%BC%8F">基础模式</a><br>- <a href="#%E5%A3%B0%E6%98%8E%E5%BC%8F%E7%9A%84deployment">声明式的Deployment</a><br>- <a href="#%E5%8F%97%E7%AE%A1%E7%90%86%E7%9A%84%E5%A3%B0%E6%98%8E%E5%91%A8%E6%9C%9F">受管理的声明周期</a></li>
<li><a href="#%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F">行为模式</a><br>- <a href="#batch-job">Batch Job</a><br>- <a href="#singleton-service">Singleton Service</a><br>- <a href="#stateful-service">Stateful Service</a><br>- <a href="#service-discovery">Service Discovery</a></li>
<li><a href="#%E7%BB%93%E6%9E%84%E6%A8%A1%E5%BC%8F">结构模式</a><br>- <a href="#init-container">Init Container</a><br>- <a href="#sidecar">Sidecar</a></li>
<li><a href="#%E9%AB%98%E7%BA%A7%E6%A8%A1%E5%BC%8F">高级模式</a><br>- <a href="#%E6%8E%A7%E5%88%B6%E5%99%A8%E6%A8%A1%E5%BC%8F">控制器模式</a><br>- <a href="#operator">Operator</a><br>- <a href="#elastic-scale">Elastic Scale</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="基础模式">基础模式</span><a href="#基础模式" class="header-anchor">#</a></h2><h5><span id="声明式的deployment">声明式的Deployment</span><a href="#声明式的deployment" class="header-anchor">#</a></h5><p>   Rolling deployment， 滚动发布<br>   Fixed deployment<br>   Blue-green release， 蓝绿部署<br>   Canary release, 灰度发布</p>
<h5><span id="受管理的声明周期">受管理的声明周期</span><a href="#受管理的声明周期" class="header-anchor">#</a></h5><p>   SIGTERM Signal：  SIGTERM 是通知进程优雅退出的信号<br>   SIGKILL Signal： SIGKILL 是硬终止的信号<br>   Poststart Hook<br>   Prestop Hook</p>
<p>   <a href="https://aleiwu.com/post/tidb-opeartor-webhook">Kubernetes 中如何保证优雅地停止 Pod</a><br>   <a href="https://mp.weixin.qq.com/s/wvg2AqOZd-WtjehGWNWFXg">Kubernetes 中如何保证优雅地停止 Pod</a><br>   PingCAP 吴叶磊</p>
<h2><span id="行为模式">行为模式</span><a href="#行为模式" class="header-anchor">#</a></h2><h5><span id="batch-job">Batch Job</span><a href="#batch-job" class="header-anchor">#</a></h5><p>参考:<br><a href="https://github.com/kubernetes-sigs/kube-batch">kube-batch</a>     </p>
<h5><span id="singleton-service">Singleton Service</span><a href="#singleton-service" class="header-anchor">#</a></h5><ul>
<li><p>Out-of-application locking mechanism<br>replicas &#x3D;1</p>
</li>
<li><p>In-application locking mechanism:<br>replicas&gt;1  + 分布式锁</p>
</li>
</ul>
<h5><span id="stateful-service">Stateful Service</span><a href="#stateful-service" class="header-anchor">#</a></h5><p>参考:</p>
<ul>
<li><a href="/www6vHomeHexo/2019/11/11/k8sStatefulSet/" title="Kubernetes StatefulSet原理和源码">Kubernetes StatefulSet原理和源码</a>  self</li>
</ul>
<h5><span id="service-discovery">Service Discovery</span><a href="#service-discovery" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">Name</th>
<th align="center">Configuration</th>
<th align="center">Client type</th>
<th align="center">Summary</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ClusterIP</td>
<td align="center">type: ClusterIP <br> .spec.selector</td>
<td align="center">Internal</td>
<td align="center">The most common internal discovery mechanism</td>
</tr>
<tr>
<td align="center">Manual IP</td>
<td align="center">type: ClusterIP <br> kind: Endpoints</td>
<td align="center">Internal</td>
<td align="center">External IP discovery</td>
</tr>
<tr>
<td align="center">Manual FQDN</td>
<td align="center">type: ExternalName <br> .spec.externalName</td>
<td align="center">Internal</td>
<td align="center">External FQDN discovery</td>
</tr>
<tr>
<td align="center">Headless Service</td>
<td align="center">type: ClusterIP <br> .spec.clusterIP: None</td>
<td align="center">Internal</td>
<td align="center">DNS-based discovery without a virtual IP</td>
</tr>
<tr>
<td align="center">NodePort</td>
<td align="center">type: NodePort</td>
<td align="center">External</td>
<td align="center">Preferred for non-HTTP traffic</td>
</tr>
<tr>
<td align="center">LoadBalancer</td>
<td align="center">type: LoadBalancer</td>
<td align="center">External</td>
<td align="center">Requires supporting cloud infrastructure</td>
</tr>
<tr>
<td align="center">Ingress</td>
<td align="center">kind: Ingress</td>
<td align="center">External</td>
<td align="center">L7&#x2F;HTTP-based smart routing mechanism</td>
</tr>
</tbody></table>
<p>参考:</p>
<ul>
<li><a href="/www6vHomeHexo/2019/11/04/k8sService/" title="Kubernetes服务">Kubernetes服务</a> self</li>
</ul>
<h2><span id="结构模式">结构模式</span><a href="#结构模式" class="header-anchor">#</a></h2><h5><span id="init-container">Init Container</span><a href="#init-container" class="header-anchor">#</a></h5><h5><span id="sidecar">Sidecar</span><a href="#sidecar" class="header-anchor">#</a></h5><p>场景:</p>
<ul>
<li>跨多语言; istio </li>
<li><a href="https://yq.aliyun.com/articles/674327">容器日志采集利器Log-Pilot</a></li>
</ul>
<h2><span id="高级模式">高级模式</span><a href="#高级模式" class="header-anchor">#</a></h2><h5><span id="控制器模式">控制器模式</span><a href="#控制器模式" class="header-anchor">#</a></h5><p>参考:<br><a href="https://github.com/kubernetes/sample-controller">sample-controller 官方控制器的例子</a> git<br><a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder</a> git</p>
<h5><span id="operator">Operator</span><a href="#operator" class="header-anchor">#</a></h5><ul>
<li><a href="/www6vHomeHexo/2019/11/19/k8sOperator/" title="Kubernetes Operator-Etcd">Kubernetes Operator-Etcd</a> self</li>
</ul>
<h5><span id="elastic-scale">Elastic Scale</span><a href="#elastic-scale" class="header-anchor">#</a></h5><ul>
<li><p>类型</p>
<ul>
<li>VPA</li>
<li>HPA</li>
<li>CA</li>
</ul>
</li>
<li><p>资源模型概念</p>
<ul>
<li>QoS</li>
<li>requests, limits</li>
</ul>
</li>
</ul>
<p>参考:</p>
<ul>
<li><a href="/www6vHomeHexo/2019/11/14/k8sResouceModel/" title="Kubenetes资源模型">Kubenetes资源模型</a> self</li>
<li><a href="/www6vHomeHexo/2019/11/16/k8sAutoScale/" title="Kubernetes自动伸缩和HPA">Kubernetes自动伸缩和HPA</a>  self</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href>《Kubernetes Patterns - Reusable Elements for Designing Cloud-Native Applications》</a>  Bilgin Ibryam@RedHat, Roland Huß@RedHat</p>
</li>
<li><p><a href="http://www.xuyasong.com/?p=2056">K8S 中的设计模式- 读《Kubernetes Patterns》</a> </p>
</li>
<li><p><a href="https://github.com/k8spatterns">Kubernetes Patterns</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes StatefulSet原理和源码</title>
    <url>/www6vHomeHexo/2019/11/11/k8sStatefulSet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/11/11/k8sStatefulSet/statefulSet.jpg" class title="StatefulSet">


<h2><span id="源码分析">源码分析</span><a href="#源码分析" class="header-anchor">#</a></h2><h2><span id="1-statefulset资源定义">1. StatefulSet资源定义</span><a href="#1-statefulset资源定义" class="header-anchor">#</a></h2><h3><span id="11-statefulset定义">1.1 StatefulSet定义</span><a href="#11-statefulset定义" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">staging/src/k8s.io/api/apps/v1beta2/types.go</span><br><span class="line"></span><br><span class="line">type StatefulSet struct &#123;</span><br><span class="line">  metal.TypeMeta</span><br><span class="line">  metal.ObjectMeta</span><br><span class="line">  Spec StatefulSetspec #1</span><br><span class="line">  Status StatefulSetStatus #2</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="12-statefulsetspec定义">1.2 StatefulSetspec定义</span><a href="#12-statefulsetspec定义" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#1</span><br><span class="line">staging/src/k8s.io/api/apps/v1beta2/types.go</span><br><span class="line"></span><br><span class="line">type StatefulSetspec struct &#123;</span><br><span class="line">  /// Pod副本数控制	</span><br><span class="line">  Replicas *int32</span><br><span class="line"></span><br><span class="line">  /// Pod创建或者删除</span><br><span class="line">  Selector *metav1.LabelSelector</span><br><span class="line">  Template v1.PodTemplateSpec</span><br><span class="line">  VolumeClaimTemplates []v1.PersistentVolumeClaim  # 存储状态【2】</span><br><span class="line">  ServiceName string</span><br><span class="line">  PodManagementPolicy PodManagementPolicyType      # 2.2节 </span><br><span class="line"></span><br><span class="line">  /// Pod升级和回滚</span><br><span class="line">  UpdateStrategy StatefulSetUpdateStrategy         # 2.4节</span><br><span class="line">  RevisionHistoryLimit *int32</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="13-statefulsetstatus定义">1.3 StatefulSetStatus定义</span><a href="#13-statefulsetstatus定义" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#2</span><br><span class="line">staging/src/k8s.io/api/apps/v1beta2/types.go</span><br><span class="line"></span><br><span class="line">type StatefulSetStatus struct &#123;</span><br><span class="line">  ObservedGeneration *int64</span><br><span class="line">  Replicas int32              # 所有属于该 StatefulSet的Pod数量</span><br><span class="line">  ReadyReplicas int32         # 所有属于该 Statefulset的Pod且状态为ready的数量</span><br><span class="line">  CurrentReplicas int32       # 所有属于该 StatefulSet当前版本的Pod数量(升级完成时会等于UpdatedReplicas)</span><br><span class="line">  UpdatedReplicas int32       # 所有属于该 StatefulSet升级版本的Pod数量</span><br><span class="line">  CurrentRevision string      # Statefulset当前版本的 set.Name+hash</span><br><span class="line">  UpdateRevision string       # Stateful|Set升级版本的 set.Name+hash</span><br><span class="line">  CollisionCount *int32</span><br><span class="line">  Conditions []StatefulSetCondition</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3><span id="14-statefulset例子">1.4 StatefulSet例子</span><a href="#14-statefulset例子" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:                    ## 对应StatefulSetspec</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: k8s.gcr.io/nginx-slim:0.8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:       ## 存储状态</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi         </span><br></pre></td></tr></table></figure>


<h2><span id="2-statefulset-controller详细说明">2. StatefulSet Controller详细说明</span><a href="#2-statefulset-controller详细说明" class="header-anchor">#</a></h2><p>前提: 应用的各实例启动需要遵循一定的<strong>顺序</strong>， 顺序需要唯一的<strong>标识和编号</strong>. </p>
<h3><span id="21-pod固定身份标识编号-gt-拓扑状态1">2.1 Pod固定身份标识（编号） -&gt; 拓扑状态【1】</span><a href="#21-pod固定身份标识编号-gt-拓扑状态1" class="header-anchor">#</a></h3><ul>
<li>名称维度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pkg/controller/statefulset/stateful_set_utils.go</span><br><span class="line"></span><br><span class="line">// getPodName gets the name of set&#x27;s child Pod with an ordinal index of ordinal</span><br><span class="line">func getPodName(set *apps.StatefulSet, ordinal int) string &#123;</span><br><span class="line">	return fmt.Sprintf(&quot;%s-%d&quot;, set.Name, ordinal)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>索引号 ordinal&#x3D;{0~ StatefulSetSpec.Replicas - 1}</p>
<ul>
<li>网络地址维度</li>
</ul>
<p>headless service对应pod的网络地址(完整<strong>域名</strong>)为: <code>&lt;hostname&gt;.&lt;subdomain&gt;.&lt;ns&gt;.svc.cluster.local</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pkg/controller/statefulset/stateful_set_utils.go</span><br><span class="line"></span><br><span class="line">func initIdentity(set *apps.StatefulSet, pod *v1.Pod) &#123;</span><br><span class="line">	updateIdentity(set, pod)</span><br><span class="line">	// Set these immutable fields only on initial Pod creation, not updates.</span><br><span class="line">	pod.Spec.Hostname = pod.Name</span><br><span class="line">	pod.Spec.Subdomain = set.Spec.ServiceName</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从名称维度可知，podName是固定的，所以pod的网络地址也是固定的。</p>
<ul>
<li>存储维度 -&gt;  存储状态</li>
</ul>
<p>Pod的各个 volume是通过PVC来管理的，所以只要 Volume对应的PVC能保持不变，那就可以保证存储不变。那么顺其自然一定会想到，只要PVC的名称也和Pod的索引位置绑定，那问题就解决了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pkg/controller/statefulset/stateful_set_utils.go</span><br><span class="line"></span><br><span class="line">// getPersistentVolumeClaimName gets the name of PersistentVolumeClaim for a Pod with an ordinal index of ordinal. claim</span><br><span class="line">// must be a PersistentVolumeClaim from set&#x27;s VolumeClaims template.</span><br><span class="line">func getPersistentVolumeClaimName(set *apps.StatefulSet, claim *v1.PersistentVolumeClaim, ordinal int) string &#123;</span><br><span class="line">	// NOTE: This name format is used by the heuristics for zone spreading in ChooseZoneForVolume</span><br><span class="line">	return fmt.Sprintf(&quot;%s-%s-%d&quot;, claim.Name, set.Name, ordinal)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>pod的名字和pvc的名字一一对应。</p>
<p><strong>总结</strong>: 可以看到3个维度的固定，本质都是依赖Pod的<strong>索引位置</strong>(ordinal)来固定的。</p>
<h3><span id="22-statefulset-pod创建和删除">2.2 StatefulSet Pod创建和删除</span><a href="#22-statefulset-pod创建和删除" class="header-anchor">#</a></h3><p>根据<code>StatefulSetSpec.PodManagementPolicy</code>的设置，Pod创建分为<code>OrderedReady</code>和 <code>Parallel</code>两种模式。</p>
<p><code>OrderedReady模式</code>: 按索引号<code>0 ~ replicas-1</code>的顺序，前序Pod创建成功后，才会接下来创建下一个Pod。<br><code>Parallel模式</code>:并发创建各个Pod。</p>
<p>而对于Pod<strong>删除</strong>，用户直接删除StatefulSet的Pod是无效的，因为 StatefulSet马上就会重建。<br>如果要删除Pod,必须通过调整 StatefulSet的 Spec.Replicas来达到删除目的。即为Pod扩缩容处理。</p>
<h3><span id="23-pod扩缩容">2.3 Pod扩缩容</span><a href="#23-pod扩缩容" class="header-anchor">#</a></h3><p>扩容处理: Replicas增大的情况，则直接是Pod创建的逻辑。<br>缩容处理: 因为需要减少Pod，为了不和Pod创建过程冲突，缩容是从最大索引号开始删除Pod。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pkg/controller/statefulset/stateful_set_control.go</span><br><span class="line"></span><br><span class="line">if ord := getOrdinal(pods[i]); 0 &lt;= ord &amp;&amp; ord &lt; replicaCount &#123;</span><br><span class="line">    replicas[ord] = pods[i]</span><br><span class="line">&#125; else if ord &gt;= replicaCount &#123;</span><br><span class="line">    //pod索引号大于最新的Spec.Replicas,说明Replicas减小了，这些Pod需要被缩容掉</span><br><span class="line">    condemned = append(condemned, pods[i])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 从最大索引号开始缩容</span><br><span class="line">for target := len(condemned) - 1; target &gt;= 0; target-- &#123;</span><br><span class="line">    // 如果该Pod正在被删除,则等待被删除完成即可</span><br><span class="line">    if isTerminating(condemned[target]) &#123;</span><br><span class="line">        if monotonic &#123;</span><br><span class="line">            return &amp;status, nil</span><br><span class="line">        &#125;</span><br><span class="line">        continue</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 如果被删除pod的前序pod中有不健康的,那么需要等待前序pod恢复为正常状态后才能继续缩容</span><br><span class="line">    if !isRunningAndReady(condemned[target]) &amp;&amp; monotonic &amp;&amp; condemned[target] != firstUnhealthyPod &#123;</span><br><span class="line">        return &amp;status, nil</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 到这里可以执行pod缩容删除了</span><br><span class="line">    if err := ssc.podControl.DeleteStatefulPod(set, condemned[target]); err != nil &#123;</span><br><span class="line">        return &amp;status, err</span><br><span class="line">    &#125;</span><br><span class="line">    // 更新StatefulSet.Status</span><br><span class="line">    if getPodRevision(condemned[target]) == currentRevision.Name &#123;</span><br><span class="line">        status.CurrentReplicas--</span><br><span class="line">    &#125; else if getPodRevision(condemned[target]) == updateRevision.Name &#123;</span><br><span class="line">        status.UpdatedReplicas--</span><br><span class="line">    &#125;</span><br><span class="line">    if monotonic &#123;</span><br><span class="line">        return &amp;status, nil</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="24-pod的升级">2.4 Pod的升级</span><a href="#24-pod的升级" class="header-anchor">#</a></h3><p>Pod升级策略由<code>Spec.Update.Strategy</code>字段指定，目前支持<code>OnDelete</code>和<code>RollingUpdate</code>两种模式。</p>
<h5><span id="241-ondelete-模式">2.4.1 OnDelete 模式</span><a href="#241-ondelete-模式" class="header-anchor">#</a></h5><p><code>Spec.UpdateStrategy.Type=OnDelete</code>: <code>Spec.Template</code>更新后，需要用户手动删除旧Pod，然后StatefulSet Controller会利用新的<code>Spec.Template</code>创建新Pod。代码中处理如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType &#123;</span><br><span class="line">    return &amp;status, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当升级策略为OnDelete时，执行直接返回，等待用户手动删除pod。</p>
<h5><span id="242-rollingupdate-模式">2.4.2 RollingUpdate 模式</span><a href="#242-rollingupdate-模式" class="header-anchor">#</a></h5><p><code>Spec.UpdateStrategy.Type=RollingUpdate</code>: <code>Spec.Template</code>更新后，StatefulSet Controller会从最大索引号开始逐个升级Pod。即先删除pod，然后等到删除的pod被创建好后再进行下一个索引号的Pod升级。</p>
<p>RollingUpdate模式的Pod升级，可以只升级部分Pod。新旧Pod分水岭的索引号由<code>Spec.UpdateStrategy.RollingUpdate.Partition</code>指定。其中<code>[0 ~ partition-1]</code>索引号的Pod为旧版本，而<code>[partition ~ replicas-1]</code>索引号的Pod为新版本。当然如果 <code>partition &gt; Spec.Replicas</code>，则不会升级任何Pod。</p>
<p>RollingUpdate模式的代码如下(下面主要为删除旧Pod)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">updateMin := 0</span><br><span class="line">if set.Spec.UpdateStrategy.RollingUpdate != nil &#123;</span><br><span class="line">    updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 从最大索引号开始执行Pod升级处理(此处为旧Pod删除)</span><br><span class="line">for target := len(replicas) - 1; target &gt;= updateMin; target-- &#123;</span><br><span class="line">    // 如果pod为旧版本并且不在被删除状态，则执行Pod删除</span><br><span class="line">    if getPodRevision(replicas[target]) != updateRevision.Name &amp;&amp; !isTerminating(replicas[target]) &#123;</span><br><span class="line">        err := ssc.podControl.DeleteStatefulPod(set, replicas[target])</span><br><span class="line">        status.CurrentReplicas--</span><br><span class="line">        return &amp;status, err // 直接退出,等待被删除Pod被创建</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 等待到被删除pod创建且ready，才进行下一个pod的升级。否则就退出for循环</span><br><span class="line">    if !isHealthy(replicas[target]) &#123;</span><br><span class="line">        return &amp;status, nil</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2><span id="3-statefulset的控制流程">3. Statefulset的控制流程</span><a href="#3-statefulset的控制流程" class="header-anchor">#</a></h2><p>经过上面代码级别的细节说明，下面大致梳理一下 StatefulSet Controller的控制流程。具体如下:</p>
<ul>
<li>获取 StatefulSet: 由key从<code>set.Lister</code>(本地缓存)中获取到需要处理的 StatefulSet实例</li>
<li>获取 Statefulset所有Pod: 由<code>StatefulSet.Spec.Selector</code>从 pod.Lister(本地缓存)中过滤所有符合条件的Pod(且podName和 set.Name匹配) （1.2节）</li>
<li>获取当前版本和升级版本的 Controller Revision: 如果升级版本的 Controller Revision不存在，就新创建一个。(StatefulSet创建时，当前版本和升级版本相同。当前升级完成后,他们也相同)</li>
<li>从<code>0 ~ Spec.Replicas-1</code>逐个索引，创建StatefulSet Pod。 (2.2节)</li>
<li>所有pod创建完成后，进入扩缩容逻辑处理(如果需要扩缩容操作的话)  （2.3节）</li>
<li>扩缩容操作完成后，进入Pod升级逻辑(如果需要Pod升级操作的话) （2.4节）</li>
<li>更新 StatefulSet的 Status （1.3节）</li>
</ul>
<h2><span id="draft-here">Draft Here</span><a href="#draft-here" class="header-anchor">#</a></h2>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes服务</title>
    <url>/www6vHomeHexo/2019/11/04/k8sService/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-kubenetes服务">一. Kubenetes服务</span><a href="#一-kubenetes服务" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/68105556-d4257280-ff19-11e9-9d19-f18f5c454f34.jpg" alt="Kubernetes服务发现架构"><br>Kubernetes服务发现架构</p>
</div>

<img src="/www6vHomeHexo/2019/11/04/k8sService/k8sService.jpg" class title="Kubernetes服务"> 

<ul>
<li>ClusterIP 模式的 Service: <strong>稳定的 IP 地址</strong>，即 VIP.  ClusterIP是VIP, 虚拟IP.    </li>
<li>Headless Service: <strong>稳定的 DNS 名字</strong>, 名字是通过 Pod 名字和 Service 名字拼接出来.</li>
</ul>
<h3><span id="11-服务对外暴露方式">1.1  服务对外暴露方式</span><a href="#11-服务对外暴露方式" class="header-anchor">#</a></h3><ol>
<li>NodePort  四层<div style="text-align: center; width:60%; height: 60%"></div></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/5608425/68234082-7d17be80-003b-11ea-891f-90a9e174bbc8.png" alt="node-port"></p>


<ul>
<li>一定要对流出的包做 SNAT操作[2][6]</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          client</span><br><span class="line">            \ ^</span><br><span class="line">             \ \</span><br><span class="line">              v \</span><br><span class="line">  node 1 &lt;--- node 2</span><br><span class="line">  | ^    SNAT</span><br><span class="line">  | |    ---&gt;</span><br><span class="line">  v |</span><br><span class="line">endpoint</span><br></pre></td></tr></table></figure>

<ul>
<li>获取真实客户端IP, 设置Service 的 spec.externalTrafficPolicy 字段设置为 local，[2][6]</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    client</span><br><span class="line">    ^ / \</span><br><span class="line">   / /   \</span><br><span class="line">  / v     X</span><br><span class="line">node 1   node 2</span><br><span class="line">  ^ |</span><br><span class="line">  | |</span><br><span class="line">  | v</span><br><span class="line">endpoint</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Service LoadBalancer  四层<div style="text-align: center; width:60%; height: 60%"></div></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/5608425/68290997-e216f700-00c3-11ea-82d3-b5e3f4c565a1.jpg" alt="loadbalancer"></p>


<p><strong>LoadBalancer</strong>类型的Service被提交后，Kubernetes就会调用<strong>CloudProvider</strong>[5]在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP地址配置给负载均衡服务做后端.</p>
<ol start="3">
<li><p>ExternalName<br>ExternalName 类型的 Service，其实是在 kubedns里为你添加了一条 CNAME 记录.<br>1.7 之后支持的一个新特性.</p>
</li>
<li><p>Ingress Controller  七层</p>
<div style="text-align: center; width:60%; height: 60%"></div></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/5608425/68234079-7c7f2800-003b-11ea-8ada-2c034db8b25a.png" alt="ingress-1"><br><img src="https://user-images.githubusercontent.com/5608425/68234081-7c7f2800-003b-11ea-804c-1c5d87164d06.png" alt="ingress-2">   </p>


<ul>
<li>Ingress 服务: 全局的、为了代理不同后端 Service 而设置的负载均衡服务.</li>
<li>Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</li>
<li>Ingress Controller: Nginx、HAProxy、Envoy、Traefik</li>
</ul>
<p><strong>Ingress controller</strong></p>
<p>为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。</p>
<ul>
<li>Kubernetes 当前支持并维护 GCE 和 nginx 两种 controller</li>
<li>F5（公司）支持并维护 F5 BIG-IP Controller for Kubernetes</li>
<li>Kong 同时支持并维护 社区版 与 企业版 的 Kong Ingress Controller for Kubernetes</li>
<li>Traefik 是功能齐全的 ingress controller（Let’s Encrypt, secrets, http2, websocket…）, Containous 也对其提供商业支持。</li>
<li>Istio 使用 CRD Gateway 来 控制 Ingress 流量。</li>
</ul>
<h3><span id="12-通过dns发现服务">1.2 通过DNS发现服务</span><a href="#12-通过dns发现服务" class="header-anchor">#</a></h3><blockquote>
<p>每个Service对象相关的DNS记录有两个：<br>{SVCNAME}.{NAMESPACE}.{CLUSTER_DOMAIN}<br>{SVCNAME}.{NAMESPACE}.svc.{CLUSTER_DOMAIN}</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@kubia-9nvx7:/# cat /etc/resolv.conf</span><br><span class="line">nameserver 172.17.0.2</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">（1）拥有ClusterIP的Service资源，要具有以下类型的资源记录</span><br><span class="line">A记录：&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt;  IN  A  &lt;cluster-ip&gt;</span><br><span class="line"></span><br><span class="line">（2）Headless类型的Service资源，要具有以下类型的资源记录</span><br><span class="line">A记录：&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt;</span><br><span class="line"></span><br><span class="line">（3）ExternalName类型的Service资源，要具有CNAME类型的资源记录</span><br><span class="line">CNAME记录：&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN CNAME &lt;extname&gt;.</span><br></pre></td></tr></table></figure>

<h3><span id="13-clusterip模式的yaml配置">1.3 ClusterIP模式的yaml配置</span><a href="#13-clusterip模式的yaml配置" class="header-anchor">#</a></h3><p>Service（接口声明） + Deployment（ endpoint 接口实现）</p>
<h2><span id="二-kubenetes服务工作原理">二. Kubenetes服务工作原理</span><a href="#二-kubenetes服务工作原理" class="header-anchor">#</a></h2><ul>
<li>Service是由<strong>kube-proxy</strong>组件，加上<strong>iptables</strong>来共同实现的。</li>
<li>kube-proxy的作用: 网络配置</li>
</ul>
<h5><span id="21-kube-proxy">2.1 kube-proxy</span><a href="#21-kube-proxy" class="header-anchor">#</a></h5><h5><span id="userspace-代理模式">userspace 代理模式</span><a href="#userspace-代理模式" class="header-anchor">#</a></h5><div style="text-align: center;">
<img width="450" alt="user-space-proxy" src="https://user-images.githubusercontent.com/5608425/68077955-2b3b2280-fe08-11e9-8672-3210219a7372.png">
userspace 代理模式
</div>

<h5><span id="iptables-proxy-mode">iptables Proxy Mode</span><a href="#iptables-proxy-mode" class="header-anchor">#</a></h5><div style="text-align: center;">
<img width="450" alt="iptables-proxy" src="https://user-images.githubusercontent.com/5608425/68077954-2b3b2280-fe08-11e9-8231-cb9bc177ba21.png">
 Iptables Proxy Mode
</div>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-A OUTPUT -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line"></span><br><span class="line"># 访问10.107.54.95后跳转到KUBE-SVC-4N57TFCL4MD7ZTDA链</span><br><span class="line">-A KUBE-SERVICES -d 10.107.54.95/32 -p tcp -m comment --comment &quot;default/nginx: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-4N57TFCL4MD7ZTDA</span><br><span class="line"></span><br><span class="line"># 随机转发的目的地，分别是 KUBE-SEP-UZXILYFQQ2IZUWN5 和 KUBE-SEP-43IWXJI557JKCKCF</span><br><span class="line">-N KUBE-SVC-4N57TFCL4MD7ZTDA</span><br><span class="line">-A KUBE-SVC-4N57TFCL4MD7ZTDA -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-UZXILYFQQ2IZUWN5</span><br><span class="line">-A KUBE-SVC-4N57TFCL4MD7ZTDA -j KUBE-SEP-43IWXJI557JKCKCF</span><br><span class="line"></span><br><span class="line">## DNAT到pod的ip和端口</span><br><span class="line">-A KUBE-SEP-UZXILYFQQ2IZUWN5 -p tcp -m tcp -j DNAT --to-destination 172.17.0.4:80</span><br><span class="line">-A KUBE-SEP-43IWXJI557JKCKCF -p tcp -m tcp -j DNAT --to-destination 172.17.0.5:80</span><br></pre></td></tr></table></figure>

<h5><span id="ipvs-proxy-mode">IPVS proxy mode</span><a href="#ipvs-proxy-mode" class="header-anchor">#</a></h5><blockquote>
<p>IPVS是LVS一个组件，提供高性能、高可靠性的四层负载均衡器。IPVS 是IP Virtual Server的简写。IPVS构建在netfilter上，作为Linux 内核的一部分，从传输层实现了负载均衡。</p>
</blockquote>
<p>与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：</p>
<pre><code>rr：轮询调度
lc：最小连接数
dh：目标哈希
sh：源哈希
sed：最短期望延迟
nq： 不排队调度
</code></pre>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.kubernetes.org.cn/5992.html">深入理解 Kubernetes之：Service</a> good</li>
<li><a href="https://edu.aliyun.com/lesson_1651_17064#_17064">第14 章 ： Kubernetes Services</a>  阿里</li>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》</a>  七牛容器云团队</li>
<li><a href="https://mp.weixin.qq.com/s/3THiWFt52tZckFGxg3Cx-g">Kubernetes中的服务发现机制与方式</a> 马永亮 </li>
<li><a href="https://mp.weixin.qq.com/s/a_540yJ1EGVroJ9TpvYtPw">从 K8S 的 Cloud Provider 到 CCM 的演进之路</a>  毛宏斌 百度</li>
<li><a href="https://docs.ucloud.cn/compute/uk8s/service/getresourceip">获取真实客户端IP</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU1OTAzNzc5MQ==&mid=2247485610&idx=1&sn=e092e55c848af62368835d530c57da15&chksm=fc1c249acb6bad8c940c587e59e0dc63ba4863c7063f0a0e322dcbd6ad5f610cd2ad1b4ba87d&scene=21#wechat_redirect">华为云在 K8S 大规模场景下的 Service 性能优化实践</a> 未</li>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html">Service</a>  jimmysong</li>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html">Ingress</a> jimmysong</li>
</ol>
<hr>
<p>《深入剖析Kubernetes》  张磊</p>
<ol>
<li><a href>《37  找到容器不容易：Service、DNS与服务发现》</a> </li>
<li><a href>《38  从外界连通Service与Service调试“三板斧”》</a></li>
<li><a href>《39  谈谈Service与Ingress》</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes技能图谱</title>
    <url>/www6vHomeHexo/2019/11/03/k8sSkill/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63923539-4f315f00-ca79-11e9-900a-22b66a75922c.jpg" alt="k8s技能图谱"><br>Kubernetes技能图谱</p>
</div>]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenAPI 设计</title>
    <url>/www6vHomeHexo/2019/11/01/apiDesign/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="一-rest-api-设计-规范">一. REST API 设计 规范</span><a href="#一-rest-api-设计-规范" class="header-anchor">#</a></h2><p>OpenAPI Specification 业界标准</p>
<ol>
<li><a href="https://blog.readme.io/an-example-filled-guide-to-swagger-3-2/">A Visual Guide to What’s New in Swagger 3.0</a></li>
<li><a href="https://swagger.io/specification/">OpenAPI Specification - Version 3.0.2</a></li>
</ol>
<p>Google API Design Guide</p>
<ol>
<li><a href="https://www.bookstack.cn/read/API-design-guide/API-design-guide-README.md">谷歌API设计指南</a></li>
</ol>
<h2><span id="二-api-设计模式">二. API 设计模式</span><a href="#二-api-设计模式" class="header-anchor">#</a></h2><ol>
<li>RPC</li>
<li>ROA(Rest-Oriented Architecture)</li>
</ol>
<p>通常RESTful风格对API设计者的要求是比较高的，主要的难点在于面向资源设计要求开发者事先做好规划，将后端数据模型与API服务模型相匹配。</p>
<h2><span id="三-面向资源设计api">三. 面向资源设计API</span><a href="#三-面向资源设计api" class="header-anchor">#</a></h2><ul>
<li>资源模型<br>资源分类管理</li>
<li>资源关系</li>
</ul>
<p><a href="https://yq.aliyun.com/articles/497936">ECS TAG功能详解</a><br><a href="https://www.alibabacloud.com/help/zh/doc-detail/54182.htm">资源组</a></p>
<h2><span id="四-服务容错处理">四. 服务容错处理</span><a href="#四-服务容错处理" class="header-anchor">#</a></h2><ol>
<li>同步请求的Timeout[2]</li>
<li>异步请求方式</li>
<li>错误码<br><a href="https://help.aliyun.com/document_detail/110424.html">TagResources</a> 错误码</li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&mid=2247491596&idx=1&sn=9df1fac5f35771dc6aa065d8fac67f2e&chksm=e92add03de5d541504f6087a5f571ca838bc8ff23a045e7b4d4513454be8c34ba9d56a20215a&scene=0&xtrack=1#rd">云服务OpenAPI的7大挑战，架构师如何应对？</a>  阿里技术 虚明  </li>
<li><a href="../../../../2016/01/17/timeout/">超时和重试总结</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>API</category>
      </categories>
      <tags>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库-分类与选型</title>
    <url>/www6vHomeHexo/2019/10/27/dbClassify/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E7%B1%BB">数据库分类</a><ul>
<li><a href="#%E5%85%A8%E6%99%AF%E5%9B%BE-1">全景图 [1]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E7%B1%BB-1">数据库分类</a></li>
</ul>
</li>
<li><a href="#%E9%80%89%E5%9E%8B-3">选型 [3]</a><ul>
<li><a href="#%E5%85%B3%E6%B3%A8%E7%82%B9">关注点</a></li>
<li><a href="#%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%89%E5%9E%8B">关系型数据库选型</a></li>
<li><a href="#nosql%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%89%E5%9E%8B">NoSQL数据库选型</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据库分类">数据库分类</span><a href="#数据库分类" class="header-anchor">#</a></h1><h3><span id="全景图-1">全景图 [1]</span><a href="#全景图-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/10/27/dbClassify/overviw.jpg" class title="结构化数据库分类">


<h3><span id="数据库分类">数据库分类</span><a href="#数据库分类" class="header-anchor">#</a></h3><ul>
<li>NoSQL<ul>
<li>Key-Value 键值对<br>  Redis</li>
<li>Ordered Key-Value 有序键值对<br>  HBase</li>
<li>Document databases 文档模型<br>  MongoDB</li>
<li>Graph data models 图式数据库<br>  Neo4J</li>
</ul>
</li>
<li>NewSQL <ul>
<li>Google Spanner</li>
<li>TiDB</li>
<li>OceanBase</li>
<li>CockroachDB</li>
<li>Vitess</li>
</ul>
</li>
<li>RMDB<ul>
<li>MySQL</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2019/10/27/dbClassify/classify.JPG" class>

<h1><span id="选型-3">选型 [3]</span><a href="#选型-3" class="header-anchor">#</a></h1><h3><span id="关注点">关注点</span><a href="#关注点" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/10/27/dbClassify/xuqiu.JPG" class>

<h3><span id="关系型数据库选型">关系型数据库选型</span><a href="#关系型数据库选型" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/10/27/dbClassify/rmdb.JPG" class>

<h3><span id="nosql数据库选型">NoSQL数据库选型</span><a href="#nosql数据库选型" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/10/27/dbClassify/nosql.JPG" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/icycode/article/details/81008607">5分钟理解数据库全景图(SQL,NoSQL,NewSQL,OLAP,OLTP)</a> icycode</li>
<li><a href="/www6vHomeHexo/2018/07/19/NoSQL/" title="NoSQL总结">NoSQL总结</a>  self</li>
<li>《爱奇艺实用数据库选型树：不同场景如何快速选择<br>数据库？》 数据库架构选型指南 pdf</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>分类</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式系统学习资源-个人</title>
    <url>/www6vHomeHexo/2019/10/13/distributedStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="book">Book</span><a href="#book" class="header-anchor">#</a></h3><ol>
<li><a href="https://item.jd.com/10079452.html">《世界著名计算机教材精选：分布式系统原理与范型》</a></li>
<li><a href="http://www.antonfagerberg.com/files/intensive.pdf">《Designing Data‑Intensive Applications》</a></li>
</ol>
<h2><span id="个人博客-阿里">个人博客 - 阿里</span><a href="#个人博客-阿里" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">内容</th>
<th align="center">注释-最后更新时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><a href="http://leaver.me/">碧远 蚂蚁金服中间件</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="http://monkeyhorse.cn/">蚂蚁财富研发工程师</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="http://blog.lichengwu.cn/">李小武，coding@alibaba。</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="http://www.blogjava.net/BlueDavy/">阿里人</a></td>
<td align="center">2009停更</td>
</tr>
<tr>
<td align="center"><a href="http://blog.zephyrleaves.net/">黄若慧 高级技术专家 淘宝文通</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://www.sczyh30.com/">Software Engineer @ Alibaba Middleware Team</a></td>
<td align="center">2017停更</td>
</tr>
<tr>
<td align="center"><a href="http://blog.yufeng.info/">系统技术非业余研究</a></td>
<td align="center">不写技术了</td>
</tr>
<tr>
<td align="center"><a href="http://arebya.com/">arebya</a> 质量不高</td>
<td align="center">2018 停更</td>
</tr>
<tr>
<td align="center"><a href="https://agapple.iteye.com/">canal作者 agapple</a></td>
<td align="center">2016停更</td>
</tr>
<tr>
<td align="center"><a href="https://www.jianshu.com/u/b230a86fb7ad">阿里加多</a></td>
<td align="center"><strong>更新-2023.01</strong></td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/6bmpl5rdwpqu4">乒乓狂魔</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/ejn34jbxf5evs">冯嘉 rocketmq</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/jwbhxydfk6qyi/article">木洛 阿里云技术专家，表格存储</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/u3vivzisyg2pc/">成喆 阿里云存储服务</a></td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/zh5kxxfngqw2m">文意 阿里云存储服务产品</a>  serverless 场景</td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://yq.aliyun.com/users/fbt6ovijrs2zi">scorpion 阿里云函数计算研发经理</a> 函数计算</td>
<td align="center">已失效</td>
</tr>
<tr>
<td align="center"><a href="https://www.cnblogs.com/bodhitree/default.html">Freewill 阿里云的人 SDN ceph</a></td>
<td align="center">2020 停更</td>
</tr>
<tr>
<td align="center"><a href="http://www.seflerzhou.net/">阿里  周遥（玄胤）</a></td>
<td align="center">一般</td>
</tr>
<tr>
<td align="center">入境繁华  蚂蚁金服中间件团队 <br> <a href="http://zhengjianglong.cn/">http://zhengjianglong.cn</a> <br> <a href="http://blog.leanote.com/along">http://blog.leanote.com/along</a></td>
<td align="center">失效</td>
</tr>
<tr>
<td align="center">阿里人的博客 canel <br> <a href="http://agapple.iteye.com/">http://agapple.iteye.com/</a> <br>  <a href="https://github.com/agapple">https://github.com/agapple</a></td>
<td align="center">2016 停更</td>
</tr>
<tr>
<td align="center"><a href="http://bluedavy.me/">阿里 林昊</a></td>
<td align="center">失效</td>
</tr>
<tr>
<td align="center"><a href="https://dinglin.iteye.com/">阿里 丁奇 dba mysql</a></td>
<td align="center">2016 停更</td>
</tr>
<tr>
<td align="center"><a href="http://www.cnblogs.com/LBSer/">占利军   后端架构、算法  阿里巴巴 美团；</a></td>
<td align="center">2019 停更</td>
</tr>
<tr>
<td align="center"><a href="http://hengyunabc.github.io/">横云断岭&#x2F;hengyunabc hexo</a>  <a href="https://blog.csdn.net/hengyunabc">横云断岭&#x2F;hengyunabc csdn</a></td>
<td align="center">2019 停更</td>
</tr>
</tbody></table>
<p>taobao   毕玄博客   做服务化的<br><a href="http://hellojava.info/">http://hellojava.info/</a><br>hellojavacases微信公众号网站  加过  </p>
<h2><span id="个人博客-其他">个人博客 - 其他</span><a href="#个人博客-其他" class="header-anchor">#</a></h2><p><a href="https://coolshell.cn/">陈皓 左耳耗子</a> ***<br><a href="https://www.helight.cn/">黑光技术</a>  *** linux 云原生<br><a href="https://crossoverjie.top/">crossoverjie</a><br><a href="http://abloz.com/">瀚海星空</a><br><a href="https://blog.csdn.net/yinwenjie/article/list/1">银文杰 &lt;&lt;高性能服务系统构建与实战&gt;&gt;</a></p>
<h5><span id="停更">停更</span><a href="#停更" class="header-anchor">#</a></h5><p><a href="https://www.cnblogs.com/java-zhao/">水寒  网易研发工程师</a>  2018停更<br><a href="http://www.jianshu.com/u/90ab66c248e6">占小狼 – 点评</a>  2020停更 jvm<br><a href="http://blog.csdn.net/mindfloating/article/details/51221780">Microservice 微服务的理论模型和现实路径 – 京东的人</a>  2019停更<br><a href="https://jinnianshilongnian.iteye.com/">京东 张开涛</a>  2016停更<br><a href="http://icyfenix.iteye.com/">周志明 的博客 </a>  《java虚拟机》 作者 2013停更<br><a href="http://my.oschina.net/andylucc/home">Float_Luuu  大众点评网 - 高级程序员</a>  失效<br><a href="https://www.cnblogs.com/wayfarer/default.html">张逸的博客</a> * 2018停更<br><a href="http://www.cnblogs.com/netfocus/">汤雪华，DDD&#x2F;CQRS</a> 2019停更<br><a href="http://mingxinglai.com/">赖明星  网易</a>  2016停更</p>
<h5><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h5><p><a href>花钱的年华 江南白衣</a>  基础架构<br>零度 jvm<br>朱小厮 朱忠华 kafka rabbitmq 唯品会<br>闪电侠 netty<br>老钱 redis 阅文集团<br>程序猿DD springCloud</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>宕机检测-Lease、心跳</title>
    <url>/www6vHomeHexo/2019/10/12/crashDetect/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="lease">Lease</span><a href="#lease" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/66694573-605fc380-ece7-11e9-8213-9fd6cfe8ae1c.jpg" alt="lease"></p>
</div>

<p>Kubernetes: 引入了一个新的 build-in Lease API[2]</p>
<h2><span id="心跳">心跳</span><a href="#心跳" class="header-anchor">#</a></h2><p> 模式： ping-ping模式, ping-pong模式<br> raft： 心跳选主</p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/gaowenbin/archive/2012/07/08/2581948.html">【分布式系统工程实现】如何检测一台机器是否宕机？</a>  阿里中间件  ***</li>
<li><a href="https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg">当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？</a>  曾凡松（逐灵） ***</li>
<li><a href>《面向模式的软件架构-卷4》</a>   20.15节</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>宕机</category>
      </categories>
      <tags>
        <tag>宕机</tag>
      </tags>
  </entry>
  <entry>
    <title>Serverless</title>
    <url>/www6vHomeHexo/2019/10/10/serverless/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="serverless定义">Serverless定义</span><a href="#serverless定义" class="header-anchor">#</a></h2><p>Serverless &#x3D; Faas + Baas[5]</p>
<blockquote>
<p>Faas: 相对通用的形式表示无服务计算<br>Faas例子: Cloud Functions</p>
</blockquote>
<blockquote>
<p>Baas:特定领域的、高度优化的无服务计算实现<br>Baas例子: S3(Object Storage), DynamoDB（Key-Value Database），Cloud Pub&#x2F;Sub (Messaging) </p>
</blockquote>
<div style="text-align: center; width: 70%; height: 70%"> 

<p><img src="https://user-images.githubusercontent.com/5608425/66545940-5fefed00-eb6e-11e9-85d8-3279f6db2d61.jpg" alt="serverless"><br>Serverless</p>
</div>



<h2><span id="serverless和serverful的对比">Serverless和Serverful的对比</span><a href="#serverless和serverful的对比" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/66544428-af341e80-eb6a-11e9-9a82-527cb0268300.jpg" alt="serverless-serverful"><br>serverless和serverful的对比</p>
</div>

<h5><span id="serverless">Serverless</span><a href="#serverless" class="header-anchor">#</a></h5><ol>
<li>事件驱动的</li>
<li><strong>计算与存储解耦</strong><br>状态存储在外部（Baas） </li>
<li>资源受限的</li>
<li><strong>以实际使用的资源量付费，而不是根据分配的资源数</strong></li>
<li><strong>执行代码而不需要管理资源分配</strong><br>自动提供如下服务:<br>选择实例、扩缩容、部署、容错、监控、日志、安全补丁</li>
<li><strong>自动扩缩容</strong><br>有负载时自动<strong>扩容</strong>;<br>没有负载的情况下将资源一直<strong>缩容</strong>到零;</li>
<li>多租户，安全隔离</li>
</ol>
<h2><span id="使用的场景">使用的场景</span><a href="#使用的场景" class="header-anchor">#</a></h2><p><a href="https://help.aliyun.com/document_detail/65565.html">Serverless应用场景</a> 阿里云文档 未</p>
<h5><span id="事件驱动以及响应式架构">事件驱动以及响应式架构</span><a href="#事件驱动以及响应式架构" class="header-anchor">#</a></h5><ul>
<li>IoT物联网,低频请求;      </li>
<li>视频转码;</li>
<li>定制图片;</li>
<li>抽取数据  ETL 大数据;</li>
<li>人脸识别;</li>
<li><strong>固定时间触发</strong>计算资源利用低的业务;<br>夜间或者服务空闲时间来处理繁忙时候的交易数据(大数据)<br>运行批量数据，来生成数据报表(大数据)</li>
<li>定制事件<br>用户注册时发邮件验证邮箱地址</li>
</ul>
<h5><span id="流量突发场景">流量突发场景</span><a href="#流量突发场景" class="header-anchor">#</a></h5><ul>
<li>短周期内的<strong>流量峰值</strong><br>外卖企业的用餐时期负载高峰，<br>安防行业的负载高峰是夜间</li>
</ul>
<h5><span id="大数据处理">大数据处理</span><a href="#大数据处理" class="header-anchor">#</a></h5><h2><span id="产品-hosted-platform-0">产品- Hosted Platform [0]</span><a href="#产品-hosted-platform-0" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>Serverless , Function as a Service(Faas)</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>AWS Lambda</td>
<td></td>
</tr>
<tr>
<td>Google Cloud Functions</td>
<td></td>
</tr>
<tr>
<td>阿里 Function Compute 2.0</td>
<td><a href="https://yq.aliyun.com/articles/719694">函数计算 2.0 重磅发布，Serverless Computing 开启新篇章</a><br>  开发工具 fun，vscode插件</td>
</tr>
<tr>
<td>Azure Funcions</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Serverless Container[6] , Container as a Service(Caas)</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>AWS Fargate</td>
<td></td>
</tr>
<tr>
<td>Azure ACI</td>
<td></td>
</tr>
<tr>
<td>华为CCI</td>
<td></td>
</tr>
<tr>
<td>阿里 Serverless Kubernetes、本身是Kubernetes集群</td>
<td></td>
</tr>
<tr>
<td>容器服务 ACS</td>
<td><a href="https://yq.aliyun.com/articles/59483">大道至简 - 基于Docker的Serverless探索之旅</a> 产品</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>面向应用的 Serverless 服务, Paas</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://tech.antfin.com/products/SAS">Serverless 应用服务(蚂蚁金服)</a></td>
<td><a href="https://yq.aliyun.com/articles/713292">Serverless 落地挑战与蚂蚁金服实践</a> 产品 <br>SAS , 兼容标准Knative， toB不toC</td>
</tr>
<tr>
<td>Serverless 应用引擎（阿里云） <br> SAE是DevOps最佳实践</td>
<td><a href="https://developer.aliyun.com/article/782846?utm_content=g_1000253942">Serverless 时代 DevOps 的最佳打开方式</a>  未<br>   Serverless 应用引擎（SAE）是阿里云 Serverless 产品矩阵中提供的 DevOps 最佳实践。<br>   SAE 是一款面向应用 Serverless PaaS 平台，支持 Spring Cloud、Dubbo、HSF 等主流的应用开发框架。</td>
</tr>
<tr>
<td>EDAS Serverless , Rpc产品</td>
<td><a href="https://yq.aliyun.com/articles/683675">0基础快速入门运维-EDAS Serverless(FAAS) 产品评测</a></td>
</tr>
</tbody></table>
<h2><span id="产品-开源产品-0">产品- 开源产品 [0]</span><a href="#产品-开源产品-0" class="header-anchor">#</a></h2><ul>
<li>Installable Platform<ul>
<li>Apache OpenWhisk [IBM]</li>
<li>Knative[Faas + Caas] ***   </li>
<li>kubeless</li>
<li>Fission</li>
<li>OpenFaaS</li>
<li>OpenFunction[Kubesphere]</li>
</ul>
</li>
<li>Framework<ul>
<li>dapr ***</li>
<li>EventMesh</li>
<li>serverless</li>
<li>AWS SAM</li>
<li>Spring Cloud Function</li>
</ul>
</li>
<li>Tool<ul>
<li>Serverless Devs [99]<br>[Ali Serverless工具链项目-多云,所见即所得]</li>
</ul>
</li>
</ul>
<h2><span id="产品-其他">产品- 其他</span><a href="#产品-其他" class="header-anchor">#</a></h2><p>Nest 美团[17]</p>
<div style="text-align: center; width: 80%; height: 80%">

<p><img src="https://user-images.githubusercontent.com/5608425/66989009-1d8e5900-f0f6-11e9-9902-2efb75d27da7.png" alt="caas-faas"></p>
</div>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li>0.<a href="https://landscape.cncf.io/serverless">CNCF Cloud Native Interactive Landscape</a></li>
<li>99.<a href="https://developer.aliyun.com/live/245624">Serverless Devs-让你像使用手机一样玩转 Serverless</a>   v </li>
<li>5.<a href="https://mp.weixin.qq.com/s/7qJUzf8xrGihPPLsvwPEig">无服务计算的未来和挑战: A Berkeley View on Serverless Computing</a>  ***</li>
<li>6.<a href="https://yq.aliyun.com/articles/574222">当我们在聊Serverless时你应该知道这些</a>  阿里 竹涧， 场景 产品 架构</li>
<li>8.<a href="https://yq.aliyun.com/articles/717318">Serverless 与容器决战在即？有了弹性伸缩就不一样了</a>  阿里 莫源 未   </li>
<li>16.<a href="https://yq.aliyun.com/articles/78172">15+文章详细讲述Serverless：开启函数计算时代！（含PDF下载）</a></li>
<li>17.<a href="https://tech.meituan.com/2021/04/21/nest-serverless.html">美团Serverless平台Nest的探索与实践</a></li>
</ul>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serverless</category>
      </categories>
      <tags>
        <tag>serverless</tag>
      </tags>
  </entry>
  <entry>
    <title>存储</title>
    <url>/www6vHomeHexo/2019/10/08/storage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">块存储</th>
<th align="center">文件存储</th>
<th align="center">对象存储</th>
</tr>
</thead>
<tbody><tr>
<td align="center">概念</td>
<td align="center">本地存储</td>
<td align="center">有目录树结构</td>
<td align="center">将数据和元数据当做一个对象</td>
</tr>
<tr>
<td align="center">分布式</td>
<td align="center">×</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">文件大小</td>
<td align="center">大小都可以</td>
<td align="center">适合大文件</td>
<td align="center">适合各种大小</td>
</tr>
<tr>
<td align="center">接口</td>
<td align="center">Diver，kernel module</td>
<td align="center">POSIX<br> NFS&#x2F;SMB协议</td>
<td align="center">语义简单，基于HTTP协议访问，Restful API</td>
</tr>
<tr>
<td align="center">典型技术</td>
<td align="center">直连，FC SAN， IP SAN(iSCSI)</td>
<td align="center">HDFS、GFS<br>  NAS存储: 以太网承载的NFS <br></td>
<td align="center">Swift， Amazion S3， 阿里OSS</td>
</tr>
<tr>
<td align="center">场景</td>
<td align="center">专用领域<br> 高性能，低延迟<br> 云主机本地存储</td>
<td align="center">通用 <br> 高并发，共享<br> HPC高性能计算<br> 共享存储</td>
<td align="center">海量，并发 <br>读多写少 <br> 音频，视频，图片。 <br> 网站资源动静分离; 网盘</td>
</tr>
<tr>
<td align="center">数据结构（类比）</td>
<td align="center">数组</td>
<td align="center">二叉树</td>
<td align="center">hash表</td>
</tr>
</tbody></table>
<h2><span id="存储设备5">存储设备[5]</span><a href="#存储设备5" class="header-anchor">#</a></h2><ul>
<li>DAS<br>IDE, SATA, SCSI, SAS, USB</li>
<li>NAS<br>NFS, CIFS</li>
<li>SAN<br>SCSI, FC SAN, iSCSI(基于因特网及SCSI-3协议下的存储技术)</li>
</ul>
<h2><span id="块存储">块存储</span><a href="#块存储" class="header-anchor">#</a></h2><p>必须连接到相关的虚拟机，才能访问它里面的数据。</p>
<h2><span id="对象存储">对象存储</span><a href="#对象存储" class="header-anchor">#</a></h2><ul>
<li>通过高层的 API 和 SDK 来和它进行交互</li>
<li>更接近一个键值（Key-Value）形式的存储服务<br>（url，对象的二进制）</li>
<li>存储分层<img src="/www6vHomeHexo/2019/10/08/storage/object-storage.JPG" class title="存储分层"></li>
<li>对象的版本管理（Versioning）</li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/hukey/p/8323853.html">块存储、文件存储、对象存储意义及差异</a></li>
<li><a href="https://www.zhihu.com/question/21536660">块存储、文件存储、对象存储这三者的本质差别是什么？</a></li>
<li><a href="https://help.aliyun.com/video_detail/71173.html?spm=5176.13394938.0.0.4f436b24YVoclI">【云吞铺子】阿里云三种存储产品该怎么选择</a></li>
<li><a href>深入浅出云计算 - 10 | 对象存储：看似简单的存储服务都有哪些玄机？</a> 何恺铎</li>
<li><a href="https://www.bilibili.com/video/BV17p4y1a7Em?p=2&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">马哥教育2021-Ceph分布式存储系统快速入门【涨薪30%+】</a> P2</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>存储</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令行工具</title>
    <url>/www6vHomeHexo/2019/10/01/linuxTool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-进程与网络">一. 进程与网络</span><a href="#一-进程与网络" class="header-anchor">#</a></h2><ol>
<li>知道占用的端口，找进程号</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@iZuf6jcjzd6wfh2fhp6315Z:~# netstat -tunlp | grep 18090</span><br><span class="line">tcp        0      0 0.0.0.0:18090           0.0.0.0:*               LISTEN      12921/java</span><br><span class="line"></span><br><span class="line">root@iZuf6jcjzd6wfh2fhp6315Z:~# ps -ef | grep 12921</span><br><span class="line">root     12921 24298  0  2018 ?        01:06:37 java -jar services/onlinetraining-service.jar</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>知道进程名，找占用的端口号 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@iZuf6jcjzd6wfh2fhp6315Z:~# ps aux | grep java</span><br><span class="line">root     12921  0.0  3.7 21046096 2305228 ?    Sl    2018  66:37 java -jar services/onlinetraining-service.jar</span><br><span class="line"></span><br><span class="line">root@iZuf6jcjzd6wfh2fhp6315Z:~# lsof -p 12921 -nP | grep TCP</span><br><span class="line">... ...</span><br><span class="line">java    12921 root   30u     IPv4          286953901      0t0       TCP *:18090 (LISTEN)</span><br><span class="line">... ...</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="二-其他">二. 其他</span><a href="#二-其他" class="header-anchor">#</a></h2><ol>
<li><p>systemctl： 负责控制systemd系统和服务管理器 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#检查单元或服务是否正在运行</span><br><span class="line">systemctl status firewalld.service</span><br><span class="line">#列出所有服务</span><br><span class="line">systemctl list-unit-files --type=service</span><br></pre></td></tr></table></figure>
</li>
<li><p>进程有访问了哪些文件句柄</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lsof -p 11825</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看连接你服务器 top10 用户端的 IP 地址：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -nat | awk &#x27;&#123;print $5&#125;&#x27; | awk -F &#x27;:&#x27; &#x27;&#123;print $1&#125;&#x27; | sort | uniq -c | sort -rn | head -n 10</span><br></pre></td></tr></table></figure>
</li>
<li><p>alias</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alias nis=&quot;npm install --save &quot;</span><br><span class="line">alias install=&#x27;sudo apt get install&#x27;</span><br><span class="line">alias update=&#x27;sudo apt-get update; sudo apt-get upgrade&#x27;</span><br><span class="line">alias ..=&quot;cd ..&quot;</span><br><span class="line">alias ...=&quot;cd ..; cd ..&quot;</span><br><span class="line">alias sock5=&#x27;ssh -D 8080 -q -C -N -f user@your.server&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>rtt  网络延迟</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-25-3-55 ~]# hping3 -c 3 -S -p 80 baidu.com</span><br><span class="line">HPING baidu.com (eth0 39.156.69.79): S set, 40 headers + 0 data bytes</span><br><span class="line">len=40 ip=39.156.69.79 ttl=43 id=22691 sport=80 flags=SA seq=0 win=8192 rtt=31.9 ms</span><br><span class="line">len=40 ip=39.156.69.79 ttl=43 id=19208 sport=80 flags=SA seq=1 win=8192 rtt=33.0 ms</span><br><span class="line">len=40 ip=39.156.69.79 ttl=43 id=2359 sport=80 flags=SA seq=2 win=8192 rtt=36.0 ms</span><br><span class="line"></span><br><span class="line">[root@10-25-3-55 ~]# traceroute  --tcp -p 80 -n  baidu.com</span><br><span class="line">traceroute to baidu.com (220.181.38.148), 30 hops max, 60 byte packets</span><br></pre></td></tr></table></figure>
</li>
<li><p>xxx to root</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Ubuntu</span><br><span class="line">sudo su root</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看inode详情</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@172-16-244-142]$df -i</span><br><span class="line">Filesystem       Inodes  IUsed    IFree IUse% Mounted on</span><br><span class="line">devtmpfs         485574    346   485228    1% /dev</span><br><span class="line">tmpfs            488560      1   488559    1% /dev/shm</span><br><span class="line">tmpfs            488560    500   488060    1% /run</span><br><span class="line">tmpfs            488560     17   488543    1% /sys/fs/cgroup</span><br><span class="line">/dev/vda1      10485248 195694 10289554    2% /</span><br><span class="line">tmpfs            488560      1   488559    1% /run/user/0</span><br></pre></td></tr></table></figure>
</li>
<li><p>杀xxx进程 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 杀tomat</span><br><span class="line">ps -ef | grep tomcat| awk &#x27;&#123;print &quot;kill -9 &quot; $2&#125;&#x27; | sh</span><br><span class="line">ps -ef | grep tomcat|grep -v grep | awk &#x27;&#123;print &quot;kill -9 &quot; $2&#125;&#x27; | sh</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="三-oh-my-zsh安装">三. oh-my-zsh安装</span><a href="#三-oh-my-zsh安装" class="header-anchor">#</a></h2><p>参考 zsh</p>
<h2><span id="四-tmux使用和安装">四. tmux使用和安装</span><a href="#四-tmux使用和安装" class="header-anchor">#</a></h2><p>参考 tmux</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://linux265.com/news/3385.html">systemctl 命令详解及使用教程</a>  systemd Systemctl</li>
<li><a href="https://coolshell.cn/articles/19219.html">打造高效的工作环境 – Shell 篇</a>  good</li>
<li><a href="https://wangchujiang.com/linux-command/">Linux 命令搜索</a>  good</li>
<li><a href="https://help.aliyun.com/knowledge_detail/42531.html?spm=5176.11065259.1996646101.searchclickresult.4f0c88c9FG1Wbp#h2-u5904u7406u529Eu6CD53">Linux实例磁盘空间满和inode满的问题排查方法</a>  inode</li>
</ol>
<h3><span id="zsh">zsh</span><a href="#zsh" class="header-anchor">#</a></h3><ol>
<li><a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Installing-ZSH">Installing ZSH</a></li>
<li><a href="https://imroc.io/posts/geek/oh-my-zsh/">极客工具之 oh-my-zsh</a></li>
</ol>
<h3><span id="tmux">tmux</span><a href="#tmux" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.cnblogs.com/liuguanglin/p/9290345.html">tmux基本操作</a> 快捷键</li>
<li><a href="https://mp.weixin.qq.com/s/nuBUKT8WFtvCxURE7m1E3w">linux多Session神器Tmux安装使用场景和常用命令</a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>命令行工具</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Knative</title>
    <url>/www6vHomeHexo/2019/09/27/knative/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="knative服务模块serving">Knative服务模块（Serving）</span><a href="#knative服务模块serving" class="header-anchor">#</a></h2><p>Kubernetes-based, scale-to-zero, request-driven compute </p>
<img src="/www6vHomeHexo/2019/09/27/knative/knative-service.png" class title="knative-service">

<h2><span id="knative事件模块eventing">Knative事件模块（Eventing）</span><a href="#knative事件模块eventing" class="header-anchor">#</a></h2><h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://knative.dev/docs/serving/">Knative Serving</a></p>
<h5><span id="ibm-knative课程">ibm Knative课程</span><a href="#ibm-knative课程" class="header-anchor">#</a></h5><ul>
<li><a href="https://developer.ibm.com/cn/os-academy-knative/">Knative课程主页</a></li>
<li><a href="https://live.bilibili.com/21285133">IBM开源微讲堂 - Knative</a>  Knative系列视频课程</li>
<li><a href="https://github.com/dWChina/ibm-opentech-ma/blob/master/knative/knative01.pdf">Knative培训课程 - 第一课讲义</a> ibm</li>
</ul>
<h5><span id="阿里牛秋霖冬岛-knative教程">阿里牛秋霖（冬岛） Knative教程</span><a href="#阿里牛秋霖冬岛-knative教程" class="header-anchor">#</a></h5><ul>
<li><a href="https://yq.aliyun.com/articles/719274">Knative 系列文章目录</a>  冬岛</li>
<li><a href="https://yq.aliyun.com/articles/712252?spm=a2c4e.11153940.0.0.22b048fe1pIjdr">Knative 初体验：Serving Hello World</a>  做过</li>
<li><a href="https://yq.aliyun.com/articles/705438?spm=a2c4e.11153940.0.0.22b048fe1pIjdr">Knative 初体验：Eventing Hello World</a> 做过</li>
</ul>
<h5><span id="knative安装">Knative安装</span><a href="#knative安装" class="header-anchor">#</a></h5><ol>
<li><a href="https://knative.dev/docs/install/knative-with-any-k8s/?spm=a2c4e.10696291.0.0.7d8719a434fv2C">Install on a Kubernetes cluster</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serverless</category>
        <category>knative</category>
      </categories>
      <tags>
        <tag>knative</tag>
      </tags>
  </entry>
  <entry>
    <title>混沌工程</title>
    <url>/www6vHomeHexo/2019/09/24/chaosEngineering/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="混沌工程实践12">混沌工程实践[1,2]</span><a href="#混沌工程实践12" class="header-anchor">#</a></h2><h5><span id="原则">原则</span><a href="#原则" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/09/24/chaosEngineering/priciple.jpg" class title="原则">

<h5><span id="成熟度">成熟度</span><a href="#成熟度" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/09/24/chaosEngineering/mature.jpg" class title="成熟度"> 

<h5><span id="混沌工程实施步骤">混沌工程实施步骤</span><a href="#混沌工程实施步骤" class="header-anchor">#</a></h5><ul>
<li>制订混沌实验计划</li>
<li>定义系统稳态指标</li>
<li>做出系统容错行为假设</li>
<li>执行混沌实验</li>
<li>检查系统稳态指标</li>
<li>记录&amp;恢复混沌实验</li>
<li>修复发现的问题</li>
<li>自动化持续进行验证</li>
</ul>
<h2><span id="混沌工程产品">混沌工程产品</span><a href="#混沌工程产品" class="header-anchor">#</a></h2><ul>
<li>相关产品ChaosBlade<br>故障注入</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/xxscj/article/details/96840307">分布式服务架构下的混沌工程实践</a> 阿里 穹谷 ***</li>
<li><a href="https://github.com/StabilityMan/StabilityGuide/blob/master/docs/prevention/resilience/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E8%B7%B5.md">混沌工程介绍与实践</a>  阿里 穹谷</li>
<li><a href="https://www.infoq.cn/article/jjp0c2bR4*Ulld0wb88r">Netflix 混沌工程手册 Part 1：混沌工程简介</a></li>
<li><a href="https://www.infoq.cn/article/AsN34J2T9QDXB0s-t9JN">Netflix 混沌工程手册 Part 2：混沌工程原则</a></li>
<li><a href="https://www.infoq.cn/article/M3EktXxYGRYYm*t5vKga">Netflix 混沌工程手册 Part 3：实践方法</a></li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>混沌工程</category>
      </categories>
      <tags>
        <tag>混沌工程</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot</title>
    <url>/www6vHomeHexo/2019/09/20/springboot/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#core">Core</a><ul>
<li><a href="#spring-boot%E5%AE%9A%E4%B9%89">Spring Boot定义</a></li>
<li><a href="#features-%E5%AE%98%E6%96%B9">Features (官方)</a></li>
<li><a href="#%E7%89%B9%E6%80%A7">特性</a></li>
</ul>
</li>
<li><a href="#auto-configuration">Auto Configuration</a><ul>
<li><a href="#%E5%BA%95%E5%B1%82%E8%A3%85%E9%85%8D%E6%8A%80%E6%9C%AF-3">底层装配技术 [3]</a></li>
<li><a href="#%E5%A4%96%E9%83%A8%E5%8C%96%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F">外部化配置加载顺序</a></li>
</ul>
</li>
<li><a href="#starter-dependency">Starter Dependency</a></li>
<li><a href="#production-ready">production-ready</a><ul>
<li><a href="#actuator">Actuator</a></li>
<li><a href="#actuator-endpoint">Actuator Endpoint</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%B5%8C%E7%9A%84web%E5%AE%B9%E5%99%A8">内嵌的Web容器</a><ul>
<li><a href="#%E5%8F%AF%E9%80%89%E5%AE%B9%E5%99%A8%E5%88%97%E8%A1%A8">可选容器列表</a></li>
<li><a href="#%E7%AB%AF%E5%8F%A3">端口</a></li>
<li><a href="#%E5%8E%8B%E7%BC%A9">压缩</a></li>
<li><a href="#tomcat%E7%89%B9%E6%80%A7%E9%85%8D%E7%BD%AE">Tomcat特性配置</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="core">Core</span><a href="#core" class="header-anchor">#</a></h1><h3><span id="spring-boot定义">Spring Boot定义</span><a href="#spring-boot定义" class="header-anchor">#</a></h3><p><strong>Spring Boot</strong> is designed to get you up and running as quickly as possible,<br>with <strong>minimal</strong> upfront <strong>configuration</strong> of Spring.<br>Spring Boot takes an opinionated view of building production-ready applications.</p>
<h3><span id="features-官方">Features (官方)</span><a href="#features-官方" class="header-anchor">#</a></h3><ul>
<li>Create stand-alone Spring applications</li>
<li><strong>Embed</strong> Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>
<li>Provide opinionated <strong>‘starter’</strong> dependencies to simplify your build configuration</li>
<li><strong>Automatically configure</strong> Spring and 3rd party libraries whenever possible</li>
<li>Provide <strong>production-ready</strong> features such as metrics, health checks, and externalized configuration</li>
<li>Absolutely no code generation and no requirement for XML configuration</li>
</ul>
<h3><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h3><ul>
<li><p>自动配置   Auto Configuration<br>为Spring及第三方库提供自动配置;<br>简化了项目的构建配置;</p>
<ul>
<li>无需生成代码或进行xml配置;<br> 约定优于配置(Convention Over Configuration) CoC</li>
</ul>
</li>
<li><p>Starter Dependency</p>
</li>
<li><p>Springboot CLI</p>
</li>
<li><p>内嵌的服务器<br>方便地创建可独立运行的Spring应用程序;<br>直接内嵌的Tomcat， Jetty或者Undertow;</p>
</li>
<li><p>生产级<br>提供生产级特性;<br>Actuator（Runtime）</p>
</li>
</ul>
<h1><span id="auto-configuration">Auto Configuration</span><a href="#auto-configuration" class="header-anchor">#</a></h1><h3><span id="底层装配技术-3">底层装配技术  [3]</span><a href="#底层装配技术-3" class="header-anchor">#</a></h3><ul>
<li><p>Spring 模式注解装配</p>
</li>
<li><p>Spring @Enable 模块装配</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 组件</span></span><br><span class="line">   <span class="meta">@EnableXXX</span></span><br><span class="line">        <span class="meta">@Importer</span> <span class="meta">@ImportXXXSelector</span></span><br><span class="line">   <span class="meta">@Conditional</span></span><br><span class="line">   <span class="meta">@ConditionalOnClass</span></span><br><span class="line">   <span class="meta">@ConditionalOnBean</span></span><br><span class="line">   ... </span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 开启自动配置</span></span><br><span class="line"><span class="meta">@EnableAutoConfiguration</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Spring 条件装配装配</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实现原理 - 有条件的加载机制</span></span><br><span class="line"><span class="meta">@ConditionalOnClass</span></span><br><span class="line"><span class="meta">@ConditionalOnBean</span></span><br><span class="line"><span class="meta">@ConditionalOnMissingBean</span></span><br><span class="line"><span class="meta">@ConditionalOnProperty</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>Spring 工厂加载机制</p>
<ul>
<li>实现类： SpringFactoriesLoader</li>
<li>配置资源： META-INF&#x2F;spring.factories</li>
</ul>
</li>
</ul>
<h3><span id="外部化配置加载顺序">外部化配置加载顺序</span><a href="#外部化配置加载顺序" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">命令行参数（--server.port=9000）</span><br><span class="line">...</span><br><span class="line">System.getProperties()</span><br><span class="line">操作系统环境变量</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jar包外的application-&#123;profile&#125;.properties 或 .yml</span><br><span class="line">jar包内的application-&#123;profile&#125;.properties 或 .yml</span><br><span class="line">jar包外的application.properties或 .yml</span><br><span class="line">jar包内的application.properties或 .yml</span><br></pre></td></tr></table></figure>

<h1><span id="starter-dependency">Starter Dependency</span><a href="#starter-dependency" class="header-anchor">#</a></h1><ul>
<li>直接面向功能</li>
<li>官方Starters spring-boot-starter-*</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 使用方 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 统一管理依赖 --&gt;</span><span class="comment">&lt;!-- spring cloud的依赖 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 定义方 --&gt;</span></span><br><span class="line">Bill of Materials - bom</span><br><span class="line">BOM本质上是一个普通的POM文件</span><br></pre></td></tr></table></figure>

<ul>
<li>扩展自定义starter<br>…</li>
</ul>
<h1><span id="production-ready">production-ready</span><a href="#production-ready" class="header-anchor">#</a></h1><h3><span id="actuator">Actuator</span><a href="#actuator" class="header-anchor">#</a></h3><ul>
<li>目的： 监控并管理应用程序</li>
<li>访问方式： HTTP, JMX</li>
<li>依赖： spring-boot-starter-actuator</li>
</ul>
<h3><span id="actuator-endpoint">Actuator Endpoint</span><a href="#actuator-endpoint" class="header-anchor">#</a></h3><ul>
<li>http访问<br>&#x2F;actuator&#x2F;<id></id></li>
<li>端口与路径</li>
<li>management.server.address&#x3D;</li>
<li>management.server.port&#x3D;</li>
</ul>
<h1><span id="内嵌的web容器">内嵌的Web容器</span><a href="#内嵌的web容器" class="header-anchor">#</a></h1><h3><span id="可选容器列表">可选容器列表</span><a href="#可选容器列表" class="header-anchor">#</a></h3><ul>
<li>spring-boot-starter-tomcat</li>
<li>spring-boot-starter-jetty</li>
<li>spring-boot-starter-undertow</li>
<li>spring-boot-starter-reactor-netty</li>
</ul>
<h3><span id="端口">端口</span><a href="#端口" class="header-anchor">#</a></h3><ul>
<li>server.port</li>
<li>server.address</li>
</ul>
<h3><span id="压缩">压缩</span><a href="#压缩" class="header-anchor">#</a></h3><h3><span id="tomcat特性配置">Tomcat特性配置</span><a href="#tomcat特性配置" class="header-anchor">#</a></h3><ul>
<li>server.tomcat.max-connections&#x3D;10000</li>
<li>server.tomcat.max-http-post-size</li>
<li>server.tomcat.max-threads</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《玩转Spring全家桶》 67, 68, 71, 73,  75, 79  丁雪峰 V</li>
<li>《黑马程序员SpringBoot教程，6小时快速入门Java微服务架构Spring Boot》 V</li>
<li>《mksz252 - Spring Boot 2.0深度实践之核心技术篇》 第2章 走向自动装配 V *** </li>
<li><a href="https://www.cnblogs.com/crazymakercircle/p/14365487.html">SpringBoot面试题 (史上最全、持续更新、吐血推荐) </a>  尼恩  未</li>
<li><a href="https://www.cnblogs.com/crazymakercircle/p/14465630.html">spring + spring mvc + tomcat 面试题（史上最全）</a> 尼恩 未</li>
<li><a href="https://www.cnblogs.com/crazymakercircle/p/13895735.html">SpringBoot 基础知识 核心知识 【收藏版】</a>  尼恩 未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>spring</category>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据存储</title>
    <url>/www6vHomeHexo/2019/09/15/bigDataStorage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<table>
<thead>
<tr>
<th align="center">分类</th>
<th align="center">存<br>储<br>成<br>本</th>
<th align="center">数<br>据<br>规<br>模</th>
<th align="center">查询性能</th>
</tr>
</thead>
<tbody><tr>
<td align="center">关系数据库</td>
<td align="center">高</td>
<td align="center">中</td>
<td align="center">高，支持SQL查询语言，关联查询和索引加速，对复杂条件过滤查询和检索支持较弱。</td>
</tr>
<tr>
<td align="center">高速缓存</td>
<td align="center">极高</td>
<td align="center">低</td>
<td align="center">极高，满足高速Key-Value形式结果数据查询，或者是高速的内存数据交换通道。</td>
</tr>
<tr>
<td align="center">搜索引擎</td>
<td align="center">高</td>
<td align="center">高</td>
<td align="center">高，对复杂条件过滤查询和检索支持较好，支持数据相关性排序，也支持轻量级数据分析。</td>
</tr>
<tr>
<td align="center">非结构化</td>
<td align="center">低</td>
<td align="center">高</td>
<td align="center">面向吞吐优化，为在线查询和离线计算都提供高吞吐的数据读取，提供极为出色的高吞吐数据写入能力。</td>
</tr>
<tr>
<td align="center">结构化</td>
<td align="center">低</td>
<td align="center">高</td>
<td align="center">首先满足数据高吞吐写入以及大规模存储，数据缓存和索引技术提供高并发低延迟的数据访问，面向离线计算也提供高吞吐的数据扫描。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">分类</th>
<th align="center">+数据访问特征</th>
<th align="center">常见数据类型</th>
<th align="center">典型产品</th>
<th align="center">数据结构</th>
</tr>
</thead>
<tbody><tr>
<td align="center">关系数据库</td>
<td align="center">强一致事务型访问，关联查询</td>
<td align="center">交易、账单、应用元数据等关系数据</td>
<td align="center">MySQL</td>
<td align="center"><a href="../../../../2019/09/10/mysql/">+面向行 <br> 索引 B+tree</a></td>
</tr>
<tr>
<td align="center">高速缓存</td>
<td align="center">低延迟Key-Value随机查询</td>
<td align="center">复杂结果集数据，或者是需要通过内存高速交换的数据。</td>
<td align="center">Redis</td>
<td align="center"><a href="../../../../2016/11/12/redis/">Key-Value</a></td>
</tr>
<tr>
<td align="center">搜索引擎</td>
<td align="center">多字段联合条件过滤，全文检索</td>
<td align="center">面向搜索查询的数据</td>
<td align="center">Elasticsearch</td>
<td align="center"><a href="../../../../2019/08/02/elasticsearch/">逆向索引</a></td>
</tr>
<tr>
<td align="center">非结构化</td>
<td align="center">读取单个数据文件，或者是大批量扫描文件集</td>
<td align="center">图片和视频数据，数据库归档数据</td>
<td align="center">OSS，HDFS</td>
<td align="center"><a href="../../../../2019/10/08/storage/">块、对象、文件</a></td>
</tr>
<tr>
<td align="center">结构化</td>
<td align="center">单行随机访问，或者是大批量范围扫描</td>
<td align="center">1.作为关系数据库的补充，存储历史归档数据。<br>2.非关系模型数据，例如时序、日志等。</td>
<td align="center">HBase，Cassandra，<br>Tablestore</td>
<td align="center">+ <a href="../../../../2018/07/19/NoSQL/">面向列<br>Ordered Key-Value(列族)<br> 索引 LSM-tree</a>[3][4]</td>
</tr>
</tbody></table>
<div style="text-align: center;">云计算中的大数据产品</div>

<table>
<thead>
<tr>
<th align="center">服务类别</th>
<th align="center">AWS</th>
<th align="center">aliyun阿里云</th>
<th align="center">Azure</th>
</tr>
</thead>
<tbody><tr>
<td align="center">大数据计算</td>
<td align="center">EMR</td>
<td align="center">E-MapReduce,<br>MaxCompute</td>
<td align="center">HDInsight</td>
</tr>
<tr>
<td align="center">大数据存储</td>
<td align="center">S3（EMRFS）</td>
<td align="center">OSS，JindoFS</td>
<td align="center">Blob Storage，<br>Data Lake Storage Gen2</td>
</tr>
<tr>
<td align="center">分析型数据库</td>
<td align="center">Redshift</td>
<td align="center">AnalyticDB for MySQL&#x2F;PostgreSQL</td>
<td align="center">SQL Data Warehouse(Synapse Analytics)</td>
</tr>
<tr>
<td align="center">无服务器查询 serverless</td>
<td align="center">Athena</td>
<td align="center">Data Lake Analytics</td>
<td align="center">Azure Data Lake Analytics</td>
</tr>
</tbody></table>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://yq.aliyun.com/articles/715254?spm=a2c4e.11155435.0.0.2eba5b6e3DhL3A">数据中台之结构化大数据存储设计</a></li>
<li><a href="../../../../2018/07/19/NoSQL/">NoSQL总结</a> self</li>
<li><a href="https://www.cnblogs.com/siegfang/archive/2013/01/12/lsm-tree.html">日志结构的合并树 The Log-Structured Merge-Tree</a></li>
<li><a href="https://kernelmaker.github.io/lsm-tree">【Paper笔记】The Log structured Merge-Tree（LSM-Tree）</a></li>
<li><a href>深入浅出云计算 - 13 | 云上大数据：云计算遇上大数据，为什么堪称天作之合？</a> 何恺铎</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux zero copy-零拷贝</title>
    <url>/www6vHomeHexo/2019/09/14/zeroCopy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D">零拷贝</a><ul>
<li><a href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D-1">零拷贝</a></li>
<li><a href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D%E7%B1%BB%E5%9E%8B">零拷贝类型</a></li>
<li><a href="#mmap">mmap</a></li>
<li><a href="#sendfile-13">sendfile [1][3]</a></li>
<li><a href="#read-write%E4%BC%A0%E7%BB%9F%E4%BC%A0%E8%BE%93%E6%96%B9%E5%BC%8F-13">read + write(传统传输方式)  [1][3]</a></li>
</ul>
<ul>
<li><a href="#%E6%80%BB%E7%BB%93-7">总结 [7]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="零拷贝">零拷贝</span><a href="#零拷贝" class="header-anchor">#</a></h1><h3><span id="零拷贝">零拷贝</span><a href="#零拷贝" class="header-anchor">#</a></h3><p>  就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术</p>
<h3><span id="零拷贝类型">零拷贝类型</span><a href="#零拷贝类型" class="header-anchor">#</a></h3><ul>
<li>mmap()<br>&lt;&lt;深入理解计算机系统&gt;&gt;mmap定义为：Linux通过将一个虚拟内存区域与一个磁盘上的对象(object)关联起来，以初始化这个虚拟内存区域的内容，这个过程称为内存映射(memory mapping)。</li>
<li>sendfile() -&gt; Java transferTo();</li>
</ul>
<h3><span id="mmap">mmap</span><a href="#mmap" class="header-anchor">#</a></h3><ul>
<li><p>mmap 内存映射 [4]</p>
<img src="/www6vHomeHexo/2019/09/14/zeroCopy/mmap1.jpg" class>
</li>
<li><p>mmap 内存映射 [5]</p>
<img src="/www6vHomeHexo/2019/09/14/zeroCopy/mmap2.jpg" class>
</li>
<li><p>利用 mmap 代替 read  &#x3D;&#x3D;  mmap + write [1][3]</p>
<img src="/www6vHomeHexo/2019/09/14/zeroCopy/mmap.jpg" class></li>
</ul>
<h3><span id="sendfile-13">sendfile [1][3]</span><a href="#sendfile-13" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/14/zeroCopy/sendfile.jpg" class>

<h3><span id="read-write传统传输方式-13">read + write(传统传输方式)  [1][3]</span><a href="#read-write传统传输方式-13" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/14/zeroCopy/read-write.jpg" class>

<h2><span id="总结-7">总结 [7]</span><a href="#总结-7" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th></th>
<th>系统调用</th>
<th>上下文切换</th>
<th>CPU拷贝</th>
<th>DMA拷贝</th>
<th>硬件依赖</th>
<th>支持任意类型输入&#x2F;输出描述符</th>
</tr>
</thead>
<tbody><tr>
<td>传统方法</td>
<td>read + write</td>
<td>4</td>
<td>2</td>
<td>2</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>内存映射</td>
<td>mmap + write</td>
<td>4</td>
<td>1</td>
<td>2</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>sendfile</td>
<td>sendfile</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>否</td>
<td>否</td>
</tr>
<tr>
<td>sendfile(scatter&#x2F;gather copy)</td>
<td>sendfile</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>splice</td>
<td>splice</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>否</td>
<td>是</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>原理</p>
<ol>
<li><a href="https://www.cnblogs.com/AaronCui/p/10528046.html">零拷贝-zero copy</a> *** </li>
<li><a href="https://developer.ibm.com/articles/j-zerocopy/">Efficient data transfer through zero copy</a><br>sendfile减少了上下文切换次数，transferTo()  ***</li>
<li><a href="https://www.linuxjournal.com/article/6345">Zero Copy I: User-Mode Perspective</a>   ***</li>
<li><a href="https://cloud.tencent.com/developer/article/1145377">理解mmap</a></li>
<li><a href="https://www.jianshu.com/p/eece39beee20">[原创] 深入剖析mmap-从三个关键问题说起</a></li>
<li><a href="/www6vHomeHexo/2019/08/23/linuxMemory/" title="Linux内存管理">Linux内存管理</a> self</li>
<li><a href="https://zhuanlan.zhihu.com/p/587695921">【万字长文】从Linux零拷贝深入了解Linux I&#x2F;O</a>    腾讯 ***</li>
</ol>
<hr>
<p>已失效<br>100. <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/">Linux 中的零拷贝技术，第 1 部分</a>  已失效<br>101. <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/">Linux 中的零拷贝技术，第 2 部分</a>  已失效<br>102. <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-directio/">Linux 中直接 I&#x2F;O 机制的介绍</a>     已失效  ***</p>
]]></content>
      <categories>
        <category>linux</category>
        <category>zero-copy</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Feed流 总结</title>
    <url>/www6vHomeHexo/2019/09/13/feed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#feed%E6%80%BB%E7%BB%93">Feed总结</a></li>
<li><a href="#%E6%B6%88%E6%81%AF%E5%90%8C%E6%AD%A5%E6%A8%A1%E5%9E%8B">消息同步模型</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Etimeline%E7%9A%84%E6%B6%88%E6%81%AF%E5%BA%93%E8%AE%BE%E8%AE%A1">基于Timeline的消息库设计</a></li>
<li><a href="#%E6%8E%A8%E6%8B%89%E7%BB%93%E5%90%88">推拉结合</a></li>
<li><a href="#%E8%AF%BB%E6%89%A9%E6%95%A3-vs-%E5%86%99%E6%89%A9%E6%95%A3">读扩散 vs  写扩散</a><ul>
<li><a href="#%E5%9C%BA%E6%99%AF">场景</a></li>
<li><a href="#rank">Rank</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考:</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="feed总结">Feed总结</span><a href="#feed总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/09/13/feed/feed.jpg" class title="Feed总结">


<h2><span id="消息同步模型">消息同步模型</span><a href="#消息同步模型" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/09/13/feed/sync-pattern.png" class title="消息同步模型">
<p>消息同步模型- 左:BCDEF的发件箱，右:A的收件箱</p>
<h2><span id="基于timeline的消息库设计">基于Timeline的消息库设计</span><a href="#基于timeline的消息库设计" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/09/13/feed/msgStore.png" class title="基于Timeline的消息库设计">
<p>基于Timeline的消息库设计 - 上：用于写扩散消息同步，下：全量历史消息，读扩散消息同步</p>
<h2><span id="推拉结合">推拉结合</span><a href="#推拉结合" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/09/13/feed/mix.jpg" class title="Timeline读扩散&#x2F;写扩散混合">
<p>基于用户类型的Timeline推拉结合(读扩散&#x2F;写扩散混合) - 上面是发布流程，下面是阅读流程</p>
<h2><span id="读扩散-vs-写扩散">读扩散 vs  写扩散</span><a href="#读扩散-vs-写扩散" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">拉模式(读扩散)</th>
<th align="center">推模式(写扩散)[推荐使用]</th>
</tr>
</thead>
<tbody><tr>
<td align="center">发布</td>
<td align="center">个人页Timeline（发件箱）</td>
<td align="center">粉丝的关注页（收件箱）</td>
</tr>
<tr>
<td align="center">阅读</td>
<td align="center">所有关注者的个人页Timeline</td>
<td align="center">自己的关注页Timeline</td>
</tr>
<tr>
<td align="center">网络最大开销</td>
<td align="center">用户刷新时</td>
<td align="center">发布Feed时</td>
</tr>
<tr>
<td align="center">读写放大</td>
<td align="center">放大读：读写比例到1万:1</td>
<td align="center">放大写减少读：读写比例到50:50</td>
</tr>
<tr>
<td align="center">优点</td>
<td align="center">只要写一次</td>
<td align="center">接收端消息同步逻辑会非常简单</td>
</tr>
<tr>
<td align="center">缺点、副作用</td>
<td align="center">1.读被大大的放大<br> 2.响应时间长</td>
<td align="center">消息写入会被放大， 数据会极大膨胀，</td>
</tr>
<tr>
<td align="center">针对副作用的优化-推拉结合</td>
<td align="center"></td>
<td align="center">1.大V采用拉模式，普通用户使用推模式<br>2.对活跃粉丝采用推模式，非活跃粉丝采用拉模式</td>
</tr>
</tbody></table>
<h3><span id="场景">场景</span><a href="#场景" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">场景</th>
<th align="center">Timeline</th>
</tr>
</thead>
<tbody><tr>
<td align="center">IM单聊</td>
<td align="center">三个Timeline</td>
</tr>
<tr>
<td align="center">IM群聊</td>
<td align="center">1 + N个Timeline</td>
</tr>
<tr>
<td align="center">朋友圈</td>
<td align="center">1 + N个Timeline</td>
</tr>
<tr>
<td align="center">微博</td>
<td align="center">大V发一条微博就是 1 + M个Timeline（M &lt;&lt; N，N是粉丝数）</td>
</tr>
</tbody></table>
<h3><span id="rank">Rank</span><a href="#rank" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/13/feed/rank.jpg" class>



<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s/HC9Ucdfih24jXY6lCAv40g">feed流拉取，读扩散，究竟是啥？</a></li>
<li><a href="https://yq.aliyun.com/articles/224132?spm=a2c4e.11153940.0.0.280655b2Qo0T2I">如何打造千万级Feed流系统</a></li>
<li><a href="https://yq.aliyun.com/articles/319138?spm=a2c4e.11153940.0.0.206d1844pmn4zn">TableStore Timeline：轻松构建千万级IM和Feed流系统</a></li>
<li><a href="https://yq.aliyun.com/articles/253242">现代IM系统中消息推送和存储架构的实现</a></li>
<li><a href="https://yq.aliyun.com/articles/706808">Feed流系统设计-总纲</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统设计</category>
        <category>feed</category>
      </categories>
      <tags>
        <tag>feed</tag>
      </tags>
  </entry>
  <entry>
    <title>Java里的设计模式</title>
    <url>/www6vHomeHexo/2019/09/10/designPatternsInJava/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<img src="/www6vHomeHexo/2019/09/10/designPatternsInJava/designPatternsInJdk.jpg" class title="JDK里的设计模式">



<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://coolshell.cn/articles/3320.html">JDK里的设计模式</a></li>
<li><a href="https://stackoverflow.com/questions/1673841/examples-of-gof-design-patterns-in-javas-core-libraries">Examples of GoF Design Patterns in Java’s core libraries</a></li>
<li><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/system-design/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.md">Java 设计模式</a> 未</li>
<li><a href="https://design-patterns.readthedocs.io/zh_CN/latest/index.html">图说设计模式</a> 未</li>
<li><a href="https://www.jianshu.com/p/503b15f155c0">常用开源框架中设计模式使用分析（全）</a>  阿里加多 *** 未</li>
<li><a href="https://colobu.com/2014/09/05/design-pattern-cheatsheet/">设计模式概览图</a> *** 未</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL的索引和优化</title>
    <url>/www6vHomeHexo/2019/09/10/mysqlIndex/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%B4%A2%E5%BC%95-%E7%BB%93%E6%9E%84">索引-结构</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E5%88%86%E7%B1%BB7">索引分类[7]</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84%E5%92%8C%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E-3">索引结构和存储引擎 [3]</a><br>- <a href="#%E5%A4%8D%E5%90%88%E7%B4%A2%E5%BC%95%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">复合索引的数据结构</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95-%E4%BD%BF%E7%94%A8">索引- 使用</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF">索引的使用场景</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E5%A4%B1%E6%95%88-127">索引的失效 [12][7]</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95-%E4%BC%98%E5%8C%96">索引-优化</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E7%BB%B4%E6%8A%A4">索引维护</a><br>- <a href="#%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE">自增主键</a><br>- <a href="#%E8%A6%86%E7%9B%96%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5">覆盖索引(优化手段)</a><br>- <a href="#%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8-icp-14">索引下推 ICP [14]</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95-%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%907">索引-性能分析[7]</a><br>- <a href="#%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E9%A2%91%E6%AC%A1">查看执行频次</a><br>- <a href="#%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97">慢查询日志</a><br>- <a href="#show-profiles">show profiles</a><br>- <a href="#explain">explain</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="索引-结构">索引-结构</span><a href="#索引-结构" class="header-anchor">#</a></h2><h5><span id="索引分类7">索引分类[7]</span><a href="#索引分类7" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>分类</th>
<th>含义</th>
<th>特点</th>
<th>关键字</th>
</tr>
</thead>
<tbody><tr>
<td><strong>主键索引</strong></td>
<td>针对于表中主键创建的索引</td>
<td>默认自动创建，只能有一个</td>
<td><strong>PRIMARY</strong></td>
</tr>
<tr>
<td><strong>唯一索引</strong></td>
<td>避免同一个表中某数据列中的值重复</td>
<td>可以有多个</td>
<td><strong>UNIQUE</strong></td>
</tr>
<tr>
<td>常规索引</td>
<td>快速定位特定数据</td>
<td>可以有多个</td>
<td></td>
</tr>
<tr>
<td>全文索引</td>
<td>全文索引查找的是文本中的关键词，而不是比较索引中的值</td>
<td>可以有多个</td>
<td>FULLTEXT</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>分类</th>
<th>含义</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>聚集索引(Clustered Index)</strong></td>
<td>将数据存储与索引放一块，索引结构的叶子节点保存了行数据</td>
<td><strong>必须有，而且只有一个</strong></td>
</tr>
<tr>
<td><strong>二级索引(Secondary Index)</strong></td>
<td>将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键</td>
<td>可以存在多个</td>
</tr>
</tbody></table>
<ul>
<li>聚集索引选取规则:<ul>
<li>如果存在主键，主键索引就是聚集索引</li>
<li>如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引。</li>
<li>如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索 引。</li>
</ul>
</li>
</ul>
<h5><span id="索引结构和存储引擎-3">索引结构和存储引擎 [3]</span><a href="#索引结构和存储引擎-3" class="header-anchor">#</a></h5><p>索引的数据结构： B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数</p>
<img src="/www6vHomeHexo/2019/09/10/mysqlIndex/mysql-index.jpg" class title="Innodb和MyISAM中的聚集索引和非聚集索引(二级索引)">




<table>
<thead>
<tr>
<th align="center">index</th>
<th align="center">MyISAM</th>
<th align="center">InnoDB</th>
<th align="center">Memory</th>
</tr>
</thead>
<tbody><tr>
<td align="center">B-Tree<br>（balanced 平衡的）</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">Hash</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">R-Tree <br>空间索引</td>
<td align="center">支持</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
</tr>
<tr>
<td align="center">Full-text</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">不支持</td>
</tr>
</tbody></table>
<h5><span id="复合索引的数据结构">复合索引的数据结构</span><a href="#复合索引的数据结构" class="header-anchor">#</a></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table people &#123;</span><br><span class="line">  last_name,</span><br><span class="line">  first_name,</span><br><span class="line">  dob,</span><br><span class="line">  gender,</span><br><span class="line">  key(last_name, first_name, dob)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/www6vHomeHexo/2019/09/10/mysqlIndex/compositeIndex.JPG" class title="复合索引的数据结构">


<h2><span id="索引-使用">索引- 使用</span><a href="#索引-使用" class="header-anchor">#</a></h2><h5><span id="索引的使用场景">索引的使用场景</span><a href="#索引的使用场景" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">索引的使用场景</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">匹配全值</td>
<td align="center">index (a,b,c) <br> a&#x3D;1 and b&#x3D;2 and c&#x3D;3</td>
</tr>
<tr>
<td align="center">范围查找</td>
<td align="center">index a&gt;1 and b&lt;3</td>
</tr>
<tr>
<td align="center">匹配最左前缀</td>
<td align="center">index(a，b，c)  <br> a OR a，b OR a、b、c OR a，c 会使用 <br>  b、c 不使用</td>
</tr>
<tr>
<td align="center">仅对索引列进行查询（覆盖索引）</td>
<td align="center">index  a <br> a&#x3D;1</td>
</tr>
<tr>
<td align="center">匹配列前缀</td>
<td align="center">index （a， b） <br> a like ‘WEER%’</td>
</tr>
<tr>
<td align="center">Index Condition Pushdown（ICP）</td>
<td align="center">减少回表IO</td>
</tr>
</tbody></table>
<h5><span id="索引的失效-127">索引的失效   [12][7]</span><a href="#索引的失效-127" class="header-anchor">#</a></h5><ul>
<li><p>非复合索引</p>
<table>
<thead>
<tr>
<th align="center">索引失效(不会使用index的场景)</th>
<th align="center">例子</th>
<th align="center">解释</th>
</tr>
</thead>
<tbody><tr>
<td align="center">在索引列上进行运算操作</td>
<td align="center">substring(phone,10,2)</td>
<td align="center"><strong>对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。</strong></td>
</tr>
<tr>
<td align="center">模糊查询, 头部模糊匹配</td>
<td align="center">like “%NI”</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">字符串类型字段使用时，不加引号[隐式转换]</td>
<td align="center">lastname&#x3D;1  不使用索引 <br>lastname&#x3D;’1’  使用索引</td>
<td align="center"><strong>隐式类型转换</strong>， <strong>隐式字符编码转换</strong>，等价于在索引字段上做函数操作而导致了全索引扫描。</td>
</tr>
<tr>
<td align="center">or连接条件</td>
<td align="center">index a <br>  a&#x3D;3 or c&#x3D;6 or d&#x3D;9</td>
<td align="center">如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到. 当or连接的条件，左右两侧字段都有索引时，索引才会生效。</td>
</tr>
</tbody></table>
</li>
<li><p>复合索引[7]</p>
<ul>
<li>最左前缀原则<br>如果索引关联了多列（联合索引），要遵守最左前缀法则，最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。<strong>如果跳跃某一列，索引将部分失效（后面的字段索引失效）。</strong>  </li>
<li>范围查询<br>联合索引中，<strong>出现范围查询(&gt;,&lt;)，范围查询右侧的列索引失效</strong>。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">explain select * from tb_user where profession = &#x27;软件工程&#x27; and age &gt;= 30 and status = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2><span id="索引-优化">索引-优化</span><a href="#索引-优化" class="header-anchor">#</a></h2><h5><span id="索引维护">索引维护</span><a href="#索引维护" class="header-anchor">#</a></h5><p><strong>页分裂</strong>， 性能会受影响， 整体空间利用率降低大约50%。<br>页合并，页分裂的逆过程。 </p>
<h5><span id="自增主键">自增主键</span><a href="#自增主键" class="header-anchor">#</a></h5><p>自增主键的插入数据模式，正符合了递增插入的场景。每次插入一条<br>新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。<br>而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。</p>
<p>除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如<br>字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？<br>由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的<br>叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是<br>8个字节。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。<br>这样，非主键索引占用的空间最小。</p>
<p>所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。<br><strong>[自增主键使得索引值是顺序插入的，而不是随机插入的， insert时性能更高。 顺序插入同时也减少了页分裂]</strong></p>
<h5><span id="覆盖索引优化手段">覆盖索引(优化手段)</span><a href="#覆盖索引优化手段" class="header-anchor">#</a></h5><p>如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值<br>已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，<br>索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引.</p>
<p>覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用<strong>覆盖索引</strong>是一个常用的性能<strong>优化手段</strong>.<br><strong>[不需要回表， 不需要回到聚集索引里查询]</strong></p>
<h5><span id="索引下推-icp-14">索引下推 ICP [14]</span><a href="#索引下推-icp-14" class="header-anchor">#</a></h5><h2><span id="索引-性能分析7">索引-性能分析[7]</span><a href="#索引-性能分析7" class="header-anchor">#</a></h2><h5><span id="查看执行频次">查看执行频次</span><a href="#查看执行频次" class="header-anchor">#</a></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SHOW GLOBAL STATUS LIKE &#x27;Com_______&#x27;;</span><br></pre></td></tr></table></figure>

<h5><span id="慢查询日志">慢查询日志</span><a href="#慢查询日志" class="header-anchor">#</a></h5><h5><span id="show-profiles">show profiles</span><a href="#show-profiles" class="header-anchor">#</a></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 查看每一条SQL的耗时情况:</span><br><span class="line">mysql&gt; show profiles;</span><br></pre></td></tr></table></figure>

<h5><span id="explain">explain</span><a href="#explain" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/09/10/mysqlIndex/sql-explain.jpg" class title="执行计划">

<ul>
<li><strong>type</strong>：表示连接类型，性能由好到差的连接类型为 NULL、system、const、eq_ref、ref、range、index、all</li>
<li>possible_key：可能应用在这张表上的索引，一个或多个</li>
<li>Key：<strong>实际使用的索引</strong>，如果为 NULL，则没有使用索引</li>
<li>Key_len：表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下，长度越短越好</li>
<li>rows：MySQL认为必须要执行的行数，在InnoDB引擎的表中，是一个估计值，可能并不总是准确的</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p>《深入浅出MySQL：数据库开发、优化与管理维护》 </p>
</li>
<li><p><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html">MySQL索引背后的数据结构及算法原理</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/hustcat/archive/2009/10/28/1591648.html">理解MySQL——索引与优化</a></p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://blog.csdn.net/voidccc/article/details/40077329">剖析Mysql的InnoDB索引</a>  ***</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/M1dLLuePpdM9vA3F1uJGyw">可能是全网最好的MySQL重要知识点</a>  已失效</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?p=78">黑马程序员 MySQL数据库入门到精通</a> P75-P82 P72<br><a href="https://github.com/www6v/mysql_note">mysql_note</a> 笔记1<br><a href="https://frxcat.fun/database/MySQL/MySQL_Advanced_index/">MySQL 索引</a> 笔记2</p>
</li>
<li><p>xxx </p>
</li>
<li><p><a href="https://github.com/alibaba/canal">ali canal</a></p>
</li>
<li><p>《MySQL实战45讲 - 深入浅出索引（上）》   丁奇</p>
</li>
<li><p>《MySQL实战45讲 - 深入浅出索引（下）》   丁奇</p>
</li>
<li><p>《Java性能调优实战 - 34 | MySQL调优之索引：索引的失效与优化》  刘超  还要再整理</p>
</li>
<li><p><a href="https://www.cnblogs.com/tongongV/p/10952102.html">MySQL索引（二）B+树在磁盘中的存储</a> </p>
<blockquote>
<p>B+树索引并不能直接找到行，只是找到行所在的页，通过把整页读入内存，再在内存中查找。<br>聚集索引的存储在物理上并不是连续的，每个数据页在不同的磁盘块，通过一个双向链表来进行连接。</p>
</blockquote>
</li>
<li><p><a href="https://juejin.cn/post/7005794550862053412">五分钟搞懂MySQL索引下推</a></p>
</li>
<li><p><a href="https://tech.meituan.com/2014/06/30/mysql-index.html">MySQL索引原理及慢查询优化</a>   美团 未  ***</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/h99sXP4mvVFsJw6Oh3aU5A">业界难题-“跨库分页”的四种方案</a>  58沈剑  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>资料收集</title>
    <url>/www6vHomeHexo/2019/09/10/others/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="架构案例">架构案例</span><a href="#架构案例" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/commond/archive/2010/04/12/1710603.html">Twitter系统结构分析</a></li>
<li><a href="https://timyang.net/architecture/twitter-cache-architecture/">Twitter架构图(cache篇)</a></li>
<li><a href="https://www.infoq.cn/article/2009/06/Twitter-Architecture/">Twitter，架构的变迁</a>  Twitter  缓存</li>
<li><a href="https://www.jdon.com/artichect/ebay.html">EBay架构案例分析</a></li>
<li><a href="https://www.infoq.cn/article/xw-cloud-in-my-view/">我眼中的云端架构</a>  百度  存储</li>
<li><a href="https://www.infoq.cn/article/qzone-architecture/">QQ 空间技术架构之深刻揭密</a></li>
</ol>
<h2><span id="架构理论">架构理论</span><a href="#架构理论" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.blogjava.net/BlueDavy/archive/2008/09/03/226749.html">大型网站架构演变和知识体系</a></li>
<li><a href="https://timyang.net/architecture/micro-vs-macro/">微观架构及宏观架构</a>  timyang</li>
<li><a href="https://wenku.baidu.com/view/f5427074f46527d3240ce0b5.html">软件架构原理讲座</a>  ibm 偏理论</li>
<li><a href="https://www.infoq.cn/article/2009/02/Architectural-Styles-Patterns/">架构风格和架构模式速览</a>  理论</li>
</ol>
<h2><span id="架构设计">架构设计</span><a href="#架构设计" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.doc88.com/p-3176763983077.html">构建高并发高可用的电商平台架构实践</a></li>
<li><a href="https://blog.csdn.net/cenwenchu79/article/details/4488374">应用架构设计“防火”经验分享</a>   岑文初(淘宝花名：放翁)</li>
<li><a href="https://wenku.baidu.com/view/fc8afe28647d27284b735104.html">2010《架构师接龙》合集</a></li>
<li><a href="https://www.infoq.cn/article/2011/02/nosql-architecture-practice/">NoSQL 架构实践（一）——以 NoSQL 为辅</a></li>
<li><a href="http://blog.zhaojie.me/2009/01/system-architecture-and-program-performance.html">计算机体系结构与程序性能</a>   Locality, False Sharing</li>
<li><a href="https://www.infoq.cn/article/cjz-architecture-corruption/">架构腐化之谜</a></li>
<li><a href>阿里巴巴中文站架构设计实践</a>  delicious有笔记</li>
<li><a href="https://www.infoq.cn/article/ebay-scalability-best-practices/">可伸缩性最佳实践：来自 eBay 的经验</a></li>
<li><a href="https://timyang.net/web/pagination/">用Twitter的cursor方式进行Web数据分页</a></li>
<li><a href="https://timyang.net/architecture/weibo/">微博架构与平台安全演讲稿</a></li>
</ol>
<h2><span id="性能">性能</span><a href="#性能" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.ha97.com/5095.html">系统吞吐量（TPS）、用户并发量、性能测试概念和公式</a></li>
<li><a href="https://coolshell.cn/articles/6470.html">由12306.cn谈谈网站性能技术</a>  coolshell</li>
</ol>
<h2><span id="分布式">分布式</span><a href="#分布式" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.docin.com/p-532877866.html">海量存储系列1-15</a>  good</li>
<li><a href="https://www.iteye.com/topic/684087">Ketama一致性Hash算法(含Java代码)</a></li>
<li><a href="https://www.infoq.cn/article/cap-twelve-years-later-how-the-rules-have-changed/">CAP 理论十二年回顾：”规则”变了</a> good</li>
</ol>
<hr>
<h2><span id="mysql">MySQL</span><a href="#mysql" class="header-anchor">#</a></h2><ol>
<li><a href="https://wenku.baidu.com/view/9daa2b8102d276a200292e9c.html">百度分布式数据库</a></li>
<li><a href="http://imysql.cn/?q=node/96">MySQL</a>  blog</li>
<li><a href="https://timyang.net/data/friendfeed-mysql-schema-less/">Friendfeed的MySQL key&#x2F;value存储</a></li>
</ol>
<h2><span id="nosql">NoSQL</span><a href="#nosql" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/zhenjing/archive/2012/04/09/noSQL.html">NoSQL生态系统</a> 中文</li>
<li><a href="http://www.aosabook.org/en/nosql.html">The NoSQL Ecosystem</a>  英文</li>
<li><a href="https://blog.csdn.net/lskyne/article/details/8930772">SQL到NOSQL的思维转变</a>  已打印</li>
<li><a href="http://nosql-database.org/">nosql-database</a>  nosql集合</li>
<li><a href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html">How Google Serves Data from Multiple Datacenters</a></li>
<li><a href="https://www.infoq.cn/article/tq-why-choose-redis/">为什么使用 Redis 及其产品定位</a></li>
<li><a href="https://www.infoq.cn/article/nosql-dynamo/">解读 NoSQL 技术代表之作 Dynamo</a></li>
<li><a href="https://www.infoq.cn/article/2011/02/nosql-architecture-practice/">NoSQL 架构实践（一）——以 NoSQL 为辅</a></li>
<li><a href="https://www.infoq.cn/article/2011/03/nosql-architecture-practice-3/">NoSQL 架构实践（三）——以 NoSQL 为缓存</a></li>
<li><a href="https://www.iteye.com/blog/forchenyun-1018324">海量数据存储之动态Schema的传说</a></li>
</ol>
<h2><span id="kafka">Kafka</span><a href="#kafka" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.infoq.cn/article/kafka-analysis-part-2/">Kafka 设计解析（二）：Kafka High Availability （上）</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Whydoesmyconsumernevergetanydata?">FAQ</a></li>
<li><a href="https://www.tuicool.com/articles/QJvu2e">Kafka深度解析，众人推荐，精彩好文！</a>  Jason Guo</li>
<li><a href="https://www.oschina.net/translate/kafka-design?cmp&p=1">分布式发布订阅消息系统 Kafka 架构设计</a></li>
<li><a href="http://www.seflerzhou.net/post-47.html">Paper Rush-3:Apache Kafka</a>  阿里  周遥（玄胤）</li>
<li><a href="https://blog.csdn.net/lizhitao/article/details/23744675?utm_source=tuicool">apache kafka系列之在zookeeper中存储结构</a></li>
<li><a href="https://www.infoq.cn/article/kafka-analysis-part-4/">Kafka 设计解析（四）：Kafka Consumer 解析</a></li>
<li><a href="https://wenku.baidu.com/view/b96ff230a32d7375a4178051.html">Kafka设计思想</a></li>
<li><a href="https://blog.csdn.net/lizhitao/article/details/39499283">apache kafka技术分享系列(目录索引)</a></li>
</ol>
]]></content>
      <categories>
        <category>资料收集</category>
      </categories>
      <tags>
        <tag>资料收集</tag>
      </tags>
  </entry>
  <entry>
    <title>微服务 总结</title>
    <url>/www6vHomeHexo/2019/09/09/microservice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E5%AE%9A%E4%B9%89">微服务 定义</a></li>
<li><a href="#core">Core</a><ul>
<li><a href="#api%E7%BD%91%E5%85%B3">API网关</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%AE%B9%E9%94%99">服务容错</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0">服务注册和发现</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E9%97%B4%E8%B0%83%E7%94%A8">服务间调用</a><ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%A5%91%E7%BA%A6">服务契约</a></li>
<li><a href="#%E8%B0%83%E7%94%A8%E5%8D%8F%E8%AE%AE">调用协议</a></li>
</ul>
</li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%92%8C%E5%8F%91%E5%B8%83">服务部署和发布</a></li>
</ul>
</li>
<li><a href="#design">Design</a><ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%88%92%E5%88%86%E5%92%8C%E7%BB%84%E5%90%88">服务划分和组合</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B1%82">服务分层</a><ul>
<li><a href="#%E4%B8%8A%E5%B1%82-%E8%81%9A%E5%90%88%E6%9C%8D%E5%8A%A1%E9%80%82%E9%85%8D%E6%9C%8D%E5%8A%A1-%E8%BE%B9%E7%95%8C%E6%9C%8D%E5%8A%A1">上层: 聚合服务（适配服务， 边界服务）</a></li>
<li><a href="#%E4%B8%8B%E5%B1%82-%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1%E6%A0%B8%E5%BF%83%E9%A2%86%E5%9F%9F%E6%9C%8D%E5%8A%A1-%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1">下层: 基础服务（核心领域服务， 公共服务）</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#design-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F">Design-微服务设计模式</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#sidecar-11">Sidecar [11]</a></li>
</ul>
</li>
<li><a href="#the-scale-cube-%E5%8F%AF%E4%BC%B8%E7%BC%A9%E6%80%A7">The Scale Cube 可伸缩性</a></li>
<li><a href="#%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E4%BC%98%E5%8A%BF%E5%92%8C%E4%BB%A3%E4%BB%B7">微服务的优势和代价</a></li>
<li><a href="#%E5%8E%9F%E5%88%99%E5%92%8C%E7%BC%BA%E7%82%B9%E6%8C%91%E6%88%98">原则和缺点（挑战）</a></li>
<li><a href="#soa-%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E4%BA%91%E5%8E%9F%E7%94%9F%E6%BC%94%E8%BF%9B">SOA、微服务、云原生演进</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="微服务-定义">微服务 定义</span><a href="#微服务-定义" class="header-anchor">#</a></h1><p>In short, the microservice architectural style [1] is an approach to developing a single application as a suite of <strong>small services</strong>, <strong>each running in its own process</strong> and <strong>communicating with lightweight mechanisms</strong>, often an HTTP resource API. These services are built around business capabilities and <strong>independently deployable</strong> by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.   –  [Martin Fowler]</p>
<h1><span id="core">Core</span><a href="#core" class="header-anchor">#</a></h1><h3><span id="api网关">API网关</span><a href="#api网关" class="header-anchor">#</a></h3><a href="/www6vHomeHexo/2021/03/03/serviceGovernanceSummary/" title="服务治理  汇总">服务治理  汇总</a>  self     

<h3><span id="服务容错">服务容错</span><a href="#服务容错" class="header-anchor">#</a></h3><a href="/www6vHomeHexo/2021/03/03/serviceGovernanceSummary/" title="服务治理  汇总">服务治理  汇总</a>  self


<h3><span id="服务注册和发现">服务注册和发现</span><a href="#服务注册和发现" class="header-anchor">#</a></h3><a href="/www6vHomeHexo/2021/03/03/serviceGovernanceSummary/" title="服务治理  汇总">服务治理  汇总</a>  self

<h3><span id="服务间调用">服务间调用</span><a href="#服务间调用" class="header-anchor">#</a></h3><p>   <a href="https://yobriefca.se/blog/2013/04/29/micro-service-architecture/">Micro Service Architecture</a><br>   <a href="http://blog.csdn.net/mindfloating/article/details/51221780">Microservice 微服务的理论模型和现实路径</a></p>
<h5><span id="服务契约">服务契约</span><a href="#服务契约" class="header-anchor">#</a></h5><ul>
<li>API，具体接口的 API 接入技术说明。</li>
<li>能力，服务能力的描述。</li>
<li>契约，提供这些能力所约定的一些限制条件说明。</li>
<li>版本，支持的最新和历史的版本说明。</li>
</ul>
<h5><span id="调用协议">调用协议</span><a href="#调用协议" class="header-anchor">#</a></h5><ul>
<li><p>同步 HTTP<br>REST（JAX-RS）<br>RPC（Dubbo）</p>
</li>
<li><p>异步消息<br>Kafka, RabbitMQ, Notify<br>AMQP, MQTT, STOMP</p>
 <div style="text-align: center;">

<p> <img src="https://user-images.githubusercontent.com/5608425/66257280-8528d800-e7c9-11e9-95f4-bfe436d9d283.png" alt="micro-service-architecture-comms"></p>
 </div></li>
</ul>
<h3><span id="服务部署和发布">服务部署和发布</span><a href="#服务部署和发布" class="header-anchor">#</a></h3><p><a href="https://my.oschina.net/xiaominmin/blog/3070053">微服务部署：蓝绿部署、滚动部署、灰度发布、金丝雀发布</a></p>
<ul>
<li>部署模式</li>
<li>Single Service per Host  </li>
<li>Multiple Services per Host patterns</li>
</ul>
<h1><span id="design">Design</span><a href="#design" class="header-anchor">#</a></h1><h3><span id="服务划分和组合">服务划分和组合</span><a href="#服务划分和组合" class="header-anchor">#</a></h3><blockquote>
<p>微服务不是指”微小”的服务, 而是如何”拆分”服务,然后”组合”服务.</p>
</blockquote>
<ul>
<li>DDD 领域驱动设计, 上下文划分（context）</li>
<li>康威定律</li>
</ul>
<h3><span id="服务分层">服务分层</span><a href="#服务分层" class="header-anchor">#</a></h3><h5><span id="上层-聚合服务适配服务-边界服务">上层: 聚合服务（适配服务， 边界服务）</span><a href="#上层-聚合服务适配服务-边界服务" class="header-anchor">#</a></h5><pre><code>比如：pc和mobile服务对商品服务返回内容的裁剪。
      聚合商品服务和目录服务的内容。   
</code></pre>
<h5><span id="下层-基础服务核心领域服务-公共服务">下层: 基础服务（核心领域服务， 公共服务）</span><a href="#下层-基础服务核心领域服务-公共服务" class="header-anchor">#</a></h5><pre><code>比如：电商的商品服务， 目录服务， 订单服务
</code></pre>
<h1><span id="design-微服务设计模式">Design-微服务设计模式</span><a href="#design-微服务设计模式" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/09/microservice/MicroservicePatternLanguage.jpg" class>

<h3><span id="sidecar-11">Sidecar [11]</span><a href="#sidecar-11" class="header-anchor">#</a></h3><p>分离业务逻辑与路由，流控，熔断，幂等，服务发现，鉴权等控制组件。</p>
<p>适用场景：<br>老系统改造扩展，Sidebar 进程与服务进程部署在同一个节点；<br>多语言混合分布式系统扩展；</p>
<p>Eg. k8s pod中日志采集sidecar</p>
<h1><span id="the-scale-cube-可伸缩性">The Scale Cube 可伸缩性</span><a href="#the-scale-cube-可伸缩性" class="header-anchor">#</a></h1><p><a href="https://akfpartners.com/techblog/2008/05/08/splitting-applications-or-services-for-scale/">The Scale Cube</a></p>
<blockquote>
<p>  X-Axis: Horizontal Duplication and Cloning of services and data<br>    Y-Axis: Functional Decomposition and Segmentation - Microservices (or micro-services)<br>    Z-Axis: Service and Data Partitioning along Customer Boundaries - Shards&#x2F;Pods</p>
</blockquote>
<blockquote>
<p>  X-Axis: Replicate &amp;&amp; Load Balance<br>    Y-Axis: Servcie<br>    Z-Axis: Data Sharding</p>
</blockquote>
<h1><span id="微服务的优势和代价">微服务的优势和代价</span><a href="#微服务的优势和代价" class="header-anchor">#</a></h1><p><a href="https://martinfowler.com/bliki/MicroservicePremium.html">MicroservicePremium</a> Martin Fowler. </p>
<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/66262608-af54b700-e816-11e9-9e16-3b95d76e14e2.png" alt="productivity"><br>生产率和复杂度之间的关系。 </p>
</div>

<blockquote>
<p>在不复杂的系统中， 更适合monolithic的应用。<br>  复杂度增长时， 微服务的生产率能持续保持，在生产率方面是可伸缩的。</p>
</blockquote>
<h1><span id="原则和缺点挑战">原则和缺点（挑战）</span><a href="#原则和缺点挑战" class="header-anchor">#</a></h1><p><a href="https://www.phodal.com/blog/microservices-is-not-a-free-lunch/">微服务架构——不是免费的午餐</a><br><a href="http://www.infoq.com/cn/news/2014/05/micro-server-architecture-debate">有关微服务架构的争论：更简单还是更复杂？</a> </p>
<table>
<thead>
<tr>
<th align="center">原则</th>
<th align="center">优点</th>
<th align="center">缺点</th>
<th align="center">挑战</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>分布式</strong>服务组成的系统； 去中心化</td>
<td align="center">可用性高</td>
<td align="center">多服务运维难度</td>
<td align="center">分布式系统的复杂性（容错，延迟，分布式事务）</td>
</tr>
<tr>
<td align="center">按照业务而不是技术来划分组织</td>
<td align="center">服务独立无依赖</td>
<td align="center">系统部署依赖</td>
<td align="center">事务、异步、测试面临挑战</td>
</tr>
<tr>
<td align="center">做有生命的<strong>产品</strong>而不是项目</td>
<td align="center">技术栈灵活</td>
<td align="center"></td>
<td align="center">运营开销</td>
</tr>
<tr>
<td align="center">Smart endpoints and dumb pipes（强服务个体和轻量级通信）; 可组合的服务</td>
<td align="center">独立按需扩展和伸缩</td>
<td align="center">服务间通信成本</td>
<td align="center">隐式接口[接口变更成本]</td>
</tr>
<tr>
<td align="center">自动化运维（<strong>DevOps</strong>）</td>
<td align="center"></td>
<td align="center">系统集成测试</td>
<td align="center">DevOps 要求</td>
</tr>
<tr>
<td align="center"><strong>容错</strong></td>
<td align="center">可用性高</td>
<td align="center">数据一致性</td>
<td align="center">性能监控; 分布式系统的复杂性</td>
</tr>
<tr>
<td align="center">快速<strong>演化</strong></td>
<td align="center">开发简单</td>
<td align="center">重复工作</td>
<td align="center">系统集成测试</td>
</tr>
</tbody></table>
<h1><span id="soa-微服务-云原生演进">SOA、微服务、云原生演进</span><a href="#soa-微服务-云原生演进" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th align="center">关注点</th>
<th align="center">SOA</th>
<th align="center">微服务</th>
<th align="center">云原生</th>
</tr>
</thead>
<tbody><tr>
<td align="center">研发过程</td>
<td align="center">CMM&#x2F;RUP</td>
<td align="center">Agile</td>
<td align="center">Agile</td>
</tr>
<tr>
<td align="center">交付流程</td>
<td align="center">手工&#x2F;自动化</td>
<td align="center">DevOps<br>DevSecOps</td>
<td align="center">GitOps[12]<br>AIOps<br>NoOps(Serverless)</td>
</tr>
<tr>
<td align="center">服务通信</td>
<td align="center">Web Service（WSDL，Soap）</td>
<td align="center">REST&#x2F;私有RPC协议（Dubbo）</td>
<td align="center">REST&#x2F;gRPC,Envoy xDS， MSI协议等开放协议</td>
</tr>
<tr>
<td align="center">功能扩展性-filter</td>
<td align="center">x</td>
<td align="center">AOP filter<br> Dubbo filter chain<br>   WEB filter&#x2F;lisnter</td>
<td align="center">Envoy filter</td>
</tr>
<tr>
<td align="center">功能扩展性-微内核</td>
<td align="center">x</td>
<td align="center">Dubbo SPI</td>
<td align="center">K8s CRD, Operator</td>
</tr>
<tr>
<td align="center">服务治理</td>
<td align="center">ESB</td>
<td align="center">微服务&#x2F;API网关（SpringCloud），去中心化, sidecar</td>
<td align="center">服务网格（ <a href="../../../../2019/07/02/istio/">istio</a> ， Linked） 分布式</td>
</tr>
<tr>
<td align="center">应用运行环境</td>
<td align="center">物理机&#x2F;虚拟机</td>
<td align="center">虚拟机&#x2F;容器</td>
<td align="center">Kubernete（操作系统）+ Serverless（Knative）</td>
</tr>
<tr>
<td align="center">基础设施</td>
<td align="center">IDC</td>
<td align="center">公有云&#x2F;私有云</td>
<td align="center">无边界的云（多云&#x2F;混合云、 云+边+端）</td>
</tr>
<tr>
<td align="center">总结</td>
<td align="center">重</td>
<td align="center">轻, 快速</td>
<td align="center">开放、融合</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.nginx.com/blog/introduction-to-microservices/">Introduction to Microservices</a>  英文  </li>
<li><a href="https://kb.cnblogs.com/page/521880/">Introduction to Microservices</a>  中文  优缺点</li>
<li><a href="https://yq.aliyun.com/articles/2764?spm=5176.100239.blogcont59193.8.R9MzN9">微服务（Microservice）那点事</a> ***</li>
<li><a href="https://microservices.io/patterns/microservices.html">Pattern: Microservice Architecture</a>  ***</li>
<li><a href="../../../2016/02/09/consistent/">一致性</a>  self</li>
<li><a href="http://www.infoq.com/cn/articles/microservices-intro">微服务：分解应用以实现可部署性和可扩展性</a>  Chris Richardson</li>
<li><a href="https://www.cnblogs.com/suter/p/3401952.html">《Linux&#x2F;Unix设计思想》随笔 ——Linux&#x2F;Unix哲学概述</a>  未</li>
<li><a href="https://www.infoq.cn/article/2014/07/microservice-learning-resources/">微服务学习资料汇总</a>  ***</li>
<li><a href="https://www.infoq.cn/article/micro-service-technology-stack/?utm_source=infoq&utm_medium=popular_widget&utm_campaign=popular_content_list&utm_content=homepage">微服务架构技术栈选型手册</a> 未</li>
<li><a href="https://mp.weixin.qq.com/s/zl0Z-bCoLDFGD8GFYh68CQ">从 SOA 到微服务，企业分布式应用架构在云原生时代如何重塑？</a> 阿里 易立  *** </li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUzNzYxNjAzMg==&mid=2247486600&idx=1&sn=0ad92a1fe535f141fe2e8c87ffbd1229&chksm=fae50747cd928e51c05c41d2cc206069babbe9dfdba5957c52ac6e77cb754192169bb6b3e898&scene=0&xtrack=1#rd">云原生时代，分布式系统设计必备知识图谱（内含22个知识点）</a> 杨泽强（竹涧） ***</li>
<li><a href="https://www.servicemesher.com/blog/202003-gitops-progressive-delivery-with-asm/">使用托管服务网格实现应用在多集群中的 GitOps 全自动化渐进式发布</a>  郝树伟 阿里云容器服务</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes存储</title>
    <url>/www6vHomeHexo/2019/09/01/k8sStorage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/09/01/k8sStorage/k8sStorage.jpg" class title="Kubernetes存储">

<h2><span id="一-kubernetes-存储的绑定流程">一. Kubernetes 存储的绑定流程</span><a href="#一-kubernetes-存储的绑定流程" class="header-anchor">#</a></h2><h3><span id="11">1.1</span><a href="#11" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/68108028-9593b600-ff21-11e9-8623-5c719772317e.jpg" alt="Kubernetes存储的绑定流程"></p>
<p><strong>三个阶段：</strong><br>第一个<strong>create</strong>阶段，主要是创建存储；<br>第二个<strong>attach</strong>阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的&#x2F;dev下面)；<br>第三个<strong>mount</strong>阶段，将对应的存储进一步挂载到 pod 可以使用的路径。</p>
<h2><span id="二-static-provisioning-ampamp-dynamic-provisioning">二. Static Provisioning &amp;&amp; Dynamic Provisioning</span><a href="#二-static-provisioning-ampamp-dynamic-provisioning" class="header-anchor">#</a></h2><h3><span id="21-static-provisioning">2.1 Static Provisioning</span><a href="#21-static-provisioning" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/01/k8sStorage/static-provision.PNG" class title="Static Provisioning">

<h3><span id="22-dynamic-provisioning">2.2 Dynamic Provisioning</span><a href="#22-dynamic-provisioning" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/09/01/k8sStorage/dynamic-provision.PNG" class title="Dynamic Provisioning">

<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64247540-aafc5c00-cf41-11e9-83af-64199e79ded7.JPG" alt="relationship"><br><strong>Kubernetes pvc 动态绑定流程</strong></p>
</div>

<blockquote>
<p>只有同属于一个 StorageClass 的PV 和 PVC，才可以绑定在一起</p>
</blockquote>
<ul>
<li>CSI driver动态创建pv ， CSI Driver 可以动态创建PV [5]<br>创建pvc之后，provisioner 会创建pv ，并做pvc和pv的绑定关系</li>
</ul>
<h2><span id="本地存储5">本地存储[5]</span><a href="#本地存储5" class="header-anchor">#</a></h2><ul>
<li>emptyDir<br>不是overlayFS ，和写主机的性能是一样的<br>和容器的生命周期一致，如果容器销毁， emptyDir也被销毁。</li>
<li>hostPath<br>需要的注意点，不建议用</li>
</ul>
<h2><span id="最佳实践5">最佳实践[5]</span><a href="#最佳实践5" class="header-anchor">#</a></h2><ul>
<li>用户去查询集群中有哪些storageclass<br>可能有本地的hostpath，或者远程的nfs， ceph 等等<ul>
<li>在storageClass中, provisioner很重要</li>
</ul>
</li>
<li>pod中声明一个pvc<br>csi  plugin把pv attach到对应的node，mount到对应的pod</li>
<li>pvc和pv是一一对应的关系</li>
</ul>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》七牛容器云团队</a></li>
<li>&lt;&lt;深入剖析Kubernetes - 28  PV、PVC、StorageClass，这些到底在说啥？&gt;&gt; 张磊</li>
<li>&lt;&lt;深入剖析Kubernetes - 29  PV、PVC体系是不是多此一举？从本地持久化卷谈起&gt;&gt; 张磊</li>
<li><a href="https://edu.aliyun.com/lesson_1651_13085#_13085">第9 章 ： 应用存储和持久化数据卷：核心知识</a> </li>
<li>云原生训练营 第0期-模块七</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>可观测性 总结</title>
    <url>/www6vHomeHexo/2019/08/31/observability/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><strong>关键词</strong>:  可观测性,  全链路,  APM， Metric， Log</p>
<h1><span id="可观测性">可观测性</span><a href="#可观测性" class="header-anchor">#</a></h1><h3><span id="10-基础支柱">1.0-基础支柱</span><a href="#10-基础支柱" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/64059064-216a2880-cbe7-11e9-9ee7-141334d93959.png" alt="metric-tracing-logging"></p>
<table>
<thead>
<tr>
<th align="center">模式</th>
<th align="center">产品&#x2F;框架</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Log aggregation</td>
<td align="center"><strong>ELK</strong>， AWS Cloud Watch</td>
</tr>
<tr>
<td align="center">Application metrics + alert</td>
<td align="center"><strong>Prometheus</strong> 、AWS Cloud Watch</td>
</tr>
<tr>
<td align="center">Distributed tracing</td>
<td align="center">Zipkin ，Jaeger，pinpoint（无侵入）, <strong>skywalking</strong>（无侵入）, CAT</td>
</tr>
<tr>
<td align="center">Exception tracking</td>
<td align="center">Zipkin ，Jaeger，pinpoint（无侵入）, <strong>skywalking</strong>（无侵入）, CAT</td>
</tr>
</tbody></table>
<h3><span id="20-统一的可观测性平台">2.0-统一的可观测性平台</span><a href="#20-统一的可观测性平台" class="header-anchor">#</a></h3><ul>
<li>OpenTelemetry - 数据采集传输的标准化</li>
</ul>
<h3><span id="30-内生的可观测性能力">3.0-内生的可观测性能力</span><a href="#30-内生的可观测性能力" class="header-anchor">#</a></h3><ul>
<li>基于ebpf</li>
</ul>
<h2><span id="监控指标和原则">监控指标和原则</span><a href="#监控指标和原则" class="header-anchor">#</a></h2><ul>
<li><p>USE 原则  [面向”资源监控指标”]</p>
<ul>
<li>利用率（Utilization），资源被有效利用起来提供服务的平均时间占比</li>
<li>饱和度（Saturation），资源拥挤的程度，比如工作队列的长度</li>
<li>错误率（Errors），错误的数量</li>
</ul>
</li>
<li><p>RED 原则  [面向”服务监控指标”] </p>
<ul>
<li>每秒请求数量（Rate）</li>
<li>每秒错误数量（Errors）</li>
<li>服务响应时间（Duration）</li>
</ul>
</li>
<li><p>google 四个黄金监控指标  [面向服务]</p>
<ul>
<li>延迟    latancy</li>
<li>通信量  throughtout</li>
<li>错误    error</li>
<li>饱和度</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://wu-sheng.github.io/me/articles/metrics-tracing-and-logging">Metrics, tracing 和 logging 的关系</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&mid=2247489564&idx=1&sn=46d9103444bef97e89e897224a896268&chksm=e8d7e7dedfa06ec8d687c1292a1d82ff9e579430afafb9d003e18c13d4ec7e1682dbd4c642d9&scene=27#wechat_redirect">观察之道：带你走进可观察性</a></li>
<li><a href="https://www.bilibili.com/video/BV1CL411777R?spm_id_from=333.880.my_history.page.click">【云原生学院#25】云原生应用可观测性实践</a>  github中有PPT ***</li>
<li><a href="https://www.pianshen.com/article/96362082048/">Monarch: 谷歌的全球级内存时序数据库</a>   监控  未</li>
</ol>
]]></content>
      <categories>
        <category>可观测性</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>可观测性</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes声明式API</title>
    <url>/www6vHomeHexo/2019/08/29/k8sDeclarativeAPI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/08/29/k8sDeclarativeAPI/ks8-declarative-api.jpg" class title="声明式API">

<h2><span id="一-crd">一. CRD</span><a href="#一-crd" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># CRD的定义和使用</span><br><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">├── controller.go                + 自定义控制器： 拿到“实际状态”，然后拿它去跟“期望状态”做对比，执行“业务逻辑”</span><br><span class="line">├── crd</span><br><span class="line">│ └── network.yaml               + resource的定义【像类定义】</span><br><span class="line">├── example</span><br><span class="line">│ └── example-network.yaml       + resource的使用【像类实例】</span><br><span class="line">├── main.go</span><br><span class="line">└── pkg</span><br><span class="line">	├── apis</span><br><span class="line">	│ 	└── samplecrd              + group </span><br><span class="line">	│ 		├── constants.go</span><br><span class="line">	│ 		└── v1                 + version</span><br><span class="line">	│ 			├── doc.go         + 代码生成，Global Tag </span><br><span class="line">	│ 			├── register.go    + 注册类型（Type）到APIServer中，全局变量</span><br><span class="line">	│ 			├── types.go       + 类型有哪些字段</span><br><span class="line">	│ 			└── zz_generated.deepcopy.go    【codegenerated】</span><br><span class="line">	└── client       +for自定义控制器【codegenerated】：  拿到“期望状态” </span><br><span class="line">		├── clientset</span><br><span class="line">		├── informers  + 见下图</span><br><span class="line">		└── listers    + 见下图</span><br></pre></td></tr></table></figure>

<h2><span id="二-自定义控制器">二. 自定义控制器</span><a href="#二-自定义控制器" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64071488-e7b02500-ccad-11e9-927e-dd71af8c3923.jpg" alt="自定义控制器">  自定义控制器</p>
</div>

<blockquote>
<p>+Informer就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。<br> +Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。<br> +Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。</p>
</blockquote>
<blockquote>
<p>+在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg&#x2F;client&#x2F;{informers, listers, clientset}里的内容。<br> +这些自动生成的代码，就为我们提供了一个可靠而高效地获取 API 对象“期望状态”的编程库。<br> +所以，接下来，作为开发者，你就只需要关注如何拿到“实际状态”，然后如何拿它去跟“期望状态”做对比，从而决定接下来要做的业务逻辑即可。  &#x2F;&#x2F;&#x2F; controller.go</p>
</blockquote>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《深入剖析Kubernetes  - 23  声明式API与Kubernetes编程范式》  张磊 </li>
<li>《深入剖析Kubernetes  - 24  深入解析声明式API（一）：API对象的奥秘》  张磊</li>
<li>《深入剖析Kubernetes  - 25  深入解析声明式API（二）：编写自定义控制器》  张磊</li>
<li><a href="https://juejin.im/post/5ba3547ae51d450e425ec6a5">Kubernetes 准入控制 Admission Controller 介绍</a></li>
<li><a href="https://github.com/resouer/k8s-controller-custom-resource">CRD 代码示例</a></li>
<li><a href="https://blog.csdn.net/boling_cavalry/article/details/88917818">k8s自定义controller三部曲之一:创建CRD（Custom Resource Definition）</a> 一、二、三</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>算法和数据结构 - 题目</title>
    <url>/www6vHomeHexo/2019/08/25/algorithmSubject/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>
 

<h2><span id="一-线性表">一. 线性表</span><a href="#一-线性表" class="header-anchor">#</a></h2><h2><span id="数组">数组</span><a href="#数组" class="header-anchor">#</a></h2><h4><span id="top-k问题-数组中最小的-k-个数">Top K问题 : 数组中最小的 k 个数</span><a href="#top-k问题-数组中最小的-k-个数" class="header-anchor">#</a></h4><p>方法一： 堆， 时间复杂度 O(n log k)<br>方法二： 快排变形， （平均）时间复杂度 O(n)</p>
<p><a href="https://mp.weixin.qq.com/s/rSr-dttis3Ubtbv892aOtg">数组中最小的 k 个数：Top K 问题的两种经典解法</a>  </p>
<h4><span id="数组的两分搜索">数组的两分搜索</span><a href="#数组的两分搜索" class="header-anchor">#</a></h4><blockquote>
<p><a href="https://github.com/grandyang/leetcode/issues/540">LeetCode 540. Single Element in a Sorted Array</a> [中等]</p>
</blockquote>
<h4><span id="合并有序数组-简单">合并有序数组 [简单]</span><a href="#合并有序数组-简单" class="header-anchor">#</a></h4><p><a href="https://leetcode.cn/problems/merge-sorted-array/solution/he-bing-liang-ge-you-xu-shu-zu-by-leetco-rrb0/">LeetCode 88</a></p>
<ul>
<li>方法一：直接合并后排序</li>
<li>方法二：双指针</li>
<li>方法三：逆向双指针</li>
</ul>
<h4><span id="leetcode-283-移动零">Leetcode 283. 移动零</span><a href="#leetcode-283-移动零" class="header-anchor">#</a></h4><h2><span id="链表">链表</span><a href="#链表" class="header-anchor">#</a></h2><h4><span id="单链表的反转-简单">单链表的反转 [简单]</span><a href="#单链表的反转-简单" class="header-anchor">#</a></h4><ul>
<li>使用3个指针遍历单链表，逐个链接点进行反转。 复杂</li>
<li>从第2个节点到第N个节点，依次逐节点插入到第1个节点(head节点)之后，最后将第一个节点挪到新表的表尾。 容易理解</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static Node reverseList(Node node) &#123;</span><br><span class="line">  Node pre = null;</span><br><span class="line">  Node next = null;</span><br><span class="line">  while (node != null) &#123;</span><br><span class="line">      next = node.next;</span><br><span class="line">      node.next = pre;</span><br><span class="line">      pre = node;</span><br><span class="line">      node = next;</span><br><span class="line">  &#125;</span><br><span class="line">  return pre;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>剑指 题16<br><a href="https://www.cnblogs.com/keeya/p/9218352.html">理解单链表的反转(java实现)</a><br><a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC206%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8.md">LeetCode206：反转链表</a></p>
</blockquote>
<h4><span id="合并两个有序链表-简单">合并两个有序链表 [简单]</span><a href="#合并两个有序链表-简单" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class ListNode &#123;</span><br><span class="line">      int val;</span><br><span class="line">      ListNode next;</span><br><span class="line">      ListNode(int x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Solution &#123;</span><br><span class="line">    public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123;</span><br><span class="line">        ListNode dummyHead = new ListNode(0);</span><br><span class="line">        ListNode lastNode = dummyHead;</span><br><span class="line">        while (l1 != null &amp;&amp; l2 != null) &#123;</span><br><span class="line">            if (l1.val &lt; l2.val) &#123;</span><br><span class="line">                lastNode.next = l1;</span><br><span class="line">                l1 = l1.next;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                lastNode.next = l2;</span><br><span class="line">                l2 = l2.next;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            lastNode = lastNode.next;</span><br><span class="line">        &#125;</span><br><span class="line">        lastNode.next = l1 != null ? l1 : l2;</span><br><span class="line">        return dummyHead.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.cnblogs.com/xugenpeng/p/9850372.html">LeetCode题解21_合并两个有序链表</a><br>  <a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC21%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8.md">LeetCode21:合并两个有序链表</a></p>
</blockquote>
<h4><span id="快指针慢指针">快指针，慢指针</span><a href="#快指针慢指针" class="header-anchor">#</a></h4><ul>
<li><p>链表中环的检测 </p>
<ol>
<li>硬解 </li>
<li>每走一步，把节点的地址存在set中，走下一步的时候查找地址是否已经在set中</li>
<li>快指针是慢指针的两倍， 最后快指针和慢指针相遇</li>
</ol>
</li>
<li><p>链表倒数第k个结点 </p>
<blockquote>
<p>剑指 题15<br>题解： 快指针先于慢指针走若干步</p>
</blockquote>
</li>
<li><p>删除链表倒数第n个结点  [中等]<br>LeetCode 19 </p>
</li>
<li><p>求链表的中间结点 -&gt; 快指针是慢指针的两倍<br><a href="https://mp.weixin.qq.com/s/Jdbr_-UACicLU_Akn4czSw">LeetCode 例题精讲 | 05 双指针×链表问题：快慢指针</a></p>
</li>
</ul>
<h4><span id="在o1时间删除链表结点">在O(1)时间删除链表结点</span><a href="#在o1时间删除链表结点" class="header-anchor">#</a></h4><blockquote>
<p>剑指Offer</p>
</blockquote>
<ul>
<li>最常规的做法是从链表的头结点开始，顺序遍历查找要删除的结点，并在链表中删除该结点。<br>这种思路由于需要顺序查找，时间复杂度自然就是O(n)。</li>
<li>然而，可以把下一个结点的内容复制到需要删除的结点上覆盖原有的内容，再把下一个结点删除，<br>就相当于把当前需要删除的结点删除了，这样的时间复杂度是O（1）。</li>
</ul>
<h4><span id="两数相加-合并两个链表中的数并返回一个新的链表">两数相加 : 合并两个链表中的数，并返回一个新的链表</span><a href="#两数相加-合并两个链表中的数并返回一个新的链表" class="header-anchor">#</a></h4><p><a href="https://github.com/www6v/jDemo">Leetcode 2.两数相加</a><br><a href="https://leetcode.cn/problems/add-two-numbers/solution/liang-shu-xiang-jia-by-leetcode-solution/">Leetcode 2.两数相加</a></p>
<h2><span id="栈队列">栈,队列</span><a href="#栈队列" class="header-anchor">#</a></h2><h4><span id="两个栈实现一个队列-简单">两个栈实现一个队列 [简单]</span><a href="#两个栈实现一个队列-简单" class="header-anchor">#</a></h4><blockquote>
<p>剑指 7题，指南 P5<br>Leetcode 232 </p>
</blockquote>
<h4><span id="用队列实现栈">用队列实现栈</span><a href="#用队列实现栈" class="header-anchor">#</a></h4><p><a href="https://leetcode.cn/problems/implement-stack-using-queues/solution/yong-dui-lie-shi-xian-zhan-by-leetcode-solution/">Leetcode 225. 用队列实现栈</a></p>
<h4><span id="包含min函数的栈-简单">包含min函数的栈 [简单]</span><a href="#包含min函数的栈-简单" class="header-anchor">#</a></h4><blockquote>
<p>剑指 21题 , 指南 P1<br>Leetcode 155<br>题解：方法一：辅助栈</p>
</blockquote>
<h4><span id="有效的括号简单">有效的括号[简单]</span><a href="#有效的括号简单" class="header-anchor">#</a></h4><blockquote>
<p>Leetcode 20<br>题解：辅助栈</p>
</blockquote>
<h4><span id="用数组来实现栈">用数组来实现栈</span><a href="#用数组来实现栈" class="header-anchor">#</a></h4><p><a href="https://github.com/www6v/jDemo">用数组来实现栈</a></p>
<h2><span id="二-hashtable-哈希表">二. HashTable 哈希表</span><a href="#二-hashtable-哈希表" class="header-anchor">#</a></h2><h4><span id="两数之和">两数之和</span><a href="#两数之和" class="header-anchor">#</a></h4><blockquote>
<p>Leetcode 1  a + b &#x3D; 9<br>  a; b&#x3D; 9 -a &#x3D;&gt; Set(查询) ;<br>  O(n)</p>
</blockquote>
<h4><span id="三数之和">三数之和</span><a href="#三数之和" class="header-anchor">#</a></h4><blockquote>
<p>Leetcode 15<br>   a, b  -&gt; 2 loop;<br>   c &#x3D; -(a+b) -&gt; Set(查询);<br>   O(n^2)</p>
</blockquote>
<h2><span id="三-二叉树">三. 二叉树</span><a href="#三-二叉树" class="header-anchor">#</a></h2><h4><span id="中序遍历-非递归算法">中序遍历   非递归算法</span><a href="#中序遍历-非递归算法" class="header-anchor">#</a></h4><blockquote>
<p>指南 P88<br><a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC94%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86.md">LeetCode 94：二叉树的中序遍历</a><br>  在迭代的时候需要显式地将这个栈模拟出来</p>
</blockquote>
<h4><span id="公共祖先">公共祖先</span><a href="#公共祖先" class="header-anchor">#</a></h4><blockquote>
<p>剑指 50题 ，指南 P153<br>参考17 LeetCode 235,236. 递归，非递归</p>
</blockquote>
<h4><span id="二叉树的最大距离即相距最远的两个叶子节点">二叉树的最大距离（即相距最远的两个叶子节点）。</span><a href="#二叉树的最大距离即相距最远的两个叶子节点" class="header-anchor">#</a></h4><h4><span id="二叉树中和为某一值的路径">二叉树中和为某一值的路径</span><a href="#二叉树中和为某一值的路径" class="header-anchor">#</a></h4><blockquote>
<p>剑指 25题<br>LeetCode 112: 路径总和. 用栈来存路径</p>
</blockquote>
<h4><span id="leetcode-102-二叉树的层序遍历">LeetCode 102  二叉树的层序遍历</span><a href="#leetcode-102-二叉树的层序遍历" class="header-anchor">#</a></h4><blockquote>
<p>参考 19<br> <a href="https://github.com/grandyang/leetcode/issues/102">LeetCode 102</a><br> <a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC102%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%B1%82%E5%BA%8F%E9%81%8D%E5%8E%86.md">LeetCode102</a><br> 解1： BFS  队列辅助<br> 解2： DFS </p>
</blockquote>
<h4><span id="leetcode-104-二叉树的最小x2f最大深度-二叉树的深度">LeetCode 104 二叉树的最小&#x2F;最大深度， 二叉树的深度</span><a href="#leetcode-104-二叉树的最小x2f最大深度-二叉树的深度" class="header-anchor">#</a></h4><blockquote>
<p>剑指 39题 ， 递归的方法<br><a href="https://github.com/grandyang/leetcode/issues/104">LeetCode 104</a><br>   BFS： 第一个达到的叶子节点是最小深度， 最后一个达到的叶子节点是最大深度。<br>   DFS递归。 </p>
</blockquote>
<h2><span id="四-字符串">四. 字符串</span><a href="#四-字符串" class="header-anchor">#</a></h2><h4><span id="一个字符串中-要找到其中最长的字符串并且没有重复字符">一个字符串中， 要找到其中最长的字符串，并且没有重复字符。</span><a href="#一个字符串中-要找到其中最长的字符串并且没有重复字符" class="header-anchor">#</a></h4><blockquote>
<p><a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC3%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2.md">Leetcode 3</a><br>  <a href="https://github.com/grandyang/leetcode/issues/3">LeetCode 3</a><br>   <a href="https://leetcode.cn/problems/longest-substring-without-repeating-characters/solution/wu-zhong-fu-zi-fu-de-zui-chang-zi-chuan-cshi-xian-/">3. 无重复字符的最长子串</a></p>
</blockquote>
<h4><span id="字符串转换为整数">字符串转换为整数</span><a href="#字符串转换为整数" class="header-anchor">#</a></h4><blockquote>
<p>剑指 50题<br><a href="https://blog.csdn.net/u010651249/article/details/85709554">leetcode 8</a><br><a href="https://github.com/grandyang/leetcode/issues/8">leetcode 8</a></p>
</blockquote>
<h4><span id="字符串中第一个只出现一次的字符如何优化算法使得遍历次数更少">字符串中第一个只出现一次的字符，如何优化算法使得遍历次数更少</span><a href="#字符串中第一个只出现一次的字符如何优化算法使得遍历次数更少" class="header-anchor">#</a></h4><blockquote>
<p>剑指 35题</p>
</blockquote>
<h4><span id="kmp子串查找算法">KMP子串查找算法</span><a href="#kmp子串查找算法" class="header-anchor">#</a></h4><h4><span id="leetcode-415-字符串相加-字符串表示的数字相加">Leetcode 415. 字符串相加: 字符串表示的数字相加</span><a href="#leetcode-415-字符串相加-字符串表示的数字相加" class="header-anchor">#</a></h4><p><a href="https://leetcode.cn/problems/add-strings/solution/add-strings-shuang-zhi-zhen-fa-by-jyd/">Leetcode 415. 字符串相加</a> </p>
<h4><span id="leetcode-344-反转字符串">Leetcode 344. 反转字符串</span><a href="#leetcode-344-反转字符串" class="header-anchor">#</a></h4><h4><span id="leetcode-205-同构字符串">Leetcode 205. 同构字符串</span><a href="#leetcode-205-同构字符串" class="header-anchor">#</a></h4><h2><span id="五-其他">五. 其他</span><a href="#五-其他" class="header-anchor">#</a></h2><h4><span id="最小响应时间的算法">最小响应时间的算法</span><a href="#最小响应时间的算法" class="header-anchor">#</a></h4><blockquote>
<p>有的task 执行时间长，有的task 执行时间短， 如何设计一个负载均衡策略</p>
</blockquote>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651017589&idx=2&sn=32a472b8e805666b197505f1acc9af83&chksm=bdbea9268ac92030342933d37ecb1b9754b5d1d6e5a2935c331098669b3f4a44909e628532dd&scene=27#wechat_redirect">QPS比Nginx提升60%，阿里Tengine负载均衡算法揭秘</a> </li>
<li><a href="https://blog.csdn.net/iteye_14001/article/details/82098361">超级负载均衡</a>  来源于以前百度的blog </li>
<li>[极客时间 - 程序员的数据基础课 19,20,21]  基于概率  响应时间短的概率越大， 响应时间是正态分布的</li>
<li><a href="https://github.com/gnosek/nginx-upstream-fair">nginx fair负载均衡算法</a>  按后端服务器的响应时间来分配请求，响应时间短的优先分配。</li>
</ul>
<h4><span id="某一个id访问了系统多次怎么做限流">某一个id访问了系统多次，怎么做限流</span><a href="#某一个id访问了系统多次怎么做限流" class="header-anchor">#</a></h4><blockquote>
<p>参考12 : leecode 239  Array + sliding window maximum<br>     解：<br>       + 优先级队列 大顶堆  O(n * logk)<br>       + queue deque  O(n*1)</p>
</blockquote>
<h4><span id="递归模板-recursion">递归模板  recursion</span><a href="#递归模板-recursion" class="header-anchor">#</a></h4><img src="/www6vHomeHexo/2019/08/25/algorithmSubject/recursion.jpg" class title="递归模板">

<h4><span id="求众数">求众数</span><a href="#求众数" class="header-anchor">#</a></h4><blockquote>
<p> <a href="https://github.com/MisterBooo/LeetCodeAnimation/blob/master/notes/LeetCode%E7%AC%AC169%E5%8F%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E6%B1%82%E4%BC%97%E6%95%B0.md">LeetCode第169号问题：求众数</a><br>   <a href="https://github.com/grandyang/leetcode/issues/169">LeetCode 169. Majority Element</a><br>   剑指 29题， 参考18<br>   解1： map O(N)<br>   解2：sort O（n logn）<br>   解3: 摩尔投票法  特殊高效算法</p>
</blockquote>
<h4><span id="lru-cache">LRU cache</span><a href="#lru-cache" class="header-anchor">#</a></h4><p>   <a href="https://leetcode.cn/problems/lru-cache/solution/lruhuan-cun-ji-zhi-by-leetcode-solution/">Leetcode 146. LRU 缓存</a><br>   参考20  双向链表实现<br>   <a href="https://my.oschina.net/andylucc/blog/741965">Redis内存淘汰机制</a>  allkeys-lru<br>   <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">Cache replacement policies</a><br>   <a href="http://dennis-zane.iteye.com/blog/128278">简单LRU算法实现缓存</a></p>
<h4><span id="布隆过滤器">布隆过滤器</span><a href="#布隆过滤器" class="header-anchor">#</a></h4><blockquote>
<p>+有误判率<br>  +难以删除<br>  应用： <a href="https://github.com/google/guava/blob/master/guava/src/com/google/common/hash/BloomFilter.java">Guava  Bloomfilter</a><br>       Mapredue bloomfilter<br>       <a href="http://www.fullstackyang.com/bu-long-guo-lu-qi-google-guavalei-ku-yuan-ma-fen-xi-ji-ji-yu-redis-bitmapsde-zhong-gou/">Google Guava之BloomFilter源码分析及基于Redis的重构</a></p>
</blockquote>
<h4><span id="如何在海量数据中判断某个数据是否存在">如何在海量数据中判断某个数据是否存在</span><a href="#如何在海量数据中判断某个数据是否存在" class="header-anchor">#</a></h4><blockquote>
<p>布隆过滤器</p>
</blockquote>
<h4><span id="面试题-1626-计算器">面试题 16.26. 计算器</span><a href="#面试题-1626-计算器" class="header-anchor">#</a></h4><p><a href="https://leetcode.cn/problems/calculator-lcci/solution/java-zhe-dao-ti-yong-dan-zhan-ji-ke-by-f-dgm7/">Leetcode 面试题 16.26. 计算器</a><br>用Stack来实现</p>
<h2><span id="六-动态规划">六. 动态规划</span><a href="#六-动态规划" class="header-anchor">#</a></h2><p><a href="http://dongxicheng.org/structure/knapsack-problems/">背包问题研究与应用</a><br><a href>算法面试通关40讲  - 动态规划</a>  覃超</p>
<p><a href="https://leetcode.cn/problems/climbing-stairs/solution/pa-lou-ti-by-leetcode-solution/">70. 爬楼梯</a><br><a href="https://leetcode.cn/problems/maximum-subarray/solution/dong-tai-gui-hua-fen-zhi-fa-python-dai-ma-java-dai/">53. 最大子数组和</a></p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol start="3">
<li><a href>数据结构与算法之美 - 07链表（下）：如何轻松写出正确的链表代码？</a> 王争</li>
<li><a href="http://wuchong.me/blog/2014/03/25/interview-link-questions/">面试精选：链表问题集锦</a> good</li>
<li>《剑指 offer》</li>
<li>《程序员代码面试指南》</li>
<li><a href="http://blog.csdn.net/scape1989/article/details/21085659">LVS集群之十种调度算法及负载均衡——理论</a></li>
</ol>
<hr>
<pre><code> 算法面试通关40讲  覃超
</code></pre>
<ol start="8">
<li><a href>03.如何计算算法的复杂度</a>  </li>
<li><a href>06.面试题：反转一个单链表&amp;判断链表是否有环</a>  </li>
<li><a href>07.理论讲解：堆栈&amp;队列</a> </li>
<li><a href>10.理论讲解：优先队列</a> </li>
<li><a href>12.面试题：返回滑动窗口中的最大值</a></li>
</ol>
<hr>
<pre><code> 算法面试通关40讲  覃超
</code></pre>
<ol start="17">
<li><a href>19.面试题：二叉树&amp;二叉搜索树的最近公共祖先</a> </li>
<li><a href>23.面试题：求众数</a>  </li>
<li><a href>28.面试题：二叉树层次遍历</a> </li>
<li><a href>55.理论讲解： LRU Cache</a> LRU， LFU</li>
</ol>
<hr>
<h5><span id="git">git</span><a href="#git" class="header-anchor">#</a></h5><ul>
<li><a href="https://github.com/wangzheng0822/algo">https://github.com/wangzheng0822/algo</a>   王争</li>
<li><a href="https://github.com/www6v/leetcode-2">https://github.com/www6v/leetcode-2</a>   作者出版了算法书</li>
<li><a href="https://github.com/www6v/algorithm">https://github.com/www6v/algorithm</a>  star 100K<br><a href="https://labuladong.github.io/algo/">https://labuladong.github.io/algo/</a>   labuladong 的算法网站</li>
</ul>
]]></content>
      <categories>
        <category>基础</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Socket总结</title>
    <url>/www6vHomeHexo/2019/08/25/linuxSocket/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>

<h2><span id="一-tcp-socket建立过程">一. TCP socket建立过程</span><a href="#一-tcp-socket建立过程" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int bind(int sockfd, const struct sockaddr *addr,socklen_t addrlen);           // server sockfd</span><br><span class="line">int listen(int sockfd, int backlog);                                           // server sockfd</span><br><span class="line">int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);             // int 返回的socket</span><br><span class="line">int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);       </span><br></pre></td></tr></table></figure>
<blockquote>
<p>监听的socket和真正用来传送数据的socket，是两个socket，一个叫作监听socket，一个叫作已连接socket。</p>
</blockquote>
<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63642036-00876c00-c6eb-11e9-8df2-a9063c81a640.jpg" alt="TCP socket建立过程"> TCP socket建立过程<br><img src="https://user-images.githubusercontent.com/5608425/63642037-00876c00-c6eb-11e9-9287-55a68ec11892.jpg" alt="UDP socket建立过程"> UDP socket建立过程<br><img src="https://user-images.githubusercontent.com/5608425/63642038-01200280-c6eb-11e9-823e-20bb365f604f.jpg" alt="总结">  总结</p>
</div>

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>趣谈Linux操作系统 - 43-Socket通信：遇上特大项目，要学会和其他公司合作</a>   刘超</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>网络</category>
        <category>socket</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux文件系统</title>
    <url>/www6vHomeHexo/2019/08/24/linuxFile/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>

<p>关键字: 虚拟文件系统,  pagecache, inode, 硬链接, 软链接</p>
<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63632440-9fb74f80-c668-11e9-9b71-6538f4d86b9b.jpg" alt="Linux文件系统">  Linux文件系统</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63632442-a04fe600-c668-11e9-8e6a-368a26e1c83f.jpg" alt="文件的数据结构">  文件的数据结构</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63632218-db501a80-c664-11e9-817a-847c9ff67c47.jpg" alt="文件夹inode， 文件inode">  文件夹inode， 文件inode</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63632219-db501a80-c664-11e9-8fa6-8cc2a055ac97.jpg" alt="硬链接， 软链接">   硬链接， 软链接</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63632527-2a4c7e80-c66a-11e9-9d41-c80e88d88318.jpg" alt="读写中的buffer， cache">   读写中的buffer,cache</p>
<p><img src="https://user-images.githubusercontent.com/5608425/64937814-58e6ff00-d88e-11e9-9cb4-dbe52fa27264.png" alt="clipboard"><br>The Linux Storage Stack Diagram</p>
</div>

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>趣谈Linux操作系统 - 29-虚拟文件系统：文件多了就需要档案管理系统</a>   刘超</li>
<li><a href>趣谈Linux操作系统 - 28-硬盘文件系统：如何最合理地组织档案库的文档</a>   刘超</li>
<li><a href>趣谈Linux操作系统 - 30-文件缓存：常用文档应该放在触手可得的地方</a>  刘超</li>
<li><a href="https://www.thomas-krenn.com/de/wikiDE/images/e/e0/Linux-storage-stack-diagram_v4.10.png">The Linux Storage Stack Diagram</a> mmap， directIO， Page Cache</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>文件系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes网络</title>
    <url>/www6vHomeHexo/2019/08/23/k8sNetwork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/08/23/k8sNetwork/k8sNetwork.jpg" class title="图1.Kubenetes网络">

<h1><span id="一-容器和容器之间的网络">一. 容器和容器之间的网络</span><a href="#一-容器和容器之间的网络" class="header-anchor">#</a></h1><ul>
<li>使用Docker的一种网络模型：–net&#x3D;container</li>
<li>每个Pod容器有有一个pause容器<br> <a href="https://jimmysong.io/kubernetes-handbook/concepts/pause-container.html">Pause容器</a> 例子<br> <a href="https://o-my-chenjian.com/2017/10/17/The-Pause-Container-Of-Kubernetes/">Kubernetes之Pause容器</a><br> <a href="https://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjIyMA==&mid=2247542987&idx=1&sn=dad6c7723576cf1fb86486f2855b0efd&source=41#wechat_redirect">Kubernetes之“暂停”容器</a></li>
</ul>
<h1><span id="二-pod与pod之间的网络">二.  Pod与Pod之间的网络</span><a href="#二-pod与pod之间的网络" class="header-anchor">#</a></h1><h3><span id="20-overview">2.0   Overview</span><a href="#20-overview" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/08/23/k8sNetwork/k8sNetworkAll.png" class title="K8S 网络"> 

<h3><span id="21-同节点pod通信">2.1  同节点pod通信</span><a href="#21-同节点pod通信" class="header-anchor">#</a></h3><p>基础: 网桥 bridge<br>通过<strong>网桥</strong>通信</p>
<div style="text-align: center; width:60%; height: 60%">

<p><img src="https://user-images.githubusercontent.com/5608425/68114098-31c4b980-ff30-11e9-9dbd-163452ec51bc.jpg" alt="pod-to-pod-in-node"><br>图2. 同节点pod通信</p>
</div>

<h3><span id="22-不同节点中的pod通信跨主机网络通讯">2.2  不同节点中的Pod通信（跨主机网络通讯）</span><a href="#22-不同节点中的pod通信跨主机网络通讯" class="header-anchor">#</a></h3><h5><span id="221-overlay-flannel方案">2.2.1  Overlay (Flannel方案)</span><a href="#221-overlay-flannel方案" class="header-anchor">#</a></h5><ol>
<li><strong>flannel-UDP模式(三层overlay)</strong></li>
</ol>
<ul>
<li>原理：<br>  fannelId进程封装&#x2F;解开虚拟网卡docker0,fannel0的数据;<br>  三层的overlay网络;</li>
<li>组件： TUN设备是<strong>3层</strong>的虚拟网络设备 ; fannel0</li>
<li>劣势:  三次用户态和内核态切换 ; 性能差， 已弃用 <div style="text-align: center;"></div></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/5608425/65022322-50acc380-d963-11e9-8476-5e5ab22c8b4c.JPG" alt="flannel-udp">  图3. flannel-UDP模式</p>
<p><img src="https://user-images.githubusercontent.com/5608425/68364150-16dc8a00-0168-11ea-8281-272b274fdfae.jpg" alt="flannel-udp">  图4. flannel-UDP模式</p>


<ol start="2">
<li><strong>flannel-vxlan模式(两层虚拟网络)</strong></li>
</ol>
<ul>
<li>VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN<br>模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可<br>以）之间，可以像在同一个局域网（LAN）里那样自由通信</li>
<li>组件：<br>  VTEP（VXLAN Tunnel End Point）设备; fannel.1;<br>  组成一个虚拟的<strong>两层</strong>网络</li>
<li>优势：<br>  进行封装和解封装的对象，是二层数据帧（Ethernet frame）;<br>  而且这个工作的执行流程，全部是在内核里完成的（因为VXLAN本身就是内核中的一个模块）;<br>  主流的网络容器方案。<div style="text-align: center;"></div></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/5608425/65022323-51455a00-d963-11e9-9442-d4f1b84ecce5.JPG" alt="flannel-vxlan">   图5. flannel-vxlan模式</p>
<p><img src="https://user-images.githubusercontent.com/5608425/68364151-16dc8a00-0168-11ea-8935-445ad9c6b456.jpg" alt="flannel-vxlan"> 图6. flannel-vxlan模式</p>


<h5><span id="222-纯3层网络方案">2.2.2 纯3层网络方案</span><a href="#222-纯3层网络方案" class="header-anchor">#</a></h5><ol>
<li><strong>Flannel host-gw模式</strong></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/5608425/68364149-1643f380-0168-11ea-80ef-c0b218278dec.jpg" alt="flannel-host-gw"></p>
<ul>
<li><p>host-gw 模式工作原理：<br>其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0&#x2F;24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。<br>也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。</p>
</li>
<li><p>核心<br>路由规则：<br>&lt;目的容器 IP 地址段&gt; via &lt;网关的 IP 地址&gt; dev eth0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">...</span><br><span class="line">10.244.1.0/24 via 10.168.0.3 dev eth0</span><br></pre></td></tr></table></figure>
</li>
<li><p>优势：<br>根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。</p>
</li>
</ul>
<ol start="2">
<li><strong>Calico</strong></li>
</ol>
<ul>
<li><p>原理:<br>基于iptable&#x2F;linux kernel包转发;<br>根据iptables规则进行路由转发;<br>非overlay, Calico 没有使用 CNI 的网桥模式;</p>
</li>
<li><p>核心<br>路由规则：<br>&lt;目的容器 IP 地址段&gt; via &lt;网关的 IP 地址&gt; dev eth0</p>
</li>
<li><p>组件:<br>路由规则; iptables的配置组件Felix;<br>路由广播组件BGP Speaker;</p>
</li>
</ul>
<ol start="3">
<li><strong>Host Network模式</strong><br>容器的网络和宿主机的网络打平，在同一层;<br>underlay方案;</li>
</ol>
<h5><span id="223-总结">2.2.3 总结:</span><a href="#223-总结" class="header-anchor">#</a></h5><ul>
<li>flannel-UDP模式和flannel-vxlan模式都可以称作”隧道”机制；都是是overlay的。</li>
<li>普适性最强 flannel-VxLan (OpenShift使用)</li>
<li>二层可直连可选用Calico &#x2F; Flannel host-gw</li>
</ul>
<table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">overlay（隧道）</th>
<th align="center">underlay（路由）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">L2</td>
<td align="center">Vxlan(大二层 通讯双方在同一逻辑网段内)</td>
<td align="center">IPvlan L2模式<br> Macvlan <a href="https://larioy.gst.monster/2021/09/06/k8s-ji-chong-cni-fang-an-jie-xi/macvlan-yu-ipvlan/macvlan-he-ipvlan-jie-shao/">9</a></td>
</tr>
<tr>
<td align="center">L3</td>
<td align="center">flannel-UDP（类似L2 overlay， 在节点上增加一个网关）</td>
<td align="center">IPvlan L3模式<br> flannel host-gw（不能跨两层网络） <br> Calico BGP组网方式（大三层）</td>
</tr>
</tbody></table>
<h1><span id="三-pod与service之间的网络">三. Pod与Service之间的网络</span><a href="#三-pod与service之间的网络" class="header-anchor">#</a></h1><p><a href="../../../../2019/11/04/k8sService/">Kubernetes服务</a></p>
<h1><span id="四-internet与service之间的网络">四. Internet与Service之间的网络</span><a href="#四-internet与service之间的网络" class="header-anchor">#</a></h1><h3><span id="41-service到internet">4.1 Service到Internet</span><a href="#41-service到internet" class="header-anchor">#</a></h3><p>iptables执行源NAT( SNAT )</p>
<h3><span id="42-internet到service">4.2 Internet到Service</span><a href="#42-internet到service" class="header-anchor">#</a></h3><p><a href="../../../../2019/11/04/k8sService/">Kubernetes服务</a></p>
<h1><span id="五-网络隔离-多租户">五. 网络隔离 多租户</span><a href="#五-网络隔离-多租户" class="header-anchor">#</a></h1><p>NetworkPolicy</p>
<h1><span id="六-访问公网99">六. 访问公网[99]</span><a href="#六-访问公网99" class="header-anchor">#</a></h1><h3><span id="通过nat访问公网">通过NAT访问公网</span><a href="#通过nat访问公网" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/08/23/k8sNetwork/internet-access.jpg" class title="通过NAT访问公网">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><p>1.<a href="https://edu.aliyun.com/lesson_1651_13087#_13087">第13 章 ： Kubernetes网络概念及策略控制</a>  CNCF × Alibaba 云原生技术公开课</p>
</li>
<li><p>2.<a href="https://blog.csdn.net/hxpjava1/article/details/79566192">calico网络原理及与flannel对比</a></p>
</li>
<li><p>3.<a href="https://mp.weixin.qq.com/s/GQc8XPV4MaCWiTcN2wVzbw">Kubernetes CNI网络最强对比：Flannel、Calico、Canal和Weave</a></p>
</li>
<li><p>8.<a href="https://mp.weixin.qq.com/s/spw8fHkIjiyf4kg5RQIL_w">K8s网络模型</a>  阿里 加多  ***</p>
</li>
<li><p>9.<a href="https://larioy.gst.monster/2021/09/06/k8s-ji-chong-cni-fang-an-jie-xi/macvlan-yu-ipvlan/macvlan-he-ipvlan-jie-shao/">macvlan和ipvlan介绍及在k8s中的使用</a></p>
</li>
<li><p>趣谈网络协议  刘超<br>+《30容器网络之Flannel：每人一亩三分地》<br>+《31容器网络之Calico：为高效说出善意的谎言》</p>
</li>
<li><p>深入剖析Kubernetes  张磊<br>+《32  浅谈容器网络》<br>+《33  深入解析容器跨主机网络》<br>+《34  Kubernetes网络模型与CNI网络插件》<br>+《35  解读Kubernetes三层网络方案》</p>
</li>
<li><p>99.<a href="https://www.aliyundrive.com/s/dXxngxjTkZE">K8S在UCloud内部的应用-高鹏</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内存管理</title>
    <url>/www6vHomeHexo/2019/08/23/linuxMemory/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>

<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63573728-b00ff180-c5b8-11e9-9038-4cda1cd99148.jpg" alt="进程空间管理-64位"> 进程空间管理-64位</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63572472-873a2d00-c5b5-11e9-9253-5282537c00b2.JPG" alt="NUMA架构下的物理内存管理">    NUMA架构下的物理内存管理</p>
</div>

<blockquote>
<p>Linux中的内存管理的“页”大小为4KB。把所有的空闲页分组为11个页块链表，每个块链表分别包含很多个大小的页块，有1、2、4、8、16、32、64、128、256、512和1024个连续页的页块。最大可以申请1024个连续页，对应4MB大小的连续内存。每个页块的第一个页的物理地址是该页块大小的整数倍。</p>
</blockquote>
<div style="text-align: center;">	

<p><img src="https://user-images.githubusercontent.com/5608425/63574111-9d49ec80-c5b9-11e9-8e74-42248f7b2757.jpg" alt="用户态内存映射-MMAP">   用户态内存映射-MMAP</p>
<p><img src="https://user-images.githubusercontent.com/5608425/63574862-58bf5080-c5bb-11e9-8fcc-f342ecbb7f76.jpg" alt="内核态内存映射">  内核态内存映射</p>
</div>


<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>趣谈Linux操作系统 - 22-进空间管理：项目组还可以自行布置会议室</a>  刘超</li>
<li><a href>趣谈Linux操作系统 - 23-物理内存管理（上）：会议室管理员如何分配会议室？</a>   刘超</li>
<li><a href>趣谈Linux操作系统 - 25-用户态内存映射：如何找到正确的会议室？</a>  刘超</li>
<li><a href>趣谈Linux操作系统 - 26-内核态内存映射：如何找到正确的会议室？</a>   刘超</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>内存管理</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 进程</title>
    <url>/www6vHomeHexo/2019/08/22/linuxProcess/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63564515-e6894480-c597-11e9-90c2-eba751ad0c08.jpg" alt="进程执行"> 进程执行</p>
</div>

<h2><span id="内核初始化">内核初始化</span><a href="#内核初始化" class="header-anchor">#</a></h2><ul>
<li>Systemd 1号进程在用户态将运行一个用户进程, 1号进程和子进程形成一棵进程树。</li>
<li>内核态2号进程： 使用 kernel_thread 函数创建进程。  线程 &#x3D;&#x3D; 轻量级进程</li>
</ul>
<h2><span id="进程间通信">进程间通信</span><a href="#进程间通信" class="header-anchor">#</a></h2><ul>
<li>管道，命令行中常用的模式</li>
<li>消息队列其实很少使用，因为有太多的用户级别的消息队列，功能更强大。</li>
<li>共享内存加信号量是常用的模式。这个需要牢记，常见到一些知名的以C语言开发的开源软件都会用到<br>它。</li>
<li>信号更加常用，机制也比较复杂。</li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ul>
<li><a href>趣谈Linux操作系统 - 08_内核初始化：生意做大了就得成立公</a>    刘超</li>
<li><a href>趣谈Linux操作系统 - 10_进程：公司接这么多项目，如何管</a>   刘超</li>
<li><a href>趣谈Linux操作系统 - 36-进程间通信：遇到大项目需要项目组之间的合作才行</a> 刘超</li>
</ul>
]]></content>
      <categories>
        <category>linux</category>
        <category>kernel</category>
        <category>进程</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx总结</title>
    <url>/www6vHomeHexo/2019/08/22/nginx/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#nginx%E6%80%BB%E7%BB%93">Nginx总结</a></li>
<li><a href="#nginx%E6%9E%B6%E6%9E%84">Nginx架构</a></li>
<li><a href="#nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86">Nginx反向代理</a><ul>
<li><a href="#%E7%B1%BB%E5%9E%8B">类型</a></li>
<li><a href="#%E5%8F%AF%E6%89%A9%E5%B1%95%E7%AB%8B%E6%96%B9%E4%BD%93">可扩展立方体</a></li>
<li><a href="#%E5%A4%9A%E7%A7%8D%E5%8D%8F%E8%AE%AE%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86">多种协议反向代理</a></li>
<li><a href="#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E6%B5%81%E7%A8%8B">反向代理流程</a></li>
</ul>
</li>
<li><a href="#%E8%8A%82%E7%82%B9%E7%83%AD%E6%9B%B4%E6%96%B0">节点热更新</a><ul>
<li><a href="#master%E8%8A%82%E7%82%B9%E7%83%AD%E6%9B%B4%E6%96%B0">master节点热更新</a></li>
<li><a href="#worker%E8%8A%82%E7%82%B9%E7%83%AD%E6%9B%B4%E6%96%B0">worker节点热更新</a></li>
</ul>
</li>
<li><a href="#%E5%9F%9F%E5%90%8D%E8%BD%AC%E5%8F%91%E5%88%B0%E5%85%B6%E4%BB%96%E5%9F%9F%E5%90%8D2">域名转发到其他域名[2]</a></li>
<li><a href="#%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD">文件下载</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="nginx总结">Nginx总结</span><a href="#nginx总结" class="header-anchor">#</a></h2><p><img src="https://user-images.githubusercontent.com/5608425/64508349-cedbeb00-d30f-11e9-836c-2f920725e1bb.jpg" alt="Nginx总结"></p>
<h2><span id="nginx架构">Nginx架构</span><a href="#nginx架构" class="header-anchor">#</a></h2><ol>
<li>共享内存 Slab<br>分页 4K， 8K， 16K</li>
</ol>
<h2><span id="nginx反向代理">Nginx反向代理</span><a href="#nginx反向代理" class="header-anchor">#</a></h2><h3><span id="类型">类型</span><a href="#类型" class="header-anchor">#</a></h3><ul>
<li>带权重的round-robin算法是基础 </li>
<li>hash负载均衡算法<br> ip-hash算法 -&gt; real-ip<br> hash算法 -&gt; 自定义可以hash的参数（比如?userName）<blockquote>
<p>问题: 如果有upstream的机器宕机， hash算法还会路由到这台机器<br>  解决方案：使用一致性hash(consistent),hash 环</p>
</blockquote>
</li>
<li>least-connection算法， 如果所有节点的connection都一致，<br> 会退化成为round-robin算法。</li>
</ul>
<h3><span id="可扩展立方体">可扩展立方体</span><a href="#可扩展立方体" class="header-anchor">#</a></h3><ol>
<li>X-axis 基于round-robin或者least-connected算法分发请求 -&gt; 相对简单</li>
<li>Y-axis 基于URL对功能进行分发。 -&gt; 相对复杂</li>
<li>Z-axis 将用户IP地址或者其他信息映射到某个特定的服务或者集群 -&gt; 相对简单</li>
</ol>
<h3><span id="多种协议反向代理">多种协议反向代理</span><a href="#多种协议反向代理" class="header-anchor">#</a></h3><ol>
<li>tcp udp 透传</li>
<li>http -&gt; memcached , scgi, fastcgi, uwsgi, grpc, http, websocket</li>
</ol>
<h3><span id="反向代理流程">反向代理流程</span><a href="#反向代理流程" class="header-anchor">#</a></h3><p>修改发送到upstream机器的请求的nginx指令。</p>
<h2><span id="节点热更新">节点热更新</span><a href="#节点热更新" class="header-anchor">#</a></h2><h3><span id="master节点热更新">master节点热更新</span><a href="#master节点热更新" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/08/22/nginx/master-graceful-showdown.jpg" class>

<h3><span id="worker节点热更新">worker节点热更新</span><a href="#worker节点热更新" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/08/22/nginx/worker-graceful-showdown1.jpg" class>
<img src="/www6vHomeHexo/2019/08/22/nginx/worker-graceful-showdown.jpg" class>

<h2><span id="域名转发到其他域名2">域名转发到其他域名[2]</span><a href="#域名转发到其他域名2" class="header-anchor">#</a></h2><ul>
<li>return 指令</li>
<li>rewrite</li>
<li>proxy_pass</li>
</ul>
<h2><span id="文件下载">文件下载</span><a href="#文件下载" class="header-anchor">#</a></h2><p>nginx.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">location /userlab.dat &#123;</span><br><span class="line">    charset  gbk;</span><br><span class="line">    # alias /home/hp/home/frontend/indicator/userlab.dat;</span><br><span class="line"></span><br><span class="line">    root /home/cms/indicator;</span><br><span class="line"></span><br><span class="line">    if ($request_filename ~* ^.*?\.(txt)$)&#123;</span><br><span class="line">    add_header Content-Disposition &#x27;attachment&#x27;;</span><br><span class="line">    add_header Content-Type: &#x27;APPLICATION/OCTET-STREAM&#x27;;&#125;</span><br><span class="line"></span><br><span class="line">    autoindex on;</span><br><span class="line">    autoindex_exact_size   off;</span><br><span class="line">    autoindex_localtime    on;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651010416&idx=4&sn=dfa07f0e065d273b028e662e87e780ff&chksm=bdbecd238ac9443511c4e7eadf9e59cc9139fac25c52b44f7a93787b940826c5f61f06e10224&scene=27#wechat_redirect">深入Nginx 思维导图</a></p>
</li>
<li><p><a href="https://blog.csdn.net/yeguxin/article/details/94020476">nginx配置域名转发到其他域名的几种方法</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>iptables总结</title>
    <url>/www6vHomeHexo/2019/08/19/iptables/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-overview">一. Overview</span><a href="#一-overview" class="header-anchor">#</a></h2><div style="width: 70%; height: 70%">
    
<p><img src="https://user-images.githubusercontent.com/5608425/63585097-ebb6b580-c5d0-11e9-8d8e-dcb116d4ca65.jpg" alt="iptables"></p>
</div>

<p><img src="https://user-images.githubusercontent.com/5608425/63585098-ebb6b580-c5d0-11e9-88a3-dbb557f0d838.jpg" alt="iptables-overview2"></p>
<p><img src="https://user-images.githubusercontent.com/5608425/63585099-ec4f4c00-c5d0-11e9-936d-9057cfe4f75e.jpg" alt="iptables-overview3"></p>
<h2><span id="二-规则">二. 规则</span><a href="#二-规则" class="header-anchor">#</a></h2><p><img src="https://user-images.githubusercontent.com/5608425/63585100-ec4f4c00-c5d0-11e9-8518-64669984ff82.png" alt="iptables-rule"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/">理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持</a></li>
<li><a href="https://wangchujiang.com/linux-command/c/iptables.html">iptables</a></li>
<li><a href="http://www.zsythink.net/archives/1199/">iptables概念</a></li>
<li><a href="https://www.cnblogs.com/frankb/p/7427944.html">iptables 从入门到应用</a></li>
<li><a href>趣谈Linux操作系统 - 46-发送网络包（下）：如何表达我们想让合作伙伴做什么</a>  刘超</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>网络</category>
        <category>iptables</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP和HTTPS总结</title>
    <url>/www6vHomeHexo/2019/08/14/https/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<div style="width: 70%; height: 70%">
<img src="/www6vHomeHexo/2019/08/14/https/https.jpg" class>
<img src="/www6vHomeHexo/2019/08/14/https/https1.jpg" class title="HTTPS总结">
</div>


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>&lt;&lt;趣谈网络协议 - 第15讲 HTTPS协议：点外卖的过程原来这么复杂&gt;&gt; 刘超</li>
<li><a href="https://github.com/bagder/http2-explained/">http2</a>  未</li>
<li><a href="https://github.com/bagder/http3-explained/">http3</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>http</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes开放接口</title>
    <url>/www6vHomeHexo/2019/08/11/k8sInterface/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-开放接口">一. 开放接口</span><a href="#一-开放接口" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/08/11/k8sInterface/k8sInterface.jpg" class title="Kubenetes开放接口">


<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65022324-51455a00-d963-11e9-9338-26675af8d3b7.JPG" alt="k8s-interface"><br>Kubenetes开放接口</p>
</div>



<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://feisky.xyz/kubernetes-handbook/network/cni/">CNI (Container Network Interface)</a></li>
<li><a href="https://www.jianshu.com/p/62e71584d1cb">容器开放接口规范（CRI OCI CNI）</a></li>
<li>&lt;&lt;深入剖析Kubernetes - 09  从容器到容器云：谈谈Kubernetes的本质&gt;&gt; 张磊</li>
</ol>
<h2><span id="self">self</span><a href="#self" class="header-anchor">#</a></h2><ol>
<li><a href="../../../../2019/08/23/k8sNetwork/">Kubernetes网络</a></li>
<li><a href="../../../../2019/09/01/k8sStorage/">Kubernetes存储</a></li>
<li><a href="../../../../2019/11/19/k8sRuntime/">Kubernetes Runtime</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux性能优化</title>
    <url>/www6vHomeHexo/2019/08/08/linuxPerformance/</url>
    <content><![CDATA[<p hidden></p>

<span id="more"></span>

<h3><span id="最常用的cpu工具">最常用的cpu工具</span><a href="#最常用的cpu工具" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/08/08/linuxPerformance/cpu-tool.jpg" class title="最常用的cpu工具">

<h3><span id="memory-tool">memory tool</span><a href="#memory-tool" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/65083667-b76dc380-d9db-11e9-8e74-3e80c8692fa8.jpg" alt="memory"></p>
<h3><span id="io-tool">IO tool</span><a href="#io-tool" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/65083666-b6d52d00-d9db-11e9-9b57-e6a9f74235be.JPG" alt="io"></p>
<blockquote>
<p>Buffer是对磁盘数据的缓存，而Cache是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。</p>
</blockquote>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>&lt;&lt;Linux性能优化实战  11 - 套路篇：如何迅速分析出系统CPU的瓶颈在哪里？&gt;&gt; 倪朋飞</li>
<li>&lt;&lt;Linux性能优化实战  21 - 套路篇：如何“快准狠”找到系统内存的问题？&gt;&gt; 倪朋飞</li>
<li>&lt;&lt;Linux性能优化实战  30 - 套路篇：如何迅速分析出系统IO的瓶颈在哪里？&gt;&gt; 倪朋飞</li>
<li>&lt;&lt;Linux性能优化实战  16 - 基础篇：怎么理解内存中的Buffer和Cache？&gt;&gt; 倪朋飞</li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>性能优化</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>算法和数据结构</title>
    <url>/www6vHomeHexo/2019/08/07/algorithm/</url>
    <content><![CDATA[<p hidden></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2019/08/07/algorithm/algorithm.jpg" class title="算法和数据结构总结">

<h2><span id="一-master-thereom-主定律">一. Master Thereom  主定律</span><a href="#一-master-thereom-主定律" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64620811-25801c80-d417-11e9-88aa-182269f0bc02.JPG" alt="图1.主定律在常用算法中的应用"><br>图1.主定律在常用算法中的应用</p>
</div>

<h2><span id="二-数据结构操作-on">二. 数据结构操作 O(n)</span><a href="#二-数据结构操作-on" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64620788-1a2cf100-d417-11e9-82d3-7ca7864f6129.JPG" alt="图2.数据结构操作"><br>图2.数据结构操作</p>
</div>

<blockquote>
<h4><span id="线性表">线性表:</span><a href="#线性表" class="header-anchor">#</a></h4><pre><code>   Array, Stack, Queue, Singly-Linked List, Doubly-Linked List
</code></pre>
</blockquote>
<blockquote>
<h4><span id="skip-list-ampamp-binary-search-tree">Skip List &amp;&amp; Binary Search Tree:</span><a href="#skip-list-ampamp-binary-search-tree" class="header-anchor">#</a></h4><pre><code>   + Average &amp;&amp; Worst 相同
   + Redis的SortedSet用Skip List实现
</code></pre>
</blockquote>
<blockquote>
<h4><span id="二叉搜索树">二叉搜索树:</span><a href="#二叉搜索树" class="header-anchor">#</a></h4><pre><code>   + 左子树上所有结点的值均小于它的根结点的值； 
     右子树上所有结点的值均大于它的根结点的值； 
     它的左、右子树也分别为二叉搜索树。
   + 最差时间复杂度的二叉搜索树是O(n)，退化成链表;
     所以提出了平衡二叉搜索树， 最差时间复杂度是O(logn)。
     平衡二叉搜索树: Red-Black Tree, Splay Tree, AVL Tree
</code></pre>
</blockquote>
<blockquote>
<h4><span id="b-tree">B-tree:</span><a href="#b-tree" class="header-anchor">#</a></h4><pre><code>   多路搜索树，并不是二叉的
</code></pre>
</blockquote>
<h2><span id="三-数组排序算法-on">三. 数组排序算法 O(n)</span><a href="#三-数组排序算法-on" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/08/07/algorithm/arraySortAlg.JPG" class title="图3.数组排序算法">


<p><strong>稳定排序</strong>:有两个排序关键字的时候，稳定排序可以让第一个关键字排序的结果服务于第二个关键字排序中数值相等的那些数. [5][7]</p>
<h2><span id="四-x3d-二-三">四 &#x3D;  二 +  三</span><a href="#四-x3d-二-三" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64620785-19945a80-d417-11e9-97bf-5af847bdbd5a.png" alt="图4.BigO总结"><br>图4.BigO总结</p>
</div>

<h2><span id="五-堆实现-on">五. 堆实现 O(n)</span><a href="#五-堆实现-on" class="header-anchor">#</a></h2><div style="text-align: center;">
  
<p><img src="https://user-images.githubusercontent.com/5608425/64620806-244eef80-d417-11e9-9b17-39538ad19573.JPG" alt="图5.堆实现"><br>图5.堆实现</p>
</div>

<blockquote>
<h4><span id="堆实现">堆实现:</span><a href="#堆实现" class="header-anchor">#</a></h4><p>Binary， Binomial， Fibonacci， Strict Fibonacci（性能好，用的比较多）</p>
</blockquote>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">算法大牛的git</a></li>
<li><a href="http://www.codeceo.com/article/algorithm-complexity-table.html">每个程序员都应该收藏的算法复杂度速查表</a> good</li>
<li><a href="https://yq.aliyun.com/articles/38838">从头到尾彻底解析Hash表算法</a></li>
<li><a href="https://www.bigocheatsheet.com/">big O</a> good</li>
<li><a href="http://dongxicheng.org/structure/sort/">董的博客 - 算法之排序算法</a>   稳定排序，原地排序</li>
<li><a href="https://www.cs.usfca.edu/~galles/visualization/Algorithms.html">Data Structure Visualizations</a>  good</li>
<li><a href="https://mp.weixin.qq.com/s/UuMzvp3hoqRx5j4slvpPUw">五分钟小知识：为什么要分稳定排序和非稳定排序？</a></li>
</ol>
]]></content>
      <categories>
        <category>基础</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP流控和拥塞控制</title>
    <url>/www6vHomeHexo/2019/08/07/tcpUdpControlCongestion/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/08/07/tcpUdpControlCongestion/tcp&udp.jpg" class title="TCP流控和拥塞控制">

<h3><span id="tcp拥塞控制">TCP拥塞控制</span><a href="#tcp拥塞控制" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/63579884-31ba4c00-c5c6-11e9-97d1-28defaa3286f.jpg" alt="TCP拥塞控制">   </p>
<h3><span id="tcp拥塞控制-快速重传">TCP拥塞控制-快速重传</span><a href="#tcp拥塞控制-快速重传" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/63579883-31ba4c00-c5c6-11e9-83ca-78a892b3243d.jpg" alt="TCP拥塞控制-快速重传">   </p>
<h3><span id="快速重传">快速重传</span><a href="#快速重传" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/63579882-3121b580-c5c6-11e9-8483-c53b4ec03fe6.jpg" alt="快速重传">  </p>
<blockquote>
<p>拥塞窗口是为了怕把网络塞满，在出现丢包的时候减少发送速度.<br>  滑动窗口就是为了怕把接收方塞满，而控制发送速度.</p>
</blockquote>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://coolshell.cn/articles/11609.html">TCP 的那些事儿（下）</a></li>
<li><a href="https://www.kancloud.cn/digest/wireshark/62473">（四）：网络性能排查之TCP重传与重复ACK</a></li>
<li><a href="http://www.dataguru.cn/article-12653-1.html">怎么让不可靠的UDP可靠？</a></li>
<li><a href>趣谈Linux操作系统 - 45-发送网络包（上）：如何表达我们想让合作伙伴做什么？</a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>网络</category>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker总结</title>
    <url>/www6vHomeHexo/2019/08/05/docker/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/08/05/docker/docker.jpg" class title="docker总结">


<h2><span id="一-docker全景图">一. docker全景图</span><a href="#一-docker全景图" class="header-anchor">#</a></h2><div style="text-align: center;">
    
<p><img src="https://user-images.githubusercontent.com/5608425/64622251-99232900-d419-11e9-8f56-4f88831828ad.JPG" alt="docker-overview"><br>docker全景图</p>
</div>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$find  /var/lib/docker/  -name image         /// [只读层]</span><br><span class="line">$find  /var/lib/docker/  -name containers    /// containers 目录: 体积大说明日志输出量大</span><br><span class="line">$find  /var/lib/docker -name diff            /// [可读写层] diff 子目录: 容器可写层，体积大说明可写层数据量大(程序在容器里写入文件)。 </span><br><span class="line">$find  /var/lib/docker/  -name mnt           ///  mnt 子目录: 联合挂载点，内容为容器里看到的内容，即包含镜像本身内容以及可写层内容</span><br></pre></td></tr></table></figure>

<h2><span id="二-docker-namespace">二. docker namespace</span><a href="#二-docker-namespace" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-25-152-177 ~]# docker inspect --format &#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27; 23dfea495611</span><br><span class="line">23777</span><br><span class="line">[root@10-25-152-177 ~]# ls -l /proc/23777/ns</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 ipc -&gt; ipc:[4026532754]</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 mnt -&gt; mnt:[4026533007]</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 net -&gt; net:[4026532757]</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 pid -&gt; pid:[4026533009]</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx 1 1337 1337 0 Aug  8 16:05 uts -&gt; uts:[4026533008]</span><br></pre></td></tr></table></figure>

<h2><span id="三-docker-run的背后-6">三. docker run的背后 [6]</span><a href="#三-docker-run的背后-6" class="header-anchor">#</a></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker run -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure>

<p>When you run this command, the following happens (assuming you are using the default registry configuration):</p>
<ol>
<li><p>If you do not have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually.</p>
</li>
<li><p><strong>Docker creates a new container, as though you had run a docker container create command manually.</strong></p>
</li>
<li><p><strong>Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem.</strong></p>
</li>
<li><p>Docker creates a network interface to connect the container to the default network, since you did not specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine’s network connection.</p>
</li>
<li><p>Docker starts the container and executes &#x2F;bin&#x2F;bash. Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while the output is logged to your terminal.</p>
</li>
<li><p>When you type exit to terminate the &#x2F;bin&#x2F;bash command, the container stops but is not removed. You can start it again or remove it.</p>
</li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p>深入剖析Kubernetes  张磊</p>
<ol>
<li>《05  白话容器基础（一）：从进程说开去》 </li>
<li>《06  白话容器基础（二）：隔离与限制》 张磊</li>
<li>《07  白话容器基础（三）：深入理解容器镜像》</li>
<li>《08  白话容器基础（四）：重新认识Docker容器》</li>
</ol>
<hr>
<ol start="5">
<li><a href="https://tencentcloudcontainerteam.github.io/2019/06/08/kubernetes-best-practice-handle-disk-full/">kubernetes 最佳实践：处理容器数据磁盘被写满</a>  腾讯容器团队</li>
<li><a href="https://docs.docker.com/engine/docker-overview/">Docker overview</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker网络</title>
    <url>/www6vHomeHexo/2019/08/04/dockerNetwork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2019/08/04/dockerNetwork/docker-network.jpg" class title="docker网络">


<div style="text-align: center;">
![docker-bridge](https://user-images.githubusercontent.com/5608425/64622453-f4edb200-d419-11e9-9083-2af25a99b289.JPG)
图1.docker的bridge网桥

<p><img src="https://user-images.githubusercontent.com/5608425/64622455-f5864880-d419-11e9-8f24-c983a2bbed3b.JPG" alt="docker-overlay-gre"><br>图2.docker基于GRE的overlay</p>
</div>

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《Docker+容器与容器云》第4章 浙江大学SEL实验室</li>
<li>《深入剖析Kubernetes-32  浅谈容器网络》张磊</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch 分布式集群</title>
    <url>/www6vHomeHexo/2019/08/03/elasticsearchDistributed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="es的集群">ES的集群</span><a href="#es的集群" class="header-anchor">#</a></h2><h5><span id="分片-shard-1">分片 shard [1]</span><a href="#分片-shard-1" class="header-anchor">#</a></h5><ul>
<li>分片 shard<br>最小的工作单元<br>是一个lucene的index。<br>在lucene中，单个倒排索引文件称为segment，多个segments汇总在一起称为lucence的index</li>
<li>主分片（Primary Shard）<br>主分片数设置好后不能变更，如要修改, 需要重建索引。</li>
<li>副本分片（Replica Shard）<br>数据可用性<br>副本数可以动态调整， 提高读取的吞吐量</li>
</ul>
<h5><span id="节点类型2">节点类型[2]</span><a href="#节点类型2" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">节点类型</th>
<th align="center">配置参数</th>
<th align="center">默认值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">master <br> 要部署多个master节点</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">master eligible <br> master的候选节点</td>
<td align="center">node.master</td>
<td align="center">true</td>
</tr>
<tr>
<td align="center">data <br> 保存分片数据</td>
<td align="center">node.data</td>
<td align="center">true <br> 默认是数据节点，可以禁止掉</td>
</tr>
<tr>
<td align="center">ingest</td>
<td align="center">node.ingest</td>
<td align="center">true</td>
</tr>
<tr>
<td align="center">coordinating only</td>
<td align="center">无</td>
<td align="center">每个节点默认都是coordinating节点。设置其他类型全部为false。</td>
</tr>
<tr>
<td align="center">machine learning</td>
<td align="center">node.ml</td>
<td align="center">true（需enable x-pack）</td>
</tr>
</tbody></table>
<ul>
<li>一个节点默认情况下是Master eligible, data, ingest的node</li>
</ul>
<h5><span id="集群健康状态-1">集群健康状态 [1]</span><a href="#集群健康状态-1" class="header-anchor">#</a></h5><ul>
<li>三种颜色<ul>
<li>绿色： 主分片和副本分片都可用</li>
<li>黄色： 主分片可用， 部分副本分片不可用 </li>
<li>红色： 部分主分片不可用</li>
</ul>
</li>
</ul>
<h5><span id="脑裂问题-2">脑裂问题 [2]</span><a href="#脑裂问题-2" class="header-anchor">#</a></h5><ul>
<li>7.0之前的版本<ul>
<li>只有在Master eligible节点数大于quorum时, 才能进行选举</li>
<li>quorum &#x3D; master&#x2F;2 + 1</li>
<li>3个Master eligible时, 设置discovery.zen.minimum_master_nodes为2, 可以避免脑裂</li>
</ul>
</li>
<li>7.0开始<ul>
<li>无需配置minimum_master_nodes</li>
</ul>
</li>
</ul>
<h5><span id="doc到shard的路由算法-3">doc到shard的路由算法 [3]</span><a href="#doc到shard的路由算法-3" class="header-anchor">#</a></h5><ul>
<li>shard &#x3D; hash(_ routing) + number_of_primary_shards<br>primary数, 不能随意修改的根本原因</li>
</ul>
<h2><span id="写流程-45">写流程 [4][5]</span><a href="#写流程-45" class="header-anchor">#</a></h2><div style="text-align: center; width: 70%; height: 70%">

<p><img src="https://user-images.githubusercontent.com/5608425/65385405-d7491280-dd60-11e9-9a73-895afca03bb8.png" alt="Elasticsearch 数据写入"><br>Elasticsearch 数据写入</p>
</div>

<ul>
<li><p>write：文档数据到内存缓存，并存到 translog</p>
<ul>
<li>translog<br>高版本， tanslog默认落盘.<br>每个shard有一个translog</li>
</ul>
</li>
<li><p>refresh<br>内存缓存中的文档数据，到文件缓存中的segment<br><strong>此时可以被搜到</strong></p>
</li>
<li><p>flush </p>
<ul>
<li>缓存中的 segment 文档数据写入到磁盘</li>
<li>触发条件<br>默认30min调用一次<br>或者  tanlogs写满(512MB)       </li>
<li>过程<ol>
<li>调用refresh</li>
<li>调用fsync， segments写入磁盘</li>
<li>清空translog</li>
</ol>
</li>
</ul>
</li>
<li><p>merge</p>
<ul>
<li>segments 定期合并<br>自动进行merge操作</li>
<li>真正删除已经惰性删除的文档</li>
</ul>
</li>
<li><p>refresh过程	</p>
<ul>
<li>index buffer 写入segment的过程<br>index buffer会被清空, translog不会清空。<br>断电后, 可以从tanslog中做索引的recover</li>
<li>不会执行fsync操作</li>
<li>一秒一次</li>
<li><strong>refresh之后， 数据就可以被搜索到</strong></li>
</ul>
</li>
</ul>
<h5><span id="倒排索引的不可变性">倒排索引的不可变性</span><a href="#倒排索引的不可变性" class="header-anchor">#</a></h5><ul>
<li>不可变性 <ul>
<li>不可变，一旦生产， 不可变更</li>
<li>好处<br>无需考虑并发写文件的问题<br>一旦读入内核的文件系统缓存，便流在那里<br>容易缓存、 数据可被压缩</li>
<li>坏处<br>让一个新的文档可以被搜索，需要重建整个索引</li>
</ul>
</li>
</ul>
<h2><span id="读流程-6">读流程 [6]</span><a href="#读流程-6" class="header-anchor">#</a></h2><h5><span id="两个阶段">两个阶段</span><a href="#两个阶段" class="header-anchor">#</a></h5><ul>
<li>Query</li>
<li>Fetch</li>
</ul>
<h5><span id="问题">问题</span><a href="#问题" class="header-anchor">#</a></h5><ul>
<li><p>性能问题<br>深度分页</p>
</li>
<li><p>相关性算分</p>
<ul>
<li>问题<br>数据量小，打分偏离</li>
<li>解决方案<ul>
<li>数据量小时， 主分片设置为1[有例子]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="排序-7">排序 [7]</span><a href="#排序-7" class="header-anchor">#</a></h2><ul>
<li>默认采用<strong>相关性算分</strong>对结果进行<strong>降序排序</strong></li>
<li>自定义排序<br>sorting参数</li>
<li>排序过程<ul>
<li>针对字段原始内容进行的<br><strong>倒排索引无法发挥作用</strong></li>
<li>需要正排索引<ul>
<li>DocValue<br>列式存储，对Text类型无效<br>默认开启，可以通过mapping设置关闭<br>如果重新打开，要重建索引<br><strong>明确不需要做排序及聚合分析</strong>           </li>
<li>Fielddata</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="并发读写乐观锁并发控制">并发读写(乐观锁并发控制)</span><a href="#并发读写乐观锁并发控制" class="header-anchor">#</a></h2><ul>
<li>doc不可变<br>更新文档时, doc标记为删除, 同时增加一个全新的doc<br>同时doc的version字段加1</li>
<li>内部版本控制<br>if_seq_no + if_primary_term</li>
<li>使用外部文档<br>version_type(external) + version</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>38丨分片与集群的故障转移</li>
<li>37丨集群分布式模型及选主与脑裂问题</li>
<li>39丨文档分布式存储</li>
<li><a href="https://mp.weixin.qq.com/s/BSjA_TBuapPHrE4COCp9VA">Elasticsearch 数据写入原理</a> </li>
<li>40丨分片及其生命周期</li>
<li>41丨剖析分布式查询及相关性算分</li>
<li>42丨排序及DocValues&amp;Fielddata</li>
<li>44丨处理并发读写操作</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch基础(ES)</title>
    <url>/www6vHomeHexo/2019/08/02/elasticsearch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="es的概念">ES的概念</span><a href="#es的概念" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">DB</th>
<th align="center">Elastic Search</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Table</td>
<td align="center">Index(Type)</td>
</tr>
<tr>
<td align="center">Row</td>
<td align="center">Document</td>
</tr>
<tr>
<td align="center">Column</td>
<td align="center">Field</td>
</tr>
<tr>
<td align="center">Schema</td>
<td align="center">Mapping</td>
</tr>
<tr>
<td align="center">SQL</td>
<td align="center">DSL</td>
</tr>
</tbody></table>
<h5><span id="倒排索引-3">倒排索引 [3]</span><a href="#倒排索引-3" class="header-anchor">#</a></h5><ul>
<li>倒排索引包括2部分<ul>
<li>单词词典(term dictionary)</li>
<li>倒排列表(posting list)<br>文档id: doc id<br>词频 TF<br>位置 Position<br>偏移 Offset</li>
</ul>
</li>
</ul>
<h5><span id="analysis-分词-4">analysis 分词 [4]</span><a href="#analysis-分词-4" class="header-anchor">#</a></h5><ul>
<li>Analyzer 分词器<ol>
<li>Character Filters<br>针对原始文本处理</li>
<li>Tokenizer<br>切分单词</li>
<li>Token Filters<br>将切分的单词进行加工</li>
</ol>
</li>
<li>中文分词器<ul>
<li>icu_analyzer</li>
<li>IK</li>
<li>THULAC</li>
</ul>
</li>
</ul>
<h5><span id="term搜索-vs-全文搜索5">term搜索 vs 全文搜索[5]</span><a href="#term搜索-vs-全文搜索5" class="header-anchor">#</a></h5><ul>
<li>term  不分词<ul>
<li>term 返回算分结果</li>
<li>keyword 完全匹配</li>
<li>query 转成 filter<br>忽略tf-idf计算, 避免相关性算分的开销<br>filter可以有效利用缓存</li>
</ul>
</li>
<li>全文索引  会分词<ul>
<li>索引和搜索时都会进行分词<br>查询时，会对输入的查询进行分词</li>
</ul>
</li>
</ul>
<p>function score query</p>
<h5><span id="索引更新策略">索引更新策略</span><a href="#索引更新策略" class="header-anchor">#</a></h5><ul>
<li>完全更新策略 -&gt; index </li>
<li>再合并策略</li>
<li>原地更新策略</li>
</ul>
<h5><span id="搜索模型">搜索模型</span><a href="#搜索模型" class="header-anchor">#</a></h5><ul>
<li><p>向量空间模型<br>相似性计算 tf, idf(5.0)  </p>
</li>
<li><p>概率检索模型<br>BM25(5.0之后)</p>
</li>
</ul>
<h5><span id="search-api-6">search api [6]</span><a href="#search-api-6" class="header-anchor">#</a></h5><ul>
<li>uri search<br>url中的查询参数</li>
<li>request body search<br>json DSL</li>
</ul>
<h5><span id="衡量相关性-information-retrieval-6-pic">衡量相关性 information retrieval [6] [pic]</span><a href="#衡量相关性-information-retrieval-6-pic" class="header-anchor">#</a></h5><ul>
<li>精确率(Presicion)<br>尽可能返回较少的无关文档</li>
<li>召回率(recall)<br>尽可能返回较多的相关文档</li>
<li>ranking<br>是否能按照相关度进行排序</li>
</ul>
<h2><span id="geoip">geoip</span><a href="#geoip" class="header-anchor">#</a></h2><div style="text-align: center; width: 70%; height: 70%">
    
<p><img src="https://user-images.githubusercontent.com/5608425/64664368-8d6b4d00-d481-11e9-88bd-1b4cbf99379c.JPG" alt="elastic geoip"><br>elastic geoip</p>
</div>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="3">
<li>12丨倒排索引介绍</li>
<li>13丨通过Analyzer进行分词</li>
<li>24丨基于词项和基于全文的搜索</li>
<li>14丨SearchAPI概览</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 总结</title>
    <url>/www6vHomeHexo/2019/07/29/streamingFlink/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E7%BB%84%E4%BB%B6">组件</a></li>
<li><a href="#runtime-6">Runtime [6]</a></li>
<li><a href="#%E6%A6%82%E5%BF%B5">概念</a></li>
<li><a href="#flink-table-store">Flink Table Store</a></li>
<li><a href="#flink-cdc">Flink CDC</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/07/29/streamingFlink/flink.jpg" class title="flink"> 


<h2><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/07/29/streamingFlink/flink-arch.png" class title="flink架构">


<h2><span id="runtime-6">Runtime [6]</span><a href="#runtime-6" class="header-anchor">#</a></h2><ul>
<li><p>JobMaster AM</p>
<ul>
<li>JobManager </li>
<li>ResourceManager</li>
<li>Dispatcher</li>
</ul>
</li>
<li><p>TaskManager </p>
<ul>
<li>Task Slot</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2019/07/29/streamingFlink/flink-runtime.png" class title="flink架构">


<h2><span id="概念">概念</span><a href="#概念" class="header-anchor">#</a></h2><ol>
<li>数据流图</li>
<li>并行度</li>
<li>算子链</li>
</ol>
<pre><code> (1)一对一(One-to-one，forwarding)
    这种关系类似于 Spark 中的窄依赖。
 (2)重分区(Redistributing)
    这种算子间的关系类似于 Spark 中的宽依赖。
</code></pre>
<ol start="4">
<li>执行图</li>
<li>任务(Tasks)和任务槽(Task Slots)</li>
</ol>
<h2><span id="flink-table-store">Flink Table Store</span><a href="#flink-table-store" class="header-anchor">#</a></h2><p><a href="https://zhuanlan.zhihu.com/p/575040340">Flink Table Store：流批一体存储</a></p>
<h2><span id="flink-cdc">Flink CDC</span><a href="#flink-cdc" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/code2one/p/10123112.html">Flink架构及其工作原理</a>  ***</li>
<li>&lt;&lt;Flink原理、实战与性能优化&gt;&gt;  张利兵</li>
<li><a href="http://ju.outofmemory.cn/entry/371335">Flink使用Broadcast State实现流处理配置实时更新</a></li>
<li><a href="https://www.iteblog.com/archives/2417.html">Apache Flink状态管理和容错机制介绍</a></li>
<li>尚硅谷 flink(Java) - bilibili </li>
<li><a href="https://www.infoq.cn/article/RWTM9o0SHHV3Xr8o8giT">Apache Flink 进阶（一）：Runtime 核心机制剖析</a> 阿里-高赟（云骞） ***</li>
<li><a href="https://www.infoq.cn/article/UXwLxU0D85E9eTU66E2A">数据实时化技术创新进展 | 一文览尽 Flink Forward Asia 2022 重磅干货内容</a>  *** 未</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio、Kubernetes和Spring Cloud中服务的比对</title>
    <url>/www6vHomeHexo/2019/07/20/istio-k8s-service/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">k8s-服务编排,资源调度</th>
<th align="center">istio-流量管理</th>
</tr>
</thead>
<tbody><tr>
<td align="center">定义</td>
<td align="center">Service<br> Deployment<br> Endpoint</td>
<td align="center">Service<br> 服务版本 <br> 服务实例</td>
</tr>
<tr>
<td align="center">入口</td>
<td align="center">NodePort<br> LoadBalance<br> Ingress</td>
<td align="center">Gateway（LoadBalance Service）</td>
</tr>
<tr>
<td align="center">出口</td>
<td align="center">Egress</td>
<td align="center">ServiceEntry</td>
</tr>
<tr>
<td align="center">服务发现</td>
<td align="center">DNS（CoreDNS）<br> apiserver + kube-proxy</td>
<td align="center"><br>Pilot + Envoy</td>
</tr>
<tr>
<td align="center">服务路由</td>
<td align="center"></td>
<td align="center">VirtualService</td>
</tr>
<tr>
<td align="center">熔断&amp;监控检查</td>
<td align="center">容器：存活探针<br>服务：就绪探针</td>
<td align="center">DestinationRule</td>
</tr>
<tr>
<td align="center">负载均衡</td>
<td align="center"></td>
<td align="center">DestinationRule</td>
</tr>
<tr>
<td align="center">生命周期管理<br>故障迁移，自愈<br>伸缩</td>
<td align="center">原生</td>
<td align="center">基于k8s</td>
</tr>
<tr>
<td align="center">有状态调度</td>
<td align="center">StatefulSet</td>
<td align="center">Operator（CRD+Controller）</td>
</tr>
<tr>
<td align="center">配置管理</td>
<td align="center">ConfigMap &amp; Secrets</td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><strong>灰度</strong></td>
<td align="center">Deployment:滚动升级<br>最佳实践：两个Deployment</td>
<td align="center">VirtualService</td>
</tr>
<tr>
<td align="center"><strong>多集群</strong></td>
<td align="center">Federation</td>
<td align="center">1. 多控制面<br> 2.集群感知单控制面(Split Horizon EDS)</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">spring Cloud-服务治理<br>（Chassis模式）</th>
<th align="center">spring Cloud alibaba-Paas全栈</th>
<th align="center">istio组件-流量管理<br>（SideCar模式）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">治理-Resilience &amp; Fault Tolerance</td>
<td align="center">Hystrix <br> 白盒,代码有侵入<br>熔断（有半开状态） <br>隔离仓</td>
<td align="center">sentinel</td>
<td align="center">Envoy<br>黑盒，代码无侵入<br> 异常点检查（逐出，重试）（无半开状态）<br>连接池</td>
</tr>
<tr>
<td align="center">监控-Distributed Tracing</td>
<td align="center">Sleuth&#x2F;zipkin</td>
<td align="center">zipkin</td>
<td align="center">Mixer</td>
</tr>
<tr>
<td align="center">监控-Centralized Metrics</td>
<td align="center">Spectator&#x2F;Atlas</td>
<td align="center"></td>
<td align="center">Mixer</td>
</tr>
<tr>
<td align="center">监控-Centralized Logging</td>
<td align="center">ELK</td>
<td align="center"></td>
<td align="center">Mixer</td>
</tr>
<tr>
<td align="center">流量管理-API Gateway</td>
<td align="center">Gateway&#x2F;Zuul</td>
<td align="center">gateway</td>
<td align="center">Gateway</td>
</tr>
<tr>
<td align="center">流量管理-Load Balancing</td>
<td align="center">OpenFeign&#x2F;Ribbon</td>
<td align="center">dubbo</td>
<td align="center">Pilot + Envoy</td>
</tr>
<tr>
<td align="center">治理-Service Discovery</td>
<td align="center">Eureka，Consul</td>
<td align="center">dubbo</td>
<td align="center">Pilot + Envoy xDS</td>
</tr>
<tr>
<td align="center">治理-Routing</td>
<td align="center">Zuul</td>
<td align="center">dubbo</td>
<td align="center">Pilot + Envoy xDS</td>
</tr>
<tr>
<td align="center">治理-Service-to-service calls</td>
<td align="center"></td>
<td align="center">dubbo</td>
<td align="center">Pilot</td>
</tr>
<tr>
<td align="center">治理-Configuration Management</td>
<td align="center">Config&#x2F;Consul</td>
<td align="center">Nacos</td>
<td align="center">Calley</td>
</tr>
<tr>
<td align="center"><strong>流量管理-故障注入</strong></td>
<td align="center">x</td>
<td align="center"></td>
<td align="center">iptables</td>
</tr>
<tr>
<td align="center"><strong>流量管理-灰度发布</strong></td>
<td align="center">Nepxion Discovery等 非原生</td>
<td align="center"></td>
<td align="center">原生支持</td>
</tr>
<tr>
<td align="center"><strong>流量管理-异地容灾</strong></td>
<td align="center">x</td>
<td align="center">x</td>
<td align="center">集群感知</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th>特性</th>
<th>Istio Gateway</th>
<th>阿里云Ingress Controller</th>
<th>NGINX Ingress Controller</th>
</tr>
</thead>
<tbody><tr>
<td>根据HTTP Header选择路由规则</td>
<td>支持</td>
<td>仅支持单个Header，不支持多个Header组合</td>
<td>不支持</td>
</tr>
<tr>
<td>Header规则支持正则表达式</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>服务之间设置权重拆分流量</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>Header和权重规则组合使用</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>路由规则检查</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>路由规则粒度</td>
<td>service下的不同pod</td>
<td>service</td>
<td>service</td>
</tr>
<tr>
<td>支持的协议</td>
<td>HTTP1.1&#x2F;HTTP2&#x2F;gRPC&#x2F;TCP<br>&#x2F;Websockets&#x2F;MongoDB</td>
<td>HTTP1.1&#x2F;HTTP2&#x2F;gRPC<br>&#x2F;TCP&#x2F;Websockets</td>
<td>HTTP1.1&#x2F;HTTP2&#x2F;gRPC<br>&#x2F;TCP&#x2F;Websockets</td>
</tr>
</tbody></table>
<hr>
<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64624730-aa6e3480-d41d-11e9-84eb-2278bdb952e1.jpg" alt="SpringCloud With Kubernetes">  SpringCloud With Kubernetes</p>
<p><img src="https://user-images.githubusercontent.com/5608425/64624734-ab06cb00-d41d-11e9-8ceb-ed1b5436eec7.jpg" alt="SpringCloud .vs Kubernetes"><br>SpringCloud .vs Kubernetes</p>
</div>

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《分布式系统的技术栈》 左耳听风</li>
<li><a href="https://blog.csdn.net/zl1zl2zl3/article/details/89790643">厉害了，Spring Cloud Alibaba 发布 GA 版本！</a></li>
<li><a href="https://www.cnblogs.com/popsuper1982/p/9634578.html">一篇囊括微服务服务拆分的一切：前提，时机，方法，规范，选型</a></li>
<li><a href="https://yq.aliyun.com/articles/636511">Istio Gateway与Kubernetes Ingress Controller对比</a>  灰度例子</li>
<li><a href="https://www.servicemesher.com/blog/baidu-service-mesh-ha-practice/">Service Mesh 高可用在企业级生产中的实践</a>  百度 罗广明 未<br> Service Mesh与Spring Cloud的结合</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>流式计算[Flink Beam Spark]</title>
    <url>/www6vHomeHexo/2019/07/19/streamComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="flink-vs-spark">Flink vs. Spark</span><a href="#flink-vs-spark" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>特性</th>
<th>Flink</th>
<th>Spark</th>
</tr>
</thead>
<tbody><tr>
<td>本质</td>
<td>event</td>
<td>batch</td>
</tr>
<tr>
<td>State</td>
<td>Keyed State(Value, List, Map) <br> Operator State(List)</td>
<td>RDD</td>
</tr>
<tr>
<td>算子[What]</td>
<td>Source<br> Transform<br> Sink</td>
<td>Transformation<br> Action</td>
</tr>
<tr>
<td>API [What]</td>
<td>1. Table<br> 2. DataStream <br> 3.DataSet</td>
<td>1.SparkSQL(DataFrame， DataSet)<br> 2. Spark Streaming <br></td>
</tr>
<tr>
<td>Time[When]</td>
<td>Event Time<br> Ingestion Time<br> Processing Time</td>
<td>无</td>
</tr>
<tr>
<td>Windows[Where]</td>
<td>Tumbling<br> Sliding <br> Session</td>
<td>Sliding <br> batchDuration,windowDuration,slideDuration</td>
</tr>
<tr>
<td>可靠性</td>
<td>Savepoint<br>External Checkpoint</td>
<td>linage<br> master checkpoint</td>
</tr>
</tbody></table>
<h3><span id="beam-vs-flink-vs-spark-6">Beam vs. Flink vs. Spark  [6]</span><a href="#beam-vs-flink-vs-spark-6" class="header-anchor">#</a></h3><ul>
<li>What</li>
</ul>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="center">Beam model</th>
<th align="center">Flink</th>
<th align="center">Spark</th>
</tr>
</thead>
<tbody><tr>
<td align="right">ParDo</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">GroupByKey</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">~</td>
</tr>
<tr>
<td align="right">Flatten</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">Combine</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">Composite Transfrom</td>
<td align="center">√</td>
<td align="center">~</td>
<td align="center">~</td>
</tr>
<tr>
<td align="right">Side Inputs</td>
<td align="center">√</td>
<td align="center">~</td>
<td align="center">~</td>
</tr>
<tr>
<td align="right">Source API</td>
<td align="center">√</td>
<td align="center">~</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">Aggregators</td>
<td align="center">~</td>
<td align="center">~</td>
<td align="center">~</td>
</tr>
<tr>
<td align="right">Keyed States</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
</tbody></table>
<ul>
<li>Where</li>
</ul>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="center">Beam model</th>
<th align="center">Flink</th>
<th align="center">Spark</th>
</tr>
</thead>
<tbody><tr>
<td align="right">Global windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">Fixed windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">~</td>
</tr>
<tr>
<td align="right">Sliding windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Session windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Custom windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Custom merging windows</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Timestamp control</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
</tbody></table>
<ul>
<li>When</li>
</ul>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="center">Beam model</th>
<th align="center">Flink</th>
<th align="center">Spark</th>
</tr>
</thead>
<tbody><tr>
<td align="right">Configurable triggering</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Event-time triggers</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Proccessing-time triggers</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="right">Count triggers</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">[Meta]data driven  triggers</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Composite triggers</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Allowed lateness</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="right">Timers</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
</tbody></table>
<ul>
<li>How</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Beam model</th>
<th>Flink</th>
<th>Spark</th>
</tr>
</thead>
<tbody><tr>
<td>Discarding</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Accumulating</td>
<td>√</td>
<td>√</td>
<td>×</td>
</tr>
<tr>
<td>Accumulating @ Retracting</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
</tbody></table>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.iteblog.com/archives/2417.html">Apache Flink状态管理和容错机制介绍</a></li>
<li><a href="https://yq.aliyun.com/articles/674450">Streaming System 第二章：The What- Where- When- and How of Data Processing</a> 未</li>
<li><a href="https://yq.aliyun.com/articles/682873">Streaming System 第三章：Watermarks</a> 未</li>
<li>xxx</li>
<li>&lt;&lt;Spark大数据处理技术&gt;&gt; 10.2节</li>
<li><a href="https://www.cnblogs.com/zlslch/p/7609417.html">Apache Beam是什么？</a></li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>对比总结</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>istio常用命令</title>
    <url>/www6vHomeHexo/2019/07/15/istioCommand/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="常用命令">常用命令</span><a href="#常用命令" class="header-anchor">#</a></h2><ol>
<li><p>Envoy的启动信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ istioctl proxy-config bootstrap ratings-v1-85858fc49f-89zd5</span><br><span class="line">&#123;</span><br><span class="line">	&quot;bootstrap&quot;: &#123;</span><br><span class="line">		&quot;node&quot;: &#123;</span><br><span class="line">            ...</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;staticResources&quot;: &#123;</span><br><span class="line">			&quot;listeners&quot;: [&#123;			</span><br><span class="line">                    ...</span><br><span class="line">			&#125;],</span><br><span class="line">			&quot;clusters&quot;: [&#123;</span><br><span class="line">                   ...</span><br><span class="line">			&#125;, </span><br><span class="line">			... , ...]</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;dynamicResources&quot;: &#123;</span><br><span class="line">			&quot;ldsConfig&quot;: &#123;</span><br><span class="line">				&quot;ads&quot;: &#123;&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">			&quot;cdsConfig&quot;: &#123;</span><br><span class="line">				&quot;ads&quot;: &#123;&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">			&quot;adsConfig&quot;: &#123;</span><br><span class="line">                 ...</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;statsConfig&quot;: &#123;			</span><br><span class="line">		    ...</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;tracing&quot;: &#123;</span><br><span class="line">		    ...</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;admin&quot;: &#123;</span><br><span class="line">     		...</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;lastUpdated&quot;: &quot;2019-07-08T02:44:29.440Z&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>xDS</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ istioctl proxy-config cluster ratings-v1-85858fc49f-89zd5</span><br><span class="line">$ istioctl proxy-config endpoint reviews-v1-6db46f6486-q7nth</span><br><span class="line">$ istioctl proxy-config listener  reviews-v1-6db46f6486-q7nth</span><br><span class="line">$ istioctl proxy-config route  reviews-v1-6db46f6486-q7nth</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># API server准入机制</span><br><span class="line">$ kubectl get  ValidatingWebhookConfiguration  -o yaml</span><br><span class="line">$ kubectl get mutatingwebhookconfiguration istio-sidecar-injector -o yaml  # sidecar自动注入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#   kubectl exec  Pod名字  -c  容器名字  -- 执行的命令</span><br><span class="line">$ kubectl exec productpage-v1-8579d7b797-dhj7z   -c istio-proxy  -- ps -ef</span><br><span class="line">/usr/local/bin/pilot-agent </span><br><span class="line">/usr/local/bin/envoy </span><br><span class="line"></span><br><span class="line">$ kubectl exec productpage-v1-8579d7b797-dhj7z   -c istio-proxy curl http://127.0.0.1:15000/help</span><br><span class="line">$ kubectl exec productpage-v1-8579d7b797-dhj7z   -c istio-proxy --  netstat -ln</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Envoy的配置</span><br><span class="line">$ kubectl -n default exec ratings-v1-85858fc49f-89zd5  -c istio-proxy curl http://localhost:15000/config_dump &gt; dump-rating.json</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.servicemesher.com/blog/envoy-proxy-config-deep-dive/">Istio 的数据平面 Envoy Proxy 配置详解</a></li>
<li><a href="https://item.jd.com/12538407.html">《云原生服务网格Istio：原理、实践、架构与源码解析》 实践篇</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>故障排查的流程和方法</title>
    <url>/www6vHomeHexo/2019/07/06/findProblem/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="故障处理-可观察性-5">故障处理-可观察性 [5]</span><a href="#故障处理-可观察性-5" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/07/06/findProblem/observe.JPG" class>

<h2><span id="微服务-故障快速定位问题-3">微服务 故障快速定位问题 [3]</span><a href="#微服务-故障快速定位问题-3" class="header-anchor">#</a></h2><ol>
<li>打印日志中的异常<br>RPC 异常列表、 异常码、 异常码形成异常列表</li>
</ol>
<img src="/www6vHomeHexo/2019/07/06/findProblem/errorCode.PNG" class title="异常码">

<ol start="2">
<li>借助分布式链路跟踪 APM</li>
</ol>
<h2><span id="故障处理通用流程-4">故障处理通用流程 [4]</span><a href="#故障处理通用流程-4" class="header-anchor">#</a></h2><ul>
<li>故障处理过程中效率如何，其实取决于三个因素：<ul>
<li>技术层面的故障隔离手段是否完备； [隔离] [技术架构]</li>
<li>故障处理过程中的指挥体系是否完善，角色分工是否明确； [角色][组织架构]</li>
<li>故障处理机制保障是否经过足够的演练。 [演练][事前]</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="/www6vHomeHexo/2019/09/09/microservice/" title="微服务 总结">微服务 总结</a>  self</li>
<li><a href="/www6vHomeHexo/2019/08/31/observability/" title="可观测性 总结">可观测性 总结</a>  self   </li>
<li>《RPC实战与核心原理进 - 19 | 分布式环境下如何快速定位问题？》   何小锋</li>
<li>《SRE实战手册 - 07｜故障处理：一切以恢复业务为最高优先级 》   方法论</li>
<li>《云原生下的可观察性》 阿里张城  ***</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>istio安装 + Bookinfo示例</title>
    <url>/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#istio%E5%AE%89%E8%A3%85">istio安装</a></li>
<li><a href="#bookinfo%E7%A4%BA%E4%BE%8B">Bookinfo示例</a><ul>
<li><a href="#bookinfo%E5%BA%94%E7%94%A8">Bookinfo应用</a></li>
<li><a href="#%E8%B7%AF%E7%94%B1">路由</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E7%89%88%E6%9C%AC%E7%9A%84%E8%B7%AF%E7%94%B1">基于版本的路由</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E7%9A%84%E8%B7%AF%E7%94%B1">基于用户身份的路由</a></li>
</ul>
</li>
<li><a href="#%E9%81%A5%E6%B5%8B">遥测</a><ul>
<li><a href="#%E6%94%B6%E9%9B%86%E6%8C%87%E6%A0%87%E5%92%8C%E6%97%A5%E5%BF%97">收集指标和日志</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="istio安装">istio安装</span><a href="#istio安装" class="header-anchor">#</a></h2><p> 有以下几种方式安装Istio：</p>
<ul>
<li>使用install&#x2F;kubernetes文件夹中的istio-demo.yaml进行安装；</li>
<li>使用Helm template渲染出Istio的YAML安装文件进行安装；</li>
<li>使用Helm和Tiller方式进行安装。</li>
</ul>
<p>对于生产环境下或大规模的应用，推荐使用 Helm和 Tiller 方式安装 Istio，这样可以灵活控制Istio的<br>所有配置项，方便管理各个组件。</p>
<ul>
<li><p>第二种方式安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#资源准备</span><br><span class="line">wget https://github.com/istio/istio/releases/download/1.1.1/istio-1.1.1-linux.tar.gz</span><br><span class="line">tar xvf istio-1.1.1-linux.tar.gz</span><br><span class="line"></span><br><span class="line">#建立namespace</span><br><span class="line">$ kubectl create namespace istio-system</span><br><span class="line"></span><br><span class="line">#Istio CRD安装</span><br><span class="line">$ helm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">#CRD验证</span><br><span class="line">$ kubectl get crds | grep &#x27;istio.io\|certmanager.k8s.io&#x27; | wc -l</span><br><span class="line">53</span><br><span class="line"></span><br><span class="line">#Istio 的核心组件安装</span><br><span class="line">$ helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">#验证</span><br><span class="line">$ kubectl get svc -n istio-system</span><br><span class="line">$ kubectl get pods -n istio-system</span><br></pre></td></tr></table></figure>
</li>
<li><p>第一种方式安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 如果以上安装有问题, 可一键安装istio</span><br><span class="line">kubectl apply -f install/kubernetes/istio-demo.yaml</span><br></pre></td></tr></table></figure></li>
</ul>
<!-- more -->

<h2><span id="bookinfo示例">Bookinfo示例</span><a href="#bookinfo示例" class="header-anchor">#</a></h2><h3><span id="bookinfo应用">Bookinfo应用</span><a href="#bookinfo应用" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#手动注入</span><br><span class="line">$ kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#安装gateway</span><br><span class="line">$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br><span class="line"></span><br><span class="line">#验证gateway </span><br><span class="line">$ kubectl get gateway</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#访问应用</span><br><span class="line">$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#x27;&#123;.status.loadBalancer.ingress[0].ip&#125;&#x27;)</span><br><span class="line">$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#x27;&#123;.spec.ports[?(@.name==&quot;http2&quot;)].port&#125;&#x27;)</span><br><span class="line">$ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT</span><br><span class="line">$ curl -s http://$&#123;GATEWAY_URL&#125;/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;</span><br></pre></td></tr></table></figure>

<p>访问应用 浏览器中打开应用程序页面</p>
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/getService.JPG" class title="istio-ingressgateway的EXTERNAL-IP">
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/browserCheck.JPG" class title="浏览器中打开应用程序页面">

<h3><span id="路由">路由</span><a href="#路由" class="header-anchor">#</a></h3><h4><span id="基于版本的路由">基于版本的路由</span><a href="#基于版本的路由" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在目标规则中定义好可用的版本，命名为 subsets</span><br><span class="line">$ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml</span><br><span class="line"></span><br><span class="line">#基于版本的路由: virtual service 将所有流量路由到每个微服务的 v1 版本</span><br><span class="line">$ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml</span><br><span class="line">$ kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml</span><br></pre></td></tr></table></figure>

<h4><span id="基于用户身份的路由">基于用户身份的路由</span><a href="#基于用户身份的路由" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#基于用户身份的路由: 来自名为 Jason 的用户的所有流量将被路由到服务 reviews:v2</span><br><span class="line">$ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl delete -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml</span><br></pre></td></tr></table></figure>

<h3><span id="遥测">遥测</span><a href="#遥测" class="header-anchor">#</a></h3><h4><span id="收集指标和日志">收集指标和日志</span><a href="#收集指标和日志" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm template install/kubernetes/helm/istio --name istio --namespace istio-system \</span><br><span class="line">--set prometheus.enabled=true \</span><br><span class="line">--set prometheus.ingress.enabled=true \</span><br><span class="line">--set prometheus.service.nodePort.enabled=true \</span><br><span class="line">--set prometheus.service.nodePort.port=32090  \</span><br><span class="line">&gt; ./001-my-istio.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl apply -f 001-my-istio.yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f new_telemetry.yaml</span><br><span class="line"></span><br><span class="line">#向示例应用发送流量</span><br><span class="line">curl http://$GATEWAY_URL/productpage</span><br></pre></td></tr></table></figure>

<p>注意: 外网访问不了, 需要在uk8s集群上做NAT路由到Prometheus服务所在的Node</p>
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/prom-node.JPG" class title="Prometheus所在的Node">
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/prom-NAT-transfer.JPG" class title="Prometheus服务的NAT">


<p>打开Prometheus界面并查询 istio_double_request_count 的值</p>
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/prom1.JPG" class title="Prometheus查询">
<img src="/www6vHomeHexo/2019/07/02/istioSetup-bookinfo/prom2.JPG" class title="Prometheus查询">

<ol start="4">
<li>熔断 【参考6】</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#结果验证</span><br><span class="line">[root@10-25-3-55 istio-1.1.1]# kubectl exec -it $FORTIO_POD  -c istio-proxy  -- sh -c &#x27;curl localhost:15000/stats&#x27; | grep httpbin | grep           pending</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.circuit_breakers.default.rq_pending_open: 0</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.circuit_breakers.high.rq_pending_open: 0</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.upstream_rq_pending_active: 0</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.upstream_rq_pending_failure_eject: 0</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.upstream_rq_pending_overflow: 109</span><br><span class="line">cluster.outbound|8000||httpbin.default.svc.cluster.local.upstream_rq_pending_total: 172</span><br></pre></td></tr></table></figure>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://istio.io/zh/docs/setup/kubernetes/install/helm/">使用 Helm 进行安装</a></li>
<li><a href="https://zhaohuabing.com/2017/11/04/istio-install_and_example/">Istio及Bookinfo示例程序安装试用笔记</a></li>
<li><a href="https://istio.io/docs/examples/bookinfo/#confirm-the-app-is-accessible-from-outside-the-cluster">Bookinfo Application</a></li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/request-routing/">配置请求路由</a></li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/telemetry/metrics/collecting-metrics/">收集指标和日志</a></li>
<li><a href="https://preliminary.istio.io/zh/docs/tasks/traffic-management/circuit-breaking/">熔断</a> done</li>
<li><a href="https://istio.io/docs/setup/install/kubernetes/">Quick Start Evaluation Install</a> istio快速安装  未</li>
<li>《云原生服务网格Istio：原理、实践、架构与源码解析》  张超盟，章鑫，徐中虎，徐飞</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>istio</title>
    <url>/www6vHomeHexo/2019/07/02/istio/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="为什么用istio">为什么用istio</span><a href="#为什么用istio" class="header-anchor">#</a></h2><ul>
<li><p>k8s和istio的比较 [3]<br>k8s的service比较弱，负载均衡基于dns和iptables。<br>需要用户态应用层的负载均衡能力，是istio等mesh项目的驱动力。</p>
</li>
<li><p>SDK与istio比较 [1]<br>SDK思维向平台思维转变<br>SDK的功能下沉到平台中， SDK中保留必要的功能</p>
</li>
</ul>
<p>参考：</p>
<ol>
<li>《阿里云云原生架构实践》</li>
<li><a href="https://www.bilibili.com/video/BV1V64y1r7oU?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">基于OpenShift Service Mesh 实现微服务网格化 林斯锐 中国DevOps社区</a></li>
<li>12丨模块十二：基于Istio的高级流量管理</li>
</ol>
<h2><span id="istio-overview-1">istio Overview [1]</span><a href="#istio-overview-1" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/07/02/istio/istio-overview.jpg" class title="istio overview">


<h2><span id="istio特性">istio特性</span><a href="#istio特性" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/07/02/istio/istio.jpg" class title="istio">

<ul>
<li>性能优化:<ul>
<li>1.精简层：  sofa-mosn 在sidercar里做很多mixer的事情</li>
<li>2.优化层：  Cilium： 优化sider的网络</li>
</ul>
</li>
</ul>
<p>参考:<br>20. <a href="https://jimmysong.io/istio-handbook/setup/istio-installation.html">Istio 安装</a>  引<br>23. <a href="https://mp.weixin.qq.com/s/fSklull_8OfpdCtdwbXx9A">Istio 庖丁解牛五：多集群网格实现分析</a>  腾讯云 钟华 引</p>
<h2><span id="istio组件-5">istio组件 [5]</span><a href="#istio组件-5" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64623495-a3debd80-d41b-11e9-9599-c8c25a7153b9.jpg" alt="istio组件"><br>istio1.1组件</p>
</div>


<h2><span id="istio架构演进">istio架构演进</span><a href="#istio架构演进" class="header-anchor">#</a></h2><ul>
<li>1.5版本之后的简化 Simplify<br>从 1.5 开始，把控制平面的所有组件组合并成一个单体结构叫 istiod。</li>
</ul>
<h5><span id="istio架构v11-v14-5">istio架构（v1.1-v1.4） [5]</span><a href="#istio架构v11-v14-5" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/07/02/istio/istio-old-arch.png" class title="istio架构（1.1-1.4）">

<h5><span id="istio架构v15">istio架构（v1.5+）</span><a href="#istio架构v15" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/07/02/istio/istio1.5-arch.jpeg" class title="istio架构（1.5+）">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://mp.weixin.qq.com/s/NjMncH84uEl_PywOFFMlFA">腾讯云容器团队内部Istio专题分享</a> 腾讯云 钟华</li>
<li><a href="https://blog.csdn.net/karamos/article/details/80133231">一个商用级Service Mesh服务的设计之道</a>  华为 田晓亮</li>
<li><a href="http://www.servicemesher.com/blog/service-mesh-the-microservices-in-post-kubernetes-era/">Service Mesh——后 Kubernetes 时代的微服务</a> 宋净超 </li>
<li><a href="https://item.jd.com/12538407.html">《云原生服务网格Istio：原理、实践、架构与源码解析》</a> 3.1.2节</li>
<li><a href="https://mp.weixin.qq.com/s/VwqxrZsVmn4a5PcVckaLxA">Istio 庖丁解牛1：组件概览</a>  腾讯云 钟华</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>serviceMesh</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Raft协议</title>
    <url>/www6vHomeHexo/2019/06/21/raft/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#raft-%E5%88%86%E5%8C%BA%E8%84%91%E8%A3%82%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4%E7%9A%84%E9%97%AE%E9%A2%98">Raft-分区脑裂（成员变更的问题）</a></li>
<li><a href="#raft-%E9%A2%86%E5%AF%BC%E8%80%85%E9%80%89%E4%B8%BE">Raft-领导者选举</a></li>
<li><a href="#raft-%E5%A4%8D%E5%88%B6%E6%97%A5%E5%BF%97">Raft-复制日志</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><p>从本质上说，<strong>Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致.</strong></p>
<img src="/www6vHomeHexo/2019/06/21/raft/raft.jpg" class title="raft协议">

<p>Raft： leader + term + peers</p>
<h2><span id="raft-分区脑裂成员变更的问题">Raft-分区脑裂（成员变更的问题）</span><a href="#raft-分区脑裂成员变更的问题" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64484884-1c425480-d24b-11e9-92c1-865111cc016d.JPG" alt="raft-patition"><br>分区脑裂[非majority有uncommited log、 term1]</p>
<p><img src="https://user-images.githubusercontent.com/5608425/64484885-1c425480-d24b-11e9-8375-102d20506265.JPG" alt="raft-patition-1"><br>分区脑裂[majority可以同步log、 term2]</p>
</div>

<h2><span id="raft-领导者选举">Raft-领导者选举</span><a href="#raft-领导者选举" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/06/21/raft/raft-leader-1.JPG" class title="raft选举-步骤1-Election Timeout">
<img src="/www6vHomeHexo/2019/06/21/raft/raft-leader-2.JPG" class title="raft选举-步骤2">
<img src="/www6vHomeHexo/2019/06/21/raft/raft-leader-3.JPG" class title="raft选举-步骤3">
<img src="/www6vHomeHexo/2019/06/21/raft/raft-leader-4.JPG" class title="raft选举-步骤4">
<img src="/www6vHomeHexo/2019/06/21/raft/raft-leader-5.JPG" class title="raft选举-步骤5">

<h2><span id="raft-复制日志">Raft-复制日志</span><a href="#raft-复制日志" class="header-anchor">#</a></h2><ul>
<li>副本数据是以日志的形式存在的，其中日志项中的指令表示用户指定的数据。</li>
<li>Raft 是通过以领导者的日志为准，来实现日志的一致的。</li>
<li>在 Raft 中日志必须是<strong>连续的</strong></li>
<li><strong>日志完整性最高的节点才能当选领导者</strong></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://thesecretlivesofdata.com/raft/">raft</a>  动画 good</li>
<li><a href="https://raft.github.io/">The Raft Consensus Algorithm</a>  good 动画 各种系统实现 未</li>
<li><a href="http://kanaka.github.io/raft.js/">Raft Distributed Consensus Algorithm Visualization</a> 动画 未</li>
<li><a href="https://my.oschina.net/pingpangkuangmo/blog/782702">Raft对比ZAB协议</a></li>
<li><a href="https://www.zhihu.com/question/28242561">raft协议和zab协议有啥区别？</a></li>
<li><a href="http://www.seflerzhou.net/post-109.html">一张图看懂Raft</a> 未</li>
<li><a href="https://www.cnblogs.com/mindwind/p/5231986.html">Raft 为什么是更易理解的分布式一致性算法</a>  未</li>
<li><a href>分布式协议与算法实战 - 07 | Raft算法（一）：如何选举领导者？</a> 韩健 ***</li>
<li><a href>分布式协议与算法实战 - 08丨Raft算法（二）：如何复制日志？.pdf</a> 韩健 ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/510220698">Raft 分布式系统一致性协议探讨</a>  腾讯 未</li>
</ol>
<hr>
<ul>
<li>论文</li>
</ul>
<ol>
<li><a href="https://www.infoq.cn/article/raft-paper/">Raft一致性算法论文译文</a></li>
<li><a href="https://raft.github.io/raft.pdf">In Search of an Understandable Consensus Algorithm(Extended Version)</a>  raft</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>raft</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>RocketMQ总结</title>
    <url>/www6vHomeHexo/2019/06/18/mqRocketmq/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>关键词: 重复消息, 事务消息</p>
<img src="/www6vHomeHexo/2019/06/18/mqRocketmq/rocketMQ.jpg" class title="RocketMQ">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><h5><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h5><ol>
<li><a href="https://yq.aliyun.com/articles/71889?spm=5176.100239.blogcont55626.10.FWVVKw">Apache RocketMQ背后的设计思路与最佳实践</a> 阿里 冯嘉</li>
<li><a href="https://yq.aliyun.com/articles/66101?spm=5176.100239.blogcont55634.18.ODias7">十分钟入门RocketMQ</a>  阿里 尘央</li>
<li><a href="https://yq.aliyun.com/articles/66110?spm=a2c4e.11155435.0.0.2cb97b3fBOIG8W">RocketMQ 关键特性</a> ***</li>
<li><a href="https://zhuanlan.zhihu.com/p/396726719">分布式开放消息系统(RocketMQ)的原理与实践</a>   CHEN川  ***  消息的顺序问题  消息的重复问题</li>
<li><a href="/www6vHomeHexo/2021/05/19/mqOrdering/" title="消息系统 顺序消息">消息系统 顺序消息</a> self</li>
<li><a href="/www6vHomeHexo/2020/08/12/mqRocketmqTransaction/" title="Rocketmq中的事务">Rocketmq中的事务</a> self</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Workload</title>
    <url>/www6vHomeHexo/2019/06/09/k8sResource/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#workload">Workload</a><br>- <a href="#basic">Basic</a><br>- <a href="#config">Config</a><br>- <a href="#core">Core</a><br>- <a href="#service">Service</a><br>- <a href="#limit-%E9%99%90%E5%88%B6">limit 限制</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="workload">Workload</span><a href="#workload" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/06/09/k8sResource/k8sResource.jpg" class title="Kubenetes Workload"> 

<h5><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h5><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/node.html">Node</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/namespace.html">Namespace</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/pod.html">Pod</a> [8]</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/serviceaccount.html">ServiceAccount</a></li>
</ul>
<h5><span id="config">Config</span><a href="#config" class="header-anchor">#</a></h5><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/configmap.html">ConfigMap</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/secret.html">Secret</a></li>
</ul>
<table>
<thead>
<tr>
<th align="center">类型</th>
<th align="center">用途</th>
<th align="center">使用方式</th>
<th align="center">安全</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ConfigMap</td>
<td align="center">普通配置</td>
<td align="center">环境变量 <br> 文件挂载（卷 Volume）[7]</td>
<td align="center">纯文本</td>
</tr>
<tr>
<td align="center">Secret</td>
<td align="center">敏感数据</td>
<td align="center">环境变量 <br>文件挂载</td>
<td align="center">Base64</td>
</tr>
</tbody></table>
<h5><span id="core">Core</span><a href="#core" class="header-anchor">#</a></h5><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/statefulset.html">StatefulSet</a> +</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/deployment.html">Deployment</a> +</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/replicaset.html">ReplicaSet</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/cronjob.html">CronJob</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/job.html">Job</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/daemonset.html">DaemonSet</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/customresourcedefinition.html">CustomResourceDefinition</a></li>
</ul>
<h5><span id="service">Service</span><a href="#service" class="header-anchor">#</a></h5><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/ingress.html">Ingress</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/service.html">Service</a> +</li>
</ul>
<h6><span id="volume">Volume</span><a href="#volume" class="header-anchor">#</a></h6><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/local-volume.html">LocalVolume</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/persistent-volume.html">PersistentVolume</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/volume.html">Volume</a></li>
</ul>
<h5><span id="limit-限制">limit  限制</span><a href="#limit-限制" class="header-anchor">#</a></h5><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/network-policy.html">NetworkPolicy</a>  网络隔离与互通</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/quota.html">Resource Quota</a>  资源限制</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/security-context.html">SecurityContext</a>   安全策略</li>
</ul>
<h6><span id="other">other</span><a href="#other" class="header-anchor">#</a></h6><ul>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/autoscaling.html">Autoscaling (HPA)</a></li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/podpreset.html">PodPreset</a></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》</a>  七牛容器云团队</li>
<li><a href="https://feisky.xyz/kubernetes-handbook/concepts/objects.html">资源对象</a>    feisky ***</li>
<li><a href="https://mp.weixin.qq.com/s/E5-agHtMvW_X7znVJDkTKA">面向 Kubernetes 编程： Kubernetes 是下一代操作系统</a> ***</li>
<li><a href="https://edu.aliyun.com/lesson_1651_13079?spm=5176.254948.1334973.10.2c12cad2AHzzTw#_13079">第4 章 ： 理解 Pod 和容器设计模式</a> 阿里</li>
<li><a href="https://edu.aliyun.com/lesson_1651_13078?spm=5176.254948.1334973.8.2c12cad2AHzzTw#_13078">第3 章 ： Kubernetes 核心概念</a> 阿里 </li>
<li><a href="https://edu.aliyun.com/lesson_1651_13080?spm=5176.254948.1334973.12.2c12cad2AHzzTw#_13080">第5 章 ： 应用编排与管理：核心原理</a> 阿里</li>
<li><a href="https://blog.csdn.net/weixin_46902396/article/details/121143037">K8s 中 ConfigMap 使用介绍</a>  </li>
<li><a href="https://draveness.me/kubernetes-pod/">详解 Kubernetes Pod 的实现原理</a>  未</li>
</ol>
<details><summary>附: ConfigMap 使用</summary><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: zhangsan</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: zhangsan</span><br><span class="line">    image: busybox:1.28.4</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;env&#x27;]</span><br><span class="line">    env:									# 配置环境变量</span><br><span class="line">    - name: HostName						# 变量名</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-01						# ConfigMap 名称 (要和上面对应)</span><br><span class="line">          key: hostname						# ConfigMap 里边的 Key (要和上面对应)</span><br><span class="line">    - name: Password</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-01</span><br><span class="line">          key: password</span><br><span class="line">  restartPolicy: Never						# 当容器退出后. 不会进行重启操作</span><br></pre></td></tr></table></figure>
</details>



]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes调度器</title>
    <url>/www6vHomeHexo/2019/06/09/k8sScheduler/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="资源调度泛型-1">资源调度泛型 [1]</span><a href="#资源调度泛型-1" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65023010-96b65700-d964-11e9-9acd-7cc8edbbde85.JPG" alt="调度系统泛型"><br>调度系统泛型</p>
</div>


<table>
<thead>
<tr>
<th>类型</th>
<th>资源选择</th>
<th>排他性</th>
<th>分配粒度</th>
<th>集群策略</th>
</tr>
</thead>
<tbody><tr>
<td>中央调度器</td>
<td>全局</td>
<td>无，时序</td>
<td>全局策略</td>
<td>严格的优先级(抢占式)</td>
</tr>
<tr>
<td>两层调度调度器</td>
<td>动态资源集</td>
<td>悲观锁</td>
<td>增量囤积</td>
<td>严格公正</td>
</tr>
<tr>
<td>共享状态</td>
<td>全局</td>
<td>乐观锁</td>
<td>调度器策略</td>
<td>优先级抢占</td>
</tr>
</tbody></table>
<p>表1. 常见调度器的比较</p>
<h2><span id="kubernetes-资源调度2">Kubernetes 资源调度[2]</span><a href="#kubernetes-资源调度2" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/06/09/k8sScheduler/k8sScheduler.jpg" class title="k8s调度器"> 


<h2><span id="kubernetes-调度的两个阶段45">Kubernetes 调度的两个阶段[4][5]</span><a href="#kubernetes-调度的两个阶段45" class="header-anchor">#</a></h2><h5><span id="基于谓词和优先级的调度器predicates-and-priorities-v100-~-v1140">基于谓词和优先级的调度器（Predicates and Priorities） · v1.0.0 ~ v1.14.0</span><a href="#基于谓词和优先级的调度器predicates-and-priorities-v100-~-v1140" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/06/09/k8sScheduler/predicates-and-priorities-scheduling.png" class title="基于谓词和优先级的调度器">

<ul>
<li><p>调度器扩展（Scheduler Extender） · v1.2.0 - Scheduler extension</p>
<p>  通过调用外部调度器扩展的方式改变调度器的决策；</p>
</li>
<li><p>Map-Reduce 优先级算法 · v1.5.0 - MapReduce-like scheduler priority functions</p>
<p>  为调度器的优先级算法支持 Map-Reduce 的计算方式，通过引入可并行的 Map 阶段优化调度器的计算性能；</p>
</li>
</ul>
<h5><span id="基于调度框架的调度器scheduling-framework-v1150-~-至今">基于调度框架的调度器（Scheduling Framework） · v1.15.0 ~ 至今</span><a href="#基于调度框架的调度器scheduling-framework-v1150-~-至今" class="header-anchor">#</a></h5><img src="/www6vHomeHexo/2019/06/09/k8sScheduler/kubernetes-scheduling-queue.png" class title="基于调度框架的调度器">

<ul>
<li><p>调度框架认为 Kubernetes 中目前存在调度（Scheduling）和绑定（Binding）两个循环：<br>  调度循环在多个 Node 中为 Pod 选择最合适的 Node；<br>  绑定循环将调度决策应用到集群中，包括绑定 Pod 和 Node、绑定持久存储等工作；</p>
</li>
<li><p>除了两个大循环之外，调度框架中还包含 QueueSort、PreFilter、Filter、PostFilter、Score、Reserve、Permit、PreBind、Bind、PostBind 和 Unreserve 11 个扩展点（Extension Point），这些扩展点会在调度的过程中触发。</p>
</li>
</ul>
<h2><span id="批量计算3">批量计算[3]</span><a href="#批量计算3" class="header-anchor">#</a></h2><ul>
<li><p>K8s自带的的资源调度器，有一个明显的特点是：依次调度每个容器。</p>
</li>
<li><p>Volcano  </p>
<ul>
<li>DRF（dominant resource fairness）: Yarn和Mesos都有<br>DRF意为：“谁要的资源少，谁的优先级高”</li>
<li>Queue: Yarn调度器的功能</li>
</ul>
</li>
</ul>
<h2><span id="实战11">实战[11]</span><a href="#实战11" class="header-anchor">#</a></h2><ul>
<li>服务资源智能推算: crane+Victoria Metrics <ul>
<li>Crane 调度器 [14]<br>crane-sheduler 基于prometheus集群真实资源负载进行调度，将其应用于调度过程中的 Filter 和 Score 阶段，能够有效缓解集群资源负载不均的问题，真正实现企业的降本增效。</li>
</ul>
</li>
<li>二次调度:  descheduler [12][13]</li>
<li>弹性调度：  OpenKruise-WorkloadSpread + Virtual Kubelet</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href>《大数据日知录：架构与算法</a>  第4章  张俊林</li>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》 第11章-机理 第16章-高级调度  七牛容器云团队</a></li>
<li><a href="https://mp.weixin.qq.com/s/_6WCgqxjTR1rAv8gQqNdWw">为什么K8s需要Volcano？</a> 华为</li>
</ol>
<h5><span id="scheduling-framework调度器-ampamp-谓词">scheduling framework调度器 &amp;&amp; 谓词</span><a href="#scheduling-framework调度器-ampamp-谓词" class="header-anchor">#</a></h5><ol start="4">
<li><a href="https://draveness.me/system-design-scheduler/">调度系统设计精要</a> linux 调度器， go调度器， k8s调度器</li>
<li><a href="https://www.bilibili.com/video/BV1N7411w7M9">Kubernetes Scheduler 设计与实现</a> bili<br><a href="https://github.com/talkgo/night/issues/535">第 76 期 Kubernetes Scheduler 设计与实现</a>  ***<br><a href="https://github.com/kubernetes/enhancements/issues/895">https://github.com/kubernetes/enhancements/issues/895</a> even pod, 多个region调度</li>
<li><a href="https://mp.weixin.qq.com/s/UkVXuZU0E0LT3LaDdZG4Xg">进击的 Kubernetes 调度系统（一）：Kubernetes scheduling framework</a>  未</li>
<li><a href="https://blog.csdn.net/alisystemsoftware/article/details/107359341">进击的 Kubernetes 调度系统（二）：支持批任务的 Coscheduling&#x2F;Gang scheduling</a> 未</li>
</ol>
<h5><span id="基于谓词的调度器">基于谓词的调度器</span><a href="#基于谓词的调度器" class="header-anchor">#</a></h5><ol start="8">
<li><a href="https://mp.weixin.qq.com/s/gfq1qghLW7g4gKZBBP17IA">Kubernetes集群调度器原理剖析及思考</a> - v1.11版本 2019</li>
<li><a href="https://cloud.tencent.com/developer/article/1475940">深度解析Kubernetes核心原理之Scheduler</a> 未 - KubeCon 2018   *** </li>
<li><a href="http://dockone.io/article/2885">DockOne微信分享（一四九）：Kubernetes调度详解 </a>  FreeWheel 主任工程师-2017年-***未</li>
</ol>
<h5><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h5><ol start="11">
<li><a href="https://www.bilibili.com/video/BV1iD4y117JL?spm_id_from=333.880.my_history.page.click">容器云调度优化及实践</a>  V</li>
<li><a href="https://www.chenshaowen.com/blog/descheduler-makes-kubernetes-load-more-balanced.html">descheduler 二次调度让 Kubernetes 负载更均衡</a></li>
<li><a href="https://blog.tianfeiyu.com/2022/06/30/kubernetes_descheduler/">Kubernetes 中 Descheduler 组件的使用与扩展</a></li>
<li><a href="https://cloudnative.to/blog/crane-scheduler/">Crane 调度器介绍——一款在 Kubernetes 集群间迁移应用的调度插件</a></li>
<li><a href="https://tencentcloud.csdn.net/64f7f5a9993dd34278ee1114.html">【腾讯云Finops Crane集训营】降本增效神器Crane实战记录</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes命令</title>
    <url>/www6vHomeHexo/2019/06/09/k8sCommand/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li>kubectl get 资源名  -&gt; 简单描述</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get rc</span><br><span class="line">kubectl get pods</span><br><span class="line">kubectl get deployment</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>kubectl describe 资源名  -&gt; 详细信息</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe rc</span><br><span class="line">kubectl describe pods</span><br><span class="line">kubectl describe deployment</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>一些命令</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get rc -o wide</span><br><span class="line">kubectl logs kubia-cjrqd</span><br><span class="line">kubectl get pods --show-labels</span><br><span class="line">kubectl get rs  -o yaml| less  ## 可以看 status， annotations</span><br><span class="line">kubectl get pods kubia-cjrqd -o yaml | less   ## 可以看 status， 可以看 OwnerReference</span><br><span class="line"></span><br><span class="line">kubectl -n default get all   ## 可以看所有资源  </span><br></pre></td></tr></table></figure>

<ol start="4">
<li>watch 状态</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pods --watch</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>Deployment 回滚</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl rollout undo deployment/deployment-nginx</span><br><span class="line">kubectl rollout undo deployment.v1.apps/deployment-nginx  --to-revision=2</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>edit 命令: 直接修改k8s里的API对象</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit pods kubia-cjrqd</span><br><span class="line">kubectl edit deployment  nginx-deployment</span><br></pre></td></tr></table></figure>

<ol start="7">
<li><p>docker 磁盘使用情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker system df -v</span><br><span class="line"></span><br><span class="line">+Images space usage:</span><br><span class="line"></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE                SHARED SIZE         UNIQUE SIZE         CONTAINERS</span><br><span class="line">&lt;none&gt;              &lt;none&gt;              818d02f0fd5f        4 days ago          121.8MB             121.7MB             41.83kB             2</span><br><span class="line">maven               3-jdk-8-alpine      7445f83cd169        4 months ago        121.7MB             121.7MB             0B                  0</span><br><span class="line"></span><br><span class="line">+Containers space usage:</span><br><span class="line"></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  LOCAL VOLUMES       SIZE                CREATED             STATUS                  NAMES</span><br><span class="line">79bf1498857f        818d02f0fd5f        &quot;/bin/sh -c &#x27;mvn pac…&quot;   0                   460B                4 days ago          Exited (1) 4 days ago   elegant_ramanujan</span><br><span class="line">e6fcce289871        818d02f0fd5f        &quot;/bin/sh -c &#x27;mvn pac…&quot;   0                   460B                4 days ago          Exited (1) 4 days ago   sleepy_lichterman</span><br><span class="line"></span><br><span class="line">+Local Volumes space usage:</span><br><span class="line"></span><br><span class="line">VOLUME NAME         LINKS               SIZE</span><br><span class="line"></span><br><span class="line">+Build cache usage: 0B</span><br><span class="line"></span><br><span class="line">CACHE ID            CACHE TYPE          SIZE                CREATED             LAST USED           USAGE               SHARED</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>k8s node 不可调度&#x2F;可调度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl drain 10.23.210.145        // 不可调度</span><br><span class="line">kubectl uncordon 10.23.210.145     // 可调度</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://tencentcloudcontainerteam.github.io/2019/06/08/kubernetes-best-practice-handle-disk-full/">kubernetes 最佳实践：处理容器数据磁盘被写满</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>多级缓存(cache)</title>
    <url>/www6vHomeHexo/2019/05/25/cacheMultiLayer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98-5">多级缓存 [5?]</a><ul>
<li><a href="#%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E8%BF%9C%E7%A8%8B%E7%BC%93%E5%AD%98">二级缓存（本地缓存+远程缓存）</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E6%9F%A5%E8%AF%A2-2">多级缓存查询 [2]</a></li>
<li><a href="#%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0-2">多级缓存更新 [2]</a></li>
<li><a href="#%E7%BC%93%E5%AD%98%E5%88%86%E5%B1%82-%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98">缓存分层 多级缓存</a><ul>
<li><a href="#%E8%BE%B9%E7%BC%98cache">边缘cache</a></li>
<li><a href="#%E9%A1%B5%E9%9D%A2%E7%BA%A7%E7%BC%93%E5%AD%98">页面级缓存</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BC%93%E5%AD%98">计算结果的缓存</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%BA%90%E7%BA%A7%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E6%BA%90%E7%BB%93%E6%9E%9C%E9%9B%86">数据源级缓存： 缓存数据源结果集</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2019/05/25/cacheMultiLayer/arch.jpg" class>

<h1><span id="多级缓存-5">多级缓存 [5?]</span><a href="#多级缓存-5" class="header-anchor">#</a></h1><h3><span id="二级缓存本地缓存远程缓存">二级缓存（本地缓存+远程缓存）</span><a href="#二级缓存本地缓存远程缓存" class="header-anchor">#</a></h3><ul>
<li>二级缓存（本地缓存+远程缓存）<ul>
<li>远端Cache推全量或者部分的数据到本地cache，并设置过期时间【初始化】</li>
<li>查询流程<ul>
<li>先从本地缓存拿，如果有数据且有效，就直接返回</li>
<li>如果没有命中<ul>
<li>1.本地查询远端服务，并拿到结果</li>
<li>2.本地更新远端缓存</li>
<li>3.更新本地缓存</li>
</ul>
</li>
</ul>
</li>
<li>本地cache失效+更新流程,本地防穿透【1】<ul>
<li>同步更新缓存<ul>
<li>访问同一个key的业务线程只有一个线程穿透到远端Cache，其他线程等待穿透线程的返回结果[加锁]</li>
</ul>
</li>
<li>异步更新缓存<ul>
<li>1.过期时间到了后，产生过期事件，延长数据有效期，返回旧的数据</li>
<li><ol start="2">
<li>检查过期事件，后台线程池更新缓存数据，并重新设置有效时间</li>
</ol>
</li>
</ul>
</li>
<li>eg. Google LoadingCache 以上两种方式都有</li>
</ul>
</li>
<li>远端cache，防穿透<ul>
<li>远端cache访穿透采用永久缓存数据，每次查询都能查到值.</li>
<li>通过辅助信息判断逻辑过期, 再从远端服务异步拉数据刷新远端cache。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="多级缓存查询-2">多级缓存查询 [2]</span><a href="#多级缓存查询-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2019/05/25/cacheMultiLayer/query.png" class>

<h1><span id="多级缓存更新-2">多级缓存更新 [2]</span><a href="#多级缓存更新-2" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2019/05/25/cacheMultiLayer/update.png" class>

<h1><span id="缓存分层-多级缓存">缓存分层  多级缓存</span><a href="#缓存分层-多级缓存" class="header-anchor">#</a></h1><h3><span id="边缘cache">边缘cache</span><a href="#边缘cache" class="header-anchor">#</a></h3><p>可用CDN实现，往往是服务器端缓存，存静态数据。<br>可以存Html页面， 脚本， 样式， 图片，页面片段等。</p>
<h3><span id="页面级缓存">页面级缓存</span><a href="#页面级缓存" class="header-anchor">#</a></h3><p>往往是本地缓存， 数据相对动态。</p>
<h3><span id="计算结果的缓存">计算结果的缓存</span><a href="#计算结果的缓存" class="header-anchor">#</a></h3><p>可以存储索引聚合数据，比如  BI里的数据聚合表。也可以存储耗时查询数据 ，比如搜索的结果。也可以存储业务相关数据， 比如对象模型的有向图可以整个缓存起来。在微博系统中，所有@你的微博是相对耗时， 可以 考虑作为逻辑缓存。</p>
<h3><span id="数据源级缓存-缓存数据源结果集">数据源级缓存： 缓存数据源结果集</span><a href="#数据源级缓存-缓存数据源结果集" class="header-anchor">#</a></h3><p><del>比如Hibernate缓存中的QueryCache用来缓存查询语句, 及查询结果集中对象的Id与Type. 当再次使用已缓存的Query时, 就可以通过对象的Id与Type在二级缓存中查找实际的对象.</del></p>
<p><del>Hibernate提供了短生命周期的缓存， 也叫事务级别的缓存。长生命周期的缓存，也叫应用级别的缓存。</del></p>
<p>  缓存分层之间的失效方式： 1. 映射关系  2. 日志 ＋ 重试</p>
<p><del>## 缓存对象的粒度</del>  </p>
<p><del>有一种缓存的误用是缓存大量的数据集合，而读取其中一部分。 在很多时候，我们往往会缓存一个对象的集合，但是，我们在读取的时候，只是每次读取其中一部分。 在更新缓存时， 读出整个集合， 改变其中一部分后， 在存回去， 这样序列化与反序列化的代价相当大</del></p>
<p><del>针对这个情况， jboss cache提供了两种粒度的对象存储：核心缓存（粗粒度的），POJO 缓存（细粒度的）</del></p>
<p><del>核心缓存会直接把您传递给它的数据存储在一个树型结构中。键／值对被存储在树的节点上，出于复制或持续性的需要它们都被序列化了。</del></p>
<p><del>POJO 缓存则采用比较复杂的机制——利用字节码编织来内省（introspecting）用户类，并向用户类的域添加侦听器，一旦域值有任何变化，侦听器会立刻 通知缓存。例如，如果要在POJO缓存中存储一个庞大、复杂的对象，会导致POJO缓存内省对象的字节码，最终只把该对象的原始域存储到树结构中。一旦域 值有所变化，缓存只复制这个改变了的域值而不会去复制整个用户类，这是高效的细粒度复制。</del></p>
<p><del>在缓存了细粒度的对象后， 造成的一个问题是数据的冗余。 例如查询条件1的返回的是model1, model2, model3, 查询条件2返回的是model2, model3, model4. model2, model3在缓存里就存了两份， 造成了冗余。这时可以分离出一个索引层，索引层存储缓存对象的地址， 这样可以节约大量的存储空间。 例如可以存储model1- model4的索引， 再从缓存中取得到实际的model.  在数据库中， 这种方式叫look up table. 如果系统更复杂， 可以采取缓存的partition加多级索引的方式</del></p>
<p><del>## 缓存与一致性</del></p>
<p>  <del>缓存多副本之间的同步：</del><br>  <del>可分为replication和invalidaiton机制。 Replication机制表示一旦有数据的更新， 其余副本都会同步复制一份更新后的数据。Replication机制复制时slave会对master节点有拖累， 这时可以考虑采取invalidation机制。 Invalidation机制在jboss cache里已有实现, 一旦有更新， 广播消息， 失效所有其他的副本，让其重新去获得该值。 可通过这种方式缓存大对象以减少在实例中复制对象的代价。根据用户在一定时间段内上网地点固定不变的规律，用户始终都是访问同一个机房， 针对主节点的本地缓存在有更新时可以异步发invalidation消息，副本节点可以慢慢的再加载回这个大对象， 这样可以提高用户响应度。这种方式也可用在边缘缓存中。对于无法分组的数据， 比如在某时间段的用户认证数据需要保证副本同步，最好的方式是清除相应的副本， 让它在下次使用时初始化</del></p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.csdn.net/lee_nacl/article/details/127860463">多级缓存架构</a><br>《架构实战营-第25节课：计算架构模式之多级缓存架构》 V</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/497029871">解析分布式系统的缓存设计</a></p>
</li>
<li><p>《移动选购线缓存实践》 赵思奇 *** </p>
</li>
<li><p><a href="http://wed.xjx100.cn/news/154282.html">jetcache：阿里这款多级缓存框架一定要掌握</a> 未</p>
</li>
<li><p><a href="https://blog.51cto.com/u_15989526/6287632">聊聊微服务架构中的多级缓存设计（建议收藏）</a> 未</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/606469614">Java分布式缓存一篇文章让你明白你多级缓存的分层架构原理分析</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka  Q&amp;A</title>
    <url>/www6vHomeHexo/2019/05/15/kafkaQ-A/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="基础">基础</span><a href="#基础" class="header-anchor">#</a></h2><ul>
<li><p>Kafka的用途有哪些？使用场景如何？</p>
</li>
<li><p>Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么</p>
</li>
<li><p>Kafka中的HW、LEO、LSO、LW等分别代表什么？</p>
</li>
</ul>
<h2><span id="topic">topic</span><a href="#topic" class="header-anchor">#</a></h2><ul>
<li><p>当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</p>
</li>
<li><p>topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</p>
</li>
<li><p>创建topic时如何选择合适的分区数？</p>
</li>
<li><p>Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？</p>
</li>
<li><p>Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理</p>
</li>
</ul>
<h3><span id="producer">Producer</span><a href="#producer" class="header-anchor">#</a></h3><ul>
<li><del>Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</del></li>
<li><del>Kafka生产者客户端中使用了几个线程来处理？分别是什么？</del></li>
</ul>
<h2><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h2><ul>
<li>聊一聊Kafka的延时操作的原理</li>
</ul>
<p><a href="https://hiddenpps.blog.csdn.net/article/details/89325701">Kafka科普系列 | 轻松理解Kafka中的延时操作</a><br>这里就涉及到了Kafka延迟操作的概念。Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给follower副本。</p>
<p>延迟操作不只是拉取消息时的特有操作，在Kafka中有多种延时操作，比如延时数据删除、延时生产等。</p>
<ul>
<li><p>Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）</p>
</li>
<li><p>Kafka中是怎么体现消息顺序性的？</p>
</li>
<li><p>Kafka的那些设计让它有如此高的性能？</p>
</li>
</ul>
<h2><span id="补齐">补齐</span><a href="#补齐" class="header-anchor">#</a></h2><ul>
<li><p>Kafka中怎么实现死信队列和重试队列？</p>
</li>
<li><p>Kafka中怎么做消息审计？</p>
</li>
<li><p>Kafka中怎么做消息轨迹？</p>
</li>
</ul>
<h2><span id="监控">监控</span><a href="#监控" class="header-anchor">#</a></h2><ul>
<li><p>Kafka中有那些配置参数比较有意思？聊一聊你的看法</p>
</li>
<li><p>Kafka有哪些指标需要着重关注？</p>
</li>
</ul>
<h2><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h2><ul>
<li><p>在使用Kafka的过程中遇到过什么困难？怎么解决的？</p>
</li>
<li><p>还用过什么同质类的其它产品，与Kafka相比有什么优缺点？</p>
</li>
<li><p>Kafka有什么优缺点？</p>
</li>
</ul>
<p><a href="../../../../2016/05/11/kafka/">Kafka总结</a></p>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>IDC网络互通</title>
    <url>/www6vHomeHexo/2019/05/15/netConnection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/05/15/netConnection/netConnection.jpg" class title="IDC网络互通">

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《极客时间  趣谈网络协议》  第25讲 软件定义网络：共享基础设施的小区物业管理办法  刘超</li>
<li>《极客时间  趣谈网络协议》  第37讲 知识串讲：用双十一的故事串起碎片的网络协议（上） 刘超</li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>有用的Git命令</title>
    <url>/www6vHomeHexo/2019/05/04/gitCommand/</url>
    <content><![CDATA[<p hidden>有用的Git命令</p>

<span id="more"></span>

<div style="width:70%; height:70%; text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64622936-b4daff00-d41a-11e9-9587-6d72df96498d.png" alt="git">   git<br><img src="https://user-images.githubusercontent.com/5608425/64622939-b4daff00-d41a-11e9-8978-8aea5e666237.png" alt="git">  git</p>
</div>

<h3><span id="stage-x3dx3d-index">stage &#x3D;&#x3D; index</span><a href="#stage-x3dx3d-index" class="header-anchor">#</a></h3><ol>
<li><p>git提交到远程分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m  &#x27;相关注释文字&#x27;</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure>
</li>
<li><p>一个分支（master）代码同步到另一个分支（dev）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git stash </span><br><span class="line">git fetch origin master </span><br><span class="line">git rebase origin dev </span><br><span class="line">git stash pop</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout -b dev</span><br></pre></td></tr></table></figure>
</li>
<li><p>强制覆盖本地文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch --all</span><br><span class="line">git reset --hard origin/master</span><br><span class="line">git reset --hard origin/&lt;branch_name&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://git-scm.com/book/zh/v2/Git-%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6">Git 分支 - 分支的新建与合并</a></li>
<li><a href="https://www.cnblogs.com/cheneasternsun/p/5952830.html">Git 基础图解、分支图解、全面教程、常用命令</a></li>
<li><a href="https://www.jianshu.com/p/776916f74a41">git使用小记—比较</a></li>
<li><a href="https://www.liangzl.com/get-article-detail-31025.html">一篇极好的Git 总结</a></li>
<li><a href="https://www.jianshu.com/p/2fd2467c27bb">Git 删除具体某个提交commit的方法</a></li>
<li><a href="https://vimsky.com/article/3679.html">“git pull”如何强制覆盖本地文件？</a></li>
<li><a href="https://www.jianshu.com/p/450cd21b36a4">关于git提示“warning: LF will be replaced by CRLF”终极解答</a></li>
</ol>
]]></content>
      <categories>
        <category>devops</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生</title>
    <url>/www6vHomeHexo/2019/05/02/cloudNative/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<img src="/www6vHomeHexo/2019/05/02/cloudNative/cloudNative.jpg" class title="云原生">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://jimmysong.io/kubernetes-handbook/cloud-native/from-kubernetes-to-cloud-native.html">云原生应用之路——从Kubernetes到Cloud Native</a></li>
<li><a href="https://edu.aliyun.com/lesson_1651_13079?spm=5176.254948.1334973.10.2c12cad2AHzzTw#_13079">课时4 理解 Pod 和容器设计模式</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>中台战略</title>
    <url>/www6vHomeHexo/2019/05/02/middleStage/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%AD%E5%8F%B0%E5%85%A8%E6%99%AF%E5%9B%BE">中台全景图</a></li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E4%B8%AD%E5%8F%B0">业务中台</a><ul>
<li><a href="#%E4%B8%9A%E5%8A%A1%E4%B8%AD%E5%8F%B0-1">业务中台</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E4%B8%AD%E5%8F%B0-123">技术中台 [1][2][3]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="中台全景图">中台全景图</span><a href="#中台全景图" class="header-anchor">#</a></h1><ul>
<li><p>中台全景图[6]</p>
<img src="/www6vHomeHexo/2019/05/02/middleStage/middleStage-overview.png" class title="中台全景图">
</li>
<li><p>中台和微服务[4]</p>
<img src="/www6vHomeHexo/2019/05/02/middleStage/middleAndService.JPG" class title="中台和微服务">
</li>
<li><p>中台</p>
<ul>
<li>业务中台<br>核心业务层<ul>
<li>技术中台<br>iaas+paas</li>
</ul>
</li>
<li>数据中台</li>
</ul>
</li>
<li><p>理念<br>  阿里提出： 大中台， 小前台</p>
</li>
</ul>
<h1><span id="业务中台">业务中台</span><a href="#业务中台" class="header-anchor">#</a></h1><h3><span id="业务中台">业务中台</span><a href="#业务中台" class="header-anchor">#</a></h3><ul>
<li><p>阿里共享服务 [6]<br>淘宝 天猫  <strong>共享</strong> 商品，交易，店铺等服务</p>
<img src="/www6vHomeHexo/2019/05/02/middleStage/middleStage-biz-ali.jpeg" class title="业务中台">

</li>
<li><p>京东业务中台 [5]</p>
<img src="/www6vHomeHexo/2019/05/02/middleStage/middleStage-biz.jpeg" class title="业务中台"></li>
</ul>
<h3><span id="技术中台-123">技术中台 [1][2][3]</span><a href="#技术中台-123" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2019/05/02/middleStage/middleStage-it.jpg" class title="技术中台">



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《企业IT架构转型之道-阿里巴巴中台战略思想与架构实战》 钟华</li>
<li><a href="https://mp.weixin.qq.com/s/Cfg-7MzabvPOLWrrlTVXzA">全面异步化：淘宝反应式架构升级探索</a></li>
<li><a href="https://mp.weixin.qq.com/s/RM3ffBCJqoQ2JMPKHgmv0Q">淘宝应用柔性架构的探索</a></li>
<li>《微服务架构核心20讲-如何理解阿里巴巴提出的微服务中台战略？》 杨波</li>
<li><a href="https://www.sohu.com/na/465557053_411876">中小型电商相当适配：京东商城系统架构设计原则精炼</a></li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统架构</category>
        <category>中台战略</category>
      </categories>
      <tags>
        <tag>中台战略</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 架构</title>
    <url>/www6vHomeHexo/2019/04/25/k8s/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-kubernetes-overview">一.  Kubernetes Overview</span><a href="#一-kubernetes-overview" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/04/25/k8s/kubenetes-arch.jpg" class title="Kubenetes架构">

<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63923811-c535c600-ca79-11e9-8057-2b6264b39d80.jpg" alt="kubernetes-high-level-component-archtecture"> kubernetes-high-level-component-archtecture</p>
</div>

<h2><span id="二-kubernetes控制器">二. Kubernetes控制器</span><a href="#二-kubernetes控制器" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">控制器</th>
<th align="center">说明</th>
<th align="center">类型</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Replication Controller<br> ReplicaSet<br> Deployment</td>
<td align="center">不建议使用<br> 建议使用<br></td>
<td align="center">无状态</td>
</tr>
<tr>
<td align="center">StatefulSet<br> PersistentVolumeController</td>
<td align="center"><br>绑定PVC、PV</td>
<td align="center">有状态</td>
</tr>
<tr>
<td align="center">Servcie【1】<br>Namespace</td>
<td align="center">x</td>
<td align="center">x</td>
</tr>
</tbody></table>
<div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/63924251-85bba980-ca7a-11e9-8a3a-4cf852dfe86c.JPG" alt="控制器模型">  控制器模型</p>
</div>




<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://product.dangdang.com/26439199.html?ref=book-65152-9168_1-529800-3">《Kubenetes in Action》</a>  七牛容器云团队</li>
<li><a href="../../../../2019/05/02/cloudNative/">云原生</a> self  </li>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/">Kubernetes架构</a>  jimmysong</li>
<li><a href="https://mp.weixin.qq.com/s/E5-agHtMvW_X7znVJDkTKA">面向 Kubernetes 编程： Kubernetes 是下一代操作系统</a>   </li>
<li><a href="https://mp.weixin.qq.com/s/xmVHqTZblQsAxdaSjWjn1Q">K8S 从懵圈到熟练：读懂此文，集群节点不下线</a> 阿里</li>
<li>&lt;&lt;深入剖析Kubernetes - 16  编排其实很简单：谈谈“控制器”模型&gt;&gt; 张磊</li>
</ol>
<hr>
<p>CNCF × Alibaba 云原生技术公开课</p>
<ol>
<li><a href="https://edu.aliyun.com/lesson_1651_13079?spm=5176.254948.1334973.10.2c12cad2AHzzTw#_13079">第4 章 ： 理解 Pod 和容器设计模式</a><br>Pod 进程组;  设计模式: initContainer模式 sidecar模式;</li>
<li><a href="https://edu.aliyun.com/lesson_1651_13078?spm=5176.254948.1334973.8.2c12cad2AHzzTw#_13078">第3 章 ： Kubernetes 核心概念</a></li>
<li><a href="https://edu.aliyun.com/lesson_1651_13080?spm=5176.254948.1334973.12.2c12cad2AHzzTw#_13080">第5 章 ： 应用编排与管理：核心原理</a>  video<br><a href="https://mp.weixin.qq.com/s/T1l5ebHqo0GUFfbfBnd-tQ">从零开始入门 K8s | K8s 的应用编排与管理</a> ppt+文字<br>控制器模式: 1. 控制循环 2. Sensor;    </li>
<li><a href="https://edu.aliyun.com/lesson_1651_13081?spm=5176.10731542.0.0.e7a120beywNIVX#_13081">第6 章 ： 应用编排与管理： Deployment</a><br>Deployment控制器; ReplicaSet控制器</li>
<li><a href="https://edu.aliyun.com/lesson_1651_17058#_17058">第8 章 ： 应用配置管理</a>  ppt 未<br> <a href="https://mp.weixin.qq.com/s/8r-_Ekje__GVHsKLfJ-66A">从零开始入门 K8s | 如何实现应用配置管理？</a>  ppt+文字 未<br> ConfigMap 可变配置; Secret 敏感信息; ServiceAccount 身份认证;  Resources; SecurityContext 安全管控;  InitContainers;</li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>十二要素-12 factor</title>
    <url>/www6vHomeHexo/2019/04/09/12factor/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2019/04/09/12factor/12factor.jpg" class title="十二要素">

<img src="/www6vHomeHexo/2019/04/09/12factor/12-factor-app.png" class title="十二要素">




<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://12factor.net/zh_cn/">12-Factor应用</a></li>
<li><a href="https://talks.bingohuang.com/2017/cloud-native-12factor.article">云原生时代下的12-factor应用与实践</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>十二要素</category>
      </categories>
      <tags>
        <tag>云原生</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark公司内部培训</title>
    <url>/www6vHomeHexo/2019/03/10/streamingSparkTrain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>  

<p>关键词： Spark架构, Spark优化</p>
<p><a href="https://www6v.github.io/www6vHomeHexo/2019/03/10/sparkTrain/sparkTrain.pptx">Spark公司内部培训</a></p>
<h2><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h2><ul>
<li>可靠性  驱动节点（Driver）失效  -&gt; checkpoint</li>
<li>ReduceByKey vs GroupByKey<br> ReduceByKey 先reduce，再shuffle， 更高效。<br> GroupByKey  先shuffle，再reduce。</li>
<li>foreachRDD、foreachPartition和foreach  3个Action<br> 与connection的关系</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 总结</title>
    <url>/www6vHomeHexo/2019/03/09/streamingSpark/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>   

<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/03/09/streamingSpark/spark.jpg" class title="spark总结">


<h2><span id="structured-streaming">Structured Streaming</span><a href="#structured-streaming" class="header-anchor">#</a></h2><ol>
<li><p>time<br>事件时间 event time<br>处理时间 processing time</p>
</li>
<li><p>DSL</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df = … // 这个 DataFrame 代表学校学生的数据流，schema 是&#123;name: string, age: number, height</span><br><span class="line">df.select(&quot;name&quot;).where(&quot;age &gt; 10&quot;) // 返回年龄大于 10 岁的学生名字列表</span><br><span class="line">df.groupBy(&quot;grade&quot;).count() // 返回每个年级学生的人数</span><br><span class="line">df.sort_values([‘age’], ascending=False).head(100) // 返回 100 个年龄最大的学生</span><br></pre></td></tr></table></figure>
</li>
<li><p>API<br><strong>Spark Streaming</strong> 提供的 DStream API 与 RDD API 很类似，相对<strong>比较低 level</strong>。<br><strong>Structured Streaming</strong> 提供的 DataFrame API 就是这么一个相对<strong>高 level <strong>的 API，大部分<br>开发者都很</strong>熟悉关系型数据库和 SQL</strong>。</p>
</li>
<li><p>实时性<br><strong>Spark Streaming</strong> 是准实时的，它能做到的最小延迟在一秒左右。 <strong>秒级</strong><br><strong>Structured Streaming</strong> 引入了<strong>连续处理</strong>的模式，可以做到真正的<strong>毫秒级</strong>延迟。</p>
</li>
</ol>
<h2><span id="spark-sql">spark-sql</span><a href="#spark-sql" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/03/09/streamingSpark/rdd-dataset-dataframe.PNG" class title="RDD,DataFrame,DataSet比较">


<h2><span id="spark-streaming-kafka-反压backpressure">Spark Streaming Kafka 反压（Backpressure）</span><a href="#spark-streaming-kafka-反压backpressure" class="header-anchor">#</a></h2><h2><span id="互联网运营常用数据指标">互联网运营常用数据指标</span><a href="#互联网运营常用数据指标" class="header-anchor">#</a></h2><ol>
<li>新增用户数<br>新增用户数有日新增用户数、周新增用户数、月新增用户数等几种统计口径</li>
<li>用户留存率<br>用户留存率 &#x3D; 留存用户数 &#x2F; 当期新增用户数</li>
<li>活跃用户数<br>有日活跃用户数、月活跃用户数</li>
<li>用户转化率<br>转化率 &#x3D; 有购买行为的用户数 &#x2F; 总访问用户数</li>
<li>PV<br>用户每次点击，每个页面跳转，被称为一个PV（Page View）</li>
<li>GMV<br>GMV即成交总金额（Gross Merchandise Volume），是电商网站统计营业额（流水</li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/www6v/r-tc-bill/blob/master/src/main/java/cloud/rtc/bill/SparkStreamingKafka.scala">计费项目&#x2F;spark streaming项目</a></li>
<li>《Spark大数据处理：技术、应用与性能优化》 高彦杰</li>
<li>《Spark大数据处理技术》 夏俊鸾,黄洁,程浩等</li>
<li><a href="https://blog.csdn.net/Scapel/article/details/84030362">Spark中foreachRDD、foreachPartition和foreach解读</a></li>
<li>[Spark SQL：Spark数据查询的利器]  蔡元楠</li>
<li>[Structured Streaming：如何用DataFrame API进行实时数据分析?]  蔡元楠</li>
<li><a href="https://github.com/xy2953396112/spark-sourcecodes-analysis/blob/master/structured-streaming/Structured-Streaming-%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97.md">Structured Streaming编程指南</a>  官方文档的中文版本  good  未 </li>
<li><a href>从0开始学大数据 - 32讲互联网运营数据指标与可视化监控</a>  李智慧</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>计算</category>
        <category>流式计算</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算中的Xaas</title>
    <url>/www6vHomeHexo/2019/02/07/xaas/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="xaas">XaaS</span><a href="#xaas" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2019/02/07/xaas/xaas.jpg" class title="Xaas">

<ul>
<li>aPaaS(行业相关的Kit, OpenApi)<ul>
<li>application Platform as a Service</li>
</ul>
</li>
</ul>
<h2><span id="paas-10">PaaS [10]</span><a href="#paas-10" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65187017-8a81e500-da9d-11e9-80f6-93654e689404.JPG" alt="PaaS"><br>Paas</p>
</div>



<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://yq.aliyun.com/articles/2918">BaaS后端即服务 - 通往中台架构之路</a></li>
<li><a href="https://yq.aliyun.com/articles/8521">BaaS后端即服务 - 概念篇</a></li>
<li><a href="https://yq.aliyun.com/articles/8522">BaaS后端即服务 - 分析篇</a></li>
<li><a href="https://yq.aliyun.com/articles/8523">BaaS后端即服务 - 中台篇</a></li>
<li><a href="https://yq.aliyun.com/articles/57221">BaaS云架构核心模式之Serverless架构 - 用服务代替服务器(Martin Fowler)</a></li>
<li><a href="https://martinfowler.com/articles/serverless.html">Serverless Architectures</a></li>
<li>《企业IT架构转型之道(阿里巴巴中台战略思想与架构实战)》 钟华</li>
<li>洞悉PaaS平台的本质  左耳听风</li>
<li><a href>深入浅出云计算-07 | 云端架构最佳实践：与故障同舞，与伸缩共生</a>  何恺铎</li>
<li><a href="https://baijiahao.baidu.com/s?id=1711957365528743403&wfr=spider&for=pc">华为云开天aPaaS上线，服务千万开发者，使能行业场景化创新</a></li>
</ol>
]]></content>
      <categories>
        <category>云计算</category>
        <category>xaas</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式系统学习资源-团队</title>
    <url>/www6vHomeHexo/2019/01/21/distributedStudyTeam/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="国内技术团队blog">国内技术团队Blog</span><a href="#国内技术团队blog" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>团队</th>
<th>状态</th>
</tr>
</thead>
<tbody><tr>
<td>阿里中间件技术博客 ***</td>
<td></td>
</tr>
<tr>
<td><a href="https://tech.meituan.com/">美团点评技术博客</a> ***</td>
<td>看到 2020年</td>
</tr>
<tr>
<td><a href="https://pingcap.com/blog-cn/">pingcap</a> ***</td>
<td></td>
</tr>
<tr>
<td><a href="https://www.jianshu.com/u/5000b3e8bd79"><a href="https://www.jianshu.com/u/5000b3e8bd79">得物技术</a></a></td>
<td></td>
</tr>
<tr>
<td><a href="http://mysql.taobao.org/monthly/">数据库内核月报 淘宝数据库组</a>   ***</td>
<td></td>
</tr>
<tr>
<td>SOFA 微信公众号</td>
<td></td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/28585781">饿了么框架工具部- 知乎</a>  异地多活 压测</td>
<td>2018停更</td>
</tr>
<tr>
<td><a href="https://techblog.toutiao.com/2017/05/02/dao/">今日头条 技术博客</a></td>
<td>已失效</td>
</tr>
<tr>
<td><a href="https://xiaomi-info.github.io/">小米信息部技术团队</a>  *</td>
<td>2020停更</td>
</tr>
<tr>
<td><a href="http://tengine.taobao.org/book/index.html">Nginx开发从入门到精通</a> Tengine book</td>
<td>2013停更</td>
</tr>
<tr>
<td><a href="https://help.aliyun.com/document_detail/67252.html">阿里云redis最佳实践</a></td>
<td>已失效</td>
</tr>
</tbody></table>
<p><a href="https://tech.antfin.com/">蚂蚁金融科技</a></p>
<p>参考<br><a href="http://www.cnblogs.com/IT-Bear/p/3191423.htmls">国内各大互联网公司相关技术站点2.0版 （集合腾讯、阿里、百度、搜狐、新浪、360等共49个）</a></p>
<h2><span id="知乎-有自己关注的大佬">知乎 [有自己关注的大佬]</span><a href="#知乎-有自己关注的大佬" class="header-anchor">#</a></h2><ul>
<li><p>大厂<br><a href="https://www.zhihu.com/org/e-han-jia-gou-shi">鹅厂架构师</a>  ***<br><a href="https://www.zhihu.com/org/mei-tuan-dian-ping-ji-shu-tuan-dui/posts">美团技术团队</a><br><a href="https://www.zhihu.com/org/vivohu-lian-wang-ji-zhu-2/posts">vivo互联网技术</a> ***   看了50%左右<br><a href="https://www.zhihu.com/org/xi-cheng-ji-shu-zhong-xin/posts">携程技术</a><br><a href="https://www.zhihu.com/people/a-li-xi-tong-ruan-jian-ji-zhu-90/posts">阿里巴巴开源</a><br><a href="https://www.zhihu.com/org/you-zan-ji-zhu-tuan-dui/activities">有赞技术团队 知乎</a>  *** 干货多<br><a href="https://www.zhihu.com/org/archsummit/activities">ArchSummit 知乎</a></p>
</li>
<li><p>中间件<br><a href="https://www.zhihu.com/org/teng-xun-yun-zhong-jian-jian/posts">腾讯云中间件</a><br><a href="https://www.zhihu.com/org/streamnative/posts">StreamNative Pulsar</a></p>
</li>
<li><p>DB<br><a href="https://www.zhihu.com/org/polardb-x/posts">PolarDB-X</a><br><a href="https://www.zhihu.com/people/zoeyzhai/posts">TiDB Robot</a><br><a href="https://www.zhihu.com/org/pingcap-25/posts">PingCAP</a><br><a href="https://www.zhihu.com/org/tencentdbteng-xun-yun-shu-ju-ku/posts">TencentDB腾讯云数据库</a></p>
</li>
<li><p>大数据&amp;流计算<br><a href="https://www.zhihu.com/people/ververica/posts">Flink 中文社区</a><br><a href="https://www.zhihu.com/org/datafuntalk/posts">DataFunTalk</a></p>
</li>
<li><p>运维&amp;SRE<br><a href="https://www.zhihu.com/people/sre-googleyun-wei-jie-mi/posts">SRE布道师</a><br><a href="https://www.zhihu.com/org/jia-wei-ke-ji-30/posts">嘉为蓝鲸</a></p>
</li>
<li><p>云原生<br><a href="https://www.zhihu.com/people/yunyuansheng/posts">云原生基地  DaoCloud</a><br><a href="https://www.zhihu.com/people/teng-xun-yun-yuan-sheng/posts">腾讯云原生</a></p>
</li>
<li><p>其他<br>阿里云云栖号<br>阿里云网站<br>腾讯云开发者<br>华为云开发者联盟<br>金融级分布式架构 SOFA</p>
</li>
</ul>
<h2><span id="开发者中心">开发者中心</span><a href="#开发者中心" class="header-anchor">#</a></h2><ul>
<li><a href="https://developer.baidu.com/">百度开发者中心</a> </li>
<li><strong>阿里云 开发者社区</strong><br>我的关注-大佬</li>
<li>腾讯云 开发者社区</li>
</ul>
<h2><span id="阿里云栖社区已更名为开发者社区">阿里云栖社区(已更名为开发者社区)</span><a href="#阿里云栖社区已更名为开发者社区" class="header-anchor">#</a></h2><ol start="0">
<li><a href="https://yq.aliyun.com/tags/">博客热门标签</a></li>
<li><a href="https://yq.aliyun.com/teams">阿里云栖  技术团队入口</a>  </li>
<li><a href="https://yq.aliyun.com/tags/tagid_523/">阿里技术协会 ATA</a>   找不到</li>
<li><a href="https://yq.aliyun.com/topic?spm=a2c4e.11154022.headermainnav.11.7037aMQGaMQGE8#guid-721571">阿里云社区做过的在线峰会</a></li>
<li><a href="https://yq.aliyun.com/teams/11/type_blog-cid_450-page_1">容器服务Docker&amp;Kubernetes</a></li>
<li><a href="https://yq.aliyun.com/users/1080464764156883?spm=a2c4e.11153940.blogrightarea54004.2.22c86a7d3BdzIB">中生代技术</a>  2018停更</li>
<li><a href="https://yq.aliyun.com/teams/16">阿里技术矩阵</a></li>
<li><a href="https://yq.aliyun.com/teams/6">阿里巴巴大数据 —玩家社区</a></li>
<li><a href="https://yq.aliyun.com/teams/28">阿里云网络产品</a>  </li>
<li><a href="https://yq.aliyun.com/teams/56/type_blog">阿里云服务</a></li>
<li><a href="https://yq.aliyun.com/teams/4">阿里云存储服务</a></li>
<li><a href="https://yq.aliyun.com/teams/20">阿里云持续交付平台</a></li>
<li><a href="https://yq.aliyun.com/teams/67">阿里云实时计算</a></li>
<li><a href="https://yq.aliyun.com/teams/22">阿里中间件团队</a>  </li>
<li><a href="https://yq.aliyun.com/teams/252">阿里巴巴云原生</a></li>
</ol>
<h2><span id="国外技术公司">国外技术公司</span><a href="#国外技术公司" class="header-anchor">#</a></h2><p><a href="https://code.fb.com/">facebook</a><br><a href="https://engineering.linkedin.com/blog">linkedin techblog</a><br><a href="https://medium.com/netflix-techblog">netflix techblog</a>  失效<br><a href="https://www.techiedelight.com/">techiedelight</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes集群搭建(二进制)</title>
    <url>/www6vHomeHexo/2019/01/17/k8sSetup/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li><p>禁用swap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看这些端口是否被占用，如果被占用，请手动释放。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -ntlp |grep -E &#x27;6443|23[79,80]|1025[0,1,2]&#x27; </span><br></pre></td></tr></table></figure>
</li>
<li><p>安装 kubectl</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$  wget https://dl.k8s.io/v1.10.7/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">$  echo &#x27;169b57c6707ed8d8be9643b0088631e5c0c6a37a5e99205f03c1199cd32bc61e  kubernetes-client-linux-amd64.tar.gz&#x27; | sha256sum -c -</span><br><span class="line">$  tar zxf kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">$  sudo mv kubernetes/client/bin/kubectl /usr/local/bin/kubectl</span><br><span class="line">$  /usr/local/bin/kubectl version --client</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装 kubeadm 和 kubelet</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ wget -q https://dl.k8s.io/v1.11.3/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">如不能访问请使用以下方式下载</span><br><span class="line">链接: https://pan.baidu.com/s/1FSEcEUplQQGsjyBIZ6j2fA 提取码: cu4s</span><br><span class="line"></span><br><span class="line">$ echo &#x27;e49d0db1791555d73add107d2110d54487df538b35b9dde0c5590ac4c5e9e039 kubernetes-server-linux-amd64.tar.gz&#x27; | sha256sum -c -</span><br><span class="line">$ tar -zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">$ mv kubernetes/server/bin/kube&#123;adm,ctl,let&#125; /usr/bin/</span><br><span class="line"></span><br><span class="line">$ kubeadm version</span><br><span class="line">$ kubectl version --client</span><br><span class="line">$ kubelet --version</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>配置 kubelet</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=kubelet: The Kubernetes Agent</span><br><span class="line">Documentation=http://kubernetes.io/docs/</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/bin/kubelet</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ mkdir -p /etc/systemd/system/kubelet.service.d</span><br><span class="line">$ cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line">EnvironmentFile=-/etc/default/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS</span><br><span class="line">EOF</span><br><span class="line">$ systemctl enable kubelet</span><br></pre></td></tr></table></figure>
<p>6.<br>安装前置依赖 crictl</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.11.1/crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">$ echo &#x27;ccf83574556793ceb01717dc91c66b70f183c60c2bbec70283939aae8fdef768 crictl-v1.11.1-linux-amd64.tar.gz&#x27; | sha256sum -c -</span><br><span class="line">$ tar zxvf crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">$ mv crictl /usr/bin/</span><br></pre></td></tr></table></figure>

<p>安装前置依赖 socat</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install -y socat</span><br><span class="line">apt-get install -y socat</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>配置集群网络 + 初始化集群</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ kubeadm reset </span><br><span class="line"></span><br><span class="line"># 使用 flannel </span><br><span class="line">$ kubeadm init --pod-network-cidr=10.244.0.0/16</span><br><span class="line">  会遇到如下异常：</span><br><span class="line">   kubeadm config images pull命令一直卡死</span><br><span class="line">  解决方式：</span><br><span class="line">   国内的代码托管平台提供了一个[仓库](https://gitee.com/K8S-release/kubeadm)</span><br><span class="line">   对每个 tar 文件执行 sudo docker load -i xx.tar 即可将镜像导入.</span><br><span class="line"></span><br><span class="line"># 验证   </span><br><span class="line">$ kubectl get nodes</span><br><span class="line">  会遇到如下异常：</span><br><span class="line">  The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br><span class="line">  解决方式：</span><br><span class="line">    # mkdir -p $HOME/.kube</span><br><span class="line">    # sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">    # sudo chown $(id -u):$(id -g) $HOME/.kube/config    </span><br><span class="line"></span><br><span class="line"># 验证</span><br><span class="line">$ kubectl get nodes -o yaml</span><br><span class="line">  会遇到如下异常：</span><br><span class="line">  lastTransitionTime: 2018-09-20T01:09:48Z</span><br><span class="line">      message: &#x27;runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady</span><br><span class="line">        message:docker: network plugin is not ready: cni config uninitialized&#x27;</span><br><span class="line">      reason: KubeletNotReady</span><br><span class="line">  解决方式：      </span><br><span class="line">  $ sysctl net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">  # 注意，这里的 flannel 配置仅适用于 1.11 版本的 K8S，若安装其他版本的 K8S 需要替换掉此链接</span><br><span class="line">  $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</span><br><span class="line">  # 检查状态</span><br><span class="line">  kubectl get pods --all-namespaces</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="8">
<li><p>新增 Node</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ kubeadm join 10.4.65.33:6443 --token ci6511.ydxqtk4kx3vrqq5m --discovery-token-ca-cert-hash sha256:e2d8ba4692ead9649bae6dfd1b3459e4fc1ade5c05fb6868fccf03d8f4e87c8c</span><br><span class="line"></span><br><span class="line"># 在master上验证</span><br><span class="line">$ kubectl get nodes</span><br></pre></td></tr></table></figure>
</li>
<li><p>可能会遇到的问题</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/run/flannel/subnet.env: no such file or directory</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://juejin.im/book/5b9b2dc86fb9a05d0f16c8ac/section/5b9b8346f265da0af03375ed">动手实践：搭建一个 Kubernetes 集群 - 生产可用</a></li>
</ol>
]]></content>
      <categories>
        <category>云原生</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux性能分析</title>
    <url>/www6vHomeHexo/2018/12/26/linuxProfile/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#linux-observability-tools-linux-%E6%80%A7%E8%83%BD%E8%A7%82%E6%B5%8B%E5%B7%A5%E5%85%B7">Linux observability tools | Linux 性能观测工具</a></li>
<li><a href="#%E5%BF%AB%E9%80%9F%E6%80%A7%E8%83%BD%E8%AF%8A%E6%96%AD%E5%BF%AB%E9%80%9F%E4%BD%93%E6%A3%80">快速性能诊断(快速体检)</a><ul>
<li><a href="#1-%E7%B3%BB%E7%BB%9F%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD">1. 系统平均负载</a></li>
<li><a href="#2-dmesg-tail-%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF-%E5%AF%BC%E8%87%B4%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E7%9A%84%E9%94%99%E8%AF%AF">2. dmesg | tail : 系统信息  导致性能问题的错误</a></li>
<li><a href="#3-vmstat-1-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98">3. vmstat 1  : 虚拟内存</a></li>
<li><a href="#4-mpstat-p-all-1-cpu%E5%88%86%E8%A7%A3%E6%97%B6%E9%97%B4">4. mpstat -P ALL 1 :  CPU分解时间</a></li>
<li><a href="#5-pidstat-1-%E6%AF%8F%E4%B8%AA%E8%BF%9B%E7%A8%8B%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%91%98%E8%A6%81">5. pidstat 1 ： 每个进程的统计摘要</a></li>
<li><a href="#6-iostat-xz-1-%E7%A3%81%E7%9B%98">6. iostat -xz 1 ： 磁盘</a></li>
<li><a href="#7-free-m-%E5%86%85%E5%AD%98">7. free -m ： 内存</a></li>
<li><a href="#8-sar-n-dev-1-%E7%BD%91%E8%B7%AF%E5%90%9E%E5%90%90">8. sar -n DEV 1 : 网路吞吐</a></li>
<li><a href="#9-sar-n-tcpetcp-1-tcp%E6%8C%87%E6%A0%87">9. sar -n TCP,ETCP 1 :  TCP指标</a></li>
<li><a href="#10-top-%E5%8F%98%E5%8C%96%E7%9A%84%E8%B4%9F%E8%BD%BD%E7%9A%84%E6%B1%87%E6%80%BB">10. top ： 变化的负载的汇总</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD">性能</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="linux-observability-tools-linux-性能观测工具">Linux observability tools | Linux 性能观测工具</span><a href="#linux-observability-tools-linux-性能观测工具" class="header-anchor">#</a></h1><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65083384-fe0eee00-d9da-11e9-9b5f-c3361a273b67.jpg" alt="linux_observe_tools"><br>Linux性能观测工具</p>
</div>


<h1><span id="快速性能诊断快速体检">快速性能诊断(快速体检)</span><a href="#快速性能诊断快速体检" class="header-anchor">#</a></h1><h3><span id="1-系统平均负载">1. 系统平均负载</span><a href="#1-系统平均负载" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> # 0.25- 1分钟负载 ， 0.22-5分钟负载， 0.23-15分钟负载</span><br><span class="line"> [root@10-25-3-55 /]# uptime</span><br><span class="line"> 23:02:19 up 285 days, 11:37,  1 user,  load average: 0.25, 0.22, 0.23</span><br><span class="line"></span><br><span class="line"># 如果cpu个数是4， 则平均负载4是合理的。</span><br><span class="line">[root@10-23-25-248]$grep &#x27;model name&#x27; /proc/cpuinfo | wc -l</span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<ul>
<li><p>平均负载:<br>单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是单位时间内的活跃进程数。</p>
</li>
<li><p>场景<br>CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；<br>I&#x2F;O 密集型进程，等待 I&#x2F;O 也会导致平均负载升高，但 CPU 使用率不一定很高；<br>大量等待 CPU 的进程调度也会导致平均负载升高，此时的CPU使用率也会比较高。</p>
</li>
</ul>
<h3><span id="2-dmesg-tail-系统信息-导致性能问题的错误">2. dmesg | tail : 系统信息  导致性能问题的错误</span><a href="#2-dmesg-tail-系统信息-导致性能问题的错误" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[9927609.690053] ffmpeg invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0</span><br><span class="line">[9927609.690106]  [&lt;ffffffff81184c7e&gt;] oom_kill_process+0x24e/0x3c0</span><br><span class="line">[9927609.690109]  [&lt;ffffffff8118471d&gt;] ? oom_unkillable_task+0xcd/0x120</span><br></pre></td></tr></table></figure>
<h3><span id="3-vmstat-1-虚拟内存">3. vmstat 1  : 虚拟内存</span><a href="#3-vmstat-1-虚拟内存" class="header-anchor">#</a></h3><p>   进程(饱和度)   内存    CPU  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 8  0      0 402204 303208 8686376    0    0     2    10    0    0  5  6 89  0  0</span><br></pre></td></tr></table></figure>
<h3><span id="4-mpstat-p-all-1-cpu分解时间">4. mpstat -P ALL 1 :  CPU分解时间</span><a href="#4-mpstat-p-all-1-cpu分解时间" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">11:08:02 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">11:08:03 PM  all   28.68    0.00   60.85    0.00    0.00    0.00    0.00    0.00    0.00   10.47</span><br><span class="line">11:08:03 PM    0   28.43    0.00   60.78    0.00    0.00    0.00    0.00    0.00    0.00   10.78</span><br><span class="line">11:08:03 PM    1   28.71    0.00   61.39    0.00    0.00    0.00    0.00    0.00    0.00    9.90</span><br><span class="line">11:08:03 PM    2   29.00    0.00   59.00    0.00    0.00    0.00    0.00    0.00    0.00   12.00</span><br><span class="line">11:08:03 PM    3   28.71    0.00   61.39    0.00    0.00    0.00    0.00    0.00    0.00    9.90</span><br></pre></td></tr></table></figure>
<h3><span id="5-pidstat-1-每个进程的统计摘要">5. pidstat 1 ： 每个进程的统计摘要</span><a href="#5-pidstat-1-每个进程的统计摘要" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">11:11:14 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command</span><br><span class="line">11:11:15 PM     0      8715    1.00    0.00    0.00    1.00     3  pidstat</span><br><span class="line">11:11:15 PM     0     27930    0.00    1.00    0.00    1.00     2  java</span><br><span class="line">11:11:15 PM     0     28042    1.00    0.00    0.00    1.00     1  java</span><br><span class="line">11:11:15 PM     0     28044    1.00    0.00    0.00    1.00     1  java</span><br><span class="line"></span><br><span class="line">Average:      UID       PID    %usr %system  %guest    %CPU   CPU  Command</span><br><span class="line">Average:        0        23    0.00    0.07    0.00    0.07     -  ksoftirqd/3</span><br><span class="line">Average:       38       531    0.00    0.07    0.00    0.07     -  ntpd</span><br><span class="line">Average:        0      2642    0.07    0.00    0.00    0.07     -  aliyun-service</span><br><span class="line">Average:       27      2784    0.00    0.07    0.00    0.07     -  mysqld</span><br><span class="line">Average:        0      7462    0.73    0.20    0.00    0.93     -  java</span><br></pre></td></tr></table></figure>
<h3><span id="6-iostat-xz-1-磁盘">6. iostat -xz 1 ： 磁盘</span><a href="#6-iostat-xz-1-磁盘" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">          29.15    0.00   59.30    0.25    0.00   11.31</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">vda               0.00     0.00    1.00    1.00     8.00     4.00    12.00     0.05   26.50   53.00    0.00  26.50   5.30</span><br></pre></td></tr></table></figure>
<h3><span id="7-free-m-内存">7. free -m ： 内存</span><a href="#7-free-m-内存" class="header-anchor">#</a></h3><pre><code>buffer， cache， Swap
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:          32013       22842         391           1        8779        8723</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure>
<h3><span id="8-sar-n-dev-1-网路吞吐">8. sar -n DEV 1 : 网路吞吐</span><a href="#8-sar-n-dev-1-网路吞吐" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line">Average:    vethd4593b4      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:         eth0      1.67      1.52      0.17      1.66      0.00      0.00      0.00</span><br><span class="line">Average:         eth1      2.74     75.08      0.16      8.86      0.00      0.00      0.00</span><br><span class="line">Average:           lo      4.86      4.86      0.71      0.71      0.00      0.00      0.00</span><br><span class="line">Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br></pre></td></tr></table></figure>
<h3><span id="9-sar-n-tcpetcp-1-tcp指标">9. sar -n TCP,ETCP 1 :  TCP指标</span><a href="#9-sar-n-tcpetcp-1-tcp指标" class="header-anchor">#</a></h3><p><img src="https://user-images.githubusercontent.com/5608425/65831070-f5c28700-e2e7-11e9-9fe7-51ccdf43451b.png" alt="sar-tcp"></p>
<h3><span id="10-top-变化的负载的汇总">10. top ： 变化的负载的汇总</span><a href="#10-top-变化的负载的汇总" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="性能">性能</span><a href="#性能" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/yhSJF6Il6iJamYCosVgXYQ">超全整理！Linux性能分析工具汇总合集</a></li>
<li><a href="https://www.oschina.net/translate/linux-performance-analysis-in-60s?print">60,000毫秒内对Linux的性能诊断</a></li>
<li><a href="https://coolshell.cn/articles/7829.html">28个UNIX&#x2F;LINUX的命令行神器</a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>性能分析</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>性能优化总结</title>
    <url>/www6vHomeHexo/2018/11/21/performance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%85%B3%E9%94%AE%E8%AF%8D">关键词</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E4%BC%98%E5%8C%96">应用优化</a></li>
<li><a href="#%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96">算法优化</a></li>
<li><a href="#%E9%80%9A%E7%94%A8%E4%BC%98%E5%8C%96">通用优化</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96">数据库优化</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96">架构&#x2F;系统优化</a></li>
<li><a href="#linux%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96">Linux系统优化</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="关键词">关键词</span><a href="#关键词" class="header-anchor">#</a></h1><p>锁优化，池化，数据库优化，架构优化， 系统优化，性能测试， 监控 </p>
<h1><span id="应用优化">应用优化</span><a href="#应用优化" class="header-anchor">#</a></h1><ul>
<li>应用<ul>
<li>锁 【4】<ul>
<li>粒度<ul>
<li>粗化</li>
</ul>
</li>
<li>最小化锁范围<ul>
<li>Eg， 单线程写文件</li>
</ul>
</li>
<li>锁拆分，分散锁<ul>
<li>减少竞争，race condition</li>
<li>eg. ConcurrentHashMap，LongAdder</li>
</ul>
</li>
<li>分离锁<ul>
<li>读写锁<ul>
<li>读多写少的场景</li>
</ul>
</li>
<li>队头队尾， 两把锁</li>
</ul>
</li>
<li>乐观锁<ul>
<li>CAS</li>
</ul>
</li>
</ul>
<p>	  </p>
</li>
<li>并行  【1】<ul>
<li>多线程<ul>
<li>fork-join模式【2】</li>
<li>本地化 Eg. ThreadLocal</li>
</ul>
</li>
<li>Actor<ul>
<li>Eg. Akka</li>
</ul>
</li>
<li>CSP<ul>
<li>Eg. Goroutie</li>
</ul>
</li>
<li>函数式范性<ul>
<li>不可变对象</li>
</ul>
</li>
<li>单核单线程<ul>
<li>Eg. Redis, Nginx</li>
</ul>
</li>
</ul>
</li>
<li>池化(重用)<ul>
<li>Eg. 线程池，数据库连接池</li>
</ul>
</li>
<li>代码调优<ul>
<li>字符串操作</li>
<li>多线程调优<ul>
<li>锁 【4】</li>
<li>线程个数【9】</li>
</ul>
</li>
<li>异步操作  【8】</li>
<li>简化代码</li>
<li>热点优化  【5】</li>
<li>数值精度<ul>
<li>Eg. 双精度 单精度</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="算法优化">算法优化</span><a href="#算法优化" class="header-anchor">#</a></h1><ul>
<li>算法<ul>
<li>算法调优<ul>
<li>分而治之 【6】<ul>
<li>fork-join【2】</li>
<li>Map-Reduce</li>
</ul>
</li>
<li>哈希算法</li>
<li>预处理<ul>
<li>提前计算(预处理)，最后合并</li>
</ul>
</li>
</ul>
</li>
<li>算法和数据结构<ul>
<li>算法复杂度 大O表示【10】<ul>
<li>时间复杂度<ul>
<li>O(1)<ul>
<li>数组访问</li>
<li>栈、链表的插入&#x2F;删除</li>
<li>Hash Table</li>
</ul>
</li>
<li>O(log(n))<ul>
<li>二叉搜索树<ul>
<li>自平衡二叉搜索树<br>  AVL树<br>  红黑树</li>
</ul>
</li>
<li>B树(多路树)</li>
</ul>
</li>
<li>O(n log(n))<ul>
<li>快排、归并、堆<br>  快排(分治算法【6】)</li>
</ul>
</li>
<li>O(n)</li>
<li>O(n^2)<ul>
<li>选择、插入、冒泡</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>数据结构<ul>
<li>树，链表，栈，队列</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="通用优化">通用优化</span><a href="#通用优化" class="header-anchor">#</a></h1><ul>
<li>通用方法<ul>
<li>异步化  【8】<ul>
<li>消息<ul>
<li>进程内<br> Eg.  Disruptor, EventBus</li>
<li>Broker</li>
</ul>
</li>
<li>事件<br>Eg. EventSource</li>
<li>服务异步化<br>Eg. 异步网关</li>
</ul>
</li>
<li>Batch<ul>
<li>Eg. redis pipeline</li>
<li>buffer io</li>
</ul>
</li>
<li>Copy on Write<ul>
<li>Eg. mysql MVCC, CopyOnWriteArrayList</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="数据库优化">数据库优化</span><a href="#数据库优化" class="header-anchor">#</a></h1><ul>
<li>索引优化</li>
<li>大表优化<ul>
<li>水平拆分</li>
<li>垂直拆分</li>
</ul>
</li>
<li>慢SQL优化</li>
<li>乐观锁</li>
</ul>
<h1><span id="架构x2f系统优化">架构&#x2F;系统优化</span><a href="#架构x2f系统优化" class="header-anchor">#</a></h1><ul>
<li>架构&#x2F;系统优化<ul>
<li>弹性&#x2F;伸缩<ul>
<li>资源调度 Eg. k8s</li>
<li>资源扩容</li>
</ul>
</li>
<li>可扩展<ul>
<li>垂直拆分</li>
<li>水平拆分</li>
</ul>
</li>
<li>分布式<ul>
<li>并行 【1】</li>
<li>延迟<br>Eg.  跨IDC网络延迟 上海&lt;-&gt;北京  50ms</li>
</ul>
</li>
<li>节省空间&#x2F;时间换空间【8、11】<ul>
<li>不存储，重新计算</li>
<li>稀疏数据结构<br>  Eg. 稀疏矩阵</li>
<li>数据压缩传输<br>  RSYNC 的核心算法<br>  Huffman 编码压缩算法</li>
<li>动态分配策略<br>  Eg. ArrayList(10)，HashMap(16)的动态扩展</li>
<li>垃圾回收</li>
</ul>
</li>
<li>空间换时间<ul>
<li>缓存</li>
<li>数据冗余，replication</li>
</ul>
</li>
<li>系统结构<ul>
<li>大型系统分解成模块<br>“粗略估算”，性能分析</li>
</ul>
</li>
<li>系统软件<ul>
<li>替换更快的操作系统、中间件、数据库、编译器</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="linux系统优化">Linux系统优化</span><a href="#linux系统优化" class="header-anchor">#</a></h1><ul>
<li>系统<ul>
<li>文件系统<ul>
<li>pageCache, 预读<br>Eg. Rocketmq</li>
<li>顺序写，随机读</li>
<li>大块读优于小块读</li>
</ul>
</li>
<li>CPU<ul>
<li>CPU绑定</li>
<li>CPU上下文切换<ul>
<li>Eg. Redis单线程<br>CPU不是Redis的瓶颈，<br>瓶颈可能是内存的大小或者网络带宽</li>
</ul>
</li>
<li>CPU缓存<ul>
<li>问题：伪共享<br>解决：cacheline padding</li>
</ul>
</li>
</ul>
</li>
<li>网络I&#x2F;O<ul>
<li>NIO,非阻塞I&#x2F;O，epoll</li>
<li>协议调优<br>TCP参数</li>
</ul>
</li>
<li>内存<ul>
<li>zeroCopy</li>
<li>HugePage</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://dwz.cn/4SrP4L">高性能高并发系统的稳定性保障</a> 京东</li>
<li><a href="https://time.geekbang.org/column/intro/140">Linux性能优化实战</a>  极客时间</li>
<li><a href="https://coolshell.cn/articles/17381.html">性能测试应该怎么做？</a>  coolshell  deleted</li>
<li><a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a>  self  deleted</li>
<li><a href="http://blog.jobbole.com/88958/">关于容量预估&#x2F;性能压测的思考</a>  deleted  失效</li>
<li><a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a>   self  重复的 deleted</li>
<li>xxx</li>
<li><a href="https://coolshell.cn/articles/7490.html/comment-page-1">性能调优攻略</a>  coolshell  *** </li>
<li><a href="/www6vHomeHexo/2014/07/02/threadNum/" title="线程池最佳线程数">线程池最佳线程数</a>  self</li>
<li><a href="http://www.codeceo.com/article/algorithm-complexity-table.html">每个程序员都应该收藏的算法复杂度速查表</a></li>
<li>《编程珠玑 第2版》 </li>
<li><a href="https://yq.aliyun.com/articles/25487">wordcount设计与优化</a>  竞赛题</li>
</ol>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><ol start="100">
<li><a href="https://colin-scott.github.io/personal_website/research/interactive_latency.html">latency</a>  未  </li>
<li><a href="/www6vHomeHexo/2019/09/10/mysqlIndex/" title="MySQL的索引和优化">MySQL的索引和优化</a>     self 未</li>
<li><a href="/www6vHomeHexo/2020/03/26/nginxOptimize/" title="Nginx优化">Nginx优化</a>   self 未</li>
</ol>
]]></content>
      <categories>
        <category>性能</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>性能</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title>故障模型-应用层</title>
    <url>/www6vHomeHexo/2018/10/27/faultModel1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B-application-data">故障模型-Application &amp; Data</a><ul>
<li><a href="#oom">OOM</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E5%8F%98%E5%B7%AE">应用性能变差</a></li>
<li><a href="#load%E8%BF%87%E9%AB%98">Load过高</a></li>
<li><a href="#%E8%BF%9B%E7%A8%8Bhang">进程Hang</a></li>
<li><a href="#%E8%BF%9B%E7%A8%8B%E8%A2%AB%E6%9D%80-jvm-crash">进程被杀， JVM crash</a></li>
<li><a href="#%E5%BC%82%E5%B8%B8">异常</a></li>
<li><a href="#cicd">CI&#x2F;CD</a></li>
<li><a href="#%E7%B3%BB%E7%BB%9F%E5%8D%95%E7%82%B9">系统单点</a></li>
<li><a href="#%E5%BC%82%E6%AD%A5%E9%98%BB%E5%A1%9E%E5%90%8C%E6%AD%A5">异步阻塞同步</a></li>
<li><a href="#%E4%BE%9D%E8%B5%96%E8%B6%85%E6%97%B6%E4%BE%9D%E8%B5%96%E5%BC%82%E5%B8%B8">依赖超时，依赖异常</a></li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%BB%A1">业务线程池满</a></li>
<li><a href="#%E6%B5%81%E6%8E%A7%E4%B8%8D%E5%90%88%E7%90%86">流控不合理</a></li>
<li><a href="#%E5%85%B6%E4%BB%96-%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86">其他-数据收集</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%A4%A7%E7%BA%B2">大纲</a></li>
<li><a href="#application-data">Application &amp; Data</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="故障模型-application-amp-data">故障模型-Application &amp; Data</span><a href="#故障模型-application-amp-data" class="header-anchor">#</a></h1><h3><span id="oom">OOM</span><a href="#oom" class="header-anchor">#</a></h3><pre><code>    + 堆内 【10】
        - PermGen
            + 原因：反射类多
            + 解决
              先jmap，后btrace【11、12 case2】
        - heap
            解决：对比Fullgc后的相同对象的数量、大小
    + 堆外Native
</code></pre>
<p>参考:<br>3. <a href="/www6vHomeHexo/2014/07/16/gc/" title="垃圾收集GC总结">垃圾收集GC总结</a>  self<br>10. 如何排查Java内存泄露(内附各种排查工具介绍) 不闻<br>11. <a href="https://hllvm-group.iteye.com/group/topic/28379">生产环境下持久带满导致FullGC，如何跟踪</a><br>12. <a href="/www6vHomeHexo/2017/11/27/optimize/" title="JVM性能调优">JVM性能调优</a>  self</p>
<h3><span id="应用性能变差">应用性能变差</span><a href="#应用性能变差" class="header-anchor">#</a></h3><pre><code>    + 原因
        - 锁  
        - heap
            + fullgc后没有空间
                原因：内存泄露
                工具：heap dump
            + fullgc后有空间
                解决：设置门槛，过滤大量短生命周期对象【12 case1，13】
        - gc停顿长
            解决：【12 case3，13】
</code></pre>
<p>参考:<br>3. <a href="/www6vHomeHexo/2014/07/16/gc/" title="垃圾收集GC总结">垃圾收集GC总结</a>  self<br>12. <a href="/www6vHomeHexo/2017/11/27/optimize/" title="JVM性能调优">JVM性能调优</a>  self<br>13. <a href="https://blog.csdn.net/github_32521685/article/details/89953050">听阿里巴巴JVM工程师为你分析常见Java故障案例</a>  ***</p>
<h3><span id="load过高">Load过高</span><a href="#load过高" class="header-anchor">#</a></h3><pre><code>    + cpu load高【13】
        - 启动阶段
            原因：JIT编译器
            解决：分层编译
        - 运行阶段
            原因：有热点方法
            工具：MAT
        - 解决
            工具【8、9】
            数据收集  4、5
</code></pre>
<p>参考:<br>不正当使用HashMap导致cpu 100%的问题追究 王宏江<br><a href="http://www.cnblogs.com/study-everyday/p/7426862.html">一个由正则表达式引发的血案</a>   CPU飚高<br>8. <a href="https://github.com/vipshop/vjtools">vjtools</a><br>9. <a href="https://github.com/oldratlee/useful-scripts">useful-scripts</a><br>13. <a href="https://blog.csdn.net/github_32521685/article/details/89953050">听阿里巴巴JVM工程师为你分析常见Java故障案例</a>  ***</p>
<h3><span id="进程hang">进程Hang</span><a href="#进程hang" class="header-anchor">#</a></h3><h3><span id="进程被杀-jvm-crash">进程被杀， JVM crash</span><a href="#进程被杀-jvm-crash" class="header-anchor">#</a></h3><pre><code>    + 原因：
      JNI 
      Unsafe
    + 工具：core dump
</code></pre>
<h3><span id="异常">异常</span><a href="#异常" class="header-anchor">#</a></h3><pre><code>    + 启动异常
    + 心跳异常
</code></pre>
<h3><span id="cix2fcd">CI&#x2F;CD</span><a href="#cix2fcd" class="header-anchor">#</a></h3><pre><code>+ 环境错误
+ 部署包错误
+ 配置错误，误删
</code></pre>
<h3><span id="系统单点">系统单点</span><a href="#系统单点" class="header-anchor">#</a></h3><pre><code>    + 原因: 设计问题
    + 解决：服务去状态，多实例部署
</code></pre>
<h3><span id="异步阻塞同步">异步阻塞同步</span><a href="#异步阻塞同步" class="header-anchor">#</a></h3><h3><span id="依赖超时依赖异常">依赖超时，依赖异常</span><a href="#依赖超时依赖异常" class="header-anchor">#</a></h3><pre><code>    + 解决【15,16】
        - 区分强弱依赖
        - 压测 + 容量规划
        - 熔断 &amp;&amp; 降级
</code></pre>
<p>参考:<br>15. <a href="/www6vHomeHexo/2016/10/07/soaTolerateFramework/" title="容错框架">容错框架</a>  self<br>16. <a href="/www6vHomeHexo/2017/05/09/stability/" title="稳定性总结">稳定性总结</a>  self</p>
<h3><span id="业务线程池满">业务线程池满</span><a href="#业务线程池满" class="header-anchor">#</a></h3><pre><code>    + 最佳实践:【4】
    + 案例：【14】
        - 问题：使用blockingQueue.put
        - 解决：blockingQueue.offer（time超时机制）+限制队列长度
        - 最佳实践：生产上线程池的core Size和poolSize设置的一样，请求不在队列里排队
</code></pre>
<p>参考：<br>4. <a href="/www6vHomeHexo/2014/07/02/threadNum/" title="线程池最佳线程数">线程池最佳线程数</a>  self<br>14. <a href="http://hellojava.info/?p=464">从一个故障说说Java的三个BlockingQueue  阿里毕玄</a></p>
<h3><span id="流控不合理">流控不合理</span><a href="#流控不合理" class="header-anchor">#</a></h3><h3><span id="其他-数据收集">其他-数据收集</span><a href="#其他-数据收集" class="header-anchor">#</a></h3><pre><code>    + 1.Heap dump
    + 2. GC 日志
    + 3. core dump
    + 4. 线程stack
    + 5.os进程信息
</code></pre>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="大纲">大纲</span><a href="#大纲" class="header-anchor">#</a></h3><ol>
<li><a href="https://developer.aliyun.com/article/105551">超全总结 | 阿里如何应对电商故障？神秘演练细节曝光</a> 阿里巴巴  周洋</li>
</ol>
<h3><span id="application-amp-data">Application &amp; Data</span><a href="#application-amp-data" class="header-anchor">#</a></h3><ol start="5">
<li><a href="https://mp.weixin.qq.com/s/QA_BTF1D3GJJ7_nYQ6oAzQ">如何检测 Web 服务请求丢失问题</a>  Nginx tracing + Tomcat tracing</li>
<li><a href="https://www.infoq.cn/article/system_failure_modeling/">系统中的故障场景建模</a>  应用层</li>
<li>大方法 codecache</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障模型</category>
      </categories>
      <tags>
        <tag>故障模型</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 汇总</title>
    <url>/www6vHomeHexo/2018/10/04/awsSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="计算">计算</span><a href="#计算" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2022/03/30/awsComputing/">AWS Computing</a></li>
<li><a href="../../../../2022/06/22/awsComputingELB/">AWS Computing-ELB</a></li>
</ul>
<h2><span id="存储">存储</span><a href="#存储" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2022/10/01/awsStorageS3/">AWS Storage-S3</a></li>
<li><a href="../../../../2022/06/17/awsStorageEBS/">AWS Storage-EBS</a></li>
<li><a href="../../../../2022/06/17/awsStorageEFS/">AWS Storage-EFS</a></li>
</ul>
<h2><span id="网络">网络</span><a href="#网络" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2022/10/01/awsNetwork/">AWS Network</a></li>
<li><a href="../../../../2022/10/30/awsNetworkVPC/">AWS Network-VPC</a></li>
<li><a href="../../../../2022/06/17/awsNetworkVPCendpoint/">AWS Network-VPC Endpoint</a></li>
<li><a href="../../../../2022/10/30/awsNetworkDX/">AWS Network-Direct Connect</a></li>
<li><a href="../../../../2022/05/03/awsNetworkCDN/">AWS Network-CDN</a></li>
<li><a href="../../../../2022/05/03/awsNetworkDNS/">AWS Network-DNS</a></li>
<li><a href="../../../../2022/05/05/awsNetworkTGW/">AWS Network-TWG</a></li>
</ul>
<h2><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h2><ul>
<li><a href="../../../../2022/10/01/awsAllServices/">AWS 所有的Services</a></li>
<li><a href="../../../../2022/06/23/awsAccountOrgnization/">AWS Account和Orgnization</a></li>
<li><a href="../../../../2022/06/23/awsManagement/">AWS Management</a></li>
<li><a href="../../../../2022/05/12/awsServerless/">AWS Serverless</a></li>
<li><a href="../../../../2022/10/01/awsDatabase/">AWS Database</a></li>
<li><a href="../../../../2022/06/21/awsDatabaseNoSQL/">AWS NoSQL</a></li>
<li><a href="../../../../2022/10/01/awsSecurity/">AWS Security</a></li>
<li><a href="../../../../2022/06/18/awsDeployment/">AWS Deployment</a></li>
<li><a href="../../../../2022/06/28/awsMigrate/">AWS Migrate</a></li>
<li><a href="../../../../2022/05/01/awsArch/">AWS 三层架构</a></li>
<li><a href="../../../../2022/10/01/awsStudyResource/">AWS 学习资源</a></li>
</ul>
]]></content>
      <categories>
        <category>汇总</category>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title>设计原则</title>
    <url>/www6vHomeHexo/2018/09/28/designPrinciple/</url>
    <content><![CDATA[<p></p>
<span id="more"></span> 

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#law-%E5%AE%9A%E5%BE%8B">Law 定律</a><br>- <a href="#%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86">奥卡姆剃刀原理 ***</a><br>- <a href="#%E6%8E%92%E9%98%9F%E7%90%86%E8%AE%BA">排队理论</a><br>- <a href="#%E5%BA%B7%E5%A8%81%E5%AE%9A%E5%BE%8B">康威定律</a><br>- <a href="#amdahl%E5%AE%9A%E5%BE%8B-%E9%80%9A%E7%94%A8%E6%89%A9%E5%B1%95%E5%AE%9A%E5%BE%8Buniversal-scalability-law-usl">Amdahl定律, 通用扩展定律(Universal Scalability Law, USL)</a><br>- <a href="#capbase">CAP&#x2F;BASE</a><br>- <a href="#%E5%A4%8D%E6%9D%82%E5%BA%A6-%E7%AE%80%E5%8C%96%E6%9C%AC%E8%B4%A8%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%B6%88%E9%99%A4%E5%81%B6%E5%8F%91%E5%A4%8D%E6%9D%82%E6%80%A7">复杂度 简化本质复杂度，消除偶发复杂性.</a></li>
<li><a href="#%E5%8E%9F%E5%88%99-principle">原则 Principle</a><br>- <a href="#solid">SOLID ***</a><br>- <a href="#happy-path-sad-path-%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E8%B7%AF%E5%BE%84-happy-path-%E5%92%8C-sad-path%E5%88%86%E7%A6%BB">Happy path &amp; Sad path 代码执行路径： happy path 和 sad path分离。</a><br>- <a href="#%E7%AC%9B%E7%B1%B3%E7%89%B9%E6%B3%95%E5%88%99">笛米特法则</a><br>- <a href="#mongodb%E8%AE%BE%E8%AE%A1%E5%93%B2%E5%AD%A6">MongoDB设计哲学</a><br>- <a href="#kiss%E5%8E%9F%E5%88%99-keep-it-simple-and-stupid">“KISS”原则 - Keep it simple and stupid</a><br>- <a href="#rule-of-least-power%E5%A4%9F%E7%94%A8%E5%B0%B1%E5%A5%BD%E7%9A%84%E5%8E%9F%E5%88%99">Rule of least power（够用就好）的原则。</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="law-定律">Law 定律</span><a href="#law-定律" class="header-anchor">#</a></h2><h5><span id="奥卡姆剃刀原理">奥卡姆剃刀原理 ***</span><a href="#奥卡姆剃刀原理" class="header-anchor">#</a></h5><pre><code>     如果对于一个现象有好几种解释, 那么最简单的解释往往是最正确的.
</code></pre>
<h5><span id="排队理论">排队理论</span><a href="#排队理论" class="header-anchor">#</a></h5><pre><code>     Little&#39;s 定律 -&gt; 应用 ： 线程池中多线程个数的确定。
</code></pre>
<h5><span id="康威定律">康威定律</span><a href="#康威定律" class="header-anchor">#</a></h5><pre><code>  organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations   
  衍生: 
     1).DDD context
     2).微服务模块划分
</code></pre>
<h5><span id="amdahl定律-通用扩展定律universal-scalability-law-usl">Amdahl定律, 通用扩展定律(Universal Scalability Law, USL)</span><a href="#amdahl定律-通用扩展定律universal-scalability-law-usl" class="header-anchor">#</a></h5><h5><span id="capx2fbase">CAP&#x2F;BASE</span><a href="#capx2fbase" class="header-anchor">#</a></h5><h5><span id="复杂度-简化本质复杂度消除偶发复杂性">复杂度   简化本质复杂度，消除偶发复杂性.</span><a href="#复杂度-简化本质复杂度消除偶发复杂性" class="header-anchor">#</a></h5><p>   有三个问题可能会产生偶发复杂度。<br>      第一个：由于日程或其他外部压力而导致临时大量削减代码。<br>      第二个是复制。<br>      第三个诱因是不可逆性，您做出的无法逆转的所有决定都将最终导致某种程度的偶发复杂度。<br>   架构师： 去熵， 去复杂度。 </p>
<h2><span id="原则-principle">原则  Principle</span><a href="#原则-principle" class="header-anchor">#</a></h2><h5><span id="solid">SOLID ***</span><a href="#solid" class="header-anchor">#</a></h5><ul>
<li>开闭原则  [3]<ul>
<li>对于扩展是开放的（Open for extension）</li>
<li>对于修改是关闭的（Closed for modification）</li>
</ul>
</li>
</ul>
<h5><span id="happy-path-amp-sad-path-代码执行路径-happy-path-和-sad-path分离">Happy path &amp; Sad path 代码执行路径： happy path 和 sad path分离。</span><a href="#happy-path-amp-sad-path-代码执行路径-happy-path-和-sad-path分离" class="header-anchor">#</a></h5><p>   测试用例： happy path用例。 sad path用例， 使用@Exception（Junit4）， fail（JUnit3）。</p>
<h5><span id="笛米特法则">笛米特法则</span><a href="#笛米特法则" class="header-anchor">#</a></h5><pre><code>     只和最亲密的朋友讲话(talk only to your immediate friends). 任何对象都不需要知道与之交互的对象的任何细节.
</code></pre>
<h5><span id="mongodb设计哲学">MongoDB设计哲学</span><a href="#mongodb设计哲学" class="header-anchor">#</a></h5><pre><code>     Databases are specializing – the “one size fits all” approach no longer applies.
</code></pre>
<h5><span id="kiss原则-keep-it-simple-and-stupid">“KISS”原则 - Keep it simple and stupid</span><a href="#kiss原则-keep-it-simple-and-stupid" class="header-anchor">#</a></h5><pre><code>  衍生: 
     Rob Pike -  Simplicity is Complicated 
</code></pre>
<h5><span id="rule-of-least-power够用就好的原则">Rule of least power（够用就好）的原则。</span><a href="#rule-of-least-power够用就好的原则" class="header-anchor">#</a></h5><pre><code>  这个原则是由 WWW 发明者 Tim Berners-Lee 提出的，它被广泛用于指导各种 W3C 标准制定
</code></pre>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/www6v/hacker-laws-zh">对开发人员有用的定律、理论、原则和模式</a> ***</li>
<li><a href="https://mp.weixin.qq.com/s/HSzqIC1pOdq8H12gCvYgRA">滴滴杜欢：大型微服务框架设计实践</a></li>
<li><a href="/www6vHomeHexo/2023/04/02/designOCPspi/" title="开闭原则 - SPI">开闭原则 - SPI</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>设计原则</category>
      </categories>
      <tags>
        <tag>设计原则</tag>
      </tags>
  </entry>
  <entry>
    <title>NoSQL总结</title>
    <url>/www6vHomeHexo/2018/07/19/NoSQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#nosql%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">NoSQL数据结构</a></li>
<li><a href="#nosql-%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%8A%80%E6%9C%AF">NoSQL 数据建模技术</a></li>
<li><a href="#%E4%B8%80%E8%87%B4%E6%80%A7">一致性</a><br>- <a href="#i%E8%AF%BB%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7">I．读写一致性</a><br>- <a href="#ii%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7">II．写一致性</a><br>- <a href="#a-h-%E4%B8%80%E8%87%B4%E6%80%A7%E7%94%B1%E5%BC%B1%E5%88%B0%E5%BC%BA">(A-H) 一致性由弱到强</a><br>- <a href="#%E4%B8%BE%E4%BE%8B-cassandra%E4%B8%80%E8%87%B4%E6%80%A7">举例: Cassandra一致性</a></li>
<li><a href="#%E4%BA%8B%E5%8A%A1">事务</a><br>- <a href="#i-%E5%8D%95%E6%9C%BA%E4%BA%8B%E5%8A%A1">I. 单机事务</a><br>- <a href="#ii-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">II. 分布式事务</a></li>
<li><a href="#%E5%8A%A8%E6%80%81%E8%BF%98%E6%98%AF%E9%9D%99%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">动态还是静态的数据结构</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E5%92%8Cjoin">索引和join</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考：</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="nosql数据结构">NoSQL数据结构</span><a href="#nosql数据结构" class="header-anchor">#</a></h2><p>大规模数据时，工程实践中常见的数据结构一般有两种：哈希表与平衡树<br><strong>平衡树:</strong> B树, 同时适应磁盘和内存.  复杂度为O(logN).<br><strong>哈希表:</strong> Hash数据结构可以高效地执行根据主键的插入、删除以及查找操作. 但不支持范围查询功能，不支持快照。复杂度为O(N)。</p>
<p><strong>存储引擎：</strong><br>哈希表对应的存储引擎是随机读取存储引擎. 一致性Hash分 桶，每个桶内使用Log-Structured Hash Table存储数据。 Eg. Bitcask<br>B+树对应的存储引擎是Merge-Dump存储引擎。 Eg. Hbase, Cassandra.</p>
<h2><span id="nosql-数据建模技术">NoSQL 数据建模技术</span><a href="#nosql-数据建模技术" class="header-anchor">#</a></h2><p><strong>Key-Value 键值对:</strong> 简单， 不支持范围查找。 Eg. Oracle Coherence, Redis<br><strong>Ordered Key-Value 有序键值对：</strong>map里嵌套map。这种模型的value主要通过“列族”（column families），列，和时间戳来控制版本。Eg. Apache HBase, Apache Cassandra<br><strong>Document databases 文档模型：</strong>value中有主观的模式（scheme）,用字段名做索引. Eg. MongoDB, CouchDB<br><strong>Graph data models 图式数据库:</strong> 图结构的数据模型. neo4j, FlockDB</p>
<p>数据库有对象模型与表模型的阻抗不匹配，NoSQL的<strong>聚合（Aggregates）模型</strong>更贴合<strong>对象模型</strong>的思路。<br>数据库是面向记录的， NoSQL是面向聚集的。</p>
<h2><span id="一致性">一致性</span><a href="#一致性" class="header-anchor">#</a></h2><h5><span id="i读写一致性">I．读写一致性</span><a href="#i读写一致性" class="header-anchor">#</a></h5><p>写后读一致性。<br>读后读一致性。</p>
<h5><span id="ii写一致性">II．写一致性</span><a href="#ii写一致性" class="header-anchor">#</a></h5><p><strong>原子写</strong>：一次写入操作只能是单独的原子性的赋值。<br><strong>冲突预防</strong>：<br>分布式锁或是 PAXOS（悲观锁）。<br>避免分布式的并发写操作，将对特定数据项的所有写操作路由到单个节点上（可以是全局主节点或者分区主节点）。Eg Hbase region  server，MongoDB，大部分关系数据库。<br><strong>冲突检测</strong>：<br>    CAS （Compare And Set）模式, 检测并发更新的冲突，选择回滚其中一个版本，或是维持两个版本并交由客户端解决。 Eg.  Dynamo Vector Lock(向量时钟), memcache CAS, SVN.</p>
<p><img src="http://pic.yupoo.com/iammutex/Cr4HWbaZ/Js1Ke.png" alt="一致性"> </p>
<h5><span id="a-h-一致性由弱到强">(A-H)  一致性由弱到强</span><a href="#a-h-一致性由弱到强" class="header-anchor">#</a></h5><ol>
<li>一次性读写 （A），（B），（C）, （D）<br>（A反熵），一致性最弱. 写操作的时候选择任意一个节点更新，在读的时候如果新数据还没有通过后台的反熵协议传递到读的那个节点，那么读到的仍然是旧数据  Eg. Cassandra  Gossip协议</li>
</ol>
<p>   （B），（C）  反熵的变体<br>（D 一次性读写） 在总共返回的N个值中，如果协调器发现有的数据不是最新的。那么它可以通过读时修复机制来对这些节点进行处理。  Eg.  Dynamo 读时修复</p>
<ol start="2">
<li><p>（E），（F）<br>（E 读若干写若干） Quorum-based, 灵活可配置  Eg.  Dynamo  R+W&gt;N，<br>（F 读全部写若干） </p>
</li>
<li><p>（G）(H)<br>（G 主从）主从异步复制  Eg. Mysql  replication<br>(H PAXOS) 使用两阶段提交协议  Eg.  Oracle DBLink</p>
</li>
</ol>
<h5><span id="举例-cassandra一致性">举例: Cassandra一致性</span><a href="#举例-cassandra一致性" class="header-anchor">#</a></h5><p><img src="https://user-images.githubusercontent.com/5608425/64945268-c2710880-d8a2-11e9-8ace-dfe3dca32995.png" alt="Cassandra一致性"></p>
<h2><span id="事务">事务</span><a href="#事务" class="header-anchor">#</a></h2><h5><span id="i-单机事务">I. 单机事务</span><a href="#i-单机事务" class="header-anchor">#</a></h5><p>ACID保证<br>（C 一致性）（I 隔离性）：<br>    主要有以下几种实现方式</p>
<ol>
<li>依托锁来实现的。<br>分类：读写锁， 排他锁。<br>4个隔离级别和锁的粒度有关。</li>
<li>copy-on-write（MVCC）<br>读写之间互不影响，写不阻塞读， 读不阻塞写， 效率高。<br>实现了隔离级别中用的最多的第二，第三级别。</li>
<li>队列</li>
</ol>
<h5><span id="ii-分布式事务">II. 分布式事务</span><a href="#ii-分布式事务" class="header-anchor">#</a></h5><ol>
<li><p>常规实现<br>悲观锁： 读写锁（两阶段提交， 三阶段提交）<br>   乐观锁： Read-Test-Write， CAS（compare and set）<br>   分布式的MVCC  Eg. MegaStore, Percolator</p>
<p>   分布式锁： 分布式导致高延迟， 并且加锁解锁多次交互。</p>
</li>
<li><p>避免分布式锁的实现<br>CAP理论中适当放宽一致性。 并兼顾一致性， 响应时间，可用性。</p>
</li>
</ol>
<blockquote>
<p>参考实现<br>  <a href="http://wangyuanzju.blog.163.com/blog/static/1302920086424341932">用消息队列和消息应用状态表来消除分布式事务</a>  事务 一致性<br>  &lt;&lt;海量存储系列之六&gt;&gt;</p>
</blockquote>
<h2><span id="动态还是静态的数据结构">动态还是静态的数据结构</span><a href="#动态还是静态的数据结构" class="header-anchor">#</a></h2><p>关系数据库模型：<br>   动态更新的B+树， 写数据时，需要更新B+树。 读写依赖， 实现复杂， 有性能上限。</p>
<p>Nosql：<br>   动态数据和静态数据分离， MemTable(动态) + SSTable（静态）。牺牲读， 提升写性能。<br>   MemTable: Write-back Cache, 随机IO写变成顺序IO写，降低大量的写操作对于存储系统的压力</p>
<h2><span id="索引和join">索引和join</span><a href="#索引和join" class="header-anchor">#</a></h2><p>索引</p>
<ul>
<li>关系数据库：单机索引</li>
<li>NoSQL索引：系统层面， 全局索引。</li>
</ul>
<p>Join</p>
<ul>
<li>关系数据库Join： 存储引擎层面支持join。</li>
<li>NoSQL Join：一般根据应用来决定join的实现。</li>
</ul>
<blockquote>
<p>参考实现  &lt;&lt;Hbase二级索引&gt;&gt;</p>
</blockquote>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://my.oschina.net/juliashine/blog/88173">NoSQL数据库的分布式算法</a>   论文 ***<br><a href="https://blog.csdn.net/wxliu1989/article/details/38415933">NoSQL数据库的分布式算法</a><br><a href="https://highlyscalable.wordpress.com/2012/09/18/distributed-algorithms-in-nosql-databases/">DISTRIBUTED ALGORITHMS IN NOSQL DATABASES</a> 原文</li>
<li><a href="http://coolshell.cn/articles/7270.html">NoSQL 数据建模技术</a>   ***    </li>
<li><a href="http://www.nosqlnotes.net/archives/134">分布式系统的数据结构</a>      </li>
<li><a href="http://blog.nosqlfan.com/html/955.html">优雅的Bitcask</a>      </li>
<li><a href="http://www.nosqlnotes.net/archives/62">分布式事务</a>       </li>
<li><a href="http://www.nosqlnotes.net/archives/140">SQL到NOSQL的思维转变</a>      </li>
<li><a href="https://www.oschina.net/question/12_32573">HBase二级索引与Join</a>  </li>
<li><a href="https://www.cnblogs.com/bonelee/p/6278154.html">cassandra框架模型之二——存储机制 CommitLog MemTable SSTable</a>  一致性</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS NameNode HA 解决方案</title>
    <url>/www6vHomeHexo/2018/06/07/hdfs/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>HDFS NameNode是可靠和高效的, 简单的架构使HDFS能够在上千个节点可靠的存放生产级的数据. 但是, HDFS NameNode也是一个集群中的单点(SPOF). 开源社区也提供了几个HA的解决方案.</p>
<p>HDFS Namenode 宕机后启动时构建状态的两个相对长的阶段包括:　 I. 加载fsimage,并回放editlog.    II. datanode块的汇报, blockMap的构建. HDFS Namenode HA主要是指Ediglog的HA,  不包括FSImage</p>
<h2><span id="1-hdfs-ha-解决方案">1   HDFS  HA 解决方案</span><a href="#1-hdfs-ha-解决方案" class="header-anchor">#</a></h2><h3><span id="secondary-namenodesnn">Secondary NameNode(SNN)</span><a href="#secondary-namenodesnn" class="header-anchor">#</a></h3><p>Secondary NameNode 周期的合并edits和fsimage，能减少集群重启的时间。SNN会周期的做检查点(默认一个小时). 如果ＮＮ宕机, 宕机前和上一次checkpoint这个时间段的状态变更会丢失, 所以元数据并不是最新.　</p>
<p>当NameNode(NN)失效的时 候，Secondary NN并无法立刻提供服务.</p>
<h3><span id="backupnodebn">BackupNode(BN):</span><a href="#backupnodebn" class="header-anchor">#</a></h3><p>BackupNode是社区版Hadoop 0.21提供的一种高可用方案。它不仅实现了SecondaryNameNode的功能，还实现了复写功能，即同时向Active Namenode和Standby Namenode写入EditLog，这样也避免了共享存储会遇到的“IO Fencing”问题。 SNN  是每隔一段时间去  NN  下载  fsimage  和  edits  文件，而NN  可以实时的将日志传送给  BN ，然后将操作合并到  fsimage  里。</p>
<p><img src="http://www6v.github.io/www6vHome/hdfs.files/image001.jpg"></p>
<p>BackupNode 很便宜, 简易, 也考虑到了以下的用例: 主节点在运行, Standby 在线下维护. 主节点宕机并且不能重启.  Standby需要启动并接管主节点.   如果Standby不可用，Active Namenode会把EditLog备份到其他的地方。</p>
<p>BackupNode只能算主从温备，因为Datanode只会向Active Namenode汇报和保持心跳，一旦Active Namenode发生故障，需要人工切换到BackupNode上，并且需要在BackupNode上把BlocksMap状态完全构建起来，这样的切换 时间在分钟~小时级别。</p>
<h3><span id="avatarnode">AvatarNode:</span><a href="#avatarnode" class="header-anchor">#</a></h3><p>Facebook的AvatarNode是业界较早的Namenode HA方案，它是基于HDFS 0.20实现的，如下图所示。由于采用的是人工切换，所以实现相对简单。AvatarNode对Namenode进行了封装，处于工作状态的叫 Primary Avatar，处于热备状态的叫Standby Avatar(封装了Namenode和SecondaryNameNode)，两者通过NFS共享EditLog所在目录。在工作状态 下，Primary Avatar中的Namenode实例接收Client的请求并进行处理，Datanode会向Primary和Standby两个同时发送 blockReport和心跳，Standby Avatar不断地从共享的EditLog中持续写入的新事务，并推送给它的Namenode实例，此时Standby Avatar内部的Namenode处于安全模式状态，不对外提供服务，但是状态与Primary Avatar中的保持一致。一旦Primary发生故障，管理员进行Failover切换：首先将原来的Primary进程杀死(避免了“Split Brain”和“IO Fencing”问题)，然后将原来的Standby设置为Primary，新的Primary会保证回放完成所有的EditLog事务，然后退出安全模 式，对外接收服务请求。为了实现对客户端透明，AvatarNode主从采用相同的虚拟IP，切换时将新的Primary设置为该虚拟IP即可。整个流程 可在秒~分钟级别完成。</p>
<p><img src="http://www6v.github.io/www6vHome/hdfs.files/image002.jpg"></p>
<p><img src="http://www6v.github.io/www6vHome/hdfs.files/image003.jpg"> </p>
<h3><span id="hadoop-20-ha">Hadoop 2.0  HA</span><a href="#hadoop-20-ha" class="header-anchor">#</a></h3><p>Hadoop 2.0 HA 在很多地方借鉴了 AvatarNode:</p>
<p><img src="http://www6v.github.io/www6vHome/hdfs.files/image004.jpg"></p>
<p> I. 同步edits信息<br> NN active和NN standby之间共享存储(share storage) edits信息, 通常用NFS. 这相当于单点故障的位置转移到了中高端的存储设备内部, 依赖外部存储的可靠性.  NN内部每次元数据变动后的flush操作，加上NFS的close-to-open，数据的一致性得到了保证。在使用了NFS做editLog的standby这个阶段后, 社区下个阶段更推崇使用BookKeeper做EditLog的存储. 并且这些功能是实现为可插拔的.</p>
<p>   II. FailoverController作为NN进程的WatchDog<br>   FailoverController是一种协调器(coordination), 作为ZooKeeper的客户端, 主要的指责是领导者选举(leader election),  在NN宕机之后, 自动化的选举出领导者, 并透明的切换到standby上.<br>     错误检测机制通过ZK的临时节点(ephemeral nodes)来实现的, 用心跳和现在的系统集成在一起. 在NN full gc时, 会stop the world, 对这类GC有特殊的管理, 防止误认为NN active已宕机, 剥离出来的FailoverController能解决这个问题.</p>
<p>III. fail-over选择<br>用户想要对fail-over做手工或者自动的热备份, Hadoop 2.0  HA提供了这两种选择.</p>
<p>IV. DN block reports<br>向standby回报块(block reports)的逻辑重用了AvatarNode部分的代码.</p>
<h3><span id="quorum-journal-manager-x2fpaxos-ha">Quorum Journal Manager &#x2F;Paxos HA</span><a href="#quorum-journal-manager-x2fpaxos-ha" class="header-anchor">#</a></h3><p>目前采用最多的就是用共享存储NAS+NFS, 但这些存储设备要求是HA的. 可以使用RAID技术, 利用磁盘阵列的手段,  提升数据的安全性. 但RAID也有缺陷, 这个机器的磁盘不可能无限的增加. 单机容量受到磁盘架个数的限制.<br>   换一个思路, 可以通过集群, 将多台pc server联系到一起, 使用软件逻辑而非硬件逻辑来进行数据的多磁盘备份.由此产生了Quorum-based Journaling这个HA解决方案.</p>
<p><img src="http://www6v.github.io/www6vHome/hdfs.files/image005.gif">  </p>
<p>概述一下Quorum-based Journaling: 多个后台进程的Quorum commits，把这样的后台进程表示为JournalNodes。每一个JournalNode暴露一个简单的RPC接口，允许NameNode读 写edit log，这个log是存在本地磁盘上。当NameNode要写一个edit时，它发送这个edit给集群中所有的JournalNodes，等待大多数节 点的回复。一旦大多数回复成功，这个edit就被认为提交了。</p>
<p>Quorum-based相对于两阶段提交更轻量级, 但延迟也比较厉害. QJM有相应的对策,  QJM中存储日志的JournalNode不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JournalNode的数量增多而影响性能（因为NN向JournalNode发送日志是并行的）</p>
<h2><span id="2-机制">2.     机制</span><a href="#2-机制" class="header-anchor">#</a></h2><p>在看了些解决方案之后, 再看一下各个HA解决方案之后的实现机制</p>
<p>   I. 表决: 相同的组件输入相同的输入, 他们的计算后的简单输出发送给表决者. 如果表决者检测到异常行为, 就终止这个组件的行为. 表决算法可以是”多数规则”等算法.<br>  也叫协调器 coordination, eg. 两阶段的deterministic coordinator, PAXOS&#x2F;Qurumn coordinator .  </p>
<p>  II. 主动冗余:　系统中的组件以并行的方式接受请求, 仅仅使用一个组件的响应.　异常发生时,　系统的停机时间是通常是秒级,　因为备份组件的状态是最新的,　如果是自动切换,　停机时间会缩的更短.<br>  Eg. Hadoop 2.0 HA中DN同时向NN Active和NN Standby发送block report.</p>
<p>  扩展: mapreduce的推测执行 (speculative execution)</p>
<p>  III. 被动冗余:　系统中的一个组件接受请求,　然后通知备用组件进行状态的更新.　何时接管主组件的工作可以由备用组件或其它组件决定.　主组件会发出状态同步的命令.　<br>   Eg. AvatarNode 和 Hadoop 2.0 HA中, NN Active 同步EditLog到NN Standby中.<br>   扩展:　Datanode一备三<br>　　　<br>  IV. Checkpoint还原点: 可以看成是对组件健康状态的一次快照.<br>  Eg. SNN<br>  扩展: Oracle checkpoint, VM snapshot</p>
<p>  V. 状态同步(状态外移): 备用组件在提供服务之前需要和主组件的状态一致. 恢复组件重新提供服务前要同步到失效之前正常的状态.　<br>  Eg.  Hadoop 2.0 HA中, 通过被动冗余的editlog和主动冗余的block report, NN Standby回放editlog与fsimage合并来实时和NN Active的状态保持一致.<br>  扩展:  Mysql slave 的binlog同步, Redis slave RDD 文件同步</p>
<p>  VI. 命令&#x2F;响应(主动) :　一个组件发出一个命令,　并希望在预定义的时间内收到一个来自对方组件的响应.<br>Eg,  FailoverController发送命令给NN active, 如果NN active宕机, 就不能返回相应.<br>　<br>　VII. 心跳:　一个组件定期发出一个消息(心跳),　另一个组件收听这个消息,　知道组件是否健康.　<br>  Eg: FailoverController通过检测到NN active不健康,  会停止发心跳给zookeeper,  zookeeper知道NN acive已宕机, 会做领导者的选举.</p>
<p>  VII. 影子服务: 备用组件和主组件的状态一致, 但主组件不宕机的情况下不对外提供服务.<br>  Eg. AvatarNode 和 Hadoop 2.0 HA中的standby</p>
<p>VIII. 冷备份: standby没有运行, standby能通过读fsimage和editlog, 等待block report来建立需要的状态.</p>
<p>VIIII. 温备份: standby装载了fsimage和editlog, 但没有最近的block report.</p>
<p>X. 热备份: standby有足够的驻内存的状态来取代active.</p>
<h2><span id="3-权衡">3.  权衡</span><a href="#3-权衡" class="header-anchor">#</a></h2><p>Backup node异步写有丢失小部分数据的可能,但吞吐量好,　延迟低, 能达到最终一致性. BackupNode 在宕机时, 还是需要把状态的远端外部存储拿回, 这又成了共享存储.</p>
<p>如果对数据完整性要求比较高, 可以使用两阶段提交和PAXOS. 两阶段提交在没有timeout的机制时, 强依赖备用组件.  实现相对重量级, 由于要同步提交或者回滚, 所以延迟高, 并且吞吐量低.</p>
<p>Quorum尝试复制多份数据,　只要有过半的节点成功,　即整个备份过程成功. PAXOS的多个变体通过合理的放弃一些性能与一致性，集群可以容忍几乎多达n&#x2F;2个数的节点失效。这种折中在两阶段提交与 PAXOS 协议的区别里体现得很明显。所以社区并未出现基于两阶段提交的HA方案, 而是出现了Quorum-based Journaling in CDH4.1</p>
<p>   在failover后的客户端重定向, 生产中的解决方案, 往往通过virtual IP address提供namenode服务,  standby接管为acitive后会也使用这个虚拟ip</p>
<p>   下图从几个维度给出各种HA解决方案的比较</p>
<table>
<thead>
<tr>
<th></th>
<th>备份方式</th>
<th>协议</th>
<th>是否丢数据</th>
<th>可用性</th>
<th>切换时间</th>
<th>机制</th>
</tr>
</thead>
<tbody><tr>
<td>Backup node</td>
<td>温备</td>
<td>Master-Slave</td>
<td>丢数据</td>
<td>基本可用</td>
<td>分钟~小时级</td>
<td>Checkpoint + EditLog被动冗余</td>
</tr>
<tr>
<td>AvatarNode</td>
<td>热备</td>
<td>数据安全的Master-Slave</td>
<td>不会丢数据</td>
<td>高可用&#x2F;依赖外部可用性</td>
<td>秒~分钟级别</td>
<td>blockmap 主动冗余 +  EditLog被动冗余 + 影子服务</td>
</tr>
<tr>
<td>HDFS HA 2.0(NFS)</td>
<td>热备</td>
<td>数据安全的Master-Slave</td>
<td>不会丢数据</td>
<td>依赖外部可用性&#x2F;依赖外部可用性</td>
<td>5-20s</td>
<td>blockmap 主动冗余 +  EditLog被动冗余 + 影子服务</td>
</tr>
<tr>
<td>QJM&#x2F;Qurom Journal Manager</td>
<td>热备</td>
<td>PAXOS</td>
<td>不会丢数据</td>
<td>高可用</td>
<td>小于1分钟</td>
<td>EditLog被动冗余(Quorum-based)</td>
</tr>
</tbody></table>
<p>CAP理论是一个相对论,  说明在分布式系统中提供的是相对的可用性, 一致性.  Hadoop namenode 的可用性也是相对的可用, 以下是一些小结:</p>
<p>Backup Node:  延迟比较低, 吞吐量比较高, 并能容忍少量的数据丢失.   </p>
<p>Hadoop HA 2.0: 要求数据不能丢失.  即可以使用NFS, 也可以使用Bookkeeper, 既可以手动切换,也可以自动切换Standby NN. 切换时间短. </p>
<p>Quorum-based journaling: 要求数据不能丢失, 并且对延迟要求不高的情况, 也不希望使用昂贵的外部存储. </p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://issues.apache.org/jira/browse/HDFS-1623"> High Availability Framework for HDFS NN</a>  </li>
<li><a href="http://www.docin.com/p-532877866.html">海量存储系列1-15</a> </li>
<li><a href="http://www.infoq.com/cn/articles/hadoop-2-0-namenode-ha-federation-practice-zh">Hadoop 2.0 NameNode HA和Federation实践</a> </li>
<li><a href="http://yanbohappy.sinaapp.com/?p=205">基于QJM&#x2F;Qurom Journal Manager&#x2F;Paxos的HDFS HA原理及代码分析</a> </li>
<li><a href="http://www.tuicool.com/articles/EZnE7v">Quorum-based Journaling in CDH4.1</a> </li>
<li><a href="http://www.infoq.com/cn/news/2012/08/facebook-avatarnode">Facebook如何使用Avartarnode提升HDFS可靠性</a></li>
<li><a href="http://www.itinit.net/thread-2468-1-1.html">hadoop HA使用Quorum Journal的设计</a> </li>
<li><a href="NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95">http://blog.nosqlfan.com/html/4139.html</a></li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>存储</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>可用性</tag>
      </tags>
  </entry>
  <entry>
    <title>秒杀系统总结</title>
    <url>/www6vHomeHexo/2018/05/21/secKillSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2018/05/21/secKillSummary/secKillSummary.jpg" class title="秒杀系统总结">

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>阿里大秒系统</li>
<li>秒杀系统架构优化思路 58沈剑</li>
<li><a href="https://mp.weixin.qq.com/s/YfHszSORHP_-W7pJA8PEcg">如何设计一个高可用、高并发秒杀系统</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统设计</category>
        <category>秒杀系统</category>
      </categories>
      <tags>
        <tag>秒杀系统</tag>
      </tags>
  </entry>
  <entry>
    <title>秒杀系统和商品详情页系统(培训讲义)</title>
    <url>/www6vHomeHexo/2018/05/06/seckill/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><a href="http://www6v.github.io/www6vHome/seckill.htm">秒杀系统和商品详情页系统(培训讲义)</a></p>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统设计</category>
        <category>秒杀系统</category>
      </categories>
      <tags>
        <tag>秒杀系统</tag>
      </tags>
  </entry>
  <entry>
    <title>故障模型-基础设施层</title>
    <url>/www6vHomeHexo/2018/05/03/faultModel3/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="故障模型-基础设施层">故障模型-基础设施层</span><a href="#故障模型-基础设施层" class="header-anchor">#</a></h2><ul>
<li>故障模型-Virtualization&amp;Storage&amp;Networking<ul>
<li>服务器宕机&amp;假死</li>
<li>断电<ul>
<li>解决：异地多活</li>
</ul>
</li>
<li>超卖</li>
<li>混和部署【3】</li>
<li>存储【2】<ul>
<li>磁盘满，坏</li>
<li>不可写，不可读</li>
</ul>
</li>
<li>网络【1】<ul>
<li>网络抖动、丢包、超时</li>
<li>网卡满</li>
<li>DNS故障</li>
<li>断网</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="virtualization-amp-storage-amp-networking">Virtualization &amp; Storage &amp; Networking</span><a href="#virtualization-amp-storage-amp-networking" class="header-anchor">#</a></h3><ol>
<li><a href="https://tencentcloudcontainerteam.github.io/2019/08/12/troubleshooting-with-kubernetes-network/">Kubernetes 网络疑难杂症排查分享</a>  腾讯云 ***</li>
<li><a href="https://tencentcloudcontainerteam.github.io/2019/06/08/kubernetes-best-practice-handle-disk-full/">kubernetes 最佳实践：处理容器数据磁盘被写满</a> 腾讯云</li>
<li><a href="https://www.infoq.cn/article/aEut*ZAIffp0q4MSKDSg">百度大规模战略性混部系统演进</a></li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障模型</category>
      </categories>
      <tags>
        <tag>故障模型</tag>
      </tags>
  </entry>
  <entry>
    <title>故障模型-中间件层</title>
    <url>/www6vHomeHexo/2018/05/03/faultModel2/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="故障模型-中间件层">故障模型-中间件层</span><a href="#故障模型-中间件层" class="header-anchor">#</a></h2><ul>
<li>故障模型-Runtime&amp;Middleware&amp;OS<ul>
<li>负载均衡失效</li>
<li>数据库<ul>
<li>数据库热点</li>
<li>数据库连接满</li>
<li>数据库宕机</li>
<li>数据库同步延迟</li>
<li>数据库主备延迟【参考2】</li>
</ul>
</li>
<li>缓存<ul>
<li>缓存热点【参考1】</li>
<li>缓存限流</li>
</ul>
</li>
<li>OS资源<ul>
<li>CPU抢占<ul>
<li>案例 : HashMap并发访问，CPU100%【参考1】</li>
<li>案例：正则表达式回溯，CPU100%</li>
</ul>
</li>
<li>内存抢占<ul>
<li>案例：OOM killer</li>
</ul>
</li>
<li>上下文切换</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="大纲">大纲</span><a href="#大纲" class="header-anchor">#</a></h3><ol>
<li><a href="https://developer.aliyun.com/article/105551">超全总结 | 阿里如何应对电商故障？神秘演练细节曝光</a> 阿里巴巴  周洋</li>
</ol>
<h3><span id="runtime-amp-middleware-amp-os">Runtime &amp; Middleware &amp; OS</span><a href="#runtime-amp-middleware-amp-os" class="header-anchor">#</a></h3><ol>
<li><a href="/www6vHomeHexo/2022/06/03/redisHotkey/" title="Redis 热点Hotkey">Redis 热点Hotkey</a>  self
</li>
<li><p><a href="https://www.admin5.com/article/20190404/902952.shtml">UCloud高可用数据库UDB主从复制延时的解决</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障模型</category>
      </categories>
      <tags>
        <tag>故障模型</tag>
      </tags>
  </entry>
  <entry>
    <title>应用集成方式</title>
    <url>/www6vHomeHexo/2018/04/07/EAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E4%B9%8B%E9%97%B4%E4%B8%BB%E8%A6%81%E6%9C%894%E7%A7%8D%E9%9B%86%E6%88%90%E6%96%B9%E5%BC%8F">企业应用之间主要有4种集成方式</a><ul>
<li><a href="#%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93">文件传输</a></li>
<li><a href="#%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE%E5%BA%93">共享数据库</a></li>
<li><a href="#%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8">远程调用</a></li>
<li><a href="#%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92">消息传递</a></li>
</ul>
</li>
<li><a href="#%E6%9D%83%E8%A1%A1">权衡</a><ul>
<li><a href="#%E8%80%A6%E5%90%88%E6%80%A7%E5%92%8C%E4%BE%9D%E8%B5%96%E6%80%A7">耦合性和依赖性</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E6%80%A7">修改性</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F">数据格式</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%96%B0%E9%B2%9C%E5%BA%A6">数据新鲜度</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96">数据序列化</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3">问题的解决</a></li>
<li><a href="#%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%84%E5%90%88">模式之间的组合</a></li>
</ul>
</li>
<li><a href="#%E5%B0%8F%E7%BB%93">小结</a></li>
</ul>
<!-- tocstop -->

</div>
                             
<h2><span id="企业应用之间主要有4种集成方式">企业应用之间主要有4种集成方式</span><a href="#企业应用之间主要有4种集成方式" class="header-anchor">#</a></h2><h3><span id="文件传输">文件传输</span><a href="#文件传输" class="header-anchor">#</a></h3><ul>
<li>文件传输<br> 由各个应用产生文件, 其中包含提供其它应用使用的信息.</li>
</ul>
<p>特征:　内部数据模式自由(schema free)<br>优势: 1. 内部细节透明 2. 松耦合 3. 标准文件格式支持,如xml, json等.<br>劣势: 1. 更新慢, 数据不同步,  数据过时 2. 数据不完整, 数据不完全正确时, 数据不一致问题解决困难.3. 产生大量小文件时, 昂贵且不可靠. 4. 语义不一致<br>适用于批处理.</p>
<h3><span id="共享数据库">共享数据库</span><a href="#共享数据库" class="header-anchor">#</a></h3><ul>
<li>共享数据库<br>把应用的数据存储在一个共享数据库中来集成应用,　并定制数据库模式来处理不同应用的各种需求.</li>
</ul>
<p>特征:　数据有模式(schema)<br>优势:  1. 提供模型一致性 2. 通过事务管理数据一致性 3. 共享数据, 避免语义不一致问题.<br>劣势:  1. 统一的数据库模式难设计. 模式改变,　应用也要改变. 2. 遗留系统很难提供一个可扩展的数据库模式供新的应用使用.  3.　性能瓶颈(单点访问)  4. 数据分布迁移困难. 5.应用和数据库紧耦合.<br>可作为新老系统的集成和改造的候选方案</p>
<h3><span id="远程调用">远程调用</span><a href="#远程调用" class="header-anchor">#</a></h3><ul>
<li>远程调用<br>应用公开提供过程, 并能够被远程调用, 应用通过调用这些过程来执行操作并交换数据.</li>
</ul>
<p>特征:　公开外部接口<br>优势:　1. 提供语义一致性 2. 接口多样化, 有可兼容性并能扩展 3. 技术实现广泛, 如Java, .Net, CORBA, Web Services  4. 接口内部数据完整性和透明性.<br>劣势:　１.　性能瓶颈,　不可靠(与本地访问相比)　２.　应用之间紧耦合, 可能会有时序上的耦合</p>
<h3><span id="消息传递">消息传递</span><a href="#消息传递" class="header-anchor">#</a></h3><ul>
<li>消息传递<br>应用连接到一个公共的消息传递系统上,　并通过消息来交换数据和调用行为.</li>
</ul>
<p>特征:　隐式调用, 完全隐藏接口<br>优势:　1. 松耦合  2. 快速响应 3. 可靠  3. 通过消息转换解决语义不一致(实现方式: DDD 防腐层, ESB提供消息转换功能 )<br>劣势:  1. 设计, 开发复杂(消息消费能力不够, 会引起消息的大量堆叠); 测试, 调试困难(可通过同步方式来测试) 2. 数据不完全同步  3. 有学习曲线</p>
<h2><span id="权衡">权衡</span><a href="#权衡" class="header-anchor">#</a></h2><h3><span id="耦合性和依赖性">耦合性和依赖性</span><a href="#耦合性和依赖性" class="header-anchor">#</a></h3><p>１.　消息传递和文件传输属于非直接耦合,　耦合性最低.<br>２.　远程调用属于数据耦合,　耦合性次之.<br>３.　共享数据库属于内容耦合,　耦合性最高.</p>
<p>使用远程调用或共享数据库的应用之间是强依赖的关系<br>使用消息传递或文件传输的应用之间是弱依赖的关系.</p>
<h3><span id="修改性">修改性</span><a href="#修改性" class="header-anchor">#</a></h3><p>共享数据库中的表结构一旦修改, 应用多少会做一些修改加以应对, 有的甚至是对整个应用的改造.</p>
<h3><span id="数据格式">数据格式</span><a href="#数据格式" class="header-anchor">#</a></h3><p>文件传输保持所产生的文件内容及格式不变就可以, 以文件作为公共接口,  内部格式可以不段变化.  远程调用, 共享数据库使用接口参数定义数据格式, 内部数据格式对外不可见. 共享数据库中数据格式的演化和扩展性相对于其它方式最弱.</p>
<h3><span id="数据新鲜度">数据新鲜度</span><a href="#数据新鲜度" class="header-anchor">#</a></h3><p>文件传输使用低频率的大文件传输会造成过时的信息, 有时可以容忍数据不一致, 但也可能造成灾难. 消息传递通过频繁和立即的发送数据来提高数据的新鲜度. </p>
<h3><span id="数据序列化">数据序列化</span><a href="#数据序列化" class="header-anchor">#</a></h3><p>数据需要序列化,反序列化. 格式可以是文本或者是二进制的. 传输的格式可以是json, xml或者PB.</p>
<h3><span id="问题的解决">问题的解决</span><a href="#问题的解决" class="header-anchor">#</a></h3><p>共享数据库和远程调用解决了文件传输语义不一致的问题.<br>消息传递相对于远程调用,共享数据库提高了性能和可靠性.</p>
<h3><span id="模式之间的组合">模式之间的组合</span><a href="#模式之间的组合" class="header-anchor">#</a></h3><ol>
<li>消息传递和消息传递的组合(全异步化 SEDA)</li>
<li>消息传递和远程调用的组合(Half-Sync&#x2F;Half-Async 半同步&#x2F;半异步 POSA4)</li>
<li>远程调用和共享数据库的组合</li>
</ol>
<p><img src="/www6vHomeHexo/%22(1)%E5%BA%94%E7%94%A8%E4%B9%8B%E9%97%B4%E6%B2%A1%E6%9C%89%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E9%97%AE%E9%A2%98%22" alt="(1)应用之间没有同步数据的问题"> </p>
<p><img src="/www6vHomeHexo/%22(2" alt="(2) 变体, 应用之间有同步数据的问题, 实时或非实时同步"> 变体, 应用之间有同步数据的问题, 实时或非实时同步”)</p>
<h2><span id="小结">小结</span><a href="#小结" class="header-anchor">#</a></h2><p>4种方式, 每种模式都建立在前一种模式的基础之上, 以解决以前的集成方法所存在的问题. 相对于前一种模式也更抽象, 成熟度更高, 复杂度也更高.</p>
<p><img src="http://www6v.github.io/www6vHome/EAI.files/EAI-1726.png"><br><img src="http://www6v.github.io/www6vHome/EAI.files/EAI-1730.png"><br><img src="http://www6v.github.io/www6vHome/EAI.files/EAI-1733.png"><br><img src="http://www6v.github.io/www6vHome/EAI.files/EAI-1737.png"></p>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>应用集成</category>
      </categories>
      <tags>
        <tag>应用集成</tag>
      </tags>
  </entry>
  <entry>
    <title>领域逻辑和SQL</title>
    <url>/www6vHomeHexo/2018/03/17/DomainLogicAndSQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>在企业应用中， 业务逻辑是复杂和庞杂的。 这些业务逻辑应该是被显示， 还是被隐藏， 这是一种选择。 在工作流中， 业务逻辑被从模块中剥离出来， 形成上层的粗粒度的业务流程。 在模块内部， 业务逻辑应该放在内存中， 还是在SQL中， 这也是一种选择。</p>
<p>拨开项目的DAO层， 你可能会看到大量的SQL字符串， 业务逻辑就隐藏在这里。在有的项目你会看到hibernate之类的ORM框架， PO对象作为一种承载业务逻辑的机制。</p>
<p>大量复杂的SQL拼接， 在SQL中放入业务逻辑与企业应用架构的分层原则相违背。OO的本质是抽象和分离， 各司其责。 领域逻辑更符合OO的精神。</p>
<p>领域逻辑中的ORM不仅是一种可重用的对象装载方式， 也是一种虚拟化技术。对象与数据库的映射机制由ORM管控，对象装载和业务逻辑的分离， 表对于对象来说是透明的。就像JVM， 硬件对于Java工程师来说是透明的一样。</p>
<p>在Domain Logic和SQL之间， 中庸的是Trasaction Script（事务脚本），它根据过程组织业务逻辑，每个过程处理来自表现层的一个单一请求。事务脚本简单的可以看成Domain Logic和Native SQL的结合。</p>
<p>Native SQL的拥护者会提到SQL的性能优势。在多表查询中，Domain Logic确实没有Native SQL快。 在一条SQL即一个事务情况下，Native SQL会快些。 但在Domain Logic中， framework会使用cache做局部性的优化， 并且cache的对象是能够跨多个事务复用的（hibernate二级缓存）， 缓存策略也是可配置的。Framework并且提供Lazy load机制，在使用时加载对象， 进一步提升性能。</p>
<p>Domain Logic可以在可理解的代码上做性能的改进，找到那占用了80%时间的20%的代码。 Native SQL可以在高性能的代码上做理解性的改进， 但sql中的逻辑是隐式的， 笔者认为要做到后者不易。 维护占到了软件生命周期的很大一部分， 应该先关注可修改性， 再关注性能。性能的第一原则是“不要提早优化”。</p>
<p>在长生命周期的企业软件中，需求的迭代和代码的迭代是常态。 改变可能是人们把业务逻辑放在内存中， 使用Domain Logic的主要原因。</p>
<p>过度复杂的sql拼接让人很难理解， 面条代码， 逻辑不连贯现象容易形成。 在Domain Logic的基础上， DSL（Domain Specific Language）提供了连贯接口， 业务清晰一目了然。<br>当然在理解程度上， 有人更适应SQL， 有人更适应Domain Logic， 这也是仁者见仁了。</p>
<p>Native SQL中的sql的重用比Domain Logic中对象的重用更困难。 如果想重用一段SQL，在SQL中嵌入了判断逻辑，SQL的复杂度又就增加了。数据库视图是表的接口，可以定义一个视图， query重用定义好的视图。但视图有局限性， 只有select操作， 没有update操作， 如果有DML要求， 还需要定义存储过程。</p>
<p>使用视图和存储过程提供的封装是不完全的。 在企业应用中，数据会来源于多个数据源， 多个数据库， xml文件，nosql数据库， 遗留系统等。 在这个情况中，数据存取的完全封装确实只能在应用的分层中实现。</p>
<p>Domain Logic提供了抽象层次和模块化的机制， 对象装载和实际业务的分离， 好的对象装载机制会零入侵业务逻辑，如果Annotation用的是JSR标准， 把hibernate替换成OpenJPA也比较容易。</p>
<p>Domain Logic要求对framework有好的把握， 一定的驾驭能力， 问题的解决能力。 Domain Logic里的对象有更多的约束条件，更多的模式， 比如一对一，多对一，多对多。  SQL相对更容易掌握， 代码直接可控。</p>
<p>如果想要有可移植性，请不要使用sql。 各个语言都有自己的方言， 语法有略微的不同。Id的增长方式不同，有sequence， 有自增的， 有全局的。Mysql有limit关键字，oracle有rowid和rownum, db2有ROW_NUMBER() over(). 如果有数据库移植的需求， 已经写的Native SQL就会有大的改动。</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="http://martinfowler.com/articles/dblogic.html">Domain Logic and SQL</a></p>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>领域建模</category>
      </categories>
      <tags>
        <tag>领域建模</tag>
      </tags>
  </entry>
  <entry>
    <title>CQRS 简介和案例分析</title>
    <url>/www6vHomeHexo/2018/02/25/cqrs/</url>
    <content><![CDATA[<p></p>
<span id="more"></span> 

<p>CQRS全称是指Command Query ResponsibilitySeparation.CQRS的核心是一个简单的概念, 使用一个模型来读信息, 使用另一个模型来更新信息. 它是CQS原理在各个软件领域中的应用而产生的一种模式. CQRS把整个系统分成两个部分: 命令部分和查询部分. Command部分关注更新,  Query部分关注读取.</p>
<p>其实你可能早就接触过CQRS相关的概念,熟悉数据库的读者不会对索引陌生.<br>Query部分:如果数据表有索引,  读数据表更加的快速.<br>Command部分:如果数据表有index,update表时需要更新index, 所以update更加的慢.</p>
<p>本文主要从CQRS在高伸缩性系统和领域驱动设计(DDD)两方面的应用阐述其优势。</p>
<p>   CQRS的出现有以下两种驱动力</p>
<ol>
<li>多参与者协作的环境<br>多个参与者会使用和修改相同的数据集.  参与者可以是行为人用户, 或者是软件.</li>
<li>数据总是过时的<br>在多协作的环境中, 数据一旦显示给了一个用户, 相同的数据可能已经被其它的参与者修改了, 说明数据已经过时了.<br>在哲学领域有一个命题, 你是否能踏进同一条河两次? 在多协作的环境中也有类似的问题, 你看到的数据总是过时的.</li>
</ol>
<p><em><strong>案例</strong></em>:<br>在查询出还有电影场次后,  你开始填自己的记录信息, 这时可能别人已经订购了你已经选择的座位, 或者这个时候, 一个事件到达银行说你信用卡有拖欠, 但最后你提交了这次订购，结果订购失败。</p>
<h2><span id="cqrs与模型">CQRS与模型</span><a href="#cqrs与模型" class="header-anchor">#</a></h2><p>在与command模型的交互中产生了事件,  顺序事件的累积可以捕获状态的所有变化, 这种交互模式称为事件源(Event Sourcing) .</p>
<p>事件源(Event Sourcing)使得系统有了审计的功能, 回放事件可以使系统恢复到某个时间点的状态. 事件源(Event Sourcing)使command部分引入了异步的机制, 队列中的消息不需要马上处理, event handler可以异步的消费事件.当commands部分产生错误后, 直接向客户端回个错误并不友好, 这时可以引入回滚和重试机制.  在系统恢复正常之后, 队列中的消息重新发送并且用户接受到确认.</p>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image001.jpg" alt="单一模型(图1)，模型的分离(图2)，模型的融合(图3)" title="单一模型(图1)，模型的分离(图2)，模型的融合(图3)"></p>
<p>  Query与Command两种行为的分离使得两个服务公用模型的分离也成为自然(图2)。单一模型(图1)分离成了两个模型:查询模型和命令模型.接口相应也分离成查询接口和命令接口. 客户端通过命令接口路由变化信息到命令模型. 查询模型和命令模型之间往往通过异步方式同步数据. 客户端通过查询接口读取查询模型以得到更新后的数据.</p>
<p>但是模型在上下文中孤立的存在并不多见，更多模型之间会有相互的渗透，融合(图3)。共享内核表示了命令模型和查询模型之间重合的部分. (DDD)<br>在DDD领域中, 通用子系统可以代表更通用的服务. 在存储系统中, 通用子系统代表了在存储介质上的数据结构的融合, 公用.</p>
<p>结合Event Souring 和模型共享内核来了解一下通用存储引擎的设计思路</p>
<p><em><strong>案例: BigTable和Cassandra的通用存储引擎</strong></em></p>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image002.jpg" alt="通用存储引擎图4" title="通用存储引擎图4"></p>
<p>数据写入时需要先写操作日志, 操作日志可以看成是Event Souring的持久化保存.成功后应用到内存中的MemTable中. 当内存中的MemTable达到一定大小, 需要将MemTable dump到磁盘中生成SSTable.由于数据同时存在MemTable和可能多个SSTable中, 读取操作需要按老到新合并SSTable和内存中的MemTable数据. 可以看到写操作对应的命令模型是MemTable, 读操作对应的查询模型是MemTable和多个SSTable,MemTable在读写时成为了共享模型, 以达到’提高写性能,  亦不降低读性能’的目的.</p>
<h2><span id="cqrs与restful">CQRS与RESTFUL</span><a href="#cqrs与restful" class="header-anchor">#</a></h2><p>在REST风格的系统中, 资源动词, 名词, 表现三个维度上的分离, 形成了资源行为(统一接口),  资源状态,  资源表现形式.<br>REST的6个约束中包括统一接口, 能够使客户端和服务端独立的演化。统一接口包括PUT, GET, POST等Http方法.  PUT,  POST类的接口可以归为command部分, GET 类的接口可以归为query部分.<br>CQRS使得资源行为维度能够再分, 形成对服务层, 模型层, 数据存取层(DAO), 数据源层的纵向切分,  形成command和query两个子系统.REST统一接口是系统的水平接口，CQRS可以看成是系统的垂直接口。<br>在系统中, C和Q的分离可以看成是对系统中最粗粒度层次的划分.</p>
<p><em><strong>案例:Facebook缓存架构</strong></em></p>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image003.jpg" alt="Facebook缓存架构图5" title="Facebook缓存架构图5"></p>
<p>•  整体REST架构分成PUT(Query部分), POST(Command部分)两个部分.<br>•  Cache分Page cache, fragment cache, row cache, vector Cache, cache命中率见图。<br>•  Page Cache和Fragment cache存放了API各种请求格式的数据，包括4种资源表现形式 XML, JSON, RSS, ATOM。<br>•  发表Tweets是先放入Kestrel, 再异步处理，Kestrel用的也是memcached协议。Kestrel可以看成Event Souring, Vector Cache是Command部分和Query部分之间的共享模型. </p>
<h2><span id="cqrs与一致性">CQRS与一致性</span><a href="#cqrs与一致性" class="header-anchor">#</a></h2><p>根据弱CAP原理，在分布式系统中，往往需要达到(一致性, 可用性,分区容忍性)三者的平衡，增强其中的一方就会削弱另外两方。在分布式系统中, P总是需要保证的, 所以需要在C和A之间做取舍.  CQRS中的S(分离)隐喻了P, 即分区容忍性.</p>
<p>贯彻CQRS的系统通过多种方式来实现各种级别的一致性，其中包括MS, MM(MMS, MMM), 两阶段提交, Paxos</p>
<p>强一致性：假如A 先写入了一个值到存储系统，存储系统保证后续A，B,C的读取操作都将返回最新值。<br>弱一致性：假如A先写入了一个值到存储系统，存储系统不能保证后续A，B，C的读取操作能读取到最新值。<br>最终一致性：最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A，B，C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到A写入的最新值。</p>
<h5><span id="ms">MS</span><a href="#ms" class="header-anchor">#</a></h5><p>在分布式系统中，通过读写多个数据副本来做到读写分离。<br>MS方式中, Master会承担起写请求(Command部分)的负载, Slave会承担起读请求(Query部分)的负载.<br>多个slave副本通过同步, 异步, 半同步的方式达到与Master数据的一致性.异步同步对延时和吞吐量这两个性能指标有好处.<br>在读多写少的系统中, 增加读的副本可以相对廉价的提高Query部分(读请求端)的水平可伸缩性.  如果有大量突增请求, 可以相应调高读的副本数.–query部分的可伸缩性</p>
<h5><span id="mm">MM</span><a href="#mm" class="header-anchor">#</a></h5><p>Multi-master指一个系统存在多个master, 每个master都具有read-write能力，可以根据时间戳或业务逻辑合并版本。具备最终一致性。</p>
<p><em><strong>案例</strong></em><br>BigTable: 同一个时刻同一个tablet只能被一台Tablet Server服务. 强一致性的分布式索引.<br>   GFS:  MS实现的弱一致性分布式存储系统.<br>   Dynamo和Cassandra: MM实现的具备最终一致性的存储系统. 可能出现同一个key被多台机器操作的情况.多台机器上执行的顺序是无法保证的. 需要依赖基于vector lock的冲突合并方法解决冲突. 默认的解决方案是”last write wins”, 即在读的时候合并多个写者产生的多个版本的数据.</p>
<p>–To do</p>
<h2><span id="cqrs与数据存储">CQRS与数据存储</span><a href="#cqrs与数据存储" class="header-anchor">#</a></h2><p>  Command部分:　相对关注事务处理,持久化为关系结构数据. 在数据库中, 使用第3范式.<br>  Query部分: 相对关注性能. 使用反范式的方式来最小化数据的级联.  在数据库中, 可以使用第一范式, 也可以结合使用nosql技术.</p>
<p><em><strong>案例:　Mysql + Redies混合存储(sql + nosql混合存储)</strong></em></p>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image004.png" alt="Mysql + Redies混合存储图6" title="Mysql + Redies混合存储图6"></p>
<p>MySQL把数据同步到NoSQL中,这种架构适用于需要把数据同步到多种类型的存储中。<br>Nosql通过装做是mysql的slave, 从mysql同步数据.MySQL到NoSQL同步的实现可以使用MySQL UDF函数，MySQL binlog的解析来实现。</p>
<h2><span id="cqrs与分布式事务">CQRS与分布式事务</span><a href="#cqrs与分布式事务" class="header-anchor">#</a></h2><p>两阶段提交是实现分布式事务的常用方式, 协议比较通用. 但两阶段提交中所有事务序列化的通过master coordinator, 是吞吐率和延迟的杀手.</p>
<p>CQRS是完全建立在BASE(Basic Availability, Soft-state, Eventual consistency)事务基础上的. 在CQRS实现中, 通过降低对写端的压力, 减少锁的竞争和死锁的可能, 来增加写端的性能.各种实现方式会有自定义的协议, 相对于两阶段提交灵活但不够通用.</p>
<p>在对性能要求不高的系统中, 应该采用两阶段提交加快开发. 在对性能要求不是很高系统中,  应该考虑采用消息队列.</p>
<p><em><strong>案例1:　ebay分布式事务</strong></em><br>消息队列上的CQRS + 消息应用状态表</p>
<ol>
<li><p>更新业务表A</p>
</li>
<li><p>更新业务表B的事件放入消息队列</p>
</li>
<li><p>提交事务1(包括步骤1, 2)</p>
</li>
<li><p>查询队列中的消息,  更新业务表B.</p>
</li>
<li><p>插入消息应用状态表message_applied</p>
</li>
<li><p>提交事务2(包括步骤5, 6)</p>
</li>
<li><p>如果上述事务成功之后, dequeue message</p>
</li>
<li><p>删除消息应用状态表中的事件</p>
</li>
</ol>
<p>在关注第2点和第4点之后, 可以看到队列的插入(command)和查询(query) 放在了两个事务中.</p>
<p><em><strong>案例2:  淘宝分布式事务</strong></em><br>日志表上的CQRS + 去重表</p>
<ol>
<li><p>更新业务表A</p>
</li>
<li><p>更新业务表B的事件放入日志表, 并自动生成一个唯一的transactionID。</p>
</li>
<li><p>提交事务1(包括步骤1, 2)</p>
</li>
<li><p>消息中间件保证从主机1上读取更新业务表B的事件和transactionID, 并且这个消息路由到主机2.</p>
</li>
<li><p>更新业务表B</p>
</li>
<li><p>将transactionID插入去重表</p>
</li>
<li><p>提交事务2(包括步骤5, 6)</p>
</li>
</ol>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image005.jpg" alt="ebay分布式事务图7, 淘宝分布式事务图8" title="ebay分布式事务图7, 淘宝分布式事务图8"></p>
<p>这两个实现方式, 它们有之间的共性, 就是有一个元素被分步骤的使用了CQRS.在案例1中是消息队列，案例2中是日志表. 从CQRS的角度看, 这两种实现方式没有本质的区别, 方式2可以看成是方式1的变体.</p>
<p>案例2中的去重表等价于案例1中的消息应用状态表.<br>案例2看似复杂, 多了步骤4, 实际是保持事务1,2同步临界区的最小化, 等于是把案例1中查询队列中的消息(步骤4)剥离出事务, 防止不必要的查询错误导致回滚整个事务.<br>案例1的事务1牵涉到了业务表A和队列的混合型业务事务,实现复杂。案例2的两个事务都是数据库的系统事务, 可以使用两阶段提交, 相对通用.<br>案例2的去重表没有删除过, 能保证最终的消息都是已经成功的事务.案例1 有dequeue message步骤, 并在事务外, 在出现故障后, message_applied会留下一些垃圾内容. –</p>
<h2><span id="cqrs与cdn">CQRS与CDN</span><a href="#cqrs与cdn" class="header-anchor">#</a></h2><p>在大规模web站点中, 动态数据和静态数据(图片)的分离是优化的通用策略.<br>   CQRS能够在Query端做到极致的优化, 例如缓存, 分区, 备份(replication), 分布式的CDN. CDN是一种离用户相对近的边缘缓存, 能提高用户体验.</p>
<p>***案例:   ***<br>淘宝CDN</p>
<p><img src="http://www6v.github.io/www6vHome/cqrs.files/image006.jpg"></p>
<h2><span id="cqrs原理">CQRS原理</span><a href="#cqrs原理" class="header-anchor">#</a></h2><p>CQRS来源于Bertrand Meyer提出的CQS原理。CQS原理从OOP中推导出来，大致是说如果你返回一个值你就不能改变状态。如果你改变了状态，你的返回值必须是void类型的。</p>
<p>CQS原理其实在很多领域都有应用, 包括OOP中字段的的setter&#x2F;getter,Java String(copy-on-write), Java thread(ConcurrentHashMap<br>读写分离锁), snapshot(copy-on-write), 数据库索引, 数据库sql(DML, DDL)中都可以看到CQS原理的影子.</p>
<h2><span id="小结">小结:</span><a href="#小结" class="header-anchor">#</a></h2><p>CQS中的分离(S)粒度,小到对象状态的setter和getter方法, 大到子系统的形成.<br>就如Greg谈到的， CQRS本身是个简单的小模式，有趣的是在结合两个服务时所要考虑的架构属性。<br>CQRS在复杂性管理和提高系统伸缩性有着独特的优势。</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://wenku.baidu.com/view/002c419851e79b896802265e.html">clarified CQRS</a></li>
<li><a href="http://martinfowler.com/bliki/CQRS.html">CQRS</a> </li>
<li><a href="http://wenku.baidu.com/view/6aaa6c0690c69ec3d5bb751e.html">CQRS Documents by Greg Young</a>  </li>
<li><a href="http://www.infoq.com/cn/news/2011/02/nosql-architecture-practice/">NoSQL架构实践（一）——以NoSQL为辅</a> </li>
<li>DDD – domain driven design (共享内存)  –  Even Eric </li>
<li><a href="http://oojdon.iteye.com/blog/903203">Rethinking architecture with CQRS</a>  </li>
<li><a href="http://timyang.net/architecture/twitter-cache-architecture/">Twitter架构图(cache篇)</a> </li>
<li>Event Sourcing – Martin fowler</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>应用架构</category>
        <category>CQRS</category>
      </categories>
      <tags>
        <tag>CQRS</tag>
      </tags>
  </entry>
  <entry>
    <title>缓存(cache)总结</title>
    <url>/www6vHomeHexo/2018/01/21/cacheSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>  

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2018/01/21/cacheSummary/cacheSummary.jpg" class title="cache总结">

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.geek-share.com/detail/2615401101.html">应用系统数据缓存设计</a> 淘宝技术部 *** 失效</li>
<li>Local Cache的小TIP  阿里 放翁（文初）</li>
<li>xxx</li>
<li>xxx</li>
<li>xxx</li>
<li>xxx</li>
<li>cache 58沈剑</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>中间件</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>缓存(cache)机制</title>
    <url>/www6vHomeHexo/2017/12/07/cache/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F">缓存数据分布模式</a><br>- <a href="#%E5%88%86%E7%89%87%E6%A8%A1%E5%BC%8F-sharding">分片模式 （sharding）</a><br>- <a href="#%E5%A4%8D%E5%88%B6%E6%A8%A1%E5%BC%8F">复制模式</a></li>
<li><a href="#%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98">本地缓存</a></li>
<li><a href="#%E7%BC%93%E5%AD%98%E4%B8%8E%E5%9C%BA%E6%99%AF">缓存与场景</a><ul>
<li><a href="#1-%E9%9D%9E%E5%85%B1%E6%80%A7%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98-eg-%E5%BE%AE%E5%8D%9A-%E5%8D%9A%E5%AE%A2%E4%B8%AA%E4%BA%BA%E9%A6%96%E9%A1%B5">1. 非共性数据缓存 eg. 微博， 博客个人首页</a><ul>
<li><a href="#i-%E7%83%AD%E7%82%B9%E7%BC%93%E5%AD%98">I. 热点缓存。</a></li>
<li><a href="#ii-%E9%9D%9E%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE">II. 非热点数据，</a></li>
<li><a href="#iii-read-only%E7%BC%93%E5%AD%98">III. read-only缓存</a></li>
</ul>
</li>
<li><a href="#2-%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9B%B4%E6%96%B0%E5%9C%BA%E6%99%AF">2. 高并发更新场景</a><ul>
<li><a href="#i-%E6%82%B2%E8%A7%82%E9%94%81%E6%96%B9%E6%A1%88">I. 悲观锁方案</a></li>
<li><a href="#ii-%E4%B9%90%E8%A7%82%E9%94%81%E6%96%B9%E6%A1%88">II. 乐观锁方案</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#redies%E4%BD%9C%E4%B8%BA%E7%BC%93%E5%AD%98%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">Redies作为缓存的最佳实践</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h2><span id="缓存数据分布模式">缓存数据分布模式</span><a href="#缓存数据分布模式" class="header-anchor">#</a></h2><h5><span id="分片模式-sharding">分片模式 （sharding）</span><a href="#分片模式-sharding" class="header-anchor">#</a></h5><p>数据正交分散到大量机器， 可以做到线性的可伸缩性， 但是实现可用性比较困难。还有一个好处是可以通过集群做负载均衡来实现数据的管理。<br>根据CAP理论， 在高可用的场景下， 数据分区的容忍性需要牺牲一定的一致性。 相应各个副本的同步策略也有不同， 主要可分为同步和异步方式。由客户端做一致性hash把数据分片存放到服务端。</p>
<h5><span id="复制模式">复制模式</span><a href="#复制模式" class="header-anchor">#</a></h5><p>数据在集群中存在多个副本。 在本地缓存的集群中，数据会复制到所有的节点中，没有网络延迟和等待时间的集群成员都是可用的。 多副本通过低延迟访问来提供高性能。<br>在修改数据时需要复制新的版本数据到所有的副本， 在高并发修改的场景下会限制系统的可伸缩性， 副本数不会调的太高。</p>
<h2><span id="本地缓存">本地缓存</span><a href="#本地缓存" class="header-anchor">#</a></h2><p>   在同一机房内的， 需要适量考虑本地cache， 数据压缩传输等以节省内网数据传输量 。<br>跨机房的缓存有地域区分，用户往往访问同一机房， 这样可以做本地的cache。 机房之间通过队列的方式进行异步和压缩传输，以提高用户请求的相应度。</p>
<h2><span id="缓存与场景">缓存与场景</span><a href="#缓存与场景" class="header-anchor">#</a></h2><h3><span id="1-非共性数据缓存-eg-微博-博客个人首页">1. 非共性数据缓存 eg. 微博， 博客个人首页</span><a href="#1-非共性数据缓存-eg-微博-博客个人首页" class="header-anchor">#</a></h3><p>  问题：缓存所有的数据性价比不高，　命中率不高<br>  解决方案：</p>
<h5><span id="i-热点缓存">I.                   热点缓存。</span><a href="#i-热点缓存" class="header-anchor">#</a></h5><p>  只缓存那些热点的数据。可以缓存在线的用户，　缓存热销的商品，　缓存热点用户的数据。热点规则表示如何匹配到一个热点，即这个查询请求是否请求了热点数据。 根据2&#x2F;8原则，小部分的数据占用了大部分的访问量。 这也就是twitter  page cache  是40%，而不是90%的原因。
　</p>
<h5><span id="ii-非热点数据">II.  非热点数据，</span><a href="#ii-非热点数据" class="header-anchor">#</a></h5><p>  可以采用nosql技术（redies），可以把它看成可持久化的缓存。<br>   原理类似虚拟内存，理论上不受内存大小的限制。使用NoSQL来做缓存，我们可以把一些不常访问、不怎么更新的数据也缓存起来。比如论坛、新闻的老数据、数据列表的靠后的页面，虽然用户访问不多，但是搜索引擎爬虫会访问，也可能导致系统负载上升。<br>  从外存拿数据减少了计算的开销 ，由于其数据库结构的简单，从磁盘获取一次数 据也比从数据库一次耗时的查询划算很多。</p>
<h5><span id="iii-read-only缓存">III. read-only缓存</span><a href="#iii-read-only缓存" class="header-anchor">#</a></h5><p>   缓存是read-only的， 如果有cache数据的更新， 把cache置为失效的。 如果有多个副本，这样做能够减少replication更新数据的开销， 只需要发送置失效的消息即可。</p>
<h3><span id="2-高并发更新场景">2. 高并发更新场景</span><a href="#2-高并发更新场景" class="header-anchor">#</a></h3><h5><span id="i-悲观锁方案">I. 悲观锁方案</span><a href="#i-悲观锁方案" class="header-anchor">#</a></h5><p>高并发更新， 缓存会超时的场景可以使用mutex锁。如<br>首页top 10, 由数据库加载到memcache缓存n分钟<br>微博中名人的content cache, 一旦不存在会大量请求不能命中并加载数据库<br>在加载数据库之前先增加一个mutex key作为锁， 成功之后再去做加载数据库， 如果加锁失败则sleep，之后重试读取原cache数据。为了防止死锁，锁也需要设置过期时间。</p>
<h5><span id="ii-乐观锁方案">II. 乐观锁方案</span><a href="#ii-乐观锁方案" class="header-anchor">#</a></h5><p>MVCC是后验性的，读不阻塞写，写也不阻塞读，等到提交的时候才检验是否有冲突，由于没有锁，所以读写不会相互阻塞，从而大大提升了并发性能。修改过的副本带着版本号元数据， 多个副本在合并时， 根据版本检测冲突， 并合并数据。</p>
<p>Memcache 通过客户端cas命令实现乐观锁。 Jboss在3.0实现了mvcc。 MVCC 提供了非阻塞 (non-blocking) 读操作 ( 它并不会去阻塞 wirter threads) ，在避免死锁的同时也提供了更高级的并发机制。它采用了 fail-fast 机制，如果写操作得到了一个 write lock ，那么它们也是依次进行，不允许重叠。</p>
<h2><span id="redies作为缓存的最佳实践">Redies作为缓存的最佳实践</span><a href="#redies作为缓存的最佳实践" class="header-anchor">#</a></h2><ol>
<li>对于全局公用的，构建成本比较低的数据， 可以采用一致性hash， 无复制， 无持久化的方案。 如果缓存crash了，可以快速重新构建。</li>
<li>对于与用户相关的， 一致性要求比较低的， 构建成本较低的， 可以采用多对一的复制方式，多个小容量的节点复制到同一个大容量的节点， 但不提供持久化， 提供较高的可用性。</li>
<li>对于与用户相关的，一致性要求比较高的， 构建成本比较高，但存储占用量不高的场景下， 需要持久化， 并且一对一的复制方式， 提供最高的可用性。</li>
</ol>
<blockquote>
<p><strong>案例： Twitter缓存体系</strong><br>  Twitter:<br>     逻辑缓存<br>        －  page cache    api<br>        － fragment cache    1. 原始数据的冗余 2. 结构上的冗余<br>     数据源cache<br>        － vector cache<br>        － Row cache </p>
</blockquote>
<ul>
<li>Page， fragment － 全局与局部的分离， api， 业务逻辑</li>
<li>Vector, row cache – 索引与内容的分离</li>
<li>Google gfs cache</li>
</ul>
<blockquote>
<p><strong>缓存类型：</strong><br>   Read-through  读贯穿<br>   Write-trrough  写贯穿<br>   Write-behind</p>
</blockquote>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.infoq.com/cn/news/2011/03/nosql-architecture-practice-3">NoSQL架构实践（三）——以NoSQL为缓存</a>   </li>
<li><a href="http://wenku.baidu.com/view/018e3f2d7375a417866f8fbc.html">大型网站架构系列之五,缓存策略设计概要</a>  失效</li>
<li><a href="http://timyang.net/programming/memcache-mutex/">Memcache mutex设计模式</a> </li>
<li><a href="http://superleo.iteye.com/blog/265823">深入理解JBoss Cache3.0——Naga</a>  *</li>
<li><a href="http://www.infoq.com/cn/articles/write-behind-caching">极端事务处理模式：Write-behind缓存</a>   </li>
<li><a href="http://coolshell.cn/articles/6790.html">多版本并发控制(MVCC)在分布式系统中的应用</a></li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM性能调优</title>
    <url>/www6vHomeHexo/2017/11/27/optimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span> 


<img src="/www6vHomeHexo/2017/11/27/optimize/optimize.jpg" class title="性能调优">


<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://tech.meituan.com/2017/12/29/jvm-optimize.html">从实际案例聊聊Java应用的GC优化</a>  录录</p>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>性能</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title>服务慢响应超时排查</title>
    <url>/www6vHomeHexo/2017/10/17/slowRT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2017/10/17/slowRT/slowRT.jpg" class title="服务慢响应排查">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://calvin1978.blogcn.com/?p=1661">在你的代码之外，服务时延过长的三个追查方向(上) (下)</a> 江南白衣</li>
<li>《Release It!: Design and Deploy Production-Ready Software》 Michael Nygard</li>
<li>关于TCP 半连接队列和全连接队列 阿里中间件博客</li>
<li><a href="https://mp.weixin.qq.com/s/uUsGwEaK4bomtXfJnW1TTQ">下次遇到嚣张的候选人就先这么问：系统变慢了你怎么搞？</a><br>fullgc， cpu，线程进入WAITING状态（压测）</li>
<li><a href="https://yq.aliyun.com/articles/705634?spm=a2c6h.12873622.0.0.40826ec89ETaDy">时延敏感业务低概率超时问题分析</a> 阿里月宾  good<br>网络原因； 排查方式， 循环抓包； 观察是否有丢包重传</li>
<li><a href="https://yq.aliyun.com/articles/697773">记一次对网络抖动经典案例的分析</a> 阿里云-江冉 good<br>根因： dentry在slab中的占用量高  解决方案：释放缓存</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>慢响应超时</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
        <tag>性能</tag>
        <tag>服务化</tag>
      </tags>
  </entry>
  <entry>
    <title>线上不能下单问题排查</title>
    <url>/www6vHomeHexo/2017/09/25/mybatisBug/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>2016.6.7号，整个jso订单都不能下单，数据库的链接从4000降到几百， 只能重启所有的服务，并保留一个机器做thread dump。</p>
<p>重启后数据库链接涨到了2000多，下单基本能用。</p>
<p>运维开始以为是数据库超时引起的整个服务线程夯住，排查下来是ibatis流控的线程里没有做同步的通知（notify），导致所有的线程都lock在同一把锁上并做wait。</p>
<p><img src="http://www6v.github.io/www6vHome/mybatisBug/thread%20dump.JPG" alt="图1： thread dump中大量的WAITING (on object monitor)" title="图1： thread dump中大量的WAITING (on object monitor)"></p>
<p><img src="http://www6v.github.io/www6vHome/mybatisBug/mybatisThrottleWait.JPG" alt="图二： 线程wait" title="图二： 线程wait"></p>
<p><img src="http://www6v.github.io/www6vHome/mybatisBug/mybatisThrottleNotify.JPG" alt="图三： 线程notify -- thread dump里没有看到" title="图三： 线程notify -- thread dump里没有看到"></p>
<p><img src="http://www6v.github.io/www6vHome/mybatisBug/throttlePool.JPG" alt="图四： pop和push" title="图四： pop和push"></p>
<p>解决方案： Throttle类是公司自己封装的ibatis版本，存在这个bug，估计是高并发的时候只调用了pop方法，其他线程还来不及调用push，所有的线程都夯住了。</p>
<p>解决这个问题很简单，升级ibatis版本到3.X.X。</p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>关于任务取消相关异常的排查</title>
    <url>/www6vHomeHexo/2017/08/09/interrupted/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="背景">背景</span><a href="#背景" class="header-anchor">#</a></h2><p>最近有些应用中会时不时的抛出如下异常：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">++++ exception thrown while trying to get object from cache for key: key值; host:10.4.37.175:11241 -- null</span><br><span class="line">java.nio.channels.ClosedByInterruptException</span><br><span class="line">at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:272)</span><br><span class="line">at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:195)</span><br><span class="line">at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86)</span><br><span class="line">at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)</span><br><span class="line">at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)</span><br><span class="line">at java.io.BufferedInputStream.read(BufferedInputStream.java:317)</span><br><span class="line">at java.io.DataInputStream.read(DataInputStream.java:132)</span><br><span class="line">at com.ycache.danga.MemCached.SockIOPool$SockIO.readLine(SockIOPool.java:2023)</span><br></pre></td></tr></table></figure>
<!-- more -->

<p><img src="http://www6v.github.io/www6vHome/interrupt/interrupted_clip_image001.png"></p>
<h2><span id="异常原因">异常原因</span><a href="#异常原因" class="header-anchor">#</a></h2><p>调用方使用了 Thread.interrupt 方法 或者 Future.cancel方法。</p>
<h2><span id="解决办法">解决办法</span><a href="#解决办法" class="header-anchor">#</a></h2><p>在调用ycache的线程中，去掉Future.cancel 或者 Thread.interrupt 方法使用。</p>
<h2><span id="调用过程分析">调用过程分析</span><a href="#调用过程分析" class="header-anchor">#</a></h2><ol>
<li>当调用方使用ycache的MemCachedClient.get方法时，就会调用SocketChannelImpl.read 方法，在读操作前运行 begin方法，读操作后使用 end方法<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public int read(ByteBuffer dst) throws IOException &#123; </span><br><span class="line">    try &#123; </span><br><span class="line">          begin(); </span><br><span class="line">          bytesRead = in.read(buf, 0, bytesToRead); </span><br><span class="line">      &#125; finally &#123; </span><br><span class="line">          end(bytesRead &gt; 0); </span><br><span class="line">      &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>in.read前执行begin()，此时创建中断触发器Interruptible的对象interruptor。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">interruptor = new Interruptible() &#123; </span><br><span class="line">    public void interrupt() &#123; </span><br><span class="line">      synchronized (closeLock) &#123; </span><br><span class="line">        if (!open) </span><br><span class="line">        return; </span><br><span class="line">        interrupted = true; </span><br><span class="line">        open = false; </span><br><span class="line">        try &#123; </span><br><span class="line">            AbstractInterruptibleChannel.this.implCloseChannel(); // 回调AbstractInterruptibleChannel的方法 来关闭 Channel。</span><br><span class="line">            &#125; catch (IOException x) &#123; &#125; </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure></li>
<li>当 in.read 在执行且尚未执行完，此时调用方代码 执行了Thread.interrupt 或者 Future.cancel 时，就会调用当前线程interruptor的interrupt方法，<br>将interrupted 设置为true，同时关掉socket<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public void interrupt() &#123; </span><br><span class="line">   if (this != Thread.currentThread()) </span><br><span class="line">       checkAccess();</span><br><span class="line"></span><br><span class="line">   synchronized (blockerLock) &#123; </span><br><span class="line">       Interruptible b = blocker; </span><br><span class="line">       if (b != null) &#123; </span><br><span class="line">      interrupt0();    // Just to set the interrupt flag </span><br><span class="line">      b.interrupt(); </span><br><span class="line">      return; </span><br><span class="line">       &#125; </span><br><span class="line">   &#125; </span><br><span class="line">   interrupt0(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>in.read后执行end(true)，当没有读取到数据或者读取超时时，就会发生ClosedByInterruptException，代码如下<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">protected final void end(boolean completed) </span><br><span class="line">   throws AsynchronousCloseException </span><br><span class="line">    &#123; </span><br><span class="line">   blockedOn(null); </span><br><span class="line">   if (interrupted) throw new ClosedByInterruptException(); </span><br><span class="line">   if (!open) throw new AsynchronousCloseException(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="整体过程是">整体过程是</span><a href="#整体过程是" class="header-anchor">#</a></h2><p>1.线程执行begin方法，植入中断触发器interruptor，然后可能在channel上阻塞；<br>2.另外一个线程调用步骤1线程的interrupt方法；<br>3.interrupt方法会执行channel的close方法，并设置标志位interrupted &#x3D; true；<br>4.线程进入end方法，清空线程的中断触发器，当判断interrupted &#x3D; true时，抛出ClosedByInterruptException。</p>
<p>注：文中代码在java.nio.channels.spi.AbstractInterruptibleChannel中。</p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>NIO</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>zookeeper未通知到服务客户端的异常排查</title>
    <url>/www6vHomeHexo/2017/07/28/zookeeperBug/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="背景">背景</span><a href="#背景" class="header-anchor">#</a></h2><p>按正常的流程，服务提供者上下线，zookeeper会通知到服务客户端，也就是说服务客户端会自动感知到服务的当前状态。但是每隔数月我们会接到同样的问题，说是服务客户端的状态通知丢失了，每次涉及机器比较少只有一两台，所以怀疑是zk漏发通知导致的偶发现象（zookeeper的早期版本有这个bug）。</p>
<p>随着业务的快速发展，发布的服务和zk节点数量一直在增加，最近一次有40几台机器出现同样状况。通过这几天我的排查和实验问题已经定位，根本原因是客户端没有与zk建立有效session，导致对应的watcher没有注册到zk，最终导致zk服务端不会推送对应的节点变更。</p>
<!-- more -->

<p>下面来剖析session创建不成功的原因。</p>
<h2><span id="排查过程">排查过程</span><a href="#排查过程" class="header-anchor">#</a></h2><h3><span id="1-zookeeper服务端日志">1. zookeeper服务端日志</span><a href="#1-zookeeper服务端日志" class="header-anchor">#</a></h3><p>通过ZK日志可以看到出，10.4.31.15 这台机器创建zk连接成功，但session创建出现异常导致连接关闭。由于zkclient本身的重连机制，客户端会不断重连服务端，但在session无法创建成功的情况下，watcher并不会注册到服务端所以也不可能受到变更通知。</p>
<p><img src="http://www6v.github.io/www6vHome/zookeeperBug/zk_log.jpg"></p>
<h3><span id="2-zk源代码">2. zk源代码</span><a href="#2-zk源代码" class="header-anchor">#</a></h3><p>我们来看下导致session创建失败的异常，java.io.IOException: Len error 1323236，通过查询ZK源码我们发现，zk服务端在获取客户端Request前会优先检测请求长度，限制长度通过jute.maxbuffer参数来配置，服务端默认值为1M。如果超过限制的请求，会直接抛拒绝session创建失败。</p>
<p><img src="http://www6v.github.io/www6vHome/zookeeperBug/readLength.jpg"></p>
<h3><span id="3-怀疑是zookeeper-706">3. 怀疑是ZOOKEEPER-706</span><a href="#3-怀疑是zookeeper-706" class="header-anchor">#</a></h3><p>调研了一下，ZOOKEEPER-706里的bug和我们看到的异常是一致的。在ZOOKEEPER-706中，如果客户端在session重建的时候做了大量 “set watches”操作，这个时候session会建立失败。</p>
<p>看了ZOOKEEPER-706的patch的代码，代码已经在我们用的zk3.4.7版本里有了，那照道理线上不应该报这个错。通过ZOOKEEPER-706的patch的代码，我们发现zk客户端在重连的过程中会对大于128K的watches长度进行拆包，把大于1M的包拆成128k的Packet，然后放到outgoingQueue中，如下图。这样就不会产生java.io.IOException: Len error异常。</p>
<p><img src="http://www6v.github.io/www6vHome/zookeeperBug/zk-706-patch.JPG"></p>
<h3><span id="4-消息体验证">4. 消息体验证</span><a href="#4-消息体验证" class="header-anchor">#</a></h3><p>想通过zookeeper服务端消息和hedwig客户端来看一下收发的消息到底是什么。</p>
<h3><span id="41-zookeeper服务端抓包消息体验证">4.1） zookeeper服务端抓包，消息体验证</span><a href="#41-zookeeper服务端抓包消息体验证" class="header-anchor">#</a></h3><p>在线上的zk服务器节点抓包，看看到底是什么请求超过1M呢，进行了TCP dump结果发现都是很小的包（如下图wireshark里看到的1k左右的消息），没找到大于1M的消息。后面通过查看代码发现客户端会先把请求长度发给服务端做检测而不是直接发内容。</p>
<p><img src="http://www6v.github.io/www6vHome/zookeeperBug/wireshark.JPG"></p>
<h3><span id="42客户端watches大小验证">4.2）客户端watches大小验证</span><a href="#42客户端watches大小验证" class="header-anchor">#</a></h3><p>客户端做了一个heap dump，看到一个ZkClient里watches的大小大于1M，为1.7M左右。 会触发ZOOKEEPER-706异常 。</p>
<p><img src="http://www6v.github.io/www6vHome/zookeeperBug/client_watchers.JPG"></p>
<h2><span id="总结">总结：</span><a href="#总结" class="header-anchor">#</a></h2><p>最后找到的问题是，应用把hedwig的zk3.4.7在pom.xml里exclude掉了，用的是zk3.4.3版本，ZOOKEEPER-706的patch当然没有打上，线上产生这个错误也是正常的。</p>
<p>最后把zk3.4.3升级为zk3.4.7版本，这个问题解决了。</p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>异地多活 总结</title>
    <url>/www6vHomeHexo/2017/06/17/multiLive/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BB-3">同城双活 [3]</a></li>
<li><a href="#%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB-3">异地多活 [3]</a></li>
<li><a href="#%E6%80%BB%E7%BB%93-3">总结 [3]</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7-12">数据一致性 [1][2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83-%E5%8F%82%E8%80%83">参考 # 参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="同城双活-3">同城双活 [3]</span><a href="#同城双活-3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2017/06/17/multiLive/r-ha1.png" class>

<p>【跨机房写，同机房读】</p>
<h1><span id="异地多活-3">异地多活 [3]</span><a href="#异地多活-3" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2017/06/17/multiLive/r-ha2.png" class>

<p><strong>一般来说，数据同步的方案有两种：</strong></p>
<ol>
<li>一种基于存储系统的主从复制，比如 MySQL 和 Redis。也就是在一个机房部署主库，<br>在异地机房部署从库，两者同步主从复制, 实现数据的同步。</li>
<li>另一种是基于消息队列的方式。一个机房产生写入请求后，会写一条消息到消息队列，<br>另一个机房的应用消费这条消息后，再执行业务处理逻辑，写入到存储服务中。</li>
</ol>
<p>【异步方式同步数据】</p>
<p>无论是采取哪种方案，<strong>数据从一个机房，传输到另一个机房都会有延迟，所以，你需要尽量</strong><br><strong>保证用户在读取自己的数据时，读取数据主库所在的机房</strong>。为了达到这一点，你需要对用户<br>做分片，让一个用户每次的读写都尽量在同一个机房中。同时，在数据读取和服务调用时，<br>也要尽量调用本机房的服务。</p>
<p>【单元化，流量调度】</p>
<h1><span id="总结-3">总结 [3]</span><a href="#总结-3" class="header-anchor">#</a></h1><ul>
<li><p>不同机房的数据传输延迟，是造成多机房部署困难的主要原因，你需要知道，<strong>同城多机</strong><br><strong>房的延迟一般在 1ms~3ms，异地机房的延迟在 50ms 以下，而跨国机房的延迟在200ms 以下。</strong></p>
</li>
<li><p><strong>同城多机房方案可以允许有跨机房数据写入的发生</strong>，<strong>但是数据的读取，和服务的调用应该尽量保证在同一个机房中。</strong></p>
</li>
<li><p>异地多活方案则应该避免跨机房同步的数据写入和读取，而是采取异步的方式，将数据从一个机房同步到另一个机房。</p>
</li>
</ul>
<h1><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h1><ul>
<li>异地多活<ul>
<li>阿里 【1】<ul>
<li>基于Userid的单元化异地多活</li>
<li>主要改造整个交易链路</li>
<li>交易链路（单元）和非交易链路（中心）之间通过DRC同步数据。单元里的数据是全量、只读的</li>
</ul>
</li>
<li>饿了么 【2】<ul>
<li>思路+原则<ul>
<li>基于地理位置的异地多活。用户、商家、骑手都会在相同的机房</li>
<li>可用性优先，放宽数据一致性</li>
</ul>
</li>
<li>主要组件<ul>
<li>GZS（元数据）+APIRouter（流量路由）</li>
<li>SOA Proxy：内部网关、IDC之间调用</li>
<li>Data Replication Center：数据库复制、数据库和cache之间的一致性</li>
<li>Data Access Layer</li>
<li>zk,mq在IDC之间的同步</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="数据一致性-12">数据一致性 [1][2]</span><a href="#数据一致性-12" class="header-anchor">#</a></h1><ul>
<li>数据一致性<ul>
<li>强一致场景<ul>
<li>都读主节点</li>
</ul>
</li>
<li>最终一致性场景<ul>
<li>DRC异步同步数据</li>
<li>业务层异步分发数据</li>
</ul>
</li>
<li>数据丢失<ul>
<li>通过算法在不同机房都能生成相同的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考-参考">参考  # 参考</span><a href="#参考-参考" class="header-anchor">#</a></h1><ol>
<li><p>《尽在双11:阿里巴巴技术演进与超越》 1.2节</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/32009822">饿了么异地多活技术实现（一）总体介绍</a>   饿了么框架工具部  知乎专栏</p>
</li>
<li><p>《28 | 多机房部署：跨地域的分布式系统如何做？》  唐扬</p>
</li>
<li><p><a href="https://blog.csdn.net/lql_h/article/details/95588996">SET化架构设计</a>  lql_h  未</p>
</li>
<li><p><a href="https://www.jianshu.com/p/0012b44ed7c6">看完这篇异地多活的改造，我决定和架构师battle一下｜得物技术</a>  未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>架构</category>
        <category>系统架构</category>
        <category>异地多活</category>
      </categories>
      <tags>
        <tag>异地多活</tag>
      </tags>
  </entry>
  <entry>
    <title>稳定性总结</title>
    <url>/www6vHomeHexo/2017/05/09/stability/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><strong>关键词</strong>:  容量规划, 压测, 强弱依赖,<br><strong>关键词</strong>:  故障模型, 故障演练, 故障注入</p>
<h2><span id="稳定性总结">稳定性总结</span><a href="#稳定性总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2017/05/09/stability/stability.jpg" class title="稳定性总结">




<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://developer.aliyun.com/article/105551">超全总结 | 阿里如何应对电商故障？神秘演练细节曝光   阿里巴巴-周洋（花名中亭）</a> 故障注入， 故障演练</li>
<li><a href="http://jm.taobao.org/2012/10/31/stability-considerations-dependent-strength/">稳定性思考-强弱依赖</a>  阿里中间件团队博客   </li>
<li><a href="http://jm.taobao.org/2012/10/31/stability-considerations-dependent-strength-2/">稳定性思考-强弱依赖2</a>  阿里中间件团队博客</li>
<li><a href="https://developer.aliyun.com/article/25419/">中间件技术及双十一实践·稳定性平台篇</a>  阿里中间件（Aliware）<br>强弱依赖， 容量规划   </li>
<li>&lt;&lt;尽在双11阿里巴巴技术演进与超越&gt;&gt;</li>
</ol>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>稳定性</tag>
      </tags>
  </entry>
  <entry>
    <title>文件IO总结</title>
    <url>/www6vHomeHexo/2017/04/23/fileIO/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2017/04/23/fileIO/fileIO.jpg" class title="文件IO总结">


<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>文件IO操作的最佳实践 kirito-moe</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>文件IO</category>
      </categories>
      <tags>
        <tag>文件</tag>
      </tags>
  </entry>
  <entry>
    <title>幂等</title>
    <url>/www6vHomeHexo/2017/03/21/idempotent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2017/03/21/idempotent/idempotent.jpg" class title="幂等">

<h2><span id="三种实现方式">三种实现方式：</span><a href="#三种实现方式" class="header-anchor">#</a></h2><h3><span id="1-利用数据库的唯一约束实现幂等">1. 利用数据库的唯一约束实现幂等</span><a href="#1-利用数据库的唯一约束实现幂等" class="header-anchor">#</a></h3><p>在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给<strong>转账单 ID 和账户 ID <strong>这两个字段联合起来创建一个</strong>唯一约束</strong>，这样对于相同的转账单 ID 和账户 ID，表里至多只能<strong>存在一条记录</strong>。</p>
<p>基于这个思路，不光是可以使用关系型数据库，只要是支持类似<strong>“INSERT IF NOT EXIST”语义</strong>的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。</p>
<h3><span id="2-为更新的数据设置前置条件">2. 为更新的数据设置前置条件</span><a href="#2-为更新的数据设置前置条件" class="header-anchor">#</a></h3><p><strong>“将账户 X 的余额增加 100 元”这个操作并不满足幂等性</strong>，我们可以把这个操作加上一个前置条件，变为：<strong>“如果账户 X 当前的余额为 500 元，将余额加100 元”</strong>，这个操作就具备了幂等性。</p>
<p>更加通用的方法， 数据增加一个<strong>版本号属性</strong>，每次更数据前，<strong>比较当前数据的版本号是否和消息中的版本号一致</strong>，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新。</p>
<h3><span id="3-记录并检查操作">3. 记录并检查操作</span><a href="#3-记录并检查操作" class="header-anchor">#</a></h3><p>通用性最强，适用范围最广的实现幂等性方法：记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”。</p>
<p><strong>具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。</strong></p>
<p>这种方法适用范围最广，但是实现难度和复杂度也比较高，<strong>一般不推荐使用</strong>。</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://tech.meituan.com/2016/09/29/distributed-system-mutually-exclusive-idempotence-cerberus-gtis.html">分布式系统互斥性与幂等性问题的分析与解决</a> 蒋谞</li>
<li>消息总线真的能保证幂等？ 58沈剑</li>
<li>《微服务设计》 11.6节  Sam Newman</li>
<li>《消息队列高手课 - 如何处理消费过程中的重复消息？》  李玥</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>幂等</category>
      </categories>
      <tags>
        <tag>幂等</tag>
      </tags>
  </entry>
  <entry>
    <title>Split Brain</title>
    <url>/www6vHomeHexo/2017/02/19/splitBrain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><strong>关键词</strong>: 脑裂, fence, Quorums , epoch</p>
<p><strong>脑裂</strong>： 类似 CAP中的P<br><strong>Partition tolerance(分区容错性)</strong>: 网络分区发生时，一致性和可用性两难全</p>
<h2><span id="一-通用解决方案">一. 通用解决方案</span><a href="#一-通用解决方案" class="header-anchor">#</a></h2><ol>
<li>Quorums</li>
<li>Redundant communications，冗余通信的方式</li>
<li>Fencing</li>
</ol>
<h2><span id="二-系统">二. 系统</span><a href="#二-系统" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">&#x2F;</th>
<th align="center">现象</th>
<th align="center">解决方案</th>
</tr>
</thead>
<tbody><tr>
<td align="center">kafka</td>
<td align="center">kafka脑裂现象:1. 存在多个controller <br> 2. consumer的splitBrain</td>
<td align="center">controller使用epoch来避免脑裂</td>
</tr>
<tr>
<td align="center">elastic search</td>
<td align="center"></td>
<td align="center">配置discovery.zen.minimum_master_nodes，类似Quorums</td>
</tr>
<tr>
<td align="center">zookeeper</td>
<td align="center">两个leader[3]</td>
<td align="center">Quorums <br>leader单调递增的epoch</td>
</tr>
<tr>
<td align="center">raft脑裂</td>
<td align="center">两个majority</td>
<td align="center">[2] Quorums + term</td>
</tr>
</tbody></table>
<p>redis脑裂、mysql脑裂 </p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://www.cnblogs.com/yjmyzz/p/redis-split-brain-analysis.html">redis 脑裂等极端情况分析</a><blockquote>
<p>Redis Cluster is not able to guarantee strong consistency. &#x2F; In general Redis + Sentinel as a whole are a an eventually consistent system</p>
</blockquote>
</li>
<li><a href="../../../../2019/06/21/raft/">raft协议</a> self</li>
<li><a href="https://www.cnblogs.com/nicerblog/p/11232531.html">脑裂是什么？Zookeeper是如何解决的？</a></li>
</ol>
<p>​	</p>
<p>​	<br>​</p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>SplitBrain</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>稳定性</tag>
      </tags>
  </entry>
  <entry>
    <title>延迟消息 时间轮</title>
    <url>/www6vHomeHexo/2017/01/05/timedTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>关键词：延迟消息, 时间轮, Time Wheel</p>
<img src="/www6vHomeHexo/2017/01/05/timedTask/timedTask.jpg" class title="延迟消息">



<h2><span id="时钟轮的应用">时钟轮的应用</span><a href="#时钟轮的应用" class="header-anchor">#</a></h2><h5><span id="kafka中的应用-1">Kafka中的应用 [1]</span><a href="#kafka中的应用-1" class="header-anchor">#</a></h5><p>   存在大量的延迟操作，比如<strong>延迟生产</strong>、<strong>延迟拉取</strong>以及<strong>延迟删除</strong>等</p>
<h5><span id="rpc中的应用-6">RPC中的应用 [6]</span><a href="#rpc中的应用-6" class="header-anchor">#</a></h5><ul>
<li>每次创建一个 Future，我们都记录这个 Future 的创建时间与这个 Future 的超时时间，并且有一个定时任务进行检测，当这个 Future 到达超时时间并且没有被处理时，我们就对这个 Future 执行超时逻辑。<br>每发一次请求，都创建一个<strong>处理请求超时的定时任务</strong>放到时钟轮里。</li>
<li><strong>调用端与服务端启动超时</strong>也可以应用到时钟轮。<br>可以在调用端启动时创建一个处理启动超时的定时任务，放到时钟轮里。</li>
<li>RPC 框架调用端定时向服务端发送心跳，来维护连接状态，我们可以将<strong>心跳的逻辑</strong>封装为一个<strong>心跳任务</strong>，放到时钟轮里。<br>在定时任务的执行逻辑的最后，我们可以重设这个任务的执行时间，把它重新丢回到时钟轮里。</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/u013256816/article/details/80697456">Kafka解惑之时间轮（TimingWheel）</a>  朱忠华</li>
<li><a href="https://mp.weixin.qq.com/s/mvFwjgxliwx808Hn_9ruEA?ptlang=2052&ADUIN=1024616676&ADSESSION=1489673030&ADTAG=CLIENT.QQ.5497_.0&ADPUBNO=26661">10w定时任务，如何高效触发超时</a> 58沈剑</li>
<li><a href="https://www.cnblogs.com/hzmark/p/mq-delay-msg.html">如何在MQ中实现支持任意延迟的消息？</a></li>
<li><a href="https://www.cnblogs.com/haoxinyue/p/6663720.html">延迟任务的实现总结</a>  nick hao</li>
<li><a href="https://www.jianshu.com/p/33aa208ea058">rocketMq-延迟消息介绍</a>   晴天哥_王志 *</li>
<li>《20 | 详解时钟轮在RPC中的应用》 何小锋</li>
<li><a href="https://zhuanlan.zhihu.com/p/488730353">时间轮原理及其在框架中的应用</a>  未<br>Dubbo 心跳检测</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>定时调度</category>
      </categories>
      <tags>
        <tag>调度</tag>
        <tag>消息</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式锁</title>
    <url>/www6vHomeHexo/2016/12/05/distributedLock/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81">分布式锁</a></li>
<li><a href="#%E9%94%81%E7%9A%84%E7%89%B9%E6%80%A7">锁的特性</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0">实现</a><ul>
<li><a href="#redis5">Redis[5]</a></li>
<li><a href="#zookeeper4">Zookeeper[4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="分布式锁">分布式锁</span><a href="#分布式锁" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2016/12/05/distributedLock/distributedLock.jpg" class title="分布式锁">  

<h1><span id="锁的特性">锁的特性</span><a href="#锁的特性" class="header-anchor">#</a></h1><ol>
<li>排它性</li>
<li>超时释放锁<br>zk临时目录</li>
<li>高可用，锁集群容错[图2]，<br>安全性[3]，</li>
<li>可重入锁, 避免死锁 [4]</li>
<li>乐观锁, 悲观锁[6][图2]</li>
</ol>
<h1><span id="实现">实现</span><a href="#实现" class="header-anchor">#</a></h1><h3><span id="redis5">Redis[5]</span><a href="#redis5" class="header-anchor">#</a></h3><h3><span id="zookeeper4">Zookeeper[4]</span><a href="#zookeeper4" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://tech.meituan.com/2016/09/29/distributed-system-mutually-exclusive-idempotence-cerberus-gtis.html">分布式系统互斥性与幂等性问题的分析与解决</a> 点评 蒋谞 </p>
</li>
<li><p>漫画：什么是分布式锁？ 程序员小灰</p>
</li>
<li><p><a href="https://www.jianshu.com/p/31e85a18a9e7">分布式服务总结 分布式锁</a><br>通过栅栏(fencing)使得锁更安全, fencing token<br><a href="http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">How to do distributed locking</a> Martin Kleppmann  </p>
</li>
<li><a href="/www6vHomeHexo/2021/05/12/zookeeperDistributedLock/" title="Zookeeper-分布式锁">Zookeeper-分布式锁</a> self</li>
<li><a href="/www6vHomeHexo/2022/05/05/redisDistKey/" title="Redis 分布式锁">Redis 分布式锁</a> self</li>
<li><p><a href="https://www.cnblogs.com/jasonZh/p/9522772.html">Redis分布式锁实现秒杀业务(乐观锁、悲观锁)</a>  最后<br> 乐观锁: jedis的watch方法</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ahcbgxWVVmRwrH9Y4-gXBA">SOFAJRaft-RheaKV 分布式锁实现剖析 | SOFAJRaft 实现原理</a>   SOFALab 米麒麟 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 总结</title>
    <url>/www6vHomeHexo/2016/11/12/redis/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BA%8B%E5%8A%A1">事务</a></li>
<li><a href="#hash%E5%91%BD%E4%BB%A4">hash命令</a></li>
<li><a href="#io%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%80%A7%E8%83%BD">IO模型和性能</a></li>
<li><a href="#%E7%89%B9%E6%80%A7">特性</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<img src="/www6vHomeHexo/2016/11/12/redis/redis.jpg" class title="Redis 总结">

<h2><span id="事务">事务</span><a href="#事务" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>-</th>
<th>原子性</th>
<th>一致性</th>
<th>隔离性</th>
<th>持久性</th>
</tr>
</thead>
<tbody><tr>
<td>redis</td>
<td>一定的原子性，但不支持回滚</td>
<td>×</td>
<td>√</td>
<td>通过一定策略可以保证持久性</td>
</tr>
<tr>
<td>redis</td>
<td>没有进行回滚，不具备原子性.<br>操作之后写AOF日志</td>
<td>aof可以保证，但从应用层看没有回滚和原子性，所以并不能保证一致性</td>
<td>单线程天然隔离</td>
<td>纯内存(×)<br>RDB Bgsave(√) <br> RDB 异步执行(×)</td>
</tr>
<tr>
<td>mysql</td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>mysql</td>
<td>undo log</td>
<td>锁</td>
<td>锁</td>
<td>redo log</td>
</tr>
</tbody></table>
<h2><span id="hash命令">hash命令</span><a href="#hash命令" class="header-anchor">#</a></h2><ul>
<li><p>redis hash的结构：一维数组+二维链表（和java的hashmap结构一样）</p>
</li>
<li><p>redis rehash: 渐进式rehash<br>Java rehash： 一次性将旧数组下挂接的元素全部转移到新数组下面</p>
</li>
</ul>
<h2><span id="io模型和性能">IO模型和性能</span><a href="#io模型和性能" class="header-anchor">#</a></h2><ul>
<li><p>非阻塞IO： read， write时不阻塞</p>
</li>
<li><p>事件轮询和多路复用[8]</p>
</li>
<li><p>redis性能<br>最低配置: 4GB， 2核， 链接数2w， QPS 16w</p>
</li>
<li><p>redis性能高的原因</p>
<ul>
<li>高效的数据结构</li>
<li>多路复用IO模型</li>
<li>事件机制<br>总结:Reactor + 队列 [10]</li>
</ul>
</li>
</ul>
<blockquote>
<p>大体上可以说 Redis 的工作模式是，reactor 模式配合一个队列，用一个 serverAccept 线程来处理建立请求的链接，<br>并且通过 IO 多路复用模型，让内核来监听这些 socket，一旦某些 socket 的读写事件准备就绪后就对应的事件压入队列中，<br>然后 worker 工作，由文件事件分派器从中获取事件交于对应的处理器去执行，当某个事件执行完成后文件事件分派器才会从队列中获取下一个事件进行处理。<br>可以类比在 netty 中，我们一般会设置 bossGroup 和 workerGroup 默认情况下 bossGroup 为 1，workerGroup &#x3D; 2 * cpu 数量，<br>这样可以由多个线程来处理读写就绪的事件，但是其中不能有比较耗时的操作如果有的话需要将其放入线程池中，不然会降低其吐吞量。<br>在 Redis 中我们可以看做这二者的值都是 1。</p>
</blockquote>
<h2><span id="特性">特性</span><a href="#特性" class="header-anchor">#</a></h2><ul>
<li>Redis 2.6<br>lua, pubsub, Sentinel V1</li>
<li>Redis 2.8<br>Sentinel V2, ipv6</li>
<li>Redis 3.0<br><strong>Redis Cluster</strong> </li>
<li>Redis3.2<br>GEO</li>
<li>Redis 4.0<br>psync2.0, <strong>lazy-free</strong>, modules<br>RDB-AOF 混合持久化</li>
<li>Redis 5.0<br>Stream</li>
<li>Redis 6.0<br><strong>Thread I&#x2F;O</strong><br>SSL, ACL</li>
<li>Redis 7.0<br>functions, ACL v2<br>sharded-pubsub<br>client-eviction<br><strong>multi-part AOF</strong></li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>《Redis 深度历险：核心原理与应用实践》 钱文品</p>
<ol>
<li>原理 4：雷厉风行 —— 管道</li>
<li>原理 5：同舟共济 —— 事务</li>
<li>原理 3：未雨绸缪 —— 持久化</li>
<li>鞭辟入里 ——— 线程IO模型</li>
</ol>
<hr>
<ol start="5">
<li>《Redis实战》 黄健宏 3.7 ,4.4, 6.2</li>
<li><a href="https://mp.weixin.qq.com/s/fO0yoHGqtFH5lpu6688h2w">Redis 数据结构和对象系统，记住这 12 张图就够啦！</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&mid=2650780240&idx=1&sn=49fb636a97a3c21fec7d2e2b59bea09f">七问Redis，才知道我与技术大牛的差距在哪里 </a><br>*** 事务，乐观锁watch，持久化， 内存优化，主从复制，过期删除策略</li>
<li><a href="../../../../2015/02/21/transaction/">Mysql事务总结</a> self</li>
<li><a href="https://www.cnblogs.com/meituantech/p/9376472.html">美团针对Redis Rehash机制的探索和实践</a> ***</li>
<li><a href="https://mp.weixin.qq.com/s/QrvUl6Ul9DxYoRZwSsMQZw">为什么 Redis 单线程能达到百万+QPS？</a> ***</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>KV</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>容错框架</title>
    <url>/www6vHomeHexo/2016/10/07/soaTolerateFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#hystrix%E5%AE%9E%E7%8E%B0%E5%92%8C%E5%AE%B9%E9%94%99%E6%A8%A1%E5%BC%8F">Hystrix实现和容错模式</a></li>
<li><a href="#resilience4j-1">Resilience4j [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#hystrix">Hystrix</a></li>
<li><a href="#resilience4j">Resilience4j</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="hystrix实现和容错模式">Hystrix实现和容错模式</span><a href="#hystrix实现和容错模式" class="header-anchor">#</a></h1><ul>
<li>Hystrix实现和容错模式<ul>
<li>熔断【熔断器模式】<ul>
<li>三个状态<ul>
<li>开</li>
<li>闭</li>
<li>半开</li>
</ul>
</li>
<li>模块<ul>
<li>熔断请求判断机制算法<ul>
<li>维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态。<ul>
<li>超时【超时与重试模式】</li>
<li>失败（异常）</li>
<li>成功</li>
<li>拒绝<br>  线程池拒绝【1】<br>  信号量拒绝【2】</li>
</ul>
</li>
<li>默认错误超过50%且10秒内超过20个请求进行中断拦截</li>
</ul>
</li>
<li>熔断恢复<ul>
<li>每隔5s允许部分请求通过，若请求都是健康的（RT&lt;250ms）则对请求健康恢复</li>
</ul>
</li>
<li>熔断报警和Metric上报</li>
</ul>
</li>
</ul>
</li>
<li>流控【限流模式】<ul>
<li>控制速率</li>
<li>控制并发</li>
</ul>
</li>
<li>隔离【舱壁隔离模式】<ul>
<li>Hystrix实现<ul>
<li>线程池隔离 【1】</li>
<li>信号量隔离【2】</li>
</ul>
</li>
</ul>
</li>
<li>回退【回退模式】<ul>
<li>快速失败（Fail Fast ）</li>
<li>无声失败（Fail Silent ）</li>
<li>返回默认值（Fallback  Static ）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="resilience4j-1">Resilience4j [1]</span><a href="#resilience4j-1" class="header-anchor">#</a></h1><ul>
<li>断路器（Circuit Breaker）</li>
<li>重试（Retry）</li>
<li>限时器（Time Limiter） </li>
<li>限流器（Rate Limiter）</li>
<li>隔板（BulkHead）</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="hystrix">Hystrix</span><a href="#hystrix" class="header-anchor">#</a></h3><ol>
<li>微服务熔断与隔离 楚岩</li>
<li>Hystrix技术解析 王新栋</li>
<li>&lt;&lt;亿级流量网站架构核心技术&gt;&gt; 5.8节 张开涛</li>
<li>Hystrix 使用与分析 zhangyijun</li>
</ol>
<h3><span id="resilience4j">Resilience4j</span><a href="#resilience4j" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.zhihu.com/question/365162958">Resilience4j 比 Hystrix 好在哪里？</a></li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>容错</category>
      </categories>
      <tags>
        <tag>服务治理</tag>
      </tags>
  </entry>
  <entry>
    <title>限流-总结</title>
    <url>/www6vHomeHexo/2016/09/26/ratelimit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="限流总结">限流总结</span><a href="#限流总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2016/09/26/ratelimit/ratelimit.jpg" class title="限流总结">

<h2><span id="限流算法">限流算法</span><a href="#限流算法" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">流控算法</th>
<th align="center">原理</th>
<th align="center">实现</th>
<th align="center">实现复杂度</th>
<th align="center">优势</th>
<th align="center">缺点</th>
</tr>
</thead>
<tbody><tr>
<td align="center">计数器法</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">简单</td>
<td align="center"></td>
<td align="center">缺点  临界问题,不能应对突发请求</td>
</tr>
<tr>
<td align="center">滑动窗口</td>
<td align="center">滑动时间窗口划成了多格，粒度细; <br>解决了计数器法的缺点;</td>
<td align="center">基于时间窗口[5]</td>
<td align="center">简单</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">令牌桶算法</td>
<td align="center"></td>
<td align="center">Guava RateLimiter  [7]</td>
<td align="center">复杂</td>
<td align="center">能够处理突发请求; 允许某些流量的突发，被业界采用地较多</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">漏桶算法</td>
<td align="center"></td>
<td align="center">漏桶算法[6] 代码[0]</td>
<td align="center">简单</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">队列算法</td>
<td align="center"></td>
<td align="center">FIFO队列; 权重队列; Linux tc</td>
<td align="center"></td>
<td align="center">队列长度很关键</td>
<td align="center"></td>
</tr>
</tbody></table>
<h2><span id="分布式限流">分布式限流</span><a href="#分布式限流" class="header-anchor">#</a></h2><ul>
<li><p>分布式计数器</p>
<ul>
<li>实现<br>Redis(服务端)+Lua(客户端)</li>
</ul>
</li>
<li><p>限流网关</p>
<ul>
<li>缺陷<br>服务之间的调用不一定走网关</li>
</ul>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol start="0">
<li><a href="https://github.com/www6v/jDemo/blob/master/src/main/java/middleware/rateLimiter/FunnelRateLimiter.java">漏桶算法实现</a></li>
<li><a href="https://yq.aliyun.com/articles/4225">限流系统如何发现系统的热点</a>  中间件小哥  ***</li>
<li>接口限流算法总结     夜有所思，日有所梦</li>
<li><a href="https://www.iteye.com/blog/jinnianshilongnian-2305117">聊聊高并发系统之限流特技</a>  张开涛</li>
<li><a href="http://calvin1978.blogcn.com/articles/ratelimiter.html">服务化体系之－限流</a>  江南白衣  失效</li>
<li>《应用 6：断尾求生 —— 简单限流  》 Redis 深度历险：核心原理与应用实践 </li>
<li>《应用 7：一毛不拔 —— 漏斗限流》 Redis 深度历险：核心原理与应用实践      有代码实现</li>
<li><a href="https://segmentfault.com/a/1190000012875897">Guava RateLimiter源码解析</a>  林舍  manerfan</li>
<li><a href="https://mp.weixin.qq.com/s/RM3ffBCJqoQ2JMPKHgmv0Q">淘宝应用柔性架构的探索</a> 自适应负载调节</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>限流</category>
      </categories>
      <tags>
        <tag>限流</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式ID生成 发号器</title>
    <url>/www6vHomeHexo/2016/08/15/id/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2016/08/15/id/id.jpg" class title="分布式id生成">

<h2><span id="一-uuid">一. UUID</span><a href="#一-uuid" class="header-anchor">#</a></h2><h3><span id="缺点">缺点</span><a href="#缺点" class="header-anchor">#</a></h3><ul>
<li>没有业务含义</li>
<li>不是自增的， 某些场景需要按id排序， 不适合做数据库主键。<br> ID 有序也会提升数据的写入性能， 数据库索引B+树的插入。</li>
</ul>
<h2><span id="二-snowflake">二. Snowflake</span><a href="#二-snowflake" class="header-anchor">#</a></h2><p><a href="https://github.com/www6v/jDemo/blob/master/src/main/java/middleware/snowflake/SnowflakeIdWorker.java">Snowflake算法实现</a></p>
<h3><span id="实现方式">实现方式：</span><a href="#实现方式" class="header-anchor">#</a></h3><ul>
<li>嵌入在服务器中</li>
<li>独立的分布式id服务器</li>
</ul>
<h3><span id="缺点">缺点</span><a href="#缺点" class="header-anchor">#</a></h3><p>最大的<strong>缺点</strong>就是它<strong>依赖于系统的时间戳</strong>，一旦系统时间不准，就有可能生成重复的 ID。<br>所以如果我们发现系统时钟不准，就可以<strong>让发号器暂时拒绝发号，直到时钟准确为止</strong>。<br>可以使用<strong>ntp服务器</strong>同步服务的系统时间。</p>
<h3><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h3><ul>
<li>时间戳不记录毫秒而是<strong>记录秒</strong>，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均。 <strong>批量id</strong></li>
</ul>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>Leaf——美团点评分布式ID生成系统 照东</li>
<li>服务化框架－分布式Unique ID的生成方法一览 江南白衣</li>
<li><a href="https://tech.meituan.com/2019/03/07/open-source-project-leaf.html">Leaf：美团分布式ID生成服务开源</a></li>
<li><a href>发号器：如何保证分库分表后ID的全局唯一性？</a>  唐扬</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>id生成</category>
      </categories>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 可靠性总结</title>
    <url>/www6vHomeHexo/2016/07/05/kafkaReliability/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2016/07/05/kafkaReliability/Kafka-Reliability.jpg" class title="Kafka可靠性总结">


<h2><span id="kafka高可靠配置">Kafka高可靠配置</span><a href="#kafka高可靠配置" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>位置</th>
<th align="center">配置项</th>
<th align="center">可靠性</th>
</tr>
</thead>
<tbody><tr>
<td>topic的配置</td>
<td align="center">replication.factor&gt;&#x3D;3,即副本数至少是3个</td>
<td align="center">复制因子<br>replication.factor(topic级别) <br>default.replication.factor(broker级别)</td>
</tr>
<tr>
<td>-</td>
<td align="center">2&lt;&#x3D;min.insync.replicas&lt;&#x3D;replication.factor <br> 最少同步副本min.insync.replicas</td>
<td align="center">3副本（总）<br>+ 3副本，一般最少同步2副本 <br>+ 最少同步2副本时，如2副本挂了，这时不能写，只能读.<br>设置为1，单副本挂了，就会丢数据【3】</td>
</tr>
<tr>
<td>broker的配置</td>
<td align="center">leader的选举条件unclean.leader.election.enable&#x3D;false <br> unclean.leader.election -&gt; broker级别</td>
<td align="center">1.允许不同步的副本成为首领 ，有数据不可靠的风险.<br>2.不允许不同步的副本成为首领 ，降低了可用性. <br>3. 强烈建议不要开启它，还可以通过其他的方式来提升可用性</td>
</tr>
<tr>
<td>producer的配置</td>
<td align="center">request.required.acks&#x3D;-1(all)【6】 <br> producer.type&#x3D;sync【7】</td>
<td align="center"></td>
</tr>
</tbody></table>
<h2><span id="如何确保消息不会丢失">如何确保消息不会丢失</span><a href="#如何确保消息不会丢失" class="header-anchor">#</a></h2><h5><span id="生产阶段">生产阶段</span><a href="#生产阶段" class="header-anchor">#</a></h5><ul>
<li>在编写<strong>发送</strong>消息代码时，需要注意，<strong>正确处理返回值或者捕获异常</strong>，就可以保证这个阶段的消息不会丢失。<br>捕获消息发送的错误，并<strong>重发消息</strong>。</li>
<li><strong>异步发送时</strong>，则需要在回调方法里进行检查。这个地方是需要特别注意的，<strong>很多丢消息的原因就是，使用了异步发送，却没有在回调中检查发送结果</strong>。</li>
</ul>
<h5><span id="存储阶段">存储阶段</span><a href="#存储阶段" class="header-anchor">#</a></h5><ul>
<li>通过配置<strong>刷盘</strong>和<strong>复制</strong>相关的参数，让消息写入到多个副本的磁盘上，来确保消息不会因为某个 Broker 宕机或者磁盘损坏而丢失。<br>Eg. 在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘<br>Eg. <strong>表. kafka高可靠配置</strong>  topic的配置</li>
</ul>
<h5><span id="消费阶段">消费阶段</span><a href="#消费阶段" class="header-anchor">#</a></h5><ul>
<li><strong>在处理完全部消费业务逻辑之后，再发送消费确认。</strong></li>
</ul>
<h2><span id="检测消息丢失的方法">检测消息丢失的方法</span><a href="#检测消息丢失的方法" class="header-anchor">#</a></h2><p>可以利用消息队列的有序性来验证是否有消息丢失。在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。</p>
<h2><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h2><ul>
<li>怎么样才能确保Kafka极大程度上的可靠性？  </li>
<li>Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）</li>
</ul>
<p><a href="../../../../2016/07/05/kafkaReliability/">Kafka 可靠性总结</a></p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.jasongj.com/kafka/high_throughput/">Kafka设计解析（六）- Kafka高性能架构之道</a> 郭俊   </li>
<li><a href="https://blog.csdn.net/u013256816/article/details/71091774">kafka数据可靠性深度解读</a> 朱忠华</li>
<li>《Kafka权威指南》 第6 章可靠的数据传递 薛命灯</li>
<li>《消息队列高手课 - 如何确保消息不会丢失？》 李玥</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka消费者总结</title>
    <url>/www6vHomeHexo/2016/06/25/kafkaConsumer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#kafka%E6%B6%88%E8%B4%B9%E8%80%85">Kafka消费者</a><ul>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#lag">lag</a></li>
<li><a href="#%E6%B6%88%E8%B4%B9%E8%80%85">消费者</a></li>
</ul>
</li>
<li><a href="#qa">Q&amp;A</a><ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="kafka消费者">Kafka消费者</span><a href="#kafka消费者" class="header-anchor">#</a></h1><h3><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2016/06/25/kafkaConsumer/kafka-consumer.jpg" class title="Kafka消费者总结">

<h3><span id="lag">lag</span><a href="#lag" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2016/06/25/kafkaConsumer/lag.png" class title="Kafka lag">

<h3><span id="消费者">消费者</span><a href="#消费者" class="header-anchor">#</a></h3><ol>
<li>批量消费</li>
<li>消费者的ZeroCopy:<br>直接把消息从文件里发送到网络通道， 而不需要内核与用户态之间数据的来回复制。</li>
</ol>
<h1><span id="qampa">Q&amp;A</span><a href="#qampa" class="header-anchor">#</a></h1><ul>
<li><p><del>怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)</del></p>
</li>
<li><p>“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？</p>
</li>
<li><p>消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</p>
</li>
<li><p>有哪些情形会造成重复消费？<br><a href="https://cloud.tencent.com/developer/article/1665700">Kafka常见的导致重复消费原因和解决方案</a><br>原因3:（重复消费最常见的原因）：消费后的数据，当offset还没有提交时，partition就断开连接。比如，通常会遇到消费的数据，处理很耗时，导致超过了Kafka的session timeout时间（0.10.x版本默认是30秒），那么就会re-blance重平衡，此时有一定几率offset没提交，会导致重平衡后重复消费。</p>
</li>
<li><p>那些情景下会造成消息漏消费？<br><a href="../../../../2016/07/05/kafkaReliability/">Kafka 可靠性总结</a><br><a href="https://blog.csdn.net/riemann_/article/details/124534487">聊聊 Kafka：Kafka 消息丢失的场景以及最佳实践</a></p>
</li>
<li><p>KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？</p>
</li>
<li><p>简述消费者与消费组之间的关系</p>
</li>
<li><p>Kafka的旧版Scala的消费者客户端的设计有什么缺陷？</p>
</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://www.jasongj.com/2015/08/09/KafkaColumn4/">Kafka设计解析（四）- Kafka Consumer设计解析</a> 郭俊</li>
<li><a href="https://blog.csdn.net/u013256816/article/details/79955578">Kafka的Lag计算误区及正确实现</a> 朱小厮</li>
<li>《kafka权威指南》 薛命灯 第3，4 ，5章</li>
<li>Kafka Consumer机制优化-保证每条消息至少消费一次 幽灵之使</li>
</ol>
<ul>
<li><p>分区分配策略<br><a href="https://blog.csdn.net/u013256816/article/details/81123600">Kafka分区分配策略（1）——RangeAssignor</a> 朱小厮<br><a href="https://blog.csdn.net/u013256816/article/details/81123625">Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor</a> 朱小厮<br><a href="https://blog.csdn.net/u013256816/article/details/81123858">Kafka分区分配策略（3）——自定义分区分配策略</a> 朱小厮</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODY1NTcxNg==&mid=2247494909&idx=1&sn=e40cd749d060093695e39576c41a9264">图解Kafka消费者分区分配策略</a>   石臻臻 kafka contributor  *** 未</p>
</li>
</ul>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka总结</title>
    <url>/www6vHomeHexo/2016/05/11/kafka/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2016/05/11/kafka/kafka.jpg" class title="Kafka总结">


<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>Kafka剖析（一）：Kafka背景及架构介绍 郭俊</li>
<li>Kafka设计解析（六）- Kafka高性能关键技术解析 郭俊</li>
<li>《kafka权威指南》 薛命灯 第3，5章</li>
<li>Kafka文件存储机制那些事 幽灵之使</li>
<li><a href="https://www.oschina.net/translate/kafka-design?cmp&p=1">分布式发布订阅消息系统 - Kafka架构设计</a>  官方文档翻译 未</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>消息中间件总结</title>
    <url>/www6vHomeHexo/2016/04/19/mq/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2016/04/19/mq/mom1.jpg" class title="消息中间件总结">



<img src="/www6vHomeHexo/2016/04/19/mq/mom2.jpg" class title="消息中间件总结">



<img src="/www6vHomeHexo/2016/04/19/mq/mom3.jpg" class title="消息中间件总结">



<h2><span id="消息积压">消息积压</span><a href="#消息积压" class="header-anchor">#</a></h2><h3><span id="原则-消费性能要高于生产的性能">原则: 消费性能要高于生产的性能</span><a href="#原则-消费性能要高于生产的性能" class="header-anchor">#</a></h3><h3><span id="1-发送端性能优化">1. 发送端性能优化</span><a href="#1-发送端性能优化" class="header-anchor">#</a></h3><p><strong>并发</strong>和<strong>批量</strong></p>
<h3><span id="2-消费端性能优化">2. 消费端性能优化</span><a href="#2-消费端性能优化" class="header-anchor">#</a></h3><p><strong>分区partion</strong>和<strong>consumer同步扩容</strong></p>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>Jumper, JMQ 代码 文档</li>
<li>Kafka vs RocketMQ——单机系统可靠性 以夕</li>
<li>xxx</li>
<li><a href="https://www.jianshu.com/p/468176c6bc1b">分布式开放消息系统(RocketMQ)的原理与实践</a> CHEN川 ***</li>
<li><a href="https://tech.meituan.com/2016/07/01/mq-design.html">消息队列设计精要</a>  点评 王烨 失效 </li>
<li><a href="http://www.jasongj.com/kafka/high_throughput/">Kafka设计解析（六）- Kafka高性能关键技术解析</a> 郭俊</li>
<li>事务消息 -&gt; 消息队列 RocketMQ 阿里云官方文档</li>
<li>消息队列 RocketMQ、Apache RocketMQ、消息队列 Kafka、Apache Kafka、RabbitMQ 产品对比 阿里云官方文档</li>
<li><a href="https://blog.csdn.net/u013068377/article/details/72903288">RocketMQ消息堆积判断</a></li>
<li>rocketMQ消息堆积监控的java实现 c614756zhang</li>
<li><a href="https://zhuanlan.zhihu.com/p/25265380">RocketMQ原理（4）——消息ACK机制及消费进度管理</a> Jaskey Lam</li>
</ol>
]]></content>
      <categories>
        <category>消息系统</category>
        <category>特性</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>消息可靠性总结(已废弃)</title>
    <url>/www6vHomeHexo/2016/03/17/reliabilityOfMsg/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2016/03/17/reliabilityOfMsg/reliabilityOfMsg.jpg" class title="消息可靠性总结">





]]></content>
      <categories>
        <category>消息系统</category>
        <category>可靠性</category>
      </categories>
      <tags>
        <tag>消息系统</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式一致性 总结</title>
    <url>/www6vHomeHexo/2016/02/09/consistent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7">分布式一致性</a></li>
<li><a href="#%E4%B8%80%E8%87%B4%E6%80%A7">一致性</a><ul>
<li><a href="#%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B">强一致性模型</a></li>
<li><a href="#%E5%BC%B1%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B">弱一致性模型</a><ul>
<li><a href="#%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7">因果一致性</a></li>
<li><a href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7client-centric-consistency">客户端为中心的一致性（Client-centric Consistency）</a></li>
<li><a href="#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7">最终一致性</a></li>
</ul>
</li>
<li><a href="#sloppy-quorum">Sloppy quorum</a></li>
</ul>
</li>
<li><a href="#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7-%E5%B7%A5%E7%A8%8B">最终一致性-工程</a><ul>
<li><a href="#tcc">TCC</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">基于事务消息的分布式事务</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">基于本地消息的分布式事务</a></li>
<li><a href="#saga%E6%B5%81%E7%A8%8B">Saga流程</a></li>
</ul>
</li>
<li><a href="#%E5%BC%B1%E4%B8%80%E8%87%B4%E6%80%A7-%E5%B7%A5%E7%A8%8B">弱一致性-工程</a><ul>
<li><a href="#%E8%A1%A5%E5%81%BF">补偿</a></li>
</ul>
</li>
<li><a href="#state-machine-primary-copy">State Machine &amp;&amp; Primary-copy</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%B8%80%E8%87%B4%E6%80%A7-1">一致性</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E6%97%B6%E9%92%9F">向量时钟</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="分布式一致性">分布式一致性</span><a href="#分布式一致性" class="header-anchor">#</a></h1><img src="/www6vHomeHexo/2016/02/09/consistent/overview.jpg" class title="一致性级别概览（图源：https:&#x2F;&#x2F;jepsen.io&#x2F;consistency）">

<p>[粉色-Unavailable]  在某些网络故障情况下不可用。为了确保安全，一些或所有节点必须暂停操作。<br>[黄色-Sticky Available]  只要客户端只与相同的服务器通信而不切换到新的服务器，就可在每个非故障节点上使用。<br>[蓝色-Total Available]  即使网络完全瘫痪，也可在每个非故障节点上使用。</p>
<img src="/www6vHomeHexo/2016/02/09/consistent/consistent.jpg" class title="分布式一致性总结">

<h1><span id="一致性">一致性</span><a href="#一致性" class="header-anchor">#</a></h1><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65506192-989c8f00-defd-11e9-8ce4-df9b2bd8a96f.jpg" alt="consistent-relationship"></p>
</div>

<h3><span id="强一致性模型">强一致性模型</span><a href="#强一致性模型" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">强一致性</th>
<th align="center">协议</th>
<th align="center">特性</th>
<th align="center">工程</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线性一致性[chat]</td>
<td align="center">2PC<br>3PC #1</td>
<td align="center">延迟大，吞吐低。全局锁资源</td>
<td align="center">JTA(XA)<br>  <a href="/www6vHomeHexo/2022/03/24/transactionSeata/" title="Seata 总结">Seata XA,AT **非入侵**</a> self</td>
</tr>
<tr>
<td align="center">顺序一致性[chat]</td>
<td align="center">Paxos #1</td>
<td align="center">难理解，延迟大，吞吐中等，全局锁资源</td>
<td align="center">Google Chubby</td>
</tr>
<tr>
<td align="center">顺序一致性</td>
<td align="center"><a href="/www6vHomeHexo/2015/11/29/zookeeperZab/" title="Zookeeper-Zab">Zab</a> self<br>逻辑时钟</td>
<td align="center">类似多线程程序执行顺序的模型</td>
<td align="center">Zookeeper的读 <br>1.两个主流程，三个阶段 <br>2. Quorum:2f+1个节点，允许f个节点失败</td>
</tr>
<tr>
<td align="center">强一致性</td>
<td align="center"><a href="/www6vHomeHexo/2019/06/21/raft/" title="Raft协议">Raft协议</a> self</td>
<td align="center">相对Paxos简单。主从，三个阶段</td>
<td align="center"><a href="/www6vHomeHexo/2022/04/06/etcd/" title="etcd 总结">etcd 总结</a> self</td>
</tr>
</tbody></table>
<ul>
<li>逻辑时钟<br>Lamport提出<strong>逻辑时钟</strong>是为了解决分布式系统中的时序问题，即如何定义a在b之前发生.<br>Java中有happen-before  <div style="text-align: center;"></div></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/5608425/67629944-a9a03d80-f8b9-11e9-820e-7bbf4fccea20.jpg" alt="logicClock"><br>图2. 逻辑时钟 logic-clock</p>


<ul>
<li><p>线性一致性 Linearizability<br><strong>线性一致性</strong>  #1： 严格一致性（Strict Consistency）或者原子一致性（Atomic Consistency） 一个操作对于系统的其他部分是不可中断的	</p>
</li>
<li><p><strong>顺序一致性</strong>   Sequential </p>
<ul>
<li>任何一次读写操作都是按照某种特定的顺序。</li>
<li>所有进程看到的读写操作顺序都保持一致。<br><strong>顺序一致性</strong>虽然通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。而<strong>线性一致性</strong>是严格保证跟实际发生的顺序一致的。</li>
</ul>
</li>
<li><p>Paxos、ZAB 和 RAFT  有以下几个主要的<strong>共同点</strong>[Claude]: </p>
<ul>
<li>都通过<strong>选举 Leader</strong> 来接受客户端请求, Leader 接收写操作,然后同步给 Follower 节点,保持集群数据的一致性。</li>
<li>都使用**日志(log)**来记录节点状态的变更,Follower 节点通过应用相同的日志来保持数据一致。</li>
<li>都通过多个阶段来实现一致性,例如Prepare 阶段 和 Commit 阶段。</li>
<li>都需要超过半数以上的节点达成一致(<strong>quorum</strong>),才能提交日志。</li>
</ul>
</li>
</ul>
<h3><span id="弱一致性模型">弱一致性模型</span><a href="#弱一致性模型" class="header-anchor">#</a></h3><h5><span id="因果一致性">因果一致性</span><a href="#因果一致性" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th align="center">因果一致性</th>
<th align="center">协议</th>
<th align="center">特性</th>
<th align="center">工程</th>
</tr>
</thead>
<tbody><tr>
<td align="center">因果一致性<br></td>
<td align="center">向量时钟 Vector clock[向量时钟] 图1</td>
<td align="center"></td>
<td align="center">微信朋友圈的评论, Dynamo</td>
</tr>
</tbody></table>
<ul>
<li>向量时钟<div style="text-align: center;"></div></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/5608425/67629891-be300600-f8b8-11e9-931c-e0fa265f2f78.jpg" alt="vectorClock"><br>图1. 向量时钟 vector-clock</p>


<h5><span id="客户端为中心的一致性client-centric-consistency">客户端为中心的一致性（Client-centric Consistency）</span><a href="#客户端为中心的一致性client-centric-consistency" class="header-anchor">#</a></h5><ul>
<li><p>客户端为中心的一致性</p>
<ul>
<li>最终一致性</li>
<li>以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证.</li>
</ul>
</li>
<li><p>类型</p>
<ul>
<li>单调读一致性（Monotonic-read Consistency）<br><strong><a href="/www6vHomeHexo/2016/05/11/kafka/" title="Kafka总结">kafka的消费者的单调读</a></strong></li>
<li>单调写一致性（Monotonic-write Consistency）</li>
<li>读写一致性（Read-your-writes Consistency）</li>
<li>写读一致性（Writes-follow-reads Consistency）</li>
</ul>
</li>
</ul>
<h5><span id="最终一致性">最终一致性</span><a href="#最终一致性" class="header-anchor">#</a></h5><table>
<thead>
<tr>
<th>最终一致性</th>
<th>协议</th>
<th>特性</th>
<th>工程</th>
</tr>
</thead>
<tbody><tr>
<td>反熵Anti-Entropy<br></td>
<td><a href="https://rrmoelker.github.io/gossip-visualization/">Gossip</a></td>
<td></td>
<td>Cassandra， redis的集群状态的同步机制</td>
</tr>
</tbody></table>
<h3><span id="sloppy-quorum">Sloppy quorum</span><a href="#sloppy-quorum" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th align="center">Sloppy quorum</th>
<th align="center">特性</th>
<th align="center">工程</th>
</tr>
</thead>
<tbody><tr>
<td align="center">R+W&gt;N[ReadQurum-WriteQurum]</td>
<td align="center">可定制</td>
<td align="center"><a href="../../../../2018/07/19/NoSQL/">Dynamo, Cassandra</a>  定制灵活</td>
</tr>
</tbody></table>
<h1><span id="最终一致性-工程">最终一致性-工程</span><a href="#最终一致性-工程" class="header-anchor">#</a></h1><h3><span id="tcc">TCC</span><a href="#tcc" class="header-anchor">#</a></h3><ul>
<li>TCC<ul>
<li>流程<br>  1.主流程控制整个事务<br>  2.分流程提供Confirm和Cancel方法。</li>
<li>阶段<br>  Try:  阶段1的业务执行<br>  Confirm: 阶段2的业务执行<br>  Cancel: 回滚Try阶段执行的业务流程和数据</li>
<li>TCC  FMT <a href="/www6vHomeHexo/2022/03/24/transactionSeata/" title="Seata 总结">Seata TCC</a>    self</li>
</ul>
</li>
</ul>
<h3><span id="基于事务消息的分布式事务">基于事务消息的分布式事务</span><a href="#基于事务消息的分布式事务" class="header-anchor">#</a></h3><ul>
<li><p>EBay模式  [8]</p>
</li>
<li><p><strong>正向流程</strong><br></p>
<ul>
<li>[本地事务+幂等业务接口+half消息]</li>
<li>消息状态<br> <ol>
<li>初始化：消息为待处理状态<br> </li>
<li>业务成功：消息为待发送状态<br></li>
<li>业务失败：消息删除</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>反向流程</strong>（异常流程，补偿流程）</p>
<ul>
<li>中间件询问业务执行结果，更新消息状态</li>
</ul>
</li>
<li><p>工程</p>
<a href="/www6vHomeHexo/2020/08/12/mqRocketmqTransaction/" title="Rocketmq中的事务">RocketMQ事务消息</a> self</li>
</ul>
<hr>
<p><img src="https://user-images.githubusercontent.com/5608425/66023796-d2d0e680-e524-11e9-8748-1a26f3d0f157.JPG" alt="mq-normal"><br><img src="https://user-images.githubusercontent.com/5608425/66023797-d2d0e680-e524-11e9-85e6-f845863fe4a8.JPG" alt="mq-reverse"></p>
<style>
table th:first-of-type {
  width: 100px;
}
</style>

<img src="/www6vHomeHexo/2016/02/09/consistent/mqTransaction.jpg" class title="基于事务消息的分布式事务">

<h3><span id="基于本地消息的分布式事务">基于本地消息的分布式事务</span><a href="#基于本地消息的分布式事务" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2016/02/09/consistent/localBaseTransaction.jpg" class title="基于本地消息的分布式事务">

<h3><span id="saga流程">Saga流程</span><a href="#saga流程" class="header-anchor">#</a></h3><ul>
<li>Saga 1PC (一阶段) <ul>
<li>基于补偿的消息驱动的用于解决long-running process业务。 </li>
<li>工程 <a href="/www6vHomeHexo/2022/03/24/transactionSeata/" title="Seata 总结">Seata Saga</a> self</li>
</ul>
</li>
</ul>
<img src="/www6vHomeHexo/2016/02/09/consistent/saga.jpg" class title="Saga流程">

<h1><span id="弱一致性-工程">弱一致性-工程</span><a href="#弱一致性-工程" class="header-anchor">#</a></h1><h3><span id="补偿">补偿</span><a href="#补偿" class="header-anchor">#</a></h3><ul>
<li>流程<br>状态查询（成功or失败）+补偿</li>
<li>流程细节<br>定时校验异常 + 补偿</li>
</ul>
<h1><span id="state-machine-ampamp-primary-copy">State Machine &amp;&amp; Primary-copy</span><a href="#state-machine-ampamp-primary-copy" class="header-anchor">#</a></h1><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/67629999-09e3af00-f8bb-11e9-88fb-10142745bfdd.png" alt="state-machine-primary-back"><br>state machine replication &amp;&amp; primary-copy</p>
</div>

<ul>
<li><p><strong>复制状态机(state machine replication)</strong><br>多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态<br>状态机 + 命令 -&gt; 重放</p>
</li>
<li><p><strong>state machine replication</strong>例子<br>mysql主从复制 slave relay log, <strong>基于sql语句的复制</strong>[9];<br>redis AOF </p>
</li>
<li><p><strong>primary-copy</strong>例子:<br>zookeeper的主从复制;<br>mysql主从复制 slave relay log, <strong>基于行的复制</strong>[9];<br>redis RDB 快照;</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="一致性">一致性</span><a href="#一致性" class="header-anchor">#</a></h3><ol>
<li><a href="https://weibo.com/ttarticle/p/show?id=2309403965965003062676">保证分布式系统数据一致性的6种方案</a>  高可用架构  ***</li>
<li><a href="https://www.csdn.net/article/2015-01-30/2823782">深入解析NoSQL数据库的分布式算法</a>   ***</li>
<li>ZooKeeper真不是最终一致性的，而是顺序一致性 陈东明</li>
<li>为什么程序员需要关心顺序一致性（Sequential Consistency）而不是Cache一致性（Cache Coherence） carlosstephen</li>
<li><a href="https://yq.aliyun.com/articles/693187">分布式系统：一致性模型</a>  阿里 Overview ***</li>
<li>ENode 1.0 - Saga的思想与实现 汤雪华</li>
<li>《大数据日知录：架构与算法》 张俊林</li>
<li><a href="https://queue.acm.org/detail.cfm?id=1394128">Base: An Acid Alternative</a>  Ebay模式  ***</li>
<li><a href="https://mp.weixin.qq.com/s/2AL3uJ5BG2X3Y2Vxg0XqnQ">如何选择分布式事务解决方案？</a>   ali  ***</li>
<li><a href="/www6vHomeHexo/2018/07/19/NoSQL/" title="NoSQL总结">NoSQL总结</a>  self  一致性</li>
<li><a href="https://zhuanlan.zhihu.com/p/461294722">（建议收藏）万字长文总结分布式事务，总有一款适合你</a> *** 腾讯</li>
<li><a href="https://blog.csdn.net/weixin_43902592/article/details/103918630">《数据密集型应用系统设计》笔记五：第五章 数据复制</a> 未</li>
</ol>
<h3><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h3><ol>
<li><a href="https://github.com/StabilityMan/StabilityGuide/blob/master/docs/processing/lostprevention/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E6%A3%80%E6%B5%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md">数据一致性检测应用场景与最佳实践</a> 阿里 未</li>
</ol>
<h3><span id="向量时钟">向量时钟</span><a href="#向量时钟" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.cnblogs.com/yanghuahui/p/3767365.html">向量时钟Vector Clock in Riak</a></li>
<li><a href="https://riak.com/posts/technical/why-vector-clocks-are-hard/">Why Vector Clocks Are Hard</a> 未</li>
<li><a href="http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F07/Dynamo.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a> paper 未<br>向量时钟的变种   版本向量（Version vector）  版本控制机制</li>
<li><a href="https://yq.aliyun.com/articles/690584">分布式系统：向量时钟</a> 阿里 肖汉松  ***</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>超时和重试 总结</title>
    <url>/www6vHomeHexo/2016/01/17/soaTimeout/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p><strong>关键词</strong>： 超时, 降级, 重试 </p>
<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%B6%85%E6%97%B6-%E5%92%8C-%E9%87%8D%E8%AF%95">超时 和 重试</a></li>
<li><a href="#%E8%B6%85%E6%97%B6%E7%B1%BB%E5%9E%8B">超时类型</a></li>
<li><a href="#%E8%B6%85%E6%97%B6%E5%90%8E%E7%AD%96%E7%95%A5">超时后策略</a></li>
<li><a href="#%E9%99%8D%E7%BA%A7">降级</a></li>
<li><a href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">最佳实践</a></li>
<li><a href="#%E9%87%8D%E8%AF%95">重试</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="超时-和-重试">超时 和 重试</span><a href="#超时-和-重试" class="header-anchor">#</a></h1><ul>
<li>超时和延迟的原因:</li>
</ul>
<ol>
<li>服务端获得请求了，但超时了;</li>
<li>服务端没获得请求，请求失败了， 超时了;</li>
</ol>
<ul>
<li>重试方式:<br>指数级退避 Exponential Blackoff[3][4]</li>
</ul>
<h1><span id="超时类型">超时类型</span><a href="#超时类型" class="header-anchor">#</a></h1><ul>
<li>类型<ul>
<li>客户端调用超时</li>
<li>服务器端调用超时</li>
<li>提供端&#x2F;消费端与注册中心之间超时</li>
</ul>
</li>
</ul>
<h1><span id="超时后策略">超时后策略</span><a href="#超时后策略" class="header-anchor">#</a></h1><ul>
<li>超时后策略<ul>
<li>超时+快速失败<br>  超时后不重试</li>
<li>超时+降级failback<br>  返回托底（返回历史数据&#x2F;静态数据&#x2F;缓存数据）数据，等待页或者错误页</li>
<li>超时+熔断<br>  超时后重试，重试不行后熔断服务</li>
</ul>
</li>
</ul>
<h1><span id="降级">降级</span><a href="#降级" class="header-anchor">#</a></h1><ul>
<li>降级<ul>
<li>非核心服务在超时后可以自动降级</li>
<li>超时时间和超时重试次数</li>
</ul>
</li>
</ul>
<h1><span id="最佳实践">最佳实践</span><a href="#最佳实践" class="header-anchor">#</a></h1><ul>
<li>最佳实践<ul>
<li>不设置超时<br>  慢请求累积导致连锁反应，甚至造成应用雪崩</li>
<li>推荐值<ul>
<li>稍大于压测或者线上监控看到的TP99的响应时间</li>
<li>超时时间太长-不恰当设置<br>  导致本应成功的调用却失败了。超时的时候服务资源没有释放</li>
<li>超时时间太短- 不恰当设置<br>  服务调用成功率降低</li>
</ul>
<p>		</p>
</li>
<li>最重要：网络连接&#x2F;读&#x2F;写的超时</li>
<li>用户能忍受的最长超时时间 - 用户体验<br>  最坏情况下的响应时间&#x3D;重试次数*单次超时时间</li>
<li>依赖<ul>
<li>service重启时大量超时的问题<br>  服务预热功能。延迟发布</li>
<li>客户端的超时时间&lt;服务端的超时<br>  可以在服务端实施限流&#x2F;降级</li>
<li>多级依赖关系<br>  如A调用B，B调用C<br>  超时设置应该是A&gt;B&gt;C,否则可能会一直重试，引起DDos攻击效果</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="重试">重试</span><a href="#重试" class="header-anchor">#</a></h1><ul>
<li>重试<ul>
<li>客户端<ul>
<li>心跳超时<ul>
<li>关闭链路，然后由客户端发起重新连接的操作，保证链路能恢复到正常的状态</li>
<li>有负载均衡的中间件，要考虑配置心跳&#x2F;存活检查</li>
</ul>
</li>
<li>客户端调用超时<ul>
<li>重试策略,保证服务调用成功 - failover<ul>
<li>摘掉不存活节点</li>
<li>尝试其他分组服务</li>
<li>尝试其他机房服务</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>	</p>
</li>
<li>服务端 	<ul>
<li>读服务天然适合重试</li>
<li>写服务大多不能重试<ul>
<li>幂等</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Zookeeper客户端会话超时 - 服务注册反注册<ul>
<li>Zookeeper服务端，将该会话对应的Node删除，删除事件通知到所有监听该Node的消费者</li>
</ul>
<p>		</p>
</li>
<li>重试次数太多<br>导致多倍请求流量，模拟了DDos攻击</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《亿级流量网站架构核心技术》 张开涛</li>
<li>Hedwig文档</li>
<li><a href="https://www.jianshu.com/p/0a6ee8c13522">网络重试中的 指数退避抖动 算法 Exponential Backoff And Jitter</a></li>
<li><a href="https://docs.aws.amazon.com/zh_cn/general/latest/gr/api-retries.html">AWS 中的错误重试和指数退避</a></li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>超时</category>
      </categories>
      <tags>
        <tag>超时</tag>
      </tags>
  </entry>
  <entry>
    <title>异步化 总结</title>
    <url>/www6vHomeHexo/2015/12/05/async/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F-callback-0">异步编程范式-Callback [0]</a><ul>
<li><a href="#continuation-%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0">Continuation &#x3D;&#x3D; 回调函数</a></li>
<li><a href="#callback%E5%AE%9E%E7%8E%B0">Callback实现</a></li>
<li><a href="#callback%E8%AF%AD%E6%B3%95%E7%B3%96promise">Callback语法糖：Promise</a></li>
<li><a href="#%E5%8F%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8Bpromise-%E7%9A%84%E6%9E%81%E5%A4%A7%E5%A2%9E%E5%BC%BA">反应式编程(Promise 的极大增强)</a></li>
</ul>
</li>
<li><a href="#%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F-coroutine-0">异步编程范式- Coroutine [0]</a><ul>
<li><a href="#cps-%E5%8F%98%E6%8D%A2coroutine-%E4%B8%8E-asyncawait">CPS 变换：Coroutine 与 async&#x2F;await</a></li>
<li><a href="#%E7%94%A8%E6%88%B7%E6%80%81%E7%BA%BF%E7%A8%8B">用户态线程</a></li>
</ul>
</li>
<li><a href="#%E5%B9%B6%E8%A1%8C-%E8%8C%83%E5%BC%8F%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B">并行 范式&#x2F;编程模型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<img src="/www6vHomeHexo/2015/12/05/async/async.jpg" class title="服务异步化总结">

<h1><span id="异步编程范式-callback-0">异步编程范式-Callback [0]</span><a href="#异步编程范式-callback-0" class="header-anchor">#</a></h1><h3><span id="continuation-x3dx3d-回调函数">Continuation &#x3D;&#x3D; 回调函数</span><a href="#continuation-x3dx3d-回调函数" class="header-anchor">#</a></h3><ul>
<li>编程模式    Continuation-passing style（CPS）<ol>
<li>把调用者 f() 还未执行的部分包成一个函数对象 cont，一同传给被调用者 g()；</li>
<li>正常运行 g() 函数体；</li>
<li>g() 完成后，连同它的结果一起回调 cont，从而继续执行 f() 里剩余的代码。<img src="/www6vHomeHexo/2015/12/05/async/CSP.jpg" class></li>
</ol>
</li>
</ul>
<h3><span id="callback实现">Callback实现</span><a href="#callback实现" class="header-anchor">#</a></h3><p>  而异步 IO 中，进程发起 IO 操作时也会一并输入回调（也就是 Continuation），这大大解放了生产力 —— <strong>现场无需等待，可以立即返回去做其他事情</strong>。一旦 IO 成功后，AIO 的 Event Loop 会调用刚刚设置的回调函数，把剩下的工作完成。这种模式有时也被称为 Fire and Forget。<br>  通过实现的 Continuation，<strong>线程不再受 IO 阻塞，可以自由自在地跑满 CPU</strong>。</p>
<h3><span id="callback语法糖promise">Callback语法糖：Promise</span><a href="#callback语法糖promise" class="header-anchor">#</a></h3><p>  <strong>Promise 是对异步调用结果的一个封装</strong>，在 Java 中它叫作 <strong>CompletableFuture</strong>  或者 <strong>ListenableFuture</strong> (Guava)。<br>  Promise 改善了 Callback 的可读性，也让异常处理稍稍优雅了些。</p>
<h3><span id="反应式编程promise-的极大增强">反应式编程(Promise 的极大增强)</span><a href="#反应式编程promise-的极大增强" class="header-anchor">#</a></h3><p>  相比 Promise，反应式引入了流（Flow）的概念。 </p>
<h1><span id="异步编程范式-coroutine-0">异步编程范式- Coroutine [0]</span><a href="#异步编程范式-coroutine-0" class="header-anchor">#</a></h1><h3><span id="cps-变换coroutine-与-asyncx2fawait">CPS 变换：Coroutine 与 async&#x2F;await</span><a href="#cps-变换coroutine-与-asyncx2fawait" class="header-anchor">#</a></h3><p>在异步函数调用时加上 await，编译器就会自动把它转化为协程（Coroutine），而非昂贵的线程。</p>
<h3><span id="用户态线程">用户态线程</span><a href="#用户态线程" class="header-anchor">#</a></h3><p>goroutine </p>
<h1><span id="并行-范式x2f编程模型">并行 范式&#x2F;编程模型</span><a href="#并行-范式x2f编程模型" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th align="center">并行 范式&#x2F;编程模型</th>
<th align="center">系统&#x2F;语言</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线程和锁</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">函数式(Future，Promise）</td>
<td align="center">Java8 Streaming API, lambda表达式<br> Spark&#x2F;Flink 算子<br> Clojure reducer</td>
</tr>
<tr>
<td align="center">分离标识和状态</td>
<td align="center">Clojure</td>
</tr>
<tr>
<td align="center">Actor</td>
<td align="center">Scala Actor, Akka</td>
</tr>
<tr>
<td align="center">CSP</td>
<td align="center">Golang协程, Kotlin协程</td>
</tr>
<tr>
<td align="center">Reactive</td>
<td align="center">RxJava<br> Flux (Reactor Core)<br> RSocket</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><a href="http://ericfu.me/several-ways-to-aync/">异步编程的几种方式 </a> *** </li>
<li><a href="http://ifeve.com/google-guava-listenablefuture/">google Guava包的ListenableFuture解析</a> 罗立树 </li>
<li>谈谈服务化体系中的异步（上） 花钱的年华</li>
<li><a href="https://www.infoq.cn/article/member-task-platform-practice">苏宁 11.11：如何基于异步化打造会员任务平台？-基于异步化的性能优化实践</a>  葛苏杰</li>
<li><a href="https://blog.csdn.net/cenwenchu79/article/details/5703430">Web服务请求异步化介绍（概念篇）</a> 放翁（文初）  ***</li>
<li>Java中的纤程库 - Quasar 鸟窝</li>
<li><a href="https://www.researchgate.net/publication/2391753_SEDA_An_Architecture_for_Well-Conditioned_Scalable_Internet_Services">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li>《七周七并发》</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
        <category>异步化</category>
      </categories>
      <tags>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper-Zab</title>
    <url>/www6vHomeHexo/2015/11/29/zookeeperZab/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#zab%E5%8D%8F%E8%AE%AE">Zab协议</a><ul>
<li><a href="#overview-1">Overview [1]</a></li>
</ul>
</li>
<li><a href="#zab%E7%9A%84%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5-3">ZAB的四个阶段 [3]</a><ul>
<li><a href="#%E5%90%8E%E4%B8%89%E4%B8%AA%E9%98%B6%E6%AE%B5-5">后三个阶段 [5]</a></li>
<li><a href="#%E9%98%B6%E6%AE%B5%E4%B8%80-%E9%80%89%E4%B8%BE%E9%98%B6%E6%AE%B5leader-election-4">阶段一 选举阶段（Leader Election） [4]</a></li>
<li><a href="#%E9%98%B6%E6%AE%B5%E4%BA%8C-%E5%8F%91%E7%8E%B0%E9%98%B6%E6%AE%B5">阶段二 发现阶段</a></li>
<li><a href="#%E9%98%B6%E6%AE%B5%E4%B8%89-%E5%90%8C%E6%AD%A5%E9%98%B6%E6%AE%B5">阶段三 同步阶段</a></li>
<li><a href="#%E9%98%B6%E6%AE%B5%E5%9B%9B-%E5%B9%BF%E6%92%AD%E9%98%B6%E6%AE%B5">阶段四 广播阶段</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="zab协议">Zab协议</span><a href="#zab协议" class="header-anchor">#</a></h1><h3><span id="overview-1">Overview  [1]</span><a href="#overview-1" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2015/11/29/zookeeperZab/zab.jpg" class title="Zookeeper的分布式一致性协议Zab">

<h1><span id="zab的四个阶段-3">ZAB的四个阶段 [3]</span><a href="#zab的四个阶段-3" class="header-anchor">#</a></h1><h3><span id="后三个阶段-5">后三个阶段 [5]</span><a href="#后三个阶段-5" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2015/11/29/zookeeperZab/zab-overview.jpg" class title="Zookeeper的分布式一致性协议Zab">

<h3><span id="阶段一-选举阶段leader-election-4">阶段一   选举阶段（Leader Election） [4]</span><a href="#阶段一-选举阶段leader-election-4" class="header-anchor">#</a></h3><ul>
<li><p>成为Leader的条件：</p>
<ul>
<li>选epoch最大的</li>
<li>epoch相等，选zxid最大的</li>
<li>epoch和zxid都相等，选server_id最大的（zoo.cfg 中配置的 myid）</li>
</ul>
</li>
<li><p>服务器状态</p>
<ul>
<li>LOOKING 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举</li>
<li>FOLLOWING 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁</li>
<li>LEADING 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳</li>
<li>OBSERVING 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票</li>
</ul>
</li>
<li><p>快速选举（Fast Leader Election）<br>节点在选举开始时，都默认投票给自己，当接收其他节点的选票时，会根据上面的 Leader条件 判断并且更改自己的选票，然后重新发送选票给其他节点。当有一个节点的得票超过半数，该节点会设置自己的状态为 Leading ，其他节点会设置自己的状态为 Following。</p>
</li>
</ul>
<img src="/www6vHomeHexo/2015/11/29/zookeeperZab/fast-election.jpg" class>

<h3><span id="阶段二-发现阶段">阶段二   发现阶段</span><a href="#阶段二-发现阶段" class="header-anchor">#</a></h3><p>在这个阶段中，Followers和上一轮选举出的准Leader进行通信，同步Followers最近接受的事务Proposal。<strong>这个阶段主要目的是发现当前大多数节点接受的最新提议，并且准Leader生成新的epoch，让Followers接受，更新它们的acceptedEpoch。</strong></p>
<p>一个Follower只会连接一个Leader，如果有一个节点F认为另一个Follower P是Leader，F在尝试连接P时会被拒绝，F被拒绝后，就会进入选举阶段。</p>
<h3><span id="阶段三-同步阶段">阶段三   同步阶段</span><a href="#阶段三-同步阶段" class="header-anchor">#</a></h3><p>同步阶段主要是利用 Leader 前一阶段获得的最新 Proposal 历史，<strong>同步集群中所有的副本</strong>。</p>
<p>只有当 quorum（超过半数的节点） 都同步完成，准 Leader 才会成为真正的 Leader。Follower 只会接收 zxid 比自己 lastZxid 大的 Proposal。</p>
<h3><span id="阶段四-广播阶段">阶段四   广播阶段</span><a href="#阶段四-广播阶段" class="header-anchor">#</a></h3><p>到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时，如果有新的节点加入，还需要对新节点进行同步。<br>需要注意的是，Zab 提交事务并不像 2PC 一样需要全部 Follower 都 Ack，只需要得到 quorum（超过半数的节点）的Ack 就可以。</p>
<img src="/www6vHomeHexo/2015/11/29/zookeeperZab/broadcast.jpg" class>

<h3><span id></span><a href="#" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《PAXOS到ZOOKEEPER分布式一致性原理与实践》 第4章 倪超</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/">分布式服务框架 Zookeeper — 管理分布式环境中的数据</a> 失效</li>
<li><a href="https://www.cnblogs.com/Jacian/p/14212401.html">Zookeeper一致性协议——ZAB </a>   *** </li>
<li><a href="http://www.jasongj.com/zookeeper/fastleaderelection/">深入浅出Zookeeper（一） Zookeeper架构及FastLeaderElection机制 </a> *** </li>
<li><a href="https://cloud.tencent.com/developer/article/1729207">分布式一致性协议 - ZAB</a></li>
</ol>
<p><a href="https://www.semanticscholar.org/paper/Zab%3A-High-performance-broadcast-for-primary-backup-Junqueira-Reed/b02c6b00bd5dbdbd951fddb00b906c82fa80f0b3">Zab: High-performance broadcast for primary-backup systems</a>  paper 未</p>
]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Netty EpollEventLoop</title>
    <url>/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>京东JSF中的服务提供者的server可以使用两种EventLoop， 默认为false，所以使用NioEventLoop，如下图所示。</p>
<p>在Netty中, NioEventLoop使用JDK Nio中基于Selector的IO多路复用的方法。在早期的JDK1.4和1.5 update10版本之前，Selector基于select&#x2F;poll模型实现，是基于IO复用技术的非阻塞IO，不是异步IO。在JDK1.5 update10和linux core2.6以上版本，sun优化了Selctor的实现，底层使用epoll替换了select&#x2F;poll。 但是在上层的JDK Nio的api中还是沿袭了select（）函数的叫法，而JDK Nio在linux中底层其实用的也是epoll模型。</p>
<p>Netty4.0.17 提供了默认采用ET工作模式的EpollEventLoop。NioEventLoop比EpollEventLoop相对更通用，EpollEventLoop只能在linux上运行，属于Linux native transport，是不能跨操作系统的。EpollEventLoop使用了JNI，调用了linux的epoll API。EpollEventLoop的API也就沿袭了Linux epoll IO多路复用中API的风格和命名方式。相对于基于NIO的transport，JNI transport在特殊平台上增加了特别的特性，会产生更少的内存垃圾，并且也会提高性能。</p>
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/twoType.JPG" class>

<p><strong>epoll使用一组函数来完成任务，而不是像select&#x2F;poll使用单个函数。</strong></p>
<h3><span id="1-epoll通过epoll_create创建一个用于epoll轮询的描述符对应于图1中的-epollfd">1. epoll通过epoll_create创建一个用于epoll轮询的描述符，对应于图1中的 epollFd。</span><a href="#1-epoll通过epoll_create创建一个用于epoll轮询的描述符对应于图1中的-epollfd" class="header-anchor">#</a></h3><p>epoll把用户关心的文件描述符上的事件放在内核里的一个事件表中，从而无需像select和poll那样每次调用都要重复传入文件的事件放在内核里的一个事件表中。但epoll需要使用一个额外的文件描述符，来唯一标识内核中的这个事件表；这个文件描述符就是图1中的eventFd。</p>
<p>Native.epollCtlAdd把epollFd和 eventFd做了关联。epoll与select&#x2F;poll不同，<strong>epoll不用每次调用都向内核拷贝事件描述信息</strong>，在第一次调用后，事件信息就会与对应的epoll描述符关联起来。</p>
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_function1.JPG" class title="图1 EpollEventLoop初始化epoll">
<h3><span id="2-epoll通过epoll_ctl添加x2f修改x2f删除事件类似于observer模式的事件注册">2. epoll通过epoll_ctl添加&#x2F;修改&#x2F;删除事件，类似于Observer模式的事件注册。</span><a href="#2-epoll通过epoll_ctl添加x2f修改x2f删除事件类似于observer模式的事件注册" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_functionAdd.JPG" class title="图2 epollFd中添加事件">
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_functionModify.JPG" class title="图3 epollFd中修改事件">
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_functionDelete.JPG" class title="图4 epollFd中删除事件">
<h3><span id="3-epoll使用事件的就绪通知方式通过在等待的描述符上注册回调函数epoll_ctl注册fd当事件发生事件fd就绪时回调函数负责把发生的事件存储在就绪事件链表中最后写到用户空间-epoll_wait便可以收到通知-下图是处理就绪事件的流程">3. epoll使用“事件”的就绪通知方式，通过在等待的描述符上注册回调函数（epoll_ctl注册fd），当事件发生（事件fd就绪）时，回调函数负责把发生的事件存储在就绪事件链表中，最后写到用户空间， epoll_wait便可以收到通知。 下图是处理就绪事件的流程。</span><a href="#3-epoll使用事件的就绪通知方式通过在等待的描述符上注册回调函数epoll_ctl注册fd当事件发生事件fd就绪时回调函数负责把发生的事件存储在就绪事件链表中最后写到用户空间-epoll_wait便可以收到通知-下图是处理就绪事件的流程" class="header-anchor">#</a></h3><img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_functionWait.JPG" class title="图5 循环阻塞调用epollWait方法，等待就绪的事件">
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_function_wait.JPG" class title="图6 epollwait收到通知后返回epollFd对应的已经就绪的事件id，也就是ready变量">
<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_function_handle_ready_event.JPG" class title="图7 根据事件id，调用就绪处理事件方法">

<img src="/www6vHomeHexo/2015/10/03/nettyEpollEventLoop/epoll_function_handle_ready_event_1.JPG" class title="图8 消费已经就绪的事件">                  
<h3><span id="总结epoll基于cqrs的理念分离了commandepollctl和queuyepollwait-epoll在性能上也比selectx2fpoll高出很多-epoll不需要一直轮询节省了cpu时间">总结：epoll基于CQRS的理念，分离了command（epollCtl）和queuy（epollWait）。epoll在性能上也比select&#x2F;poll高出很多。epoll不需要一直轮询，节省了cpu时间。</span><a href="#总结epoll基于cqrs的理念分离了commandepollctl和queuyepollwait-epoll在性能上也比selectx2fpoll高出很多-epoll不需要一直轮询节省了cpu时间" class="header-anchor">#</a></h3><p>在复制问题上，epoll使用mmap减少复制开销。</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://github.com/netty/netty/wiki/Native-transports">Netty  Native-transports</a> </li>
<li>netty-all-4.0.33.Final 源代码 EpollEventLoop</li>
<li><a href="http://blog.csdn.net/turkeyzhou/article/details/8504554">select,poll,epoll的归纳总结区分</a></li>
<li><a href="http://www.cnblogs.com/wiessharling/p/4106295.html">select poll epoll三者之间的比较</a></li>
<li><a href="http://www.cnblogs.com/Anker/p/3265058.html">select、poll、epoll之间的区别总结[整理]</a></li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title>Netty中NioEventLoop的accept过程</title>
    <url>/www6vHomeHexo/2015/09/06/nettyEventLoop-Accept/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/09/06/nettyEventLoop-Accept/nettyEventLoop-Accept.jpg" class title="Netty中NioEventLoop的accept过程">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>Netty源码</li>
<li>Netty源码分析之accept过程 占小狼</li>
<li>netty源码分析之揭开reactor线程的面纱（一） the_flash</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title>Netty总结</title>
    <url>/www6vHomeHexo/2015/08/23/nettySummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/08/23/nettySummary/nettySummary.jpg" class title="Netty总结">


<h2><span id="netty的粘包和拆包">netty的粘包和拆包</span><a href="#netty的粘包和拆包" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th align="center">类型</th>
<th align="center">实现</th>
</tr>
</thead>
<tbody><tr>
<td align="center">固定长度解码器</td>
<td align="center">使用FixedLengthFrameDecoder</td>
</tr>
<tr>
<td align="center">自定义字符进行分隔</td>
<td align="center">使用DelimiterBasedFrameDecoder</td>
</tr>
<tr>
<td align="center">自定义长度</td>
<td align="center">进行长度字段解码，就是消息体传了消息的长度 <br> LengthFieldBasedFrameDecoder和LengthFieldPrepender结合</td>
</tr>
</tbody></table>
<h2><span id="参考">参考 :</span><a href="#参考" class="header-anchor">#</a></h2><h3><span id="线程模型">线程模型</span><a href="#线程模型" class="header-anchor">#</a></h3><ol>
<li>Netty源代码</li>
<li>Netty源码分析之NioEventLoop 占小狼</li>
<li>Netty案例集锦之多线程篇(续) 李林锋</li>
<li>Netty案例集锦之多线程篇 李林锋</li>
<li>Netty系列之Netty线程模型 李林锋</li>
<li>Netty精粹之基于EventLoop机制的高效线程模型 Float_Luuu</li>
<li>Netty系列之Netty可靠性分析 李林锋</li>
<li>实例浅析epoll的水平触发和边缘触发，以及边缘触发为什么要使用非阻塞IO yuuyuu</li>
<li>Netty高性能编程备忘录(上) 江南白衣</li>
</ol>
<h3><span id="bytebuf-amp-zerocopy">ByteBuf &amp; zerocopy</span><a href="#bytebuf-amp-zerocopy" class="header-anchor">#</a></h3><ol>
<li>Netty源代码</li>
<li>netty系列之netty高性能之道 李林锋</li>
<li>《Netty权威指南》 第15章 李林锋</li>
<li><a href="https://www.cnblogs.com/xys1228/p/6088805.html">对于 Netty ByteBuf 的零拷贝(Zero Copy) 的理解</a>  xys1228</li>
<li>jvm堆外直接内存实现高性能接入层 天空的蜗牛</li>
</ol>
<h3><span id="plain-nio">Plain NIO</span><a href="#plain-nio" class="header-anchor">#</a></h3><ol>
<li>《Netty in Action》 第4章 <a href="https://github.com/www6v/netty-in-action-cn/blob/ChineseVersion/chapter4/src/main/java/nia/chapter4/PlainNioServer.java">PlainNioServer</a></li>
</ol>
<h3><span id="粘包和拆包">粘包和拆包</span><a href="#粘包和拆包" class="header-anchor">#</a></h3><p><a href="https://baijiahao.baidu.com/s?id=1716387532053553497&wfr=spider&for=pc">通过大量实战案例分解Netty中是如何解决拆包黏包问题的？</a><br><a href="https://blog.csdn.net/wwwzhouzy/article/details/119154039">netty解决拆包粘包的三种方案</a></p>
<h2><span id="面试">面试</span><a href="#面试" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/crazymakercircle/p/16181994.html">Netty内存池（史上最全 + 5W字长文）</a>  尼恩 *** 未<br><a href="https://www.cnblogs.com/crazymakercircle/p/13903625.html">Netty 面试题 （史上最全、持续更新）</a>  尼恩 未</p>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>网络</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title>京东服务框架JSF服务提供者线程模型</title>
    <url>/www6vHomeHexo/2015/07/09/jsfThreadModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="京东服务框架jsf">京东服务框架JSF</span><a href="#京东服务框架jsf" class="header-anchor">#</a></h3><p>JSF是京东基础架构组的服务化中间件产品，全名是Jingdong Service Framework（中文名：杰夫）。<br>JSF整体是依据netty来构建的，本文从代码层面简单介绍一下JSF服务端的线程模型。</p>
<h3><span id="1jsf的服务端线程模型整体上是-boss线程池-worker线程池-业务线程池-boss线程池和worker线程池称为reactor线程池">1.JSF的服务端线程模型整体上是 boss线程池 + worker线程池 + 业务线程池。boss线程池和worker线程池称为Reactor线程池。</span><a href="#1jsf的服务端线程模型整体上是-boss线程池-worker线程池-业务线程池-boss线程池和worker线程池称为reactor线程池" class="header-anchor">#</a></h3><p>三类线程池各自的参数详见下图1。</p>
<p>worker线程池和业务线程池之间的关系详见下图2，在图中可以看到业务线程和worker线程是解耦的，请求放入业务线程后，IO线程即worker线程就返回了，业务线程和I&#x2F;O线程隔离。 在没有解耦IO线程和业务ChannelHandler的情况时，如果在业务ChannelHandler中进行数据库等同步I&#x2F;O操作，很有可能会导致IO线程中的pipeline链路被阻塞。</p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/jsf%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B_%E5%8F%82%E6%95%B0.bmp" alt="图1 三类线程池各自的参数" title="图1 三类线程池各自的参数"></p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/netty%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B.png" alt="图2 worker线程池和业务线程池关系" title="图2 worker线程池和业务线程池关系"></p>
<h3><span id="2-图3是boss线程池-线程数为max4cpux2f2用户不可以配置">2. 图3是boss线程池， 线程数为Max(4,cpu&#x2F;2)，用户不可以配置</span><a href="#2-图3是boss线程池-线程数为max4cpux2f2用户不可以配置" class="header-anchor">#</a></h3><p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/boss%E7%BA%BF%E7%A8%8B%E6%B1%A0.JPG" alt="图3 boss线程池" title="图3 boss线程池"></p>
<h3><span id="3-图4是worker线程池-线程数为max8cpu1用户可以配置">3. 图4是worker线程池， 线程数为Max(8,cpu+1)，用户可以配置</span><a href="#3-图4是worker线程池-线程数为max8cpu1用户可以配置" class="header-anchor">#</a></h3><p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/worker%E7%BA%BF%E7%A8%8B%EF%BC%88io%E7%BA%BF%E7%A8%8B%EF%BC%89.JPG" alt="图4 worker线程池" title="图4 worker线程池"></p>
<h3><span id="4图5和图6是业务线程池的构建cached线程池大小是20-200默认queue的大小是0-任务来了直接分配线程直到线程池满得不到执行线程抛异常">4.图5和图6是业务线程池的构建，cached线程池大小是20-200，默认queue的大小是0。 任务来了直接分配线程，直到线程池满，得不到执行线程抛异常。</span><a href="#4图5和图6是业务线程池的构建cached线程池大小是20-200默认queue的大小是0-任务来了直接分配线程直到线程池满得不到执行线程抛异常" class="header-anchor">#</a></h3><p>图7中一个服务端口对应一个业务线程池。</p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0-%E5%88%9D%E5%A7%8B%E5%8C%961.JPG" alt="图5 业务线程池构建1" title="图5 业务线程池构建1"></p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0-%E5%88%9D%E5%A7%8B%E5%8C%962.JPG" alt="图6 业务线程池构建2" title="图6 业务线程池构建2"></p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0-%E6%9E%84%E5%BB%BA.JPG" alt="图7 服务端口和业务线程池的关系" title="图7 服务端口和业务线程池的关系"></p>
<h3><span id="5-在channelpipeline中serverhandler根据服务端的配置获取对应的业务线程池然后在serverhandler的handlerrequest中提交业务任务默认的任务是jsftask">5. 在ChannelPipeline中ServerHandler根据服务端的配置获取对应的业务线程池，然后在ServerHandler的handlerRequest中提交业务任务，默认的任务是JSFTask。</span><a href="#5-在channelpipeline中serverhandler根据服务端的配置获取对应的业务线程池然后在serverhandler的handlerrequest中提交业务任务默认的任务是jsftask" class="header-anchor">#</a></h3><p>具体实现如图8,9,10.</p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0-%E8%8E%B7%E5%8F%96.JPG" alt="图8 ServerHandler中获取业务线程池" title="图8 ServerHandler中获取业务线程池"></p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E6%8F%90%E4%BA%A4task%E5%88%B0%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B.JPG" alt="图9 ServerHandler中提交业务任务" title="图9 ServerHandler中提交业务任务"></p>
<p><img src="http://www6v.github.io/www6vHome/jsf%20%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E4%B8%9A%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%89%A7%E8%A1%8Ctask.JPG" alt="图10 提交task到业务线程Executor" title="图10 提交task到业务线程Executor"></p>
<p>可以看到，JSF服务提供者线程模型整体还是按照boss+worker+biz这种netty官方推荐的方式构建的。</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>京东 jsf 源代码和文档</li>
<li>Netty案例集锦之多线程篇（续） 李林锋</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>案例-京东JSF</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>服务框架</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式服务框架 容错机制</title>
    <url>/www6vHomeHexo/2015/06/17/soaTolerate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E9%99%8D%E7%BA%A7">降级</a></li>
<li><a href="#%E8%B6%85%E6%97%B6%E5%92%8C%E9%87%8D%E8%AF%95-retry">超时和重试 Retry</a></li>
<li><a href="#%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99">集群容错</a></li>
<li><a href="#%E9%9A%94%E7%A6%BB-bulkhead">隔离 BulkHead</a></li>
<li><a href="#%E6%A1%86%E6%9E%B6">框架</a></li>
<li><a href="#%E7%8A%B6%E6%80%81%E7%9B%91%E6%B5%8B">状态监测</a></li>
<li><a href="#%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95-rate-limiter">流量控制（算法） Rate Limiter</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#self">self</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<p><strong>关键词</strong>： 容错, 降级, 隔离, 超时, 重试, 高可用, 监控, 开关</p>
<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><ul>
<li>超时重试机制[self 1][self 2]</li>
<li>限流</li>
<li>熔断机制</li>
<li>隔离</li>
<li>降级（本地缓存）</li>
<li>流量调度、负载均衡<br><a href="https://yq.aliyun.com/articles/7443?spm=5176.100238.yqhn2.2.XS3jCO">微服务熔断与隔离</a></li>
</ul>
<h1><span id="降级">降级</span><a href="#降级" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th align="center"><strong>降级策略</strong></th>
<th align="center">场景</th>
<th align="center">实现</th>
</tr>
</thead>
<tbody><tr>
<td align="center">降低一致性约束</td>
<td align="center"></td>
<td align="center">[1]</td>
</tr>
<tr>
<td align="center">关闭非核心业务</td>
<td align="center">人工开关 （非核心服务）, 强制降级,简化功能</td>
<td align="center">开关存放位置:配置文件,数据库, Redis&#x2F;Zk</td>
</tr>
<tr>
<td align="center">关闭非核心业务</td>
<td align="center">自动开关(非核心服务), 超时降级</td>
<td align="center">1. 统计失败次数降级-不稳定的api<br> 2. 限流降级-大促秒杀<br> 3. 实现-熔断器</td>
</tr>
</tbody></table>
<h1><span id="超时和重试-retry">超时和重试 Retry</span><a href="#超时和重试-retry" class="header-anchor">#</a></h1><ul>
<li>超时和重试<ul>
<li>网络连接&#x2F;读&#x2F;写的超时时间（重要）</li>
<li>服务<ul>
<li>读服务 - 可重试</li>
<li>写服务 - 幂等可重试</li>
</ul>
</li>
<li>服务<ul>
<li>客户端超时与重试</li>
<li>服务端超时 - 业务超时<br>  任务型<br>  服务调用型</li>
</ul>
</li>
<li>超时后策略<ul>
<li>failover<br>  其它分组<br>  其它机房</li>
<li>failcache</li>
<li>托底默认数据</li>
<li>等待页&#x2F;错误页</li>
<li>降级</li>
</ul>
</li>
<li>超时时间<ul>
<li>太短<br>  调用成功率降低</li>
<li>太长<br>  后续正常请求挤压</li>
<li>经验值<br>  稍微大于tp99的响应时间</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="集群容错">集群容错</span><a href="#集群容错" class="header-anchor">#</a></h1><ul>
<li>集群容错 <ul>
<li>Fail over（重试其他节点）<br>  超时异常</li>
<li>Fail fast（快速失败）</li>
<li>Fail cache（重试故障节点）（hedwig）<br>  网路异常</li>
<li>Fail back（回退）</li>
</ul>
</li>
</ul>
<h1><span id="隔离-bulkhead">隔离 BulkHead</span><a href="#隔离-bulkhead" class="header-anchor">#</a></h1><ul>
<li>隔离<ul>
<li>线程池隔离(hystirx)</li>
<li>vm隔离（资源隔离）</li>
<li>物理机隔离</li>
<li>集群隔离 分组隔离</li>
<li>机房隔离</li>
</ul>
</li>
</ul>
<h1><span id="框架">框架</span><a href="#框架" class="header-anchor">#</a></h1><ul>
<li>框架<ul>
<li>淘宝Dubbo</li>
<li>一号店Hedwig</li>
<li>京东JSF</li>
<li>点评pegion</li>
<li>唯品会OSP</li>
</ul>
</li>
</ul>
<h1><span id="状态监测">状态监测</span><a href="#状态监测" class="header-anchor">#</a></h1><ul>
<li>状态监测<ul>
<li>服务注册中心状态监测（hedwig）</li>
<li>服务提供者和消费者之间的链路有效性检测（pegion）</li>
<li>服务健康检查（打分）<br>  反推回消费者的路由表</li>
</ul>
</li>
</ul>
<h1><span id="流量控制算法-rate-limiter">流量控制（算法） Rate Limiter</span><a href="#流量控制算法-rate-limiter" class="header-anchor">#</a></h1><ul>
<li>流量控制（算法）　<ul>
<li>限流算法<ul>
<li>令牌桶（控制入口）</li>
<li>漏桶（控制出口）</li>
<li>计数器(hedwig)</li>
</ul>
</li>
<li>接口的总请求数（hedwig客户端）</li>
<li>接口的时间窗口请求数（hedwig服务端）</li>
<li>平滑限流,整形（netty）</li>
<li>整体流控<ul>
<li>静态流控(整体qps固定)<br>  预先分配<br>  动态配额分配置（推）<br>  动态配额申请制（拉）</li>
<li>动态流控<br>  分级流控-拒绝流量</li>
</ul>
</li>
<li>连接控制</li>
<li>并发控制（线程的并发执行数）</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h2><span id="self">self</span><a href="#self" class="header-anchor">#</a></h2><ol>
<li><a href="/www6vHomeHexo/2015/06/17/soaTolerate/" title="分布式服务框架 容错机制">分布式服务框架 容错机制</a> self</li>
<li><a href="/www6vHomeHexo/2016/01/17/soaTimeout/" title="超时和重试 总结">超时和重试 总结</a> self</li>
</ol>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>容错</category>
      </categories>
      <tags>
        <tag>服务框架</tag>
        <tag>容错</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式服务框架功能</title>
    <url>/www6vHomeHexo/2015/05/07/soaFeature/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/05/07/soaFeature/soaFeature.jpg" class title="分布式服务框架功能">

<ul>
<li>负载均衡<br>RR<br>Least Connections<br>Least Time<br>“Power of Two Choices”</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.nginx.com/blog/nginx-power-of-two-choices-load-balancing-algorithm/">NGINX and the “Power of Two Choices” Load-Balancing Algorithm</a><br><a href="https://www.bilibili.com/video/BV1Gb4y187un?zw&vd_source=f6e8c1128f9f264c5ab8d9411a644036">【直播回放】海量并发微服务框架设计</a>  重要公式</p>
]]></content>
      <categories>
        <category>服务治理</category>
        <category>功能</category>
      </categories>
      <tags>
        <tag>服务框架</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP总结</title>
    <url>/www6vHomeHexo/2015/04/25/tcp/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="tcp基础总结">TCP基础总结</span><a href="#tcp基础总结" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2015/04/25/tcp/tcp-base.jpg" class title="TCP基础总结">

<h2><span id="tcp优化">TCP优化</span><a href="#tcp优化" class="header-anchor">#</a></h2><img src="/www6vHomeHexo/2015/04/25/tcp/tcp-optimize.jpg" class title="TCP优化">

<p>Tcp五元组: 源ip, 源port, 目的ip, 目的port, 协议</p>
<h2><span id="tcp-keep-alive">TCP Keep-Alive</span><a href="#tcp-keep-alive" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>【TCP Keep-Alive】</th>
<th>基础</th>
<th>并行请求</th>
</tr>
</thead>
<tbody><tr>
<td>HTTP1.1</td>
<td>Connection  Header <br>[Connection: keep-alive<br>Connection: close]</td>
<td>1. 浏览器默认不开启 HTTP Pipelining, 不可行(一个TCP连接中的多个并行HTTP请求) <br> 2. 一个 HOST 上建立多个 TCP 连接,Chrome 最多允许对同一个 Host 建立六个 TCP 连接</td>
</tr>
<tr>
<td>HTTP2.0</td>
<td>HTTP2是在HTTPS上实现的</td>
<td>HTTP2的Multiplexing多路传输特性， 可行</td>
</tr>
</tbody></table>
<h2><span id="tcp-握手-挥手">TCP 握手、挥手</span><a href="#tcp-握手-挥手" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/65186269-e2b7e780-da9b-11e9-926f-ea186bb3282a.jpg" alt="图1.TCP三次握手"><br>图1.TCP三次握手</p>
<p><img src="https://user-images.githubusercontent.com/5608425/65186267-e21f5100-da9b-11e9-9f92-3fcab8f30679.jpg" alt="图2.TCP四次挥手"><br>图2.TCP四次挥手</p>
<p><img src="https://user-images.githubusercontent.com/5608425/65186270-e3507e00-da9b-11e9-8419-bab1f09e35eb.jpg" alt="tcp-sync-queue-and-accept-queue">  图3.全&#x2F;半连接队列</p>
</div>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://blog.csdn.net/huang_xw/article/details/7340241">java socket参数详解:TcpNoDelay</a> huang_xw  </li>
<li><a href="http://jm.taobao.org/2017/05/25/525-1/">关于TCP 半连接队列和全连接队列 蛰剑</a></li>
<li><a href="http://jm.taobao.org/2017/06/08/20170608/">就是要你懂 TCP 蛰剑</a> </li>
<li><a href="http://jm.taobao.org/2017/06/01/20170601/">就是要你懂 TCP | 最经典的TCP性能问题 蛰剑</a>  Nagle算法, delay ack</li>
<li><a href="http://jm.taobao.org/2017/07/27/20170727/">就是要你懂 TCP | 通过案例来学习 MSS、MTU 蛰剑</a></li>
<li>Where do resets come from? MichaelPlatts [msft]</li>
<li>TCP的三次握手以及重置(Reset) 赵帅强</li>
<li>【剖析 | SOFARPC 框架】系列之连接管理与心跳剖析 SOFARPCLab</li>
<li>蚂蚁金服通信框架SOFABolt解析 | 编解码机制 SOFABoltLab水寒</li>
<li>《趣谈网络协议 - 第11讲 TCP协议（上）：因性恶而复杂，先恶后善反轻松》  刘超</li>
<li>《 Linux性能优化实战 - 40 - 案例篇：网络请求延迟变大了，我该怎么办？》 倪朋飞</li>
<li><a href="https://tencentcloudcontainerteam.github.io/2019/08/12/troubleshooting-with-kubernetes-network/">Kubernetes 网络疑难杂症排查分享</a>   腾讯云容器团队</li>
<li><a href="https://coolshell.cn/articles/11564.html">TCP 的那些事儿（上）</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUyOTk5NDQwOA==&mid=2247486314&idx=2&sn=2bd0223856ce7c7cdc3114f55090fd2d&chksm=fa59ccadcd2e45bb3230029cdece65eeeca9446c4671b99f6627e5c237b44272d608cb0d2c7f&scene=0&xtrack=1#rd">面试官问我：一个 TCP 连接可以发多少个 HTTP 请求？我竟然回答不上来…</a></li>
<li><a href="http://hengyunabc.github.io/why-we-need-heartbeat/">为什么基于TCP的应用需要心跳包（TCP keep-alive原理分析）</a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
        <category>网络</category>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper-总结</title>
    <url>/www6vHomeHexo/2015/03/26/zookeeper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>




<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B-1">读&#x2F;写流程 [1]</a><ul>
<li><a href="#%E5%86%99%E6%B5%81%E7%A8%8B">写流程</a></li>
<li><a href="#%E8%AF%BB%E6%B5%81%E7%A8%8B">读流程</a></li>
</ul>
</li>
<li><a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2">应用场景 [2]</a><ul>
<li><a href="#%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86">元数据管理</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83">分布式协调</a></li>
<li><a href="#master%E9%80%89%E4%B8%BE-ha%E6%9E%B6%E6%9E%84">Master选举 -&gt; HA架构</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81">分布式锁</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8-2">实现高可用 [2]</a><ul>
<li><a href="#%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2">主备切换</a></li>
<li><a href="#%E9%9B%86%E7%BE%A4%E9%80%89%E4%B8%BE">集群选举</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="读x2f写流程-1">读&#x2F;写流程 [1]</span><a href="#读x2f写流程-1" class="header-anchor">#</a></h1><h3><span id="写流程">写流程</span><a href="#写流程" class="header-anchor">#</a></h3><ul>
<li>写Leader</li>
<li>写Follower&#x2F;Observer<br>将写请求转发给Leader处理</li>
</ul>
<h3><span id="读流程">读流程</span><a href="#读流程" class="header-anchor">#</a></h3><ul>
<li>Leader&#x2F;Follower&#x2F;Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。</li>
</ul>
<h1><span id="应用场景-2">应用场景 [2]</span><a href="#应用场景-2" class="header-anchor">#</a></h1><h3><span id="元数据管理">元数据管理</span><a href="#元数据管理" class="header-anchor">#</a></h3><p>Dubbo， HBase</p>
<h3><span id="分布式协调">分布式协调</span><a href="#分布式协调" class="header-anchor">#</a></h3><p>kafka controller</p>
<h3><span id="master选举-gt-ha架构">Master选举 -&gt; HA架构</span><a href="#master选举-gt-ha架构" class="header-anchor">#</a></h3><p>HDFS NameNode HA</p>
<h3><span id="分布式锁">分布式锁</span><a href="#分布式锁" class="header-anchor">#</a></h3><p>案例比较少</p>
<h1><span id="实现高可用-2">实现高可用 [2]</span><a href="#实现高可用-2" class="header-anchor">#</a></h1><h3><span id="主备切换">主备切换</span><a href="#主备切换" class="header-anchor">#</a></h3><h3><span id="集群选举">集群选举</span><a href="#集群选举" class="header-anchor">#</a></h3><ul>
<li>最小节点获胜</li>
<li>抢建唯一节点</li>
<li>法官判决</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://www.jasongj.com/zookeeper/fastleaderelection/">深入浅出Zookeeper（一） Zookeeper架构及FastLeaderElection机制 </a> *** </li>
<li><a href="https://www.jianshu.com/p/9ce2600dd139">浅析如何基于ZooKeeper实现高可用架构｜得物技术</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/571732977">ZooKeeper 核心通识</a> 未</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring事务</title>
    <url>/www6vHomeHexo/2015/03/16/springTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/03/16/springTransaction/springTransaction.jpg" class title="Spring事务">


<h1><span id="三种事务模型">三种事务模型</span><a href="#三种事务模型" class="header-anchor">#</a></h1><ul>
<li>三种事务模型<ul>
<li>本地事务模型<ul>
<li>事务全部交给数据库来管理</li>
</ul>
</li>
<li>编程式事务模型<ul>
<li>事务的提交和回滚操作完全交给开发人员</li>
<li>TransactionTemplate <ul>
<li>TransactionCallback中执行业务代码</li>
<li>事务代码和业务代码可以实现分离的原理【1】</li>
</ul>
</li>
</ul>
</li>
<li>声明式事务模型【AOP】<ul>
<li>事务的提交和回滚操作全部交给Spring来管理</li>
<li>事务拦截器TransactionInterceptor<ul>
<li>事务管理器transactionManager【2】</li>
<li>事务配置的提供者transactionAttributes【3】<br>  业务方法+传播属性</li>
<li>代理的对象target<br>  业务对象</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>分布式事务系列（1.1）Spring事务管理器PlatformTransactionManager 乒乓狂魔</li>
<li>分布式事务系列（1.2）Spring的事务体系 乒乓狂魔</li>
</ol>
]]></content>
      <categories>
        <category>中间件</category>
        <category>spring</category>
        <category>事务</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL事务-总结</title>
    <url>/www6vHomeHexo/2015/02/21/mysqlTransaction/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/02/21/mysqlTransaction/transaction.jpg" class title="本地事务总结">



<h2><span id="mysql-log和事务">MySQL Log和事务</span><a href="#mysql-log和事务" class="header-anchor">#</a></h2><a href="/www6vHomeHexo/2022/02/27/mysqlLog/" title="MySQL Logs">MySQL Logs</a>  

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="https://dbaplus.cn/news-160-1729-1.html">拨开云雾见天日：剖析单机事务原理</a> CHEN川 ***</li>
<li><a href="https://coolshell.cn/articles/6790.html">多版本并发控制(MVCC)在分布式系统中的应用</a> Todd</li>
<li>阿里云分布式缓存OCS与DB之间的数据一致性 杨成虎</li>
<li><a href="https://www.zhihu.com/question/27876575">乐观锁和 MVCC 的区别？  </a></li>
<li><a href="https://blog.csdn.net/cweeyii/article/details/70991230">mysql可重复读和幻读实例  CWeeYii</a></li>
<li><a href="https://www.cnblogs.com/lz0925/articles/8988922.html">MySQL脏读、虚读、幻读 Eternity味道</a></li>
<li><a href="https://blog.csdn.net/J_java1/article/details/82025189">MySQL 中事务的实现原理</a>  失效</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>关系型</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Java并发容器总结</title>
    <url>/www6vHomeHexo/2015/01/09/concurrentCollection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2015/01/09/concurrentCollection/concurrentCollection.jpg" class title="Java并发容器总结">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>关于Java集合的小抄 白衣</li>
<li>谈谈ConcurrentHashMap1.7和1.8的不同实现 占小狼</li>
<li>高性能队列——Disruptor</li>
<li>非阻塞算法在并发容器中的实现 程晓明</li>
<li>从volatile解读ConcurrentHashMap（jdk1.6.0）无锁读 绫萱</li>
<li>JAVA并发容器代码随读 BucketLi</li>
<li>聊聊并发（四）深入分析ConcurrentHashMap 方 腾飞</li>
<li>Java8 新特性之流式数据处理 深蓝至尊</li>
<li>不止JDK7的HashMap，JDK8的ConcurrentHashMap也会造成CPU 100% 朱小厮 朱忠华</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>并发容器</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>并发集合</tag>
        <tag>concurrentCollection</tag>
      </tags>
  </entry>
  <entry>
    <title>Java集合总结</title>
    <url>/www6vHomeHexo/2014/12/03/collection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/12/03/collection/collection.jpg" class title="Java集合总结">

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>JDK Collection 源代码</li>
<li>关于Java集合的小抄 白衣</li>
<li>限流系统如何发现系统的热点 阿里中间件团队博客</li>
<li>不正当使用HashMap导致cpu 100%的问题追究 王宏江</li>
<li>并发环境下HashMap引起的full gc排查 ol_beta</li>
<li>不止JDK7的HashMap，JDK8的ConcurrentHashMap也会造成CPU 100% 朱小厮 朱忠华</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>集合</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>集合</tag>
        <tag>collection</tag>
      </tags>
  </entry>
  <entry>
    <title>AOP总结</title>
    <url>/www6vHomeHexo/2014/11/21/aop/</url>
    <content><![CDATA[<p></p>

<span id="more"></span>

<img src="/www6vHomeHexo/2014/11/21/aop/AOP.jpg" class title="AOP总结">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><p>1.AOP技术讨论 乒乓狂魔<br>2.AOP 那点事儿 黄勇<br>3.AOP 那点事儿（续集） 黄勇<br>4.AOP实现机制 fantasy<br>5.动态代理方案性能对比 javatar<br>6.Java动态代理机制详解（JDK 和CGLIB，Javassist，ASM） 亦山<br>7.Spring事务处理时自我调用的解决方案及一些实现方式的风险 张开涛</p>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>AOP</category>
      </categories>
      <tags>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat classloader调研</title>
    <url>/www6vHomeHexo/2014/10/28/tomcatClassloader/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-这次针对共享jar目录对tomcat-classloader调研大概有以下两种解决方案">一. 这次针对共享jar目录对tomcat classloader调研，大概有以下两种解决方案。</span><a href="#一-这次针对共享jar目录对tomcat-classloader调研大概有以下两种解决方案" class="header-anchor">#</a></h2><h2><span id="二-tomcat6x-向前兼容性-和-delegate开关">二. Tomcat6.x+ 向前兼容性 和 Delegate“开关”</span><a href="#二-tomcat6x-向前兼容性-和-delegate开关" class="header-anchor">#</a></h2><p>Tomcat6.x+ 向前兼容性</p>
<p>原理：tomcat6.x+为了用户的使用简化了tomcat5.x的classloader模型，但用户也能通过修改配置中的server.loader和shared.loader重新启用5.x的加载器结构。</p>
<p>server.loader下的jar文件只能由tomcat访问， 对应用不可见。</p>
<p>shared.loader下的jar文件能被各个应用共享访问，对tomcat不可见。</p>
<p>配置：</p>
<p>catalina.properites中的server.loader和shared.loader， 配置为jar所在的目录</p>
<p>代码：</p>
<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image002.jpg" alt="图0" title="图0"></p>
<p>Delegate“开关”</p>
<p>原理：tomcat6x+后默认Delegate为false，意味是打破java默认的双亲委派规则的。也就是说默认是child first的， 先加载webapp自身的jar文件。如果置Delegate&#x3D;true,优先委派双亲加载（parent first）。</p>
<p>配置：</p>
<ol>
<li>Defining a context（3中方式，两种方式略）</li>
</ol>
<p>Inside a Host element in the main conf&#x2F;server.xml.</p>
<ol start="2">
<li>Loader delegate&#x3D;”true”</li>
</ol>
<p>Loader - Configure the web application class loader that will be used to load servlet and bean classes for this web application. Normally, the default configuration of the class loader will be sufficient.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eg. &lt;context docBase=&quot;…&quot;&gt; &lt;Loader delegate=&quot;true&quot; /&gt; … &lt;/context&gt;</span><br></pre></td></tr></table></figure>
<p>注意点：</p>
<ol>
<li>Tomcat6.X 和Tomcat7. X，8. X的加载顺序有一点不同</li>
</ol>
<p>源代码： WebappClassLoader.loadClass ?</p>
<p>1）7. X比6. X多并发load class的功能。</p>
<ol start="2">
<li><ol start="7">
<li>X里 Bootstrap classLoader先加载class， 6. X里 System classLoader先加载class</li>
</ol>
</li>
</ol>
<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image004.jpg" alt="图1" title="图1"></p>
<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image006.jpg" alt="图2" title="图2"></p>
<p>Tomcat6.X</p>
<p>Delegate&#x3D;false 的加载顺序<br>Bootstrap classes of your JVM<br>System class loader classes<br>&#x2F;WEB-INF&#x2F;classes of your web application<br>&#x2F;WEB-INF&#x2F;lib&#x2F;*.jar of your web application<br>Common class loader classes</p>
<p>Delegate&#x3D;true 的加载顺序<br>Bootstrap classes of your JVM<br>System class loader classes<br>Common class loader classes<br>&#x2F;WEB-INF&#x2F;classes of your web application<br>&#x2F;WEB-INF&#x2F;lib&#x2F;*.jar of your web application</p>
<p>Tomcat7.X，8.X</p>
<p>Delegate&#x3D;false 的加载顺序<br>Bootstrap classes of your JVM<br>&#x2F;WEB-INF&#x2F;classes of your web application<br>&#x2F;WEB-INF&#x2F;lib&#x2F;*.jar of your web application<br>System class loader classes<br>Common class loader classes</p>
<p>Delegate&#x3D;true 的加载顺序<br>Bootstrap classes of your JVM<br>System class loader classes (described above)<br>Common class loader classes (described above)<br>&#x2F;WEB-INF&#x2F;classes of your web application<br>&#x2F;WEB-INF&#x2F;lib&#x2F;*.jar of your web application</p>
<ol start="2">
<li>packageTriggers变量</li>
</ol>
<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image008.jpg" alt="图3" title="图3"><br><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image009.png" alt="图4" title="图4"><br><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcat%20classloader_clip_image011.jpg" alt="图5" title="图5"></p>
<p>packageTriggers，执行child first时，排除的package列表，如果匹配了package，即时为delegate&#x3D;false，也会优先执行parent first策略。</p>
<p>但是这个变量没有作为tomcat配置项， 这个功能没有暴露出来。</p>
<h2><span id="三-webappclassloader加载指定目录的jar文件">三. WebappClassLoader加载指定目录的jar文件</span><a href="#三-webappclassloader加载指定目录的jar文件" class="header-anchor">#</a></h2><p>每个应用可以加载指定目录的jar，虽然灵活，但是配置相对复杂，对应用的入侵性比较大，不推荐使用。 具体见参考文档。</p>
<p>总结： tomcat6x+ 向前兼容性 和 Delegate“开关”这种方式对应用的入侵性比较小，应用改动比较少。</p>
<p>附：验证 detector:</p>
<p>Delegate 没配置， shared 目录配置了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$jmap -permstat 5440 | grep live</span><br><span class="line"></span><br><span class="line">$......</span><br><span class="line"></span><br><span class="line">computing per loader stat ..done.</span><br><span class="line"></span><br><span class="line">please wait.. computing liveness.............liveness analysis may be inaccurate ...</span><br><span class="line"></span><br><span class="line">class_loader    classes bytes   parent_loader   alive?  type</span><br><span class="line"></span><br><span class="line">&lt;bootstrap&gt;     2503    14928832          null          live    &lt;internal&gt;</span><br><span class="line"></span><br><span class="line">0x00000007683850b0      0       0       0x0000000767ae9360      live    java/net/URLClassLoader@0x00000007621eb548</span><br><span class="line"></span><br><span class="line">0x0000000767b3b2a8      578     4855184 0x0000000767ae9360      live    org/apache/catalina/loader/StandardClassLoader@0x0000000762315538</span><br><span class="line"></span><br><span class="line">0x0000000767b7ab00      0       0       0x0000000767b3b2a8      live    org/apache/catalina/loader/StandardClassLoader@0x0000000762315538</span><br><span class="line"></span><br><span class="line">0x0000000767ae93b0      77      466376    null          live    sun/misc/Launcher$ExtClassLoader@0x00000007621eb958</span><br><span class="line"></span><br><span class="line">0x0000000767b7aa48      5712    34794416        0x0000000767b7ab00      live    org/apache/catalina/loader/WebappClassLoader@0x0000000762b712f8</span><br><span class="line"></span><br><span class="line">0x0000000767ae9360      52      659816  0x0000000767ae93b0      live    sun/misc/Launcher$AppClassLoader@0x0000000762241680</span><br><span class="line"></span><br><span class="line">0x00000007673df848      0       0       0x0000000767ae9360      live    java/util/ResourceBundle$RBClassLoader@0x0000000762498f90</span><br><span class="line"></span><br><span class="line">total = 37      8951    55792808            N/A         alive=8, dead=29            N/A</span><br></pre></td></tr></table></figure>


<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcatClassloader_clip_image002_0000.jpg" alt="图6" title="图6"></p>
<p>Delegate 配置了， shared 目录配置了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">computing per loader stat ..done.</span><br><span class="line"></span><br><span class="line">please wait.. computing liveness..............done.</span><br><span class="line"></span><br><span class="line">class_loader    classes bytes   parent_loader   alive?  type</span><br><span class="line"></span><br><span class="line">&lt;bootstrap&gt;     2482    14861184          null          live    &lt;internal&gt;</span><br><span class="line"></span><br><span class="line">0x0000000768da77d8      77      466376    null          live    sun/misc/Launcher$ExtClassLoader@0x00000007621eb958 --</span><br><span class="line"></span><br><span class="line">0x0000000768da7788      52      659816  0x0000000768da77d8      live    sun/misc/Launcher$AppClassLoader@0x0000000762241680 --</span><br><span class="line"></span><br><span class="line">0x000000076892ffd0      0       0       0x0000000768da7788      live    java/util/ResourceBundle$RBClassLoader@0x0000000762498f90</span><br><span class="line"></span><br><span class="line">0x0000000768da76e8      0       0       0x0000000768da7738      live    org/apache/catalina/loader/StandardClassLoader@0x0000000762315538 --</span><br><span class="line"></span><br><span class="line">0x0000000768da7738      556     4709744 0x0000000768da7788      live    org/apache/catalina/loader/StandardClassLoader@0x0000000762315538 --</span><br><span class="line"></span><br><span class="line">0x0000000768da7630      5200    31904816        0x0000000768da76e8      live    org/apache/catalina/loader/WebappClassLoader@0x00000007628c7588 --</span><br><span class="line"></span><br><span class="line">total = 35      8395    52687048            N/A         alive=7, dead=28            N/A</span><br></pre></td></tr></table></figure>

<p><img src="http://www6v.github.io/www6vHome/tomcatClassloader/tomcatClassloader_clip_image004.jpg" alt="图7" title="图7"></p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><a href="http://tomcat.apache.org/tomcat-6.0-doc/config/loader.html">The Loader Component</a></li>
<li><a href="http://tomcat.apache.org/tomcat-7.0-doc/config/context.html">The Context Container</a></li>
<li><a href="http://agapple.iteye.com/blog/826661">主流web容器(jetty,tomcat,jboss)的classloader机制对比和相关问题分析</a></li>
<li><a href="http://blog.csdn.net/fjslovejhl/article/details/21328347">Tomcat源码分析之ClassLoader部分的设计详细分析 tomcat8.0</a> </li>
<li><a href="http://www.blogjava.net/heavensay/archive/2012/11/07/389685.html">class卸载、热替换和Tomcat的热部署的分析</a>  </li>
<li><a href="http://dncsoft.iteye.com/blog/336871">应用Tomcat的WebappClassLoader加载指定目录的jar文件</a>  </li>
<li>tomcat6.0和7.0的源代码</li>
</ol>
<p>Btw： WatchedResource - The auto deployer will monitor the specified static resource of the web application for updates, and will reload the web application if it is updated. The content of this element must be a string.</p>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>classloader</category>
      </categories>
      <tags>
        <tag>classloader</tag>
      </tags>
  </entry>
  <entry>
    <title>Classloader相关的故障排查</title>
    <url>/www6vHomeHexo/2014/09/06/classloader/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="一-背景">一. 背景</span><a href="#一-背景" class="header-anchor">#</a></h2><p>同一段代码在服务框架的服务端和客户端里重用, 客户端里classloader能load到资源文件, 但是服务端代码部署在tomcat容器里, classloader不能load到资源文件.</p>
<p>看来代码有个bug.直接上代码:</p>
<h2><span id="二代码">二.代码</span><a href="#二代码" class="header-anchor">#</a></h2><ol>
<li>看到客户端里的classloader是AppClassLoader, 能load到classpath下的文件, 所以只要资源文件在classpath下就能load到.</li>
</ol>
<p><img src="http://www6v.github.io/www6vHome/classloader/client%20appclassloder.PNG" alt="图一 客户端classloader" title="图一 客户端classloader"></p>
<p><img src="http://www6v.github.io/www6vHome/classloader/client%20appclassloder1.PNG" alt="图二 客户端 classloader打开流文件" title="图二 客户端 classloader打开流文件"></p>
<ol start="2">
<li>在tomcat里跑，load当前类的是WebappClassloaer，所以this.getClass().getClassLoader()得到的也是 WebappClassloaer。</li>
</ol>
<p>getSystemResouceAsStream()没有load到资源文件， 换成getResouceAsStream()后能load到。</p>
<p><img src="http://www6v.github.io/www6vHome/classloader/server%20WebappClassloader.PNG" alt="图三 clzloader没加载到文件, input是null" title="图三 clzloader没加载到文件, input是null"> </p>
<p><img src="http://www6v.github.io/www6vHome/classloader/server%20WebappClassloader1.PNG" alt="图四. clzloader在/WEB-INF/classse/下加载到了文件" title="图四. clzloader在/WEB-INF/classse/下加载到了文件"></p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>classloader</category>
      </categories>
      <tags>
        <tag>故障排查</tag>
      </tags>
  </entry>
  <entry>
    <title>Classloader总结</title>
    <url>/www6vHomeHexo/2014/08/12/classloaderSummarize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/08/12/classloaderSummarize/Classloader.jpg" class title="Classloader总结">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>深入浅出ClassLoader 你真的了解ClassLoader吗？   魏 鹏</li>
<li>class卸载、热替换和Tomcat的热部署的分析   heavensay</li>
<li>《实战Java虚拟机》 第10章</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>classloader</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>jvm</tag>
        <tag>classloader</tag>
      </tags>
  </entry>
  <entry>
    <title>两个GC案例</title>
    <url>/www6vHomeHexo/2014/07/21/twoGCcase/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p><a href="http://www6v.github.io/www6vHome/twoGCcase.html">两个GC案例</a></p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>gc</category>
      </categories>
      <tags>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title>垃圾收集GC总结</title>
    <url>/www6vHomeHexo/2014/07/16/gc/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/07/16/gc/gc.jpg" class title="垃圾收集GC总结">



<h3><span id="并发标记算法">并发标记算法</span><a href="#并发标记算法" class="header-anchor">#</a></h3><ol>
<li>CMS： 三色标记算法 + 增量更新（Incremental update）</li>
<li>G1： 三色标记算法 + STAB（snapshot-at-the-beginning）<br><a href="https://blog.csdn.net/qq_36697880/article/details/105206385">Java虚拟机 —-三色标记与G1垃圾回收器</a></li>
</ol>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>源码分析：Java对象的内存分配   iceAeterna</li>
<li>《深入理解Java虚拟机（第2版）》 第3章   周志明</li>
<li>深入理解GC ——MinorGC\MajorGC\FullGC   张硕的博客</li>
<li>触发JVM进行Full GC的情况及应对策略   yexx</li>
<li>Netty之有效规避内存泄漏   江南白衣</li>
<li><a href="https://yq.aliyun.com/articles/72217">周期性Full GC的异常排查</a>  weiplex</li>
<li>一个大对象引起的血案，GC的踩坑实录   何锦彬</li>
<li><a href="https://hllvm-group.iteye.com/group/topic/28379">生产环境下持久带满导致FullGC，如何跟踪</a> </li>
<li><a href="https://www.jianshu.com/p/f92c190f7dec">又是一个程序员粗心的代码引起频繁FullGC的案例</a> 微信公众号里的  </li>
<li>JVM初探- 使用堆外内存减少Full GC   菜鸟-翡青</li>
<li><a href="https://www.iteye.com/blog/auzll-1904081">我遇到tomcat 7 full gc频繁的问题</a>  auzll</li>
<li><a href="https://mp.weixin.qq.com/s/xICP6icb3mpj0lvj9zYbiw">一次堆外OOM问题的排查过程</a>  谢照东   占小狼的博客</li>
<li>美团三面：一个线程OOM，进程里其他线程还能运行么？    孤独烟 Java技术驿站</li>
<li><a href="https://tech.meituan.com/2018/10/18/netty-direct-memory-screening.html">netty 堆外内存泄露排查盛宴</a>   闪电侠的博客  netty的bug</li>
<li><a href="https://blog.csdn.net/fenglibing/article/details/82692169">线上故障排查(2) - Java应用故障之堆溢出OOM问题及排查方案</a>  冯立彬 未</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>内存</category>
        <category>gc</category>
      </categories>
      <tags>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title>线程池最佳线程数</title>
    <url>/www6vHomeHexo/2014/07/02/threadNum/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/07/02/threadNum/threadNum.jpg" class title="线程池最佳线程数">

<p>最佳实践:<br><strong>曾经遇到过任务被丢给线程池之后，长时间都没有被执行的诡异问题</strong>。最初，我认为这是代码的 Bug 导致的，后来经过排查发现，是因为<strong>线程池的coreThreadCount 和 maxThreadCount 设置的比较小</strong>，导致任务在线程池里面大量的堆积，在调大了这两个参数之后问题就解决了.</p>
<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>工作线程数究竟要设置为多少 架构师之路</li>
<li>蚂蚁金服技术专家总结：性能优化的常见招式 jurassic_1</li>
<li>《Java 并发编程实践》 8.2定制线程池的大小 Brian Goetz</li>
<li><a href>高并发系统设计40问 - 07 | 池化技术：如何减少频繁创建数据库连接的性能损耗？</a> 唐扬</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>多线程中的volatile和CAS</title>
    <url>/www6vHomeHexo/2014/06/02/volatile-CAS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/06/02/volatile-CAS/volatile-CAS.jpg" class title="多线程中的volatile和CAS">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>Java 理论与实践: 正确使用 Volatile 变量</li>
<li>从volatile解读ConcurrentHashMap（jdk1.6.0）无锁读</li>
<li>为什么volatile不能保证原子性而Atomic可以</li>
<li>阿里巴巴Java开发手册</li>
<li>非阻塞同步算法与CAS(Compare and Swap)无锁算法</li>
<li>非阻塞算法在并发容器中的实现 程晓明</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>多线程中的锁</title>
    <url>/www6vHomeHexo/2014/05/27/lock/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/05/27/lock/Lock-theory.jpg" class>

<img src="/www6vHomeHexo/2014/05/27/lock/Lock-java.jpg" class>

<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>《java并发编程实践》 13章 显示锁 ,13.5 读-写锁</li>
<li>轻松学习java可重入锁(ReentrantLock)的实现原理</li>
<li>锁模式 -&gt; 共享锁，排他锁</li>
<li>轻松掌握java读写锁(ReentrantReadWriteLock)的实现原理</li>
<li>Java中的读&#x2F;写锁 作者:Jakob Jenkov</li>
<li>Java同步块 作者:Jakob Jenkov</li>
<li>《Java并发编程的艺术》-Java并发包中的读写锁及其实现分析</li>
<li>怎么理解Condition 码梦为生|刘锟洋</li>
<li>多线程之：偏向锁，轻量级锁，重量级锁 无信不立</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>Java多线程总结</title>
    <url>/www6vHomeHexo/2014/04/21/thread/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/04/21/thread/thread.jpg" class title="Java多线程总结">

<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>非阻塞同步算法与CAS(Compare and Swap)无锁算法</li>
<li>高性能队列——Disruptor</li>
<li>java多线程体系 东隼</li>
<li>《并发编程实践》 Brian Goetz 第3章</li>
<li>Java并发之AQS详解 waterystone</li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzI1NDQ3MjQxNA==&mid=2247489989&idx=1&sn=76fc6b53ac9c7e9a3bf127e6ace66c3c&chksm=e9c5e074deb26962dcb2aa82df61e93847b60b106f92fa0b38f7d8cea07d12d7eb3f8cf180a5&mpshare=1&scene=24&srcid=&sharer_sharetime=1571881496774&sharer_shareid=970337f0f341cd04749ae35c84d2fc1e#rd">面试官：线程顺序执行,这么多答案你都答不上来？</a><br>Join, wait, Condition, 线程池</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>Java多线程中的取消和关闭</title>
    <url>/www6vHomeHexo/2014/04/09/cancelAndShutdown/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/04/09/cancelAndShutdown/cancelAndShutdown.jpg" class title="Java多线程中的取消和关闭">
]]></content>
      <categories>
        <category>Java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>伪共享 FalseSharing</title>
    <url>/www6vHomeHexo/2014/03/05/falseSharing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A6%82%E5%BF%B5">概念</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a></li>
<li><a href="#%E4%BC%AA%E5%85%B1%E4%BA%AB%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81">伪共享测试代码</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考：</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="概念">概念</span><a href="#概念" class="header-anchor">#</a></h2><div style="text-align: center;">

<p><img src="https://user-images.githubusercontent.com/5608425/64919353-6c399200-d7dc-11e9-8506-50583042d91a.png" alt="FalseSharing"><br>伪共享 FalseSharing</p>
</div>

<h2><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h2><ol>
<li><p>jdk8 LongAdder的Cell类</p>
</li>
<li><p>Disruptor</p>
</li>
</ol>
<h2><span id="伪共享测试代码">伪共享测试代码</span><a href="#伪共享测试代码" class="header-anchor">#</a></h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">-- 有cache line padding 测试结果</span></span><br><span class="line"><span class="comment">1. duration = 22344563916  </span></span><br><span class="line"><span class="comment">2. duration = 22012580114  </span></span><br><span class="line"><span class="comment">3. duration = 11167549431  </span></span><br><span class="line"><span class="comment">4. duration = 19736854183  </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">avg: 18815386911</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">-- 没有cache line padding 测试结果</span></span><br><span class="line"><span class="comment">1. duration = 23658423881  </span></span><br><span class="line"><span class="comment">2. duration = 33335707670  </span></span><br><span class="line"><span class="comment">3. duration = 35890190024  </span></span><br><span class="line"><span class="comment">4. duration = 29516958769  </span></span><br><span class="line"><span class="comment">5. duration = 35027213671</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">avg:  31485698803</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">result： 性能相差一倍左右  </span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">FalseSharing</span></span><br><span class="line">    <span class="keyword">implements</span> <span class="title class_">Runnable</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">NUM_THREADS</span> <span class="operator">=</span> <span class="number">4</span>; <span class="comment">// change</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">long</span> <span class="variable">ITERATIONS</span> <span class="operator">=</span> <span class="number">500L</span> * <span class="number">1000L</span> * <span class="number">1000L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> arrayIndex;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> VolatileLong[] longs = <span class="keyword">new</span> <span class="title class_">VolatileLong</span>[NUM_THREADS];</span><br><span class="line">    <span class="keyword">static</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; longs.length; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            longs[i] = <span class="keyword">new</span> <span class="title class_">VolatileLong</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FalseSharing</span><span class="params">(<span class="keyword">final</span> <span class="type">int</span> arrayIndex)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">this</span>.arrayIndex = arrayIndex;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">        runTest();</span><br><span class="line">        System.out.println(<span class="string">&quot;duration = &quot;</span> + (System.nanoTime() - start));</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">runTest</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">        Thread[] threads = <span class="keyword">new</span> <span class="title class_">Thread</span>[NUM_THREADS];</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; threads.length; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            threads[i] = <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">FalseSharing</span>(i));</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> (Thread t : threads)</span><br><span class="line">        &#123;</span><br><span class="line">            t.start();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> (Thread t : threads)</span><br><span class="line">        &#123;</span><br><span class="line">            t.join();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">i</span> <span class="operator">=</span> ITERATIONS + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="number">0</span> != --i)</span><br><span class="line">        &#123;</span><br><span class="line">            longs[arrayIndex].value = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">VolatileLong</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">volatile</span> <span class="type">long</span> <span class="variable">value</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">        <span class="comment">//public long p1, p2, p3, p4, p5, p6;   comment out </span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试环境： 笔记本电脑 – Intel（R）Core(TM) i3-3120M CPU @2.50GHz 2.50GHz 内存 10.0GB</p>
<h2><span id="参考">参考：</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://yq.aliyun.com/articles/68190?spm=5176.8067842.tagmain.150.yoI2AF">LongAdder类学习小结</a></p>
</li>
<li><p><a href="http://ifeve.com/disruptor-cacheline-padding/">剖析Disruptor:为什么会这么快？（二）神奇的缓存行填充</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>内存</category>
        <category>伪共享</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Java内存泄漏的案例和解决方案</title>
    <url>/www6vHomeHexo/2014/02/02/javaMemoryLeak/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>如果一个可达对象的生命周期很长，它有一个生命周期较短的对象引用，此时就可能出现Java内存泄漏。也即是说， 造成内存泄漏的原因是对象虽然可达但不是活动的。</p>
<h2><span id="避免内存泄漏的三种方式">避免内存泄漏的三种方式</span><a href="#避免内存泄漏的三种方式" class="header-anchor">#</a></h2><h3><span id="1-软引用">1. 软引用</span><a href="#1-软引用" class="header-anchor">#</a></h3><p>对于缓存，如果没有合适的策略让老的缓存项目到期就可能遇到内存泄漏。</p>
<p>我们希望缓存能用上所有的可用内存，只是在需要额外资源的时候才释放资源。 通过软可达可以实现资源的释放。</p>
<p>释放软可达对象的规则：</p>
<p>I. gc抛出OOM异常之前要尝试释放软引用</p>
<p>II. gc以LRU的顺序释放软引用</p>
<p>举例：</p>
<p>Google LoadingCache中的CacheBuilder.softValues()</p>
<h3><span id="2-弱引用">2. 弱引用</span><a href="#2-弱引用" class="header-anchor">#</a></h3><p>弱引用可以用于协助垃圾回收， 这是通用的解决方案。</p>
<p>举例： threadLocal</p>
<p><img src="http://www6v.github.io/www6vHome/memoryLeak/threadLocal.jpg" alt="threadLocal" title="threadLocal"></p>
<p>每个线程中有一个Map, 这个Map的类型是ThreadLocal.ThreadLocalMap. Map中的key为一个threadlocal实例. 这个Map的key使用了弱引用。 每个key都弱引用指向threadlocal. 当把threadlocal实例置为null以后,没有任何强引用指向threadlocal实例,所以threadlocal将会被gc回收.</p>
<p>ps: value却不能回收,因为存在一条从current thread连接过来的强引用. 只有当前thread结束以后, current thread就不会存在栈中,强引用断开, Current Thread, Map, value将全部被GC回收. 如果使用线程池，线程的强引用会长时间的存在，为了防止内存泄漏，最好的做法是将调用threadlocal的remove方法。</p>
<h3><span id="3-打开增加和关闭删除方法成对出现">3. 打开(增加)和关闭（删除）方法成对出现</span><a href="#3-打开增加和关闭删除方法成对出现" class="header-anchor">#</a></h3><p>资源申请和回收不成对会使资源没释放，造成内存泄漏。</p>
<p>举例：</p>
<p>I. 在finally块中关闭打开的资源和异常处理。</p>
<p>II. 使用try-with-resources语句（Java SE7新特性）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">try (BufferedReader br = new BufferedReader(new FileReader(path))) &#123;</span><br><span class="line">  return br.readLine();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2><span id="内存泄漏案例">内存泄漏案例</span><a href="#内存泄漏案例" class="header-anchor">#</a></h2><ol>
<li><p>&lt;&lt;实战Java虚拟机——JVM故障诊断与性能优化&gt;&gt;<br>7.2.2节中有关JDK6.0里String.substring()可能引发的内存泄漏</p>
</li>
<li><p>&lt;&lt; Effective.Java.中文版 - 第二版&gt;&gt; - 第六条 消除过期的对象引用<br>如果一个类自己管理内存，程序员就要警惕内存泄漏问题。</p>
</li>
<li><p>大量static字段引起的内存泄漏</p>
</li>
<li><p>不正确的equals()和hashCode()实现</p>
</li>
<li><p>常量字符串造成的内存泄漏</p>
</li>
</ol>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p>1 . 了解Java中的内存泄漏 作者:baeldung 译者:thornhill</p>
]]></content>
      <categories>
        <category>稳定性</category>
        <category>故障排查</category>
        <category>Java内存泄漏</category>
      </categories>
      <tags>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 字符串</title>
    <url>/www6vHomeHexo/2014/01/20/javaString/</url>
    <content><![CDATA[<p></p>

<span id="more"></span>

<img src="/www6vHomeHexo/2014/01/20/javaString/string.jpg" class title="字符串">]]></content>
      <categories>
        <category>Java基础</category>
        <category>String</category>
      </categories>
      <tags>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title>Java内存模型</title>
    <url>/www6vHomeHexo/2014/01/03/javaMemoryModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<img src="/www6vHomeHexo/2014/01/03/javaMemoryModel/memoryModel.jpg" class title="Java内存模型">

<h2><span id="参考">参考:</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li>JVM内部原理 原文作者：James D Bloom 翻译：梅小西</li>
<li>浅谈HotSpot逃逸分析 占小狼</li>
<li>class卸载、热替换和Tomcat的热部署的分析 heavensay</li>
<li>java自定义classloader引发的思考 editice</li>
<li>《深入理解java虚拟机》 第二章 周志明</li>
<li>浅谈Java String内幕 占小狼</li>
<li><a href="https://www.jianshu.com/p/8a58d8335270">JMM和底层实现原理</a>  王侦 *** 未</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>内存</category>
        <category>内存模型</category>
      </categories>
      <tags>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title>单例模式总结</title>
    <url>/www6vHomeHexo/2014/01/01/singleton/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>可以分成两类： 多线程安全和非多线程安全的单例</p>
<h2><span id="一-多线程安全">一. 多线程安全</span><a href="#一-多线程安全" class="header-anchor">#</a></h2><p>1 . initialization-on-demand holder idiom(IODH)</p>
<p>&#x2F;&#x2F; 用内部类的机制实现了延时加载和线程安全</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line"></span><br><span class="line">    private Singleton() &#123;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    private static class SingletonHolder &#123;</span><br><span class="line">        private static final Singleton INSTANCE = new Singleton();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static Singleton getInstance() &#123;</span><br><span class="line">        return SingletonHolder.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>通过枚举实现</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public enum EnumSingleton implements Serializable &#123;</span><br><span class="line">       INSTANCE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>double check模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line">    private volatile static Singleton singleton;</span><br><span class="line"></span><br><span class="line">    private Singleton() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static Singleton getSingleton() &#123;</span><br><span class="line">        if (singleton == null) &#123;</span><br><span class="line">            synchronized (Singleton.class) &#123;</span><br><span class="line">                if (singleton == null) &#123;</span><br><span class="line">                    singleton = new Singleton();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return singleton;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="二-非多线程安全">二. 非多线程安全</span><a href="#二-非多线程安全" class="header-anchor">#</a></h2><ol>
<li>lazy load模式<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line">    private static Singleton instance;</span><br><span class="line">    private Singleton ()&#123;&#125;</span><br><span class="line"></span><br><span class="line">    public static Singleton getInstance() &#123;</span><br><span class="line">    if (instance == null) &#123;</span><br><span class="line">        instance = new Singleton();</span><br><span class="line">    &#125;</span><br><span class="line">    return instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>非lazy load模式<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line">    private static Singleton instance = new Singleton();</span><br><span class="line"></span><br><span class="line">    private Singleton() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static Singleton getInstance() &#123;</span><br><span class="line">        return instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2><span id="三-应用">三. 应用</span><a href="#三-应用" class="header-anchor">#</a></h2><ol>
<li><p>系统中只有一个HttpClient单例，其他模块共享这个单例， 在高并发的情况下提高性能。</p>
</li>
<li><p>spring容器scope中的单例。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Java基础</category>
        <category>单例</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》 术语</title>
    <url>/www6vHomeHexo/2000/04/19/streamingSystemTerminology/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<table>
<thead>
<tr>
<th>术语[英文]</th>
<th>术语[中文]</th>
</tr>
</thead>
<tbody><tr>
<td>Streaming 101</td>
<td>流式处理101<br>流处理入门</td>
</tr>
<tr>
<td>Streaming</td>
<td>流处理<br>流式处理</td>
</tr>
<tr>
<td>processing time</td>
<td>处理时间</td>
</tr>
<tr>
<td>event time</td>
<td>事件时间</td>
</tr>
<tr>
<td>Watermark</td>
<td>水位线</td>
</tr>
<tr>
<td>Windows</td>
<td>窗口</td>
</tr>
<tr>
<td>Exactly-Once</td>
<td>精确一次</td>
</tr>
<tr>
<td>Sink</td>
<td>接收器</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th></th>
<th>scope</th>
</tr>
</thead>
<tbody><tr>
<td>流媒体    &lt;替换为&gt;· 流处理</td>
<td>chapter 1-5</td>
</tr>
<tr>
<td>水印  &lt;替换为&gt;  水位线</td>
<td>chapter 1-5</td>
</tr>
<tr>
<td>恰好一次  &lt;替换为&gt;   精确一次</td>
<td>chapter 1-5</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>仅一次  &lt;替换为&gt;   精确一次</td>
<td>chapter 4，5 [chapter 1 2 3 无结果]</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》- Chapter 1. Streaming 101[完整]</title>
    <url>/www6vHomeHexo/2000/03/18/streamingSystemChapter1Original/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#terminology-what-is-streaming"><strong>Terminology: What Is Streaming?</strong></a><ul>
<li><a href="#on-the-greatly-exaggerated-limitations-of-streaming"><strong>On the Greatly Exaggerated Limitations of Streaming</strong></a></li>
<li><a href="#event-time-versus-processing-time"><strong>Event Time Versus Processing Time</strong></a></li>
</ul>
</li>
<li><a href="#data-processing-patterns"><strong>Data Processing Patterns</strong></a><ul>
<li><a href="#bounded-data"><strong>Bounded Data</strong></a></li>
<li><a href="#unbounded-data-batch"><strong>Unbounded Data: Batch</strong></a></li>
<li><a href="#unbounded-data-streaming"><strong>Unbounded Data: Streaming</strong></a></li>
</ul>
</li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 19</p>
<p>Streaming data processing is a big deal in big data these days, and for good<br>reasons; among them are the following:</p>
<ul>
<li>Businesses crave ever-more timely insights into their data, and<br>switching to streaming is a good way to achieve lower latency</li>
<li>The massive, unbounded datasets that are increasingly common in<br>modern business are more easily tamed using a system designed for<br>such never-ending volumes of data.</li>
<li>Processing data as they arrive spreads workloads out more evenly<br>over time, yielding more consistent and predictable consumption of<br>resources.<br>Despite this business-driven surge of interest in streaming, streaming systems<br>long remained relatively immature compared to their batch brethren. It’s only<br>recently that the tide has swung conclusively in the other direction. In my<br>more bumptious moments, I hope that might be in small part due to the solid<br>dose of goading I originally served up in my “Streaming 101” and “Streaming<br>102” blog posts (on which the first few chapters of this book are rather<br>obviously based). But in reality, there’s also just a lot of industry interest in<br>seeing streaming systems mature and a lot of smart and active folks out there<br>who enjoy building them.<br>Even though the battle for general streaming advocacy has been, in my<br>opinion, effectively won, I’m still going to present my original arguments<br>from “Streaming 101” more or less unaltered. For one, they’re still very<br>applicable today, even if much of industry has begun to heed the battle cry.<br>And for two, there are a lot of folks out there who still haven’t gotten the<br>memo; this book is an extended attempt at getting these points across.<br>To begin, I cover some important background information that will help<br>frame the rest of the topics I want to discuss. I do this in three specific<br>sections:</li>
<li>Terminology<br>To talk precisely about complex topics requires precise definitions of<br>terms. For some terms that have overloaded interpretations in current use,<br>I’ll try to nail down exactly what I mean when I say them.</li>
<li>Capabilities<br>I remark on the oft-perceived shortcomings of streaming systems. I also<br>propose the frame of mind that I believe data processing system builders<br>need to adopt in order to address the needs of modern data consumers<br>going forward.</li>
<li>Time domains<br>I introduce the two primary domains of time that are relevant in data<br>processing, show how they relate, and point out some of the difficulties<br>these two domains impose.</li>
</ul>
<h1><span id="terminology-what-is-streaming"><strong>Terminology: What Is Streaming?</strong></span><a href="#terminology-what-is-streaming" class="header-anchor">#</a></h1><p>Before going any further, I’d like to get one thing out of the way: what is</p>
<p>streaming? The term streaming is used today to mean a variety of different</p>
<p>things (and for simplicity I’ve been using it somewhat loosely up until now),</p>
<p>which can lead to misunderstandings about what streaming really is or what</p>
<p>streaming systems are actually capable of. As a result, I would prefer to</p>
<p>define the term somewhat precisely.</p>
<p>The crux of the problem is that many things that ought to be described by</p>
<p><em>what</em> they are (unbounded data processing, approximate results, etc.), have</p>
<p>come to be described colloquially by <em>how</em> they historically have been</p>
<p>accomplished (i.e., via streaming execution engines). This lack of precision in</p>
<p>terminology clouds what streaming really means, and in some cases it</p>
<p>burdens streaming systems themselves with the implication that their</p>
<p>capabilities are limited to characteristics historically described as “streaming,”</p>
<p>such as approximate or speculative results.</p>
<p>Given that well-designed streaming systems are just as capable (technically</p>
<p>more so) of producing correct, consistent, repeatable results as any existing</p>
<p>batch engine, I prefer to isolate the term “streaming” to a very specific</p>
<p>meaning:</p>
<ul>
<li>Streaming system</li>
</ul>
<p>A type of data processing engine that is designed with infinite datasets in</p>
<p>mind.</p>
<p>If I want to talk about low-latency, approximate, or speculative results, I use</p>
<p>those specific words rather than imprecisely calling them “streaming.”</p>
<p>Precise terms are also useful when discussing the different types of data one</p>
<p>might encounter. From my perspective, there are two important (and</p>
<p>orthogonal) dimensions that define the shape of a given dataset: <em>cardinality</em></p>
<p>and <em>constitution</em>.</p>
<p>The cardinality of a dataset dictates its size, with the most salient aspect of</p>
<p>cardinality being whether a given dataset is finite or infinite. Here are the two</p>
<p>terms I prefer to use for describing the coarse cardinality in a dataset:</p>
<ul>
<li>Bounded data</li>
</ul>
<p>A type of dataset that is finite in size.</p>
<ul>
<li>Unbounded data</li>
</ul>
<p>A type of dataset that is infinite in size (at least theoretically).</p>
<p>Cardinality is important because the unbounded nature of infinite datasets</p>
<p>imposes additional burdens on data processing frameworks that consume</p>
<p>them. More on this in the next section.</p>
<p>The constitution of a dataset, on the other hand, dictates its physical</p>
<p>manifestation. As a result, the constitution defines the ways one can interact</p>
<p>with the data in question. We won’t get around to deeply examining</p>
<p>constitutions until Chapter 6, but to give you a brief sense of things, there are</p>
<p>two primary constitutions of importance:</p>
<ul>
<li>Table</li>
</ul>
<p>A holistic view of a dataset at a specific point in time. SQL systems have</p>
<p>traditionally dealt in tables.</p>
<ul>
<li>Stream</li>
</ul>
<p>An element-by-element view of the evolution of a dataset over time. The</p>
<p>MapReduce lineage of data processing systems have traditionally dealt in</p>
<p>streams.</p>
<p>We look quite deeply at the relationship between streams and tables in</p>
<p>Chapters 6, 8, and 9, and in Chapter 8 we also learn about the unifying</p>
<p>underlying concept of <em>time-varying relations</em> that ties them together. But until</p>
<p>then, we deal primarily in streams because that’s the constitution pipeline</p>
<p>developers directly interact with in most data processing systems today (both</p>
<p>batch and streaming). It’s also the constitution that most naturally embodies</p>
<p>the challenges that are unique to stream processing.</p>
<h3><span id="on-the-greatly-exaggerated-limitations-of-streaming"><strong>On the Greatly Exaggerated Limitations of Streaming</strong></span><a href="#on-the-greatly-exaggerated-limitations-of-streaming" class="header-anchor">#</a></h3><p>On that note, let’s next talk a bit about what streaming systems can and can’t</p>
<p>do, with an emphasis on can. One of the biggest things I want to get across in</p>
<p>this chapter is just how capable a well-designed streaming system can be.</p>
<p>Streaming systems have historically been relegated to a somewhat niche</p>
<p>market of providing low-latency, inaccurate, or speculative results, often in</p>
<p>conjunction with a more capable batch system to provide eventually correct</p>
<p>results; in other words, the Lambda Architecture.</p>
<p>For those of you not already familiar with the Lambda Architecture, the basic</p>
<p>idea is that you run a streaming system alongside a batch system, both</p>
<p>performing essentially the same calculation. The streaming system gives you</p>
<p>low-latency, inaccurate results (either because of the use of an approximation</p>
<p>algorithm, or because the streaming system itself does not provide</p>
<p>correctness), and some time later a batch system rolls along and provides you</p>
<p>with correct output. Originally proposed by Twitter’s Nathan Marz (creator of</p>
<p>Storm), it ended up being quite successful because it was, in fact, a fantastic</p>
<p>idea for the time; streaming engines were a bit of a letdown in the correctness</p>
<p>department, and batch engines were as inherently unwieldy as you’d expect,</p>
<p>so Lambda gave you a way to have your proverbial cake and eat it too.</p>
<p>Unfortunately, maintaining a Lambda system is a hassle: you need to build,</p>
<p>provision, and maintain two independent versions of your pipeline and then</p>
<p>also somehow merge the results from the two pipelines at the end.</p>
<p>As someone who spent years working on a strongly consistent streaming</p>
<p>engine, I also found the entire principle of the Lambda Architecture a bit</p>
<p>unsavory. Unsurprisingly, I was a huge fan of Jay Kreps’ “Questioning the</p>
<p>Lambda Architecture” post when it came out. Here was one of the first highly</p>
<p>visible statements against the necessity of dual-mode execution. Delightful.</p>
<p>Kreps addressed the issue of repeatability in the context of using a replayable</p>
<p>system like Kafka as the streaming interconnect, and went so far as to propose</p>
<p>the Kappa Architecture, which basically means running a single pipeline</p>
<p>using a well-designed system that’s appropriately built for the job at hand.</p>
<p>I’m not convinced that notion requires its own Greek letter name, but I fully</p>
<p>support the idea in principle.</p>
<p>Quite honestly, I’d take things a step further. I would argue that well-designed</p>
<p>streaming systems actually provide a strict superset of batch functionality.</p>
<p>Modulo perhaps an efficiency delta, there should be no need for batch</p>
<p>systems as they exist today. And kudos to the Apache Flink folks for taking</p>
<p>this idea to heart and building a system that’s all-streaming-all-the-time under</p>
<p>the covers, even in “batch” mode; I love it.</p>
<p><strong>BATCH AND STREAMING EFFICIENCY DIFFERENCES</strong></p>
<p>One which I propose is not an inherent limitation of streaming systems,</p>
<p>but simply a consequence of design choices made in most streaming</p>
<p>systems thus far. The efficiency delta between batch and streaming is</p>
<p>largely the result of the increased bundling and more efficient shuffle</p>
<p>transports found in batch systems. Modern batch systems go to great</p>
<p>lengths to implement sophisticated optimizations that allow for remarkable</p>
<p>levels of throughput using surprisingly modest compute resources. There’s</p>
<p>no reason the types of clever insights that make batch systems the</p>
<p>efficiency heavyweights they are today couldn’t be incorporated into a</p>
<p>system designed for unbounded data, providing users flexible choice</p>
<p>between what we typically consider to be high-latency, higher-efficiency</p>
<p>“batch” processing and low-latency, lower-efficiency “streaming”</p>
<p>processing. This is effectively what we’ve done at Google with Cloud</p>
<p>Dataflow by providing both batch and streaming runners under the same</p>
<p>unified model. In our case, we use separate runners because we happen to</p>
<p>have two independently designed systems optimized for their specific use</p>
<p>cases. Long term, from an engineering perspective, I’d love to see us</p>
<p>merge the two into a single system that incorporates the best parts of both</p>
<p>while still maintaining the flexibility of choosing an appropriate efficiency</p>
<p>level. But that’s not what we have today. And honestly, thanks to the</p>
<p>unified Dataflow Model, it’s not even strictly necessary; so it may well</p>
<p>never happen.</p>
<p>The corollary of all this is that broad maturation of streaming systems</p>
<p>combined with robust frameworks for unbounded data processing will in time</p>
<p>allow for the relegation of the Lambda Architecture to the antiquity of big</p>
<p>data history where it belongs. I believe the time has come to make this a</p>
<p>reality. Because to do so—that is, to beat batch at its own game—you really</p>
<p>only need two things:</p>
<ul>
<li>Correctness</li>
</ul>
<p>This gets you parity with batch. At the core, correctness boils down to</p>
<p>consistent storage. Streaming systems need a method for checkpointing</p>
<p>persistent state over time (something Kreps has talked about in his “Why</p>
<p>local state is a fundamental primitive in stream processing” post), and it</p>
<p>must be well designed enough to remain consistent in light of machine</p>
<p>failures. When Spark Streaming first appeared in the public big data scene</p>
<p>a few years ago, it was a beacon of consistency in an otherwise dark</p>
<p>streaming world. Thankfully, things have improved substantially since</p>
<p>then, but it is remarkable how many streaming systems still try to get by</p>
<p>without strong consistency.</p>
<p>To reiterate—because this point is important: strong consistency is</p>
<p>required for exactly-once processing,  which is required for correctness,</p>
<p>which is a requirement for any system that’s going to have a chance at</p>
<p>meeting or exceeding the capabilities of batch systems. Unless you just</p>
<p>truly don’t care about your results, I implore you to shun any streaming</p>
<p>system that doesn’t provide strongly consistent state. Batch systems don’t</p>
<p>require you to verify ahead of time if they are capable of producing</p>
<p>correct answers; don’t waste your time on streaming systems that can’t</p>
<p>meet that same bar.</p>
<p>If you’re curious to learn more about what it takes to get strong</p>
<p>consistency in a streaming system, I recommend you check out the</p>
<p>MillWheel, Spark Streaming, and Flink snapshotting papers. All three</p>
<p>spend a significant amount of time discussing consistency. Reuven will</p>
<p>dive into consistency guarantees in Chapter 5, and if you still find</p>
<p>yourself craving more, there’s a large amount of quality information on</p>
<p>this topic in the literature and elsewhere.</p>
<ul>
<li>Tools for reasoning about time</li>
</ul>
<p>This gets you beyond batch. Good tools for reasoning about time are</p>
<p>essential for dealing with unbounded, unordered data of varying event</p>
<p>time skew. An increasing number of modern datasets exhibit these</p>
<p>characteristics, and existing batch systems (as well as many streaming</p>
<p>systems) lack the necessary tools to cope with the difficulties they impose</p>
<p>(though this is now rapidly changing, even as I write this). We will spend</p>
<p>the bulk of this book explaining and focusing on various facets of this</p>
<p>point.</p>
<p>To begin with, we get a basic understanding of the important concept of</p>
<p>time domains, after which we take a deeper look at what I mean by</p>
<p>unbounded, unordered data of varying event-time skew. We then spend</p>
<p>the rest of this chapter looking at common approaches to bounded and</p>
<p>unbounded data processing, using both batch and streaming systems.</p>
<h3><span id="event-time-versus-processing-time"><strong>Event Time Versus Processing Time</strong></span><a href="#event-time-versus-processing-time" class="header-anchor">#</a></h3><p>To speak cogently about unbounded data processing requires a clear</p>
<p>understanding of the domains of time involved. Within any data processing</p>
<p>system, there are typically two domains of time that we care about:</p>
<ul>
<li>Event time</li>
</ul>
<p>This is the time at which events actually occurred.</p>
<ul>
<li>Processing time</li>
</ul>
<p>This is the time at which events are observed in the system.</p>
<p>Not all use cases care about event times (and if yours doesn’t, hooray! your</p>
<p>life is easier), but many do. Examples include characterizing user behavior</p>
<p>over time, most billing applications, and many types of anomaly detection, to</p>
<p>name a few.</p>
<p>In an ideal world, event time and processing time would always be equal,</p>
<p>with events being processed immediately as they occur. Reality is not so kind,</p>
<p>however, and the skew between event time and processing time is not only</p>
<p>nonzero, but often a highly variable function of the characteristics of the</p>
<p>underlying input sources, execution engine, and hardware. Things that can</p>
<p>affect the level of skew include the following:</p>
<ul>
<li>Shared resource limitations, like network congestion, network</li>
</ul>
<p>partitions, or shared CPU in a nondedicated environment</p>
<ul>
<li>Software causes such as distributed system logic, contention, and so</li>
</ul>
<p>on</p>
<ul>
<li>Features of the data themselves, like key distribution, variance in</li>
</ul>
<p>throughput, or variance in disorder (i.e., a plane full of people taking</p>
<p>their phones out of airplane mode after having used them offline for</p>
<p>the entire flight)</p>
<p>As a result, if you plot the progress of event time and processing time in any</p>
<p>real-world system, you typically end up with something that looks a bit like</p>
<p>the red line in Figure 1-1.</p>
<p><em>Figure 1-1. Time-domain mapping. The x-axis represents event-time completeness in the system; that is, the time X in event time up to which all data with event times less than X have been observed. The y axis represents the progress of processing time; that is, normal clock time as observed by the data</em></p>
<p><em>processing system as it executes.</em></p>
<p>In Figure 1-1, the black dashed line with slope of 1 represents the ideal, where</p>
<p>processing time and event time are exactly equal; the red line represents</p>
<p>reality. In this example, the system lags a bit at the beginning of processing</p>
<p>time, veers closer toward the ideal in the middle, and then lags again a bit</p>
<p>toward the end. At first glance, there are two types of skew visible in this</p>
<p>diagram, each in different time domains:</p>
<ul>
<li>Processing time</li>
</ul>
<p>The vertical distance between the ideal and the red line is the lag in the</p>
<p>processing-time domain. That distance tells you how much delay is</p>
<p>observed (in processing time) between when the events for a given time</p>
<p>occurred and when they were processed. This is the perhaps the more</p>
<p>natural and intuitive of the two skews.</p>
<ul>
<li>Event time</li>
</ul>
<p>The horizontal distance between the ideal and the red line is the amount of</p>
<p>event-time skew in the pipeline at that moment. It tells you how far</p>
<p>behind the ideal (in event time) the pipeline is currently.</p>
<p>In reality, processing-time lag and event-time skew at any given point in time</p>
<p>are identical; they’re just two ways of looking at the same thing. The</p>
<p>important takeaway regarding lag&#x2F;skew is this: Because the overall mapping</p>
<p>between event time and processing time is not static (i.e., the lag&#x2F;skew can</p>
<p>vary arbitrarily over time), this means that you cannot analyze your data</p>
<p>solely within the context of when they are observed by your pipeline if you</p>
<p>care about their event times (i.e., when the events actually occurred).</p>
<p>Unfortunately, this is the way many systems designed for unbounded data</p>
<p>have historically operated. To cope with the infinite nature of unbounded</p>
<p>datasets, these systems typically provide some notion of windowing the</p>
<p>incoming data. We discuss windowing in great depth a bit later, but it</p>
<p>essentially means chopping up a dataset into finite pieces along temporal</p>
<p>boundaries. If you care about correctness and are interested in analyzing your</p>
<p>data in the context of their event times, you cannot define those temporal</p>
<p>boundaries using processing time (i.e., processing-time windowing), as many</p>
<p>systems do; with no consistent correlation between processing time and event</p>
<p>time, some of your event-time data are going to end up in the wrong</p>
<p>processing-time windows (due to the inherent lag in distributed systems, the</p>
<p>online&#x2F;offline nature of many types of input sources, etc.), throwing</p>
<p>correctness out the window, as it were. We look at this problem in more detail</p>
<p>in a number of examples in the sections that follow, as well as the remainder</p>
<p>of the book.</p>
<p>Unfortunately, the picture isn’t exactly rosy when windowing by event time,</p>
<p>either. In the context of unbounded data, disorder and variable skew induce a</p>
<p>completeness problem for event-time windows: lacking a predictable</p>
<p>mapping between processing time and event time, how can you determine</p>
<p>when you’ve observed all of the data for a given event time <em>X</em>? For many real</p>
<p>world data sources, you simply can’t. But the vast majority of data processing</p>
<p>systems in use today rely on some notion of completeness, which puts them at</p>
<p>a severe disadvantage when applied to unbounded datasets.</p>
<p>I propose that instead of attempting to groom unbounded data into finite</p>
<p>batches of information that eventually become complete, we should be</p>
<p>designing tools that allow us to live in the world of uncertainty imposed by</p>
<p>these complex datasets. New data will arrive, old data might be retracted or</p>
<p>updated, and any system we build should be able to cope with these facts on</p>
<p>its own, with notions of completeness being a convenient optimization for</p>
<p>specific and appropriate use cases rather than a semantic necessity across all</p>
<p>of them.</p>
<p>Before getting into specifics about what such an approach might look like,</p>
<p>let’s finish up one more useful piece of background: common data processing</p>
<p>patterns.</p>
<h1><span id="data-processing-patterns"><strong>Data Processing Patterns</strong></span><a href="#data-processing-patterns" class="header-anchor">#</a></h1><p>At this point, we have enough background established that we can begin</p>
<p>looking at the core types of usage patterns common across bounded and</p>
<p>unbounded data processing today. We look at both types of processing and,</p>
<p>where relevant, within the context of the two main types of engines we care</p>
<p>about (batch and streaming, where in this context, I’m essentially lumping</p>
<p>microbatch in with streaming because the differences between the two aren’t</p>
<p>terribly important at this level).</p>
<h3><span id="bounded-data"><strong>Bounded Data</strong></span><a href="#bounded-data" class="header-anchor">#</a></h3><p>Processing bounded data is conceptually quite straightforward, and likely</p>
<p>familiar to everyone. In Figure 1-2, we start out on the left with a dataset full</p>
<p>of entropy. We run it through some data processing engine (typically batch,</p>
<p>though a well-designed streaming engine would work just as well), such as</p>
<p>MapReduce, and on the right side end up with a new structured dataset with</p>
<p>greater inherent value.</p>
<p><em>Figure 1-2. Bounded data processing with a classic batch engine. A finite pool of unstructured data on</em></p>
<p><em>the left is run through a data processing engine, resulting in corresponding structured data on the right.</em></p>
<p>Though there are of course infinite variations on what you can actually</p>
<p>calculate as part of this scheme, the overall model is quite simple. Much more</p>
<p>interesting is the task of processing an unbounded dataset. Let’s now look at</p>
<p>the various ways unbounded data are typically processed, beginning with the</p>
<p>approaches used with traditional batch engines and then ending up with the</p>
<p>approaches you can take with a system designed for unbounded data, such as</p>
<p>most streaming or microbatch engines.</p>
<h3><span id="unbounded-data-batch"><strong>Unbounded Data: Batch</strong></span><a href="#unbounded-data-batch" class="header-anchor">#</a></h3><p>Batch engines, though not explicitly designed with unbounded data in mind,</p>
<p>have nevertheless been used to process unbounded datasets since batch</p>
<p>systems were first conceived. As you might expect, such approaches revolve</p>
<p>around slicing up the unbounded data into a collection of bounded datasets</p>
<p>appropriate for batch processing.</p>
<p><strong>Fixed windows</strong></p>
<p>The most common way to process an unbounded dataset using repeated runs</p>
<p>of a batch engine is by windowing the input data into fixed-size windows and</p>
<p>then processing each of those windows as a separate, bounded data source</p>
<p>(sometimes also called <em>tumbling windows</em>), as in Figure 1-3. Particularly for</p>
<p>input sources like logs, for which events can be written into directory and file</p>
<p>hierarchies whose names encode the window they correspond to, this sort of</p>
<p>thing appears quite straightforward at first blush because you’ve essentially</p>
<p>performed the time-based shuffle to get data into the appropriate event-time</p>
<p>windows ahead of time.</p>
<p>In reality, however, most systems still have a completeness problem to deal</p>
<p>with (What if some of your events are delayed en route to the logs due to a</p>
<p>network partition? What if your events are collected globally and must be</p>
<p>transferred to a common location before processing? What if your events</p>
<p>come from mobile devices?), which means some sort of mitigation might be</p>
<p>necessary (e.g., delaying processing until you’re sure all events have been</p>
<p>collected or reprocessing the entire batch for a given window whenever data</p>
<p>arrive late).</p>
<p><em>Figure 1-3. Unbounded data processing via ad hoc fixed windows with a classic batch engine. An</em></p>
<p><em>unbounded dataset is collected up front into finite, fixed-size windows of bounded data that are then</em></p>
<p><em>processed via successive runs a of classic batch engine.</em></p>
<p><strong>Sessions</strong></p>
<p>This approach breaks down even more when you try to use a batch engine to</p>
<p>process unbounded data into more sophisticated windowing strategies, like</p>
<p>sessions. Sessions are typically defined as periods of activity (e.g., for a</p>
<p>specific user) terminated by a gap of inactivity. When calculating sessions</p>
<p>using a typical batch engine, you often end up with sessions that are split</p>
<p>across batches, as indicated by the red marks in Figure 1-4. We can reduce the</p>
<p>number of splits by increasing batch sizes, but at the cost of increased latency.</p>
<p>Another option is to add additional logic to stitch up sessions from previous</p>
<p>runs, but at the cost of further complexity.</p>
<p><em>Figure 1-4. Unbounded data processing into sessions via ad hoc fixed windows with a classic batch</em></p>
<p><em>engine. An unbounded dataset is collected up front into finite, fixed-size windows of bounded data that</em></p>
<p><em>are then subdivided into dynamic session windows via successive runs a of classic batch engine.</em></p>
<p>Either way, using a classic batch engine to calculate sessions is less than</p>
<p>ideal. A nicer way would be to build up sessions in a streaming manner,</p>
<p>which we look at later on.</p>
<h3><span id="unbounded-data-streaming"><strong>Unbounded Data: Streaming</strong></span><a href="#unbounded-data-streaming" class="header-anchor">#</a></h3><p>Contrary to the ad hoc nature of most batch-based unbounded data processing</p>
<p>approaches, streaming systems are built for unbounded data. As we talked</p>
<p>about earlier, for many real-world, distributed input sources, you not only find</p>
<p>yourself dealing with unbounded data, but also data such as the following:</p>
<ul>
<li>Highly unordered with respect to event times, meaning that you need</li>
</ul>
<p>some sort of time-based shuffle in your pipeline if you want to</p>
<p>analyze the data in the context in which they occurred.</p>
<ul>
<li>Of varying event-time skew, meaning that you can’t just assume</li>
</ul>
<p>you’ll always see most of the data for a given event time <em>X</em> within</p>
<p>some constant epsilon of time <em>Y</em>.</p>
<p>There are a handful of approaches that you can take when dealing with data</p>
<p>that have these characteristics. I generally categorize these approaches into</p>
<p>four groups: time-agnostic, approximation, windowing by processing time,</p>
<p>and windowing by event time.</p>
<p>Let’s now spend a little bit of time looking at each of these approaches.</p>
<p><strong>Time-agnostic</strong></p>
<p>Time-agnostic processing is used for cases in which time is essentially</p>
<p>irrelevant; that is, all relevant logic is data driven. Because everything about</p>
<p>such use cases is dictated by the arrival of more data, there’s really nothing</p>
<p>special a streaming engine has to support other than basic data delivery. As a</p>
<p>result, essentially all streaming systems in existence support time-agnostic use</p>
<p>cases out of the box (modulo system-to-system variances in consistency</p>
<p>guarantees, of course, if you care about correctness). Batch systems are also</p>
<p>well suited for time-agnostic processing of unbounded data sources by simply</p>
<p>chopping the unbounded source into an arbitrary sequence of bounded</p>
<p>datasets and processing those datasets independently. We look at a couple of</p>
<p>concrete examples in this section, but given the straightforwardness of</p>
<p>handling time-agnostic processing (from a temporal perspective at least), we</p>
<p>won’t spend much more time on it beyond that.</p>
<p><em><strong>Filtering</strong></em></p>
<p>A very basic form of time-agnostic processing is filtering, an example of</p>
<p>which is rendered in Figure 1-5. Imagine that you’re processing web traffic</p>
<p>logs and you want to filter out all traffic that didn’t originate from a specific</p>
<p>domain. You would look at each record as it arrived, see if it belonged to the</p>
<p>domain of interest, and drop it if not. Because this sort of thing depends only</p>
<p>on a single element at any time, the fact that the data source is unbounded,</p>
<p>unordered, and of varying event-time skew is irrelevant.</p>
<p><em>Figure 1-5. Filtering unbounded data. A collection of data (flowing left to right) of varying types is</em></p>
<p><em>filtered into a homogeneous collection containing a single type.</em></p>
<p><em><strong>Inner joins</strong></em></p>
<p>Another time-agnostic example is an inner join, diagrammed in Figure 1-6.</p>
<p>When joining two unbounded data sources, if you care only about the results</p>
<p>of a join when an element from both sources arrive, there’s no temporal</p>
<p>element to the logic. Upon seeing a value from one source, you can simply</p>
<p>buffer it up in persistent state; only after the second value from the other</p>
<p>source arrives do you need to emit the joined record. (In truth, you’d likely</p>
<p>want some sort of garbage collection policy for unemitted partial joins, which</p>
<p>would likely be time based. But for a use case with little or no uncompleted</p>
<p>joins, such a thing might not be an issue.)</p>
<p><em>Figure 1-6. Performing an inner join on unbounded data. Joins are produced when matching elements</em></p>
<p><em>from both sources are observed.</em></p>
<p>Switching semantics to some sort of outer join introduces the data</p>
<p>completeness problem we’ve talked about: after you’ve seen one side of the</p>
<p>join, how do you know whether the other side is ever going to arrive or not?</p>
<p>Truth be told, you don’t, so you need to introduce some notion of a timeout,</p>
<p>which introduces an element of time. That element of time is essentially a</p>
<p>form of windowing, which we’ll look at more closely in a moment.</p>
<p><strong>Approximation algorithms</strong></p>
<p>The second major category of approaches is approximation algorithms, such</p>
<p>as approximate Top-N, streaming k-means, and so on. They take an</p>
<p>unbounded source of input and provide output data that, if you squint at them,</p>
<p>look more or less like what you were hoping to get, as in Figure 1-7. The</p>
<p>upside of approximation algorithms is that, by design, they are low overhead</p>
<p>and designed for unbounded data. The downsides are that a limited set of</p>
<p>them exist, the algorithms themselves are often complicated (which makes it</p>
<p>difficult to conjure up new ones), and their approximate nature limits their</p>
<p>utility.</p>
<p><em>Figure 1-7. Computing approximations on unbounded data. Data are run through a complex algorithm,</em></p>
<p><em>yielding output data that look more or less like the desired result on the other side.</em></p>
<p>It’s worth noting that these algorithms typically do have some element of time</p>
<p>in their design (e.g., some sort of built-in decay). And because they process</p>
<p>elements as they arrive, that time element is usually processing-time based.</p>
<p>This is particularly important for algorithms that provide some sort of</p>
<p>provable error bounds on their approximations. If those error bounds are</p>
<p>predicated on data arriving in order, they mean essentially nothing when you</p>
<p>feed the algorithm unordered data with varying event-time skew. Something</p>
<p>to keep in mind.</p>
<p>Approximation algorithms themselves are a fascinating subject, but as they</p>
<p>are essentially another example of time-agnostic processing (modulo the</p>
<p>temporal features of the algorithms themselves), they’re quite straightforward</p>
<p>to use and thus not worth further attention, given our current focus.</p>
<p><strong>Windowing</strong></p>
<p>The remaining two approaches for unbounded data processing are both</p>
<p>variations of windowing. Before diving into the differences between them, I</p>
<p>should make it clear exactly what I mean by windowing, insomuch as we</p>
<p>touched on it only briefly in the previous section. Windowing is simply the</p>
<p>notion of taking a data source (either unbounded or bounded), and chopping it</p>
<p>up along temporal boundaries into finite chunks for processing. Figure 1-8</p>
<p>shows three different windowing patterns.</p>
<p><em>Figure 1-8. Windowing strategies. Each example is shown for three different keys, highlighting the</em></p>
<p><em>difference between aligned windows (which apply across all the data) and unaligned windows (which</em></p>
<p><em>apply across a subset of the data).</em></p>
<p>Let’s take a closer look at each strategy:</p>
<ul>
<li>Fixed windows (aka tumbling windows)</li>
</ul>
<p>We discussed fixed windows earlier. Fixed windows slice time into</p>
<p>segments with a fixed-size temporal length. Typically (as shown in</p>
<p>Figure 1-9), the segments for fixed windows are applied uniformly across</p>
<p>the entire dataset, which is an example of <em>aligned</em> windows. In some</p>
<p>cases, it’s desirable to phase-shift the windows for different subsets of the</p>
<p>data (e.g., per key) to spread window completion load more evenly over</p>
<p>time, which instead is an example of <em>unaligned</em> windows because they</p>
<p>vary across the data.</p>
<ul>
<li>Sliding windows (aka hopping windows)</li>
</ul>
<p>A generalization of fixed windows, sliding windows are defined by a</p>
<p>fixed length and a fixed period. If the period is less than the length, the</p>
<p>windows overlap. If the period equals the length, you have fixed</p>
<p>windows. And if the period is greater than the length, you have a weird</p>
<p>sort of sampling window that looks only at subsets of the data over time.</p>
<p>As with fixed windows, sliding windows are typically aligned, though</p>
<p>they can be unaligned as a performance optimization in certain use cases.</p>
<p>Note that the sliding windows in Figure 1-8 are drawn as they are to give</p>
<p>a sense of sliding motion; in reality, all five windows would apply across</p>
<p>the entire dataset.</p>
<ul>
<li>Sessions</li>
</ul>
<p>An example of dynamic windows, sessions are composed of sequences of</p>
<p>events terminated by a gap of inactivity greater than some timeout.</p>
<p>Sessions are commonly used for analyzing user behavior over time, by</p>
<p>grouping together a series of temporally related events (e.g., a sequence of</p>
<p>videos viewed in one sitting). Sessions are interesting because their</p>
<p>lengths cannot be defined a priori; they are dependent upon the actual data</p>
<p>involved. They’re also the canonical example of unaligned windows</p>
<p>because sessions are practically never identical across different subsets of</p>
<p>data (e.g., different users).</p>
<p>The two domains of time we discussed earlier (processing time and event</p>
<p>time) are essentially the two we care about. Windowing makes sense in both</p>
<p>domains, so let’s look at each in detail and see how they differ. Because</p>
<p>processing-time windowing has historically been more common, we’ll start</p>
<p>there.</p>
<p><em><strong>Windowing by processing time</strong></em></p>
<p>When windowing by processing time, the system essentially buffers up</p>
<p>incoming data into windows until some amount of processing time has</p>
<p>passed. For example, in the case of five-minute fixed windows, the system</p>
<p>would buffer data for five minutes of processing time, after which it would</p>
<p>treat all of the data it had observed in those five minutes as a window and</p>
<p>send them downstream for processing.</p>
<p><em>Figure 1-9. Windowing into fixed windows by processing time. Data are collected into windows based</em></p>
<p><em>on the order they arrive in the pipeline.</em></p>
<p>There are a few nice properties of processing-time windowing:</p>
<ul>
<li>It’s simple. The implementation is extremely straightforward because</li>
</ul>
<p>you never worry about shuffling data within time. You just buffer</p>
<p>things as they arrive and send them downstream when the window</p>
<p>closes.</p>
<ul>
<li>Judging window completeness is straightforward. Because the</li>
</ul>
<p>system has perfect knowledge of whether all inputs for a window</p>
<p>have been seen, it can make perfect decisions about whether a given</p>
<p>window is complete. This means there is no need to be able to deal</p>
<p>with “late” data in any way when windowing by processing time.</p>
<ul>
<li>If you’re wanting to infer information about the source <em>as it is</em></li>
</ul>
<p><em>observed</em>, processing-time windowing is exactly what you want.</p>
<p>Many monitoring scenarios fall into this category. Imagine tracking</p>
<p>the number of requests per second sent to a global-scale web service.</p>
<p>Calculating a rate of these requests for the purpose of detecting</p>
<p>outages is a perfect use of processing-time windowing.</p>
<p>Good points aside, there is one very big downside to processing-time</p>
<p>windowing: <em>if the data in question have event times associated with them,</em></p>
<p><em>those data must arrive in event-time order if the processing-time windows are</em></p>
<p><em>to reflect the reality of when those events actually happened.</em> Unfortunately,</p>
<p>event-time ordered data are uncommon in many real-world, distributed input</p>
<p>sources.</p>
<p>As a simple example, imagine any mobile app that gathers usage statistics for</p>
<p>later processing. For cases in which a given mobile device goes offline for</p>
<p>any amount of time (brief loss of connectivity, airplane mode while flying</p>
<p>across the country, etc.), the data recorded during that period won’t be</p>
<p>uploaded until the device comes online again. This means that data might</p>
<p>arrive with an event-time skew of minutes, hours, days, weeks, or more. It’s</p>
<p>essentially impossible to draw any sort of useful inferences from such a</p>
<p>dataset when windowed by processing time.</p>
<p>As another example, many distributed input sources might <em>seem</em> to provide</p>
<p>event-time ordered (or very nearly so) data when the overall system is</p>
<p>healthy. Unfortunately, the fact that event-time skew is low for the input</p>
<p>source when healthy does not mean it will always stay that way. Consider a</p>
<p>global service that processes data collected on multiple continents. If network</p>
<p>issues across a bandwidth-constrained transcontinental line (which, sadly, are</p>
<p>surprisingly common) further decrease bandwidth and&#x2F;or increase latency,</p>
<p>suddenly a portion of your input data might begin arriving with much greater</p>
<p>skew than before. If you are windowing those data by processing time, your</p>
<p>windows are no longer representative of the data that actually occurred within</p>
<p>them; instead, they represent the windows of time as the events arrived at the</p>
<p>processing pipeline, which is some arbitrary mix of old and current data.</p>
<p>What we really want in both of those cases is to window data by their event</p>
<p>times in a way that is robust to the order of arrival of events. What we really</p>
<p>want is event-time windowing.</p>
<p><em><strong>Windowing by event time</strong></em></p>
<p>Event-time windowing is what you use when you need to observe a data</p>
<p>source in finite chunks that reflect the times at which those events actually</p>
<p>happened. It’s the gold standard of windowing. Prior to 2016, most data</p>
<p>processing systems in use lacked native support for it (though any system</p>
<p>with a decent consistency model, like Hadoop or Spark Streaming 1.x, could</p>
<p>act as a reasonable substrate for building such a windowing system). I’m</p>
<p>happy to say that the world of today looks very different, with multiple</p>
<p>systems, from Flink to Spark to Storm to Apex, natively supporting event</p>
<p>time windowing of some sort.</p>
<p>Figure 1-10 shows an example of windowing an unbounded source into one</p>
<p>hour fixed windows.</p>
<p><em>Figure 1-10. Windowing into fixed windows by event time. Data are collected into windows based on</em></p>
<p><em>the times at which they occurred. The black arrows call out example data that arrived in processing</em></p>
<p><em>time windows that differed from the event-time windows to which they belonged.</em></p>
<p>The black arrows in Figure 1-10 call out two particularly interesting pieces of</p>
<p>data. Each arrived in processing-time windows that did not match the event</p>
<p>time windows to which each bit of data belonged. As such, if these data had</p>
<p>been windowed into processing-time windows for a use case that cared about</p>
<p>event times, the calculated results would have been incorrect. As you would</p>
<p>expect, event-time correctness is one nice thing about using event-time</p>
<p>windows.</p>
<p>Another nice thing about event-time windowing over an unbounded data</p>
<p>source is that you can create dynamically sized windows, such as sessions,</p>
<p>without the arbitrary splits observed when generating sessions over fixed</p>
<p>windows (as we saw previously in the sessions example from “Unbounded</p>
<p>Data: Streaming”), as demonstrated in Figure 1-11.</p>
<p><em>Figure 1-11. Windowing into session windows by event time. Data are collected into session windows</em></p>
<p><em>capturing bursts of activity based on the times that the corresponding events occurred. The black</em></p>
<p><em>arrows again call out the temporal shuffle necessary to put the data into their correct event-time</em></p>
<p><em>locations.</em></p>
<p>Of course, powerful semantics rarely come for free, and event-time windows</p>
<p>are no exception. Event-time windows have two notable drawbacks due to the</p>
<p>fact that windows must often live longer (in processing time) than the actual</p>
<p>length of the window itself:</p>
<ul>
<li>Buffering</li>
</ul>
<p>Due to extended window lifetimes, more buffering of data is required.</p>
<p>Thankfully, persistent storage is generally the cheapest of the resource</p>
<p>types most data processing systems depend on (the others being primarily</p>
<p>CPU, network bandwidth, and RAM). As such, this problem is typically</p>
<p>much less of a concern than you might think when using any well</p>
<p>designed data processing system with strongly consistent persistent state</p>
<p>and a decent in-memory caching layer. Also, many useful aggregations do</p>
<p>not require the entire input set to be buffered (e.g., sum or average), but</p>
<p>instead can be performed incrementally, with a much smaller,</p>
<p>intermediate aggregate stored in persistent state.</p>
<ul>
<li>Completeness</li>
</ul>
<p>Given that we often have no good way of knowing when we’ve seen all of</p>
<p>the data for a given window, how do we know when the results for the</p>
<p>window are ready to materialize? In truth, we simply don’t. For many</p>
<p>types of inputs, the system can give a reasonably accurate heuristic</p>
<p>estimate of window completion via something like the watermarks found</p>
<p>in MillWheel, Cloud Dataflow, and Flink (which we talk about more in</p>
<p>Chapters 3 and 4). But for cases in which absolute correctness is</p>
<p>paramount (again, think billing), the only real option is to provide a way</p>
<p>for the pipeline builder to express when they want results for windows to</p>
<p>be materialized and how those results should be refined over time.</p>
<p>Dealing with window completeness (or lack thereof) is a fascinating topic</p>
<p>but one perhaps best explored in the context of concrete examples, which</p>
<p>we look at next.</p>
<h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>Whew! That was a lot of information. If you’ve made it this far, you are to be</p>
<p>commended! But we are only just getting started. Before forging ahead to</p>
<p>looking in detail at the Beam Model approach, let’s briefly step back and</p>
<p>recap what we’ve learned so far. In this chapter, we’ve done the following:</p>
<ul>
<li>Clarified terminology, focusing the definition of “streaming” to refer</li>
</ul>
<p>to systems built with unbounded data in mind, while using more</p>
<p>descriptive terms like approximate&#x2F;speculative results for distinct</p>
<p>concepts often categorized under the “streaming” umbrella.</p>
<p>Additionally, we highlighted two important dimensions of large</p>
<p>scale datasets: cardinality (i.e., bounded versus unbounded) and</p>
<p>encoding (i.e., table versus stream), the latter of which will consume</p>
<p>much of the second half of the book.</p>
<ul>
<li>Assessed the relative capabilities of well-designed batch and</li>
</ul>
<p>streaming systems, positing streaming is in fact a strict superset of</p>
<p>batch, and that notions like the Lambda Architecture, which are</p>
<p>predicated on streaming being inferior to batch, are destined for</p>
<p>retirement as streaming systems mature.</p>
<ul>
<li>Proposed two high-level concepts necessary for streaming systems to</li>
</ul>
<p>both catch up to and ultimately surpass batch, those being</p>
<p>correctness and tools for reasoning about time, respectively.</p>
<ul>
<li>Established the important differences between event time and</li>
</ul>
<p>processing time, characterized the difficulties those differences</p>
<p>impose when analyzing data in the context of when they occurred,</p>
<p>and proposed a shift in approach away from notions of completeness</p>
<p>and toward simply adapting to changes in data over time.</p>
<ul>
<li>Looked at the major data processing approaches in common use</li>
</ul>
<p>today for bounded and unbounded data, via both batch and streaming</p>
<p>engines, roughly categorizing the unbounded approaches into: time</p>
<p>agnostic, approximation, windowing by processing time, and</p>
<p>windowing by event time.</p>
<p>Next up, we dive into the details of the Beam Model, taking a conceptual look</p>
<p>at how we’ve broken up the notion of data processing across four related</p>
<p>axes: what, where, when, and how. We also take a detailed look at processing</p>
<p>a simple, concrete example dataset across multiple scenarios, highlighting the</p>
<p>plurality of use cases enabled by the Beam Model, with some concrete APIs</p>
<p>to ground us in reality. These examples will help drive home the notions of</p>
<p>event time and processing time introduced in this chapter while additionally</p>
<p>exploring new concepts such as watermarks.</p>
<ol>
<li>For completeness, it’s perhaps worth calling out that this definition includes</li>
</ol>
<p>both true streaming as well as microbatch implementations. For those of you</p>
<p>who aren’t familiar with microbatch systems, they are streaming systems that</p>
<p>use repeated executions of a batch processing engine to process unbounded</p>
<p>data. Spark Streaming is the canonical example in the industry.</p>
<ol>
<li>Readers familiar with my original “Streaming 101” article might recall that I</li>
</ol>
<p>rather emphatically encouraged the abandonment of the term “stream” when</p>
<p>referring to datasets. That never caught on, which I initially thought was due</p>
<p>to its catchiness and pervasive existing usage. In retrospect, however, I think I</p>
<p>was simply wrong. There actually is great value in distinguishing between the</p>
<p>two different types of dataset constitutions: tables and streams. Indeed, most</p>
<p>of the second half of this book is dedicated to understanding the relationship</p>
<p>between those two.</p>
<ol>
<li>If you’re unfamiliar with what I mean when I say <em>exactly-once</em>, it’s referring</li>
</ol>
<p>to a specific type of consistency guarantee that certain data processing</p>
<p>frameworks provide. Consistency guarantees are typically bucketed into three</p>
<p>main classes: at-most-once processing, at-least-once processing, and exactly</p>
<p>once processing. Note that the names in use here refer to the effective</p>
<p>semantics as observed within the outputs generated by the pipeline, not the</p>
<p>actual number of times a pipeline might process (or attempt to process) any</p>
<p>given record. For this reason, the term <em>effectively-once</em> is sometimes used</p>
<p>instead of exactly-once, since it’s more representative of the underlying</p>
<p>nature of things. Reuven covers these concepts in much more detail in</p>
<p>Chapter 5.</p>
<ol>
<li>Since the original publication of “Streaming 101,” numerous individuals</li>
</ol>
<p>have pointed out to me that it would have been more intuitive to place</p>
<p>processing time on the x-axis and event time on the y-axis. I do agree that</p>
<p>swapping the two axes would initially feel more natural, as event time seems</p>
<p>like the dependent variable to processing time’s independent variable.</p>
<p>However, because both variables are monotonic and intimately related,</p>
<p>they’re effectively interdependent variables. So I think from a technical</p>
<p>perspective you just have to pick an axis and stick with it. Math is confusing</p>
<p>(especially outside of North America, where it suddenly becomes plural and</p>
<p>gangs up on you).</p>
<ol>
<li>This result really shouldn’t be surprising (but was for me, hence why I’m</li>
</ol>
<p>pointing it out), because we’re effectively creating a right triangle with the</p>
<p>ideal line when measuring the two types of skew&#x2F;lag. Maths are cool.</p>
<ol>
<li>We look at aligned fixed windows in detail in Chapter 2, and unaligned</li>
</ol>
<p>fixed windows in Chapter 4.</p>
<ol>
<li>If you poke around enough in the academic literature or SQL-based</li>
</ol>
<p>streaming systems, you’ll also come across a third windowing time domain:</p>
<p><em>tuple-based windowing</em> (i.e., windows whose sizes are counted in numbers of</p>
<p>elements). However, tuple-based windowing is essentially a form of</p>
<p>processing-time windowing in which elements are assigned monotonically</p>
<p>increasing timestamps as they arrive at the system. As such, we won’t discuss</p>
<p>tuple-based windowing in detail any further.</p>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》- 第一章：流处理入门[完整]</title>
    <url>/www6vHomeHexo/2000/03/18/streamingSystemChapter1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%AF%E8%AF%AD%E6%B5%81%E5%A4%84%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88">术语：流处理是什么？ |</a><ul>
<li><a href="#%E5%85%B3%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E7%9A%84%E4%B8%A5%E9%87%8D%E5%A4%B8%E5%A4%A7%E9%99%90%E5%88%B6">关于流处理的严重夸大限制</a></li>
<li><a href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4event-time%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4processing-time">事件时间(<strong>Event Time</strong>)与处理时间(<strong>Processing Time</strong>)</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A8%A1%E5%BC%8F">数据处理模式</a><ul>
<li><a href="#%E6%9C%89%E7%95%8C%E6%95%B0%E6%8D%AE">有界数据</a></li>
<li><a href="#%E6%97%A0%E7%95%8C%E6%95%B0%E6%8D%AE%E6%89%B9%E5%A4%84%E7%90%86">无界数据：批处理</a></li>
<li><a href="#%E6%97%A0%E7%95%8C%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86">无界数据：流处理</a><ul>
<li><a href="#%E6%97%B6%E9%97%B4%E4%B8%8D%E6%95%8F%E6%84%9F">时间不敏感</a></li>
<li><a href="#%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95">近似算法</a></li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%8C%96">窗口化</a></li>
<li><a href="#%E6%8C%89%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E5%8C%96"><em>按处理时间窗口化</em></a></li>
<li><a href="#%E6%8C%89%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E5%8C%96"><em>按事件时间窗口化</em></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li>
</ul>
<!-- tocstop -->

</div>



<p>流式数据处理在大数据领域日益受到重视，原因如下：</p>
<ul>
<li>企业需要更及时的数据洞察，转向流式处理是实现更低延迟的好方法</li>
<li>现代企业中越来越普遍的海量无界数据集，更容易使用针对这种不断增长的数据量设计的系统进行处理。</li>
<li>随着数据的到达逐步处理，将工作负载均衡，积极消耗资源，从而产生更一致和可预测的结果。</li>
</ul>
<p>尽管业务驱动的流式处理浪潮已经涌现，但与其批处理兄弟相比，流式处理系统仍然相对不成熟。直到最近，潮水才彻底向另一个方向倾斜。在我更加自大的时刻，我希望这在某种程度上是由于我最初在“流式处理101”和“流式处理102”博客文章中提供的坚实鞭策（本书的前几章明显基于这些文章）。但事实上，业界对流式处理系统成熟的兴趣很大，还有很多聪明而积极的人喜欢建立这些系统。</p>
<p>尽管我认为流式处理的普遍倡导已经取得了胜利，但我仍然会以几乎不改变的方式呈现我在“流式处理101”中提出的最初论点。首先，即使行业已经开始听从号召，这些论点仍然非常适用于今天。其次，仍有很多人没有收到这个备忘录；本书是一个长期的尝试，希望能够传达这些要点。</p>
<p>首先，我会涵盖一些重要的背景信息，这将有助于为我想讨论的其余主题提供框架。我将在三个具体的部分中进行：</p>
<ul>
<li>术语<br>要精确地讨论复杂的主题，需要准确定义术语。对于某些具有超载解释的术语，我将尝试明确我所说的意思。</li>
<li>功能<br>我评论了流处理系统经常被认为存在的缺点。我还提出了我认为数据处理系统构建者需要采用的思维模式，以满足现代数据消费者的需求。</li>
<li>时间领域<br>我介绍了数据处理中相关的两个主要时间领域，展示它们的关系，并指出这两个领域所带来的一些困难。</li>
</ul>
<h1><span id="术语流处理是什么">术语：流处理是什么？ |</span><a href="#术语流处理是什么" class="header-anchor">#</a></h1><p>在进一步讨论之前，我想要澄清一件事：什么是流处理？如今，流处理这个术语被用来指代许多不同的事物（为了简便，到目前为止我也比较随意地使用了这个词），这可能会导致对流处理的真正含义或流处理系统实际能力的误解。因此，我更喜欢对这个术语进行一些明确的定义。</p>
<p>问题的关键在于，许多应该通过描述它们的“本质”（无限数据处理、近似结果等）来描述的内容，已经被俗称为通过流处理执行引擎等方式进行的操作。这种术语上的不精确使得流处理的真正含义变得模糊，而且在某些情况下，还会给流处理系统自身带来一些负担，即它们的能力被视为仅限于历史上被描述为“流处理”的特征，例如近似或推测结果。</p>
<p>鉴于设计良好的流处理系统与任何现有的批处理引擎一样有能力（在技术上甚至更有能力）产生正确、一致、可重复的结果，我更喜欢将“流处理”这个术语限定为一个非常具体的含义：</p>
<ul>
<li><p>流处理系统</p>
<p>一种专门针对无限数据集设计的数据处理引擎。</p>
</li>
</ul>
<p>如果我想谈论低延迟、近似或推测结果，我会使用那些具体的词汇，而不是不精确地称它们为“流处理”。</p>
<p>在讨论可能遇到的不同类型的数据时，使用准确的术语也是很有用的。在我看来，有两个重要（且正交的）维度定义了给定数据集的形状：基数和构成。</p>
<p>数据集的基数决定了它的大小，基数的最突出的方面是给定数据集是否有限或无限。这里是我用来描述数据集中粗略基数的两个术语：</p>
<ul>
<li><p>有界数据<br>一种有限大小的数据集合。</p>
</li>
<li><p>无界数据<br>一种无限大小（至少在理论上）的数据集合。</p>
</li>
</ul>
<p>基数很重要，因为无限数据集的无限本质会对处理它们的数据处理框架造成额外的负担。下一节中会详细讨论这个问题。</p>
<p>另一方面，数据集的构成则决定了它的物理表现形式。因此，构成定义了人们可以与所讨论的数据交互的方式。我们将在第6章深入研究构成，但为了让你对事情有一个简要的了解，这里有两个主要的重要构成：</p>
<ul>
<li><p>表格<br>数据集在特定时间点的整体视图。传统上，SQL系统处理表格。</p>
</li>
<li><p>流<br>数据集随时间演变的逐个元素视图。MapReduce数据处理系统的传承传统上处理流。</p>
</li>
</ul>
<p>我们将在第6、8和9章深入探讨流和表格之间的关系，并且在第8章中，我们还会学习到将它们联系在一起的统一基本概念——时间可变关系。但在那之前，我们主要处理流，因为它是大多数数据处理系统（批处理和流处理）中管道开发人员直接交互的构成。它也是最自然地体现流处理所独有的挑战的构成。</p>
<h3><span id="关于流处理的严重夸大限制">关于流处理的严重夸大限制</span><a href="#关于流处理的严重夸大限制" class="header-anchor">#</a></h3><p>在这个话题上，让我们稍微谈谈流处理系统可以做什么和不能做什么，重点在于可以。我在本章中最想表达的一件事是，精心设计的流处理系统可以有多么强大。历史上，流处理系统一直被限制在提供低延迟、不准确或具有推测性的结果的某种利基市场上，通常与更有能力的批处理系统一起提供最终正确的结果；换句话说，这就是 Lambda 架构。</p>
<p>对于那些不熟悉 Lambda 架构的人，基本思路是同时运行一个流处理系统和一个批处理系统，两者都执行基本相同的计算。流处理系统提供低延迟、不准确的结果（要么是因为使用了近似算法，要么是因为流处理系统本身没有提供正确性），一段时间后，批处理系统会提供正确的输出。最初由 Twitter 的 Nathan Marz（Storm 的创作者）提出，它最终变得非常成功，因为它实际上是一个很棒的想法；流处理引擎在正确性方面有些令人失望，而批处理引擎与您所期望的一样不稳定，因此 Lambda 让您可以同时拥有两者的优点。不幸的是，维护 Lambda 系统很麻烦：您需要构建、提供和维护两个独立版本的管道，然后还要以某种方式合并两个管道的结果。</p>
<p>作为一位在强一致性流处理引擎上工作多年的人，我发现 Lambda 架构的整个原则有些不受欢迎。不出所料，当 Jay Kreps 的“质疑 Lambda 架构”文章出现时，我非常喜欢。这是对双模式执行必要性的首批高度可见声明。Kreps 在使用可回放系统（如 Kafka）作为流处理互联的情况下解决了可重复性问题，甚至提出了 Kappa 架构，这基本上意味着使用一个适合手头工作的精心设计的系统来运行单个管道。我不确定这个观点是否需要自己的希腊字母名称，但我完全支持这个原则。</p>
<p>说实话，我会更进一步。我会认为，精心设计的流处理系统实际上提供了批处理功能的严格超集。除了效率差异之外，批处理系统就像今天所存在的那样，不需要流处理系统。对于 Apache Flink 团队将这个想法贯彻到底并构建一个在“批处理”模式下始终是全流处理的系统，我表示赞赏。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">                    **批处理和流处理的效率差异**</span><br><span class="line">                    </span><br><span class="line">我认为这不是流处理系统的固有限制，而只是迄今为止大多数流处理系统所做的设计选择的结果。批处理和流处理之间的效率差异在很大程度上是批处理系统中更高效的捆绑和更有效的洗牌传输的结果。现代批处理系统采取了各种复杂的优化措施，允许使用令人惊讶的适度计算资源实现卓越的吞吐量。没有理由不能将使批处理系统成为效率重量级选手的那些聪明的洞察力类型纳入到为无界数据设计的系统中，为用户提供通常认为是高延迟、高效率的“批处理”处理和低延迟、低效率的“流处理”处理之间的灵活选择。这实际上是我们在 Google Cloud Dataflow 上所做的，通过在同一统一模型下提供批处理和流处理运行程序来实现。在我们的情况下，我们使用单独的运行程序，因为我们恰好有两个独立设计的系统，针对其特定的用例进行了优化。从工程角度来看，长期来看，我希望看到我们将两个系统合并成一个系统，同时保持选择适当效率级别的灵活性。但这不是我们今天拥有的东西。而且，由于统一的 Dataflow 模型，这甚至不是严格必要的；因此，它可能永远不会发生。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>所有这些的推论是，流处理系统的广泛成熟，加上强大的无界数据处理框架，将在时间上允许将 Lambda 架构降级到大数据历史的古董中，它所属的地方。我相信现在是时候将其变为现实了。因为要做到这一点-也就是在其自己的游戏中击败批处理-您只需要两件事：</p>
<ul>
<li><p>正确性<br>这可以使您与批处理平起平坐。在核心层面上，正确性归结为一致性存储。流处理系统需要一种方法来随着时间检查点持久状态（Kreps 在他的“为什么本地状态是流处理的基本原语”文章中谈到了这一点），并且必须设计得足够好，以在机器故障的情况下保持一致。几年前，当 Spark Streaming 第一次出现在公共大数据场景中时，它是一束光明中的一束一致性。幸运的是，从那时起，事情已经大大改善，但仍然令人惊讶的是，有多少流处理系统仍试图在没有强一致状态的情况下运行。</p>
<p>重申一下-因为这一点很重要：强一致性对于精准一次处理是必需的，这对于任何具有超越批处理系统能力的机会的系统都是必需的。除非您真的不关心结果，否则请避免使用不提供强一致状态的流处理系统。批处理系统不需要您提前验证它们是否能够产生正确的答案；不要浪费时间在无法达到同样要求的流处理系统上。</p>
<p>如果您想了解有关在流处理系统中实现强一致性所需的内容，我建议您查看 MillWheel、Spark Streaming 和 Flink 快照论文。所有这三个文件都花费了大量时间讨论一致性。Reuven 将在第 5 章深入探讨一致性保证，如果您仍然渴望更多，那么在文献和其他地方有大量关于这个话题的优质信息。</p>
</li>
<li><p>关于时间的推理工具</p>
<p>这可以让您超越批处理。对时间进行推理的良好工具对于处理不同事件时间偏移的无界、无序数据至关重要。越来越多的现代数据集呈现这些特征，而现有的批处理系统（以及许多流处理系统）缺乏应对它们所施加的困难的必要工具（尽管这正在迅速改变，即使在我写这篇文章时）。我们将在本书的大部分内容中解释和关注该点的各个方面。</p>
<p>首先，我们对时间域的重要概念有了基本的理解，然后深入探讨了我所说的具有不同事件时间偏移的无界、无序数据。然后，我们将在本章的其余部分中使用批处理和流处理系统介绍有关有界和无界数据处理的常见方法。</p>
</li>
</ul>
<h3><span id="事件时间event-time与处理时间processing-time">事件时间(<strong>Event Time</strong>)与处理时间(<strong>Processing Time</strong>)</span><a href="#事件时间event-time与处理时间processing-time" class="header-anchor">#</a></h3><p>要清晰地谈论无限数据处理，需要对所涉及的时间领域有清晰的理解。在任何数据处理系统中，通常有两个我们关心的时间领域：</p>
<ul>
<li><p>事件时间(Event time)<br>这是事件实际发生的时间。</p>
</li>
<li><p>处理时间(Processing time)<br>这是事件在系统中被观察到的时间。</p>
</li>
</ul>
<p>并非所有用例都关心事件时间（如果你的用例不关心，那太好了！你的生活就更轻松），但许多用例都关心。例如，随时间表征用户行为、大多数计费应用以及许多类型的异常检测等。</p>
<p>在理想的世界中，事件时间和处理时间总是相等的，事件会在其发生时立即被处理。然而，现实并不是那么友好，事件时间和处理时间之间的偏差不仅非零，而且通常是与底层输入源、执行引擎和硬件特性的特征高度可变的函数。会影响偏差水平的因素包括以下内容：</p>
<ul>
<li>共享资源限制，例如网络拥塞、网络分区或非专用环境中的共享CPU</li>
<li>软件原因，例如分布式系统逻辑、争用等</li>
<li>数据本身的特征，例如键分布、吞吐量方差或无序方差（即，在整个航班中使用脱机模式的人们将其手机从飞行模式中取出后）</li>
</ul>
<p>因此，如果在任何现实世界的系统中绘制事件时间和处理时间的进度，通常会得到类似于图1-1中红线的东西。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0101.png" class>  
<p><em>图1-1. 时间域映射。x轴表示系统中事件时间的完整性；即，事件时间为X的所有数据的事件时间小于X。y轴表示处理时间的进度；即，数据处理系统在执行时观察到的正常时钟时间。</em></p>
<p>在图1-1中，黑色的斜率为1的虚线代表理想状态，即处理时间和事件时间完全相等；红线代表现实。在本例中，系统在处理时间开始时稍有延迟，中间更接近理想，然后在结束时又稍有延迟。乍一看，此图中可见两种类型的偏差，每种偏差在不同的时间域内：</p>
<ul>
<li><p>处理时间(Processing time)<br>理想状态和红线之间的垂直距离是处理时间领域中的延迟。该距离告诉你在事件发生时间时，给定时间的事件的处理时间延迟有多少（在处理时间上）。这是两种偏差中可能更自然和直观的偏差。</p>
</li>
<li><p>事件时间(Event time)<br>理想状态和红线之间的水平距离是通道中事件时间偏差的量。它告诉你通道当前距理想状态（在事件时间上）有多远。</p>
</li>
</ul>
<p>在现实中，任何给定时间点的处理时间延迟和事件时间偏差是相同的；它们只是看同一件事情的两种方式。关于延迟&#x2F;偏差的重要经验教训是：由于事件时间和处理时间之间的总体映射不是静态的（即，延迟&#x2F;偏差可以任意随时间变化），这意味着如果你关心它们的事件时间（即事件实际发生的时间），那么你不能仅在你的管道观察到它们时分析你的数据。</p>
<p>不幸的是，这是历史上许多为无限数据设计的系统所操作的方式。为了应对无限数据集的无限性，这些系统通常提供一些关于传入数据的窗口的概念。我们稍后会深入讨论窗口，但它基本上意味着将数据集沿时间边界分成有限的数据块。如果你关心正确性并且有兴趣在事件时间的上下文中分析你的数据，你不能使用处理时间定义那些时间边界（即处理时间窗口），因为许多系统这样做；由于分布式系统的固有滞后性、许多类型的输入源的在线&#x2F;离线性等，处理时间和事件时间之间没有一致的相关性，因此你的事件时间数据中的一些数据将会落入错误的处理时间窗口内，从而使正确性丢失。我们将在接下来的部分以及本书的其余部分中通过许多示例来详细探讨这个问题。</p>
<p>不幸的是，当按事件时间进行窗口化时，情况并不完全乐观。在无限数据的上下文中，无序和可变的偏差会为事件时间窗口带来完整性问题：由于处理时间和事件时间之间缺乏可预测的映射，你如何确定何时观察到给定事件时间X的所有数据？对于许多现实世界的数据源，你根本无法做到这一点。但是，今天使用的绝大多数数据处理系统都依赖于某种完整性的概念，这使它们在应用于无限数据集时处于严重劣势。</p>
<p>我建议，我们应该设计一些工具，允许我们生活在这些复杂数据集所施加的不确定性世界中，而不是试图将无限数据整理成最终变得完整的有限批次的信息。新数据将到达，旧数据可能会被撤回或更新，任何我们构建的系统都应该能够自己应对这些事实，完整性的概念只是特定和适当的用例的方便优化，而不是跨所有用例的语义必要性。</p>
<p>在探讨这种方法可能看起来像什么之前，让我们完成一个有用的背景：常见的数据处理模式。</p>
<h1><span id="数据处理模式">数据处理模式</span><a href="#数据处理模式" class="header-anchor">#</a></h1><p>至此，我们已经有足够的背景知识，可以开始研究跨界和未界限数据处理今天常见的核心使用模式。我们看一下两种类型的处理，并在相关情况下，在我们关心的两种主要引擎的背景下（批处理和流处理，在这种情况下，我基本上将微批处理与流处理混合在一起，因为这两者之间的差异在这个层次上并不重要）。</p>
<h3><span id="有界数据">有界数据</span><a href="#有界数据" class="header-anchor">#</a></h3><p>处理有界数据在概念上非常简单，可能对每个人都很熟悉。在图1-2中，我们从左边开始，有一个充满熵的数据集。我们将其通过一些数据处理引擎（通常是批处理，但是一个设计良好的流处理引擎也可以）运行，例如MapReduce，在右侧得到具有更高内在价值的新结构化数据集。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0102.png" class>
<p><em>图1-2。使用经典批处理引擎的有界数据处理。在左侧是一组未结构化的有限数据池，经过数据处理引擎运行后，在右侧是相应的结构化数据。</em></p>
<p>尽管在实际计算中，你可以对此方案进行无限变化，但总体模型非常简单。更有趣的是处理未界限数据的任务。现在让我们看一下通常处理未界限数据的各种方式，从传统批处理引擎使用的方法开始，然后结束于可以用于未界限数据的系统的方法，例如大多数流处理或微批处理引擎。</p>
<h3><span id="无界数据批处理">无界数据：批处理</span><a href="#无界数据批处理" class="header-anchor">#</a></h3><p>虽然批处理引擎并没有专门设计用于未界限数据，但自从批处理系统首次被构思以来，它们一直被用于处理未界限数据集。正如你所期望的那样，这样的方法围绕将未界限数据切片成适合批处理的一组有界数据集展开。</p>
<p><strong>固定窗口</strong></p>
<p>使用批处理引擎的重复运行最常见的处理未界限数据集的方法是将输入数据分成固定大小的窗口，然后将每个窗口作为单独的有界数据源（有时也称为<em>滚动窗口</em>）进行处理，如图1-3所示。特别是对于像日志这样的输入源，对于这样一个源，事件可以写入目录和文件层次结构中，其名称编码了它们对应的窗口，这种处理方式一开始似乎非常简单，因为你本质上已经通过基于时间的洗牌将数据提前放入适当的事件时间窗口中。</p>
<p>然而，实际上，大多数系统仍然需要处理完整性问题（如果一些事件由于网络分区而延迟到日志中怎么办？如果事件是全球收集的，并且必须在处理之前传输到共同的位置？如果事件来自移动设备？），这意味着可能需要某种缓解措施（例如，推迟处理，直到你确定收集到所有事件或在数据到达后重新处理给定窗口的整个批次）。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0103.png" class>
<p><em>图1-3。使用经典批处理引擎通过自适应固定窗口处理未界限数据。一组未界限数据通过前置收集成为有限的固定大小的有界数据窗口，然后通过经典批处理引擎的连续运行进行处理。</em></p>
<p><strong>会话</strong></p>
<p>当尝试使用批处理引擎将未界限数据处理成更复杂的窗口策略（如会话）时，这种方法会更加崩溃。会话通常被定义为活动期间（例如，对于特定用户）在不活动期间终止。在使用典型批处理引擎计算会话时，您通常会得到跨批次拆分的会话，如图1-4中的红色标记所示。我们可以通过增加批次大小来减少拆分的数量，但代价是增加延迟。另一个选择是添加额外的逻辑来从之前的运行中拼接会话，但代价是增加更多的复杂性。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0104.png" class>
<p><em>图1-4。使用经典批处理引擎通过自适应固定窗口将未界限数据处理为会话。未界限数据通过前置收集成为有限的固定大小的有界数据窗口，然后通过经典批处理引擎的连续运行进行处理，再被细分为动态会话窗口。</em></p>
<p>无论哪种方式，使用经典批处理引擎计算会话都不理想。更好的方法是以流方式构建会话，我们稍后再看。</p>
<h3><span id="无界数据流处理">无界数据：流处理</span><a href="#无界数据流处理" class="header-anchor">#</a></h3><p>与大多数基于批处理的无限数据处理的临时性质不同，流处理系统是为无限数据而建立的。正如我们之前所讨论的，对于许多真实世界的分布式输入来源，你不仅需要处理无限数据，还需要处理以下类型的数据：</p>
<ul>
<li>与事件时间高度无序，这意味着如果你想在事件发生的上下文中分析数据，你需要在你的管道中进行某种基于时间的随机洗牌。</li>
<li>具有不同的事件时间偏移，这意味着你不能仅仅假设对于一个给定的事件时间<em>X</em>，你总是能在一定的时间<em>Y</em>内看到大部分数据。</li>
</ul>
<p>当处理这些特征的数据时，你可以采取一些方法。我通常将这些方法分为四类：<strong>时间不敏感、近似、按处理时间分窗口和按事件时间分窗口</strong>。</p>
<p>现在让我们花一点时间来看一下这些方法。</p>
<h5><span id="时间不敏感">时间不敏感</span><a href="#时间不敏感" class="header-anchor">#</a></h5><p>时间不敏感处理用于时间基本无关的情况；也就是说，所有相关的逻辑都由数据驱动。因为这些用例的一切都是由更多数据的到来所决定的，所以流引擎需要支持的除了基本的数据传递之外，实际上没有什么特别之处。因此，实际上所有现有的流处理系统都支持时间不敏感的用例（当然，如果你关心正确性，系统之间的一致性保证可能存在差异）。对于无限数据源的时间不敏感处理，批处理系统也非常适用，只需要将无限源切割成任意顺序的有界数据集并独立处理这些数据集即可。我们在这一部分中看一些具体的例子，但是由于处理时间不敏感（至少从时间的角度来看）的简单性，我们不会花太多时间进行详细讨论。</p>
<p><em><strong>过滤</strong></em></p>
<p>时间不敏感处理的一个非常基本的形式是过滤，如图1-5所示的例子。想象一下，你正在处理网站流量日志，你想要过滤掉所有不来自特定域名的流量。当每个记录到达时，你会查看它是否属于感兴趣的域名，如果不属于，则删除它。因为这种处理方式只取决于任何时间的单个元素，所以数据源是无限的、无序的和具有不同事件时间偏移是不相关的。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0105.png" class>
<p><em>图1-5。过滤无限数据。一个包含多种类型的数据集（从左到右流动）被过滤成一个包含单个类型的同质集合。</em></p>
<p><em><strong>内连接</strong></em></p>
<p>另一个时间不敏感的例子是内连接，如图1-6所示。当连接两个无限数据源时，如果你只关心两个源的元素到达时的连接结果，那么逻辑中就没有时间元素。当看到一个值来自一个源时，你可以简单地将其缓冲在持久状态中；只有另一个源的第二个值到达后，你才需要发出连接记录。（实际上，你可能需要为未发出的部分连接引入某种垃圾回收策略，这可能是基于时间的。但是对于没有或几乎没有未完成连接的用例来说，这种情况可能不是问题。）</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0106.png" class>
<p><em>图1-6。对无限数据执行内连接。当观察到两个源的匹配元素时，将产生连接。</em></p>
<p>将语义切换到某种外连接会引入我们所讨论的数据完整性问题：在你看到连接的一侧之后，你如何知道另一侧是否会到达？说实话，你不知道，因此需要引入某种超时的概念，这引入了时间元素。那个时间元素本质上是一种窗口，我们稍后会更仔细地看一下。</p>
<h5><span id="近似算法">近似算法</span><a href="#近似算法" class="header-anchor">#</a></h5><p>第二个主要的方法类别是近似算法，例如近似Top-N、流式K-means等。它们接受一个无限的输入源，并提供输出数据，如果你仔细看它们，它们看起来或多或少像你希望得到的，如图1-7所示。近似算法的优点是它们通过设计是低开销的，并且专为无限数据而设计。缺点是只有有限的一些算法，算法本身通常很复杂（这使得创建新算法变得困难），而且它们的近似性质限制了它们的效用。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0107.png" class>
<p><em>图1-7。计算无限数据的近似值。数据被运行通过一个复杂的算法，产生的输出数据看起来或多或少像另一边所希望的结果。</em></p>
<p>值得注意的是，这些算法通常在设计中具有一定的时间元素（例如内置的衰减）。由于它们处理元素的方式是随着其到达，所以时间元素通常是基于处理时间的。这对于那些提供一些可证明误差边界的算法特别重要。如果这些误差边界是基于数据按顺序到达的，那么当你提供无序数据和具有不同事件时间偏移的算法时，它们基本上没有什么意义。这是需要记住的事情。</p>
<p>近似算法本身是一个迷人的主题，但是由于它们本质上是另一个时间不敏感处理的例子（除了算法本身的时间特征），因此它们非常容易使用，因此在我们当前的重点下，不值得进一步关注。</p>
<h5><span id="窗口化">窗口化</span><a href="#窗口化" class="header-anchor">#</a></h5><p>剩下的两种无限数据处理方法都是窗口化的变体。在深入探讨它们之间的差异之前，我应该明确一下我所说的窗口化的含义，因为在上一节中我们只简单提到了它。窗口化的本质就是将数据源（无论是无限的还是有限的）沿着时间轴切分成有限的块进行处理。图1-8展示了三种不同的窗口化模式。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0108.png" class>
<p><em>图1-8. 窗口化策略。每个示例都显示了三个不同的键，突出了对齐窗口（适用于所有数据）和非对齐窗口（适用于数据子集）之间的差异。</em></p>
<p>让我们仔细看看每个策略：</p>
<ul>
<li><p>固定窗口（也称为滚动窗口）<br>我们之前讨论过固定窗口。固定窗口将时间分成具有固定时长的段。通常（如图1-9所示），固定窗口的时间段会均匀地应用于整个数据集，这是<em>对齐</em>窗口的一个示例。在某些情况下，为了使窗口完成负载更均匀地分布在时间上，需要对不同数据子集（例如每个键）的窗口进行相位移动，这是<em>非对齐</em>窗口的一个示例，因为它们在数据集中的位置是不同的。</p>
</li>
<li><p>滑动窗口（也称为跳跃窗口）<br>滑动窗口是固定窗口的一种推广，由固定长度和固定周期定义。如果周期小于长度，则窗口会重叠。如果周期等于长度，则您有固定窗口。如果周期大于长度，则您将得到一种奇怪的采样窗口，该窗口只在一段时间内查看数据子集。与固定窗口一样，滑动窗口通常是对齐的，但在某些用例中，它们可以是非对齐的性能优化。请注意，图1-8中的滑动窗口是这样绘制的，以便给出滑动运动的感觉；实际上，所有五个窗口都适用于整个数据集。</p>
</li>
<li><p>会话<br>作为动态窗口的一个示例，会话由一段未活动时间大于某个超时时间的事件序列组成。会话通常用于分析用户随时间变化的行为，通过将一系列时间相关事件（例如一段时间内观看的视频序列）分组在一起。会话很有趣，因为它们的长度无法预先定义；它们取决于实际涉及的数据。它们也是非对齐窗口的典型示例，因为会话在不同的数据子集（例如不同的用户）之间几乎永远不会完全相同。</p>
</li>
</ul>
<p>我们之前讨论过的两个时间域（处理时间和事件时间）是我们关心的两个域。窗口化在两个域中都有意义，因此让我们详细看看它们的区别。因为处理时间窗口化历史上更加常见，所以我们从那里开始。</p>
<h5><span id="按处理时间窗口化"><em>按处理时间窗口化</em></span><a href="#按处理时间窗口化" class="header-anchor">#</a></h5><p>当按处理时间窗口化时，系统基本上会缓冲输入数据直到一定的处理时间已经过去为止。例如，在五分钟的固定窗口的情况下，系统将缓冲五分钟的处理时间的数据，之后，将观察到的这五分钟的所有数据作为一个窗口发送到下游进行处理。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0109.png" class>
<p><em>图1-9. 按处理时间窗口化为固定窗口。数据是按照管道中到达的顺序收集到窗口中的。</em></p>
<p>处理时间窗口化有一些很好的性质：</p>
<ul>
<li>它很简单。实现非常简单，因为你无需担心时间内数据的重排。你只需随着数据到来进行缓冲，并在窗口关闭时将它们发送到下游。</li>
<li>判断窗口的完整性很简单。因为系统完全知道是否已经看到了窗口的所有输入，所以它可以对是否给定的窗口已经完成做出完美的决策。这意味着，当按处理时间窗口化时，不需要以任何方式处理“延迟”数据。</li>
<li>如果你希望推断源在观察时的信息，处理时间窗口化正是你所需要的。许多监视场景都属于这种情况。想象一下，追踪发送到全球规模网络服务的每秒请求数。为了检测故障而计算这些请求的速率是处理时间窗口化的完美用例。</li>
</ul>
<p>好的方面除外，处理时间窗口化有一个非常大的缺点：如果所涉及的数据有与之相关的事件时间，则这些数据必须按照事件时间的顺序到达，以便处理时间窗口反映这些事件实际发生的时间。不幸的是，在许多现实世界的分布式输入源中，事件时间排序的数据并不常见。</p>
<p>以一个简单的例子为例，想象一下为稍后处理而收集使用统计信息的任何移动应用。对于移动设备在任何时间段内离线的情况（短暂的连接中断、飞越全国的飞行模式等），在该时间段内记录的数据将不会上传，直到设备再次上线。这意味着数据可能具有几分钟、几小时、几天、几周或更多的事件时间偏差。当按处理时间窗口化时，从这样的数据集中绘制任何有用的推论基本上是不可能的。</p>
<p>另一个例子是，许多分布式输入源可能在系统整体正常运行时提供看似按事件时间排序的（或非常接近的）数据。不幸的是，当输入源的事件时间偏差很小时，它并不意味着它会一直保持这种状态。考虑一个处理跨多个大陆收集的数据的全球服务。如果跨带宽受限的洲际线路的网络问题（遗憾的是，这种情况很常见）进一步降低带宽和&#x2F;或增加延迟，那么一部分输入数据的到达可能会比之前的时间偏差更大。如果按处理时间窗口化这些数据，你的窗口将不再代表实际发生在其中的数据；相反，它们代表事件到达处理管道的时间窗口，这是一些旧数据和当前数据的任意混合。</p>
<p>在这两种情况下，我们真正想要的是按其事件时间窗口化数据，以一种能够抵御事件到达顺序的鲁棒方式。我们真正想要的是事件时间窗口化。</p>
<h5><span id="按事件时间窗口化"><em>按事件时间窗口化</em></span><a href="#按事件时间窗口化" class="header-anchor">#</a></h5><p>当你需要按实际发生时间的时间块观察数据来源时，就可以使用事件时间窗口化。它是窗口化的金标准。在2016年之前，大多数正在使用的数据处理系统都不支持它（尽管任何具有良好一致性模型的系统，如Hadoop或Spark Streaming 1.x，都可以作为构建这种窗口化系统的合理基础）。我很高兴地说，今天的世界看起来非常不同，从Flink到Spark再到Storm和Apex，多个系统本身就支持某种事件时间窗口化。</p>
<p>图1-10显示了将无限源窗口化为一个小时的固定窗口的示例。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0110.png" class>
<p><em>图1-10. 按事件时间窗口化为固定窗口。数据是按其发生时间收集到窗口中的。黑色箭头指出了到达的两个特别有趣的数据。每个到达了与其所属的事件时间窗口不匹配的处理时间窗口。</em></p>
<p>图1-10中的黑色箭头指出了两个特别有趣的数据。每个到达了处理时间窗口，但它们所属的事件时间窗口与之不匹配。因此，如果这些数据按照处理时间窗口化到达某个关心事件时间的用例中，计算的结果将是不正确的。如你所料，事件时间的正确性是使用事件时间窗口的另一个好处。</p>
<p>对于无限数据源的事件时间窗口化，另一个好处是你可以创建动态大小的窗口，例如会话，而无需像在固定窗口中生成会话时一样产生任意的分割（如我们在“无限数据：流”中看到的会话示例）。如图1-11所示。</p>
<img src="/www6vHomeHexo/2000/03/18/streamingSystemChapter1/stsy_0111.png" class>
<p><em>图1-11. 按事件时间窗口化为会话窗口。数据是根据相应事件发生的时间收集到捕获活动突发的会话窗口中的。黑色箭头再次指出了必要的时间混乱，以将数据放置到其正确的事件时间位置。</em></p>
<p>当然，强大的语义很少是免费的，事件时间窗口也不例外。由于窗口通常必须比窗口本身的实际长度更长（在处理时间上），因此事件时间窗口化有两个值得注意的缺点：</p>
<ul>
<li><p>缓冲<br>由于窗口寿命延长，需要缓冲更多的数据。值得庆幸的是，持久存储通常是大多数数据处理系统依赖的资源类型中最便宜的（其他资源类型主要是CPU、网络带宽和RAM）。因此，在使用任何设计良好的数据处理系统时，这个问题通常不会像你想象的那样令人担忧，该系统具有强一致性的持久状态和一个像样的内存缓存层。另外，许多有用的聚合并不需要缓冲整个输入集（例如求和或平均值），而是可以增量地执行，存储在持久状态中的中间聚合要小得多。</p>
</li>
<li><p>完整性<br>鉴于我们通常没有好的方法知道我们是否看到了给定窗口的所有数据，我们如何知道窗口的结果何时准备好了呢？事实上，我们根本不知道。对于许多类型的输入，系统可以通过MillWheel、Cloud Dataflow和Flink中找到的水位线等方式给出窗口完成的合理准确的启发式估计（我们将在第3章和第4章中更多地讨论它们）。但对于绝对正确性至关重要的情况（再次考虑计费），唯一的选择是为流水线构建者提供一种表达他们希望何时对窗口的结果进行物化以及如何随时间改进这些结果的方法。处理窗口完整性（或缺乏完整性）是一个有趣的话题，但最好在具体示例的上下文中探讨，我们将在下一节中看到。</p>
</li>
</ul>
<h1><span id="概述">概述</span><a href="#概述" class="header-anchor">#</a></h1><p>哇！这是很多信息。如果你已经走到了这一步，你应该受到赞扬！但是我们只是刚刚开始。在深入探讨Beam模型方法之前，让我们简要回顾一下我们到目前为止所学到的内容。在本章中，我们完成了以下工作：</p>
<ul>
<li>澄清术语，将“流式处理”的定义聚焦于考虑未限定数据的系统，同时使用更具描述性的术语，如近似&#x2F;推测结果，来区分经常归为“流式处理”范畴的不同概念。此外，我们强调了大规模数据集的两个重要维度：基数（即有界与无界）和编码（即表格与流式），后者将占据本书下半部分的大部分。</li>
<li>评估了精心设计的批处理和流处理系统的相对能力，认为流处理实际上是批处理的严格超集，并且像Lambda架构这样的概念，建立在流处理劣于批处理的基础上，注定将因为流处理系统的成熟而退役。</li>
<li>提出了两个高层次概念，这些概念对于流处理系统追赶并最终超越批处理至关重要，分别是正确性和关于时间推理的工具。</li>
<li>确定了事件时间和处理时间之间的重要差异，描述了这些差异在分析数据时所带来的困难，并建议从完整性的概念转向简单地适应随时间发生的数据变化的方法。</li>
<li>查看了今天常用的有界和无界数据的主要数据处理方法，通过批处理和流处理引擎大致将无界方法分类为：时间不可知、近似、通过处理时间进行窗口处理和通过事件时间进行窗口处理。</li>
</ul>
<p>接下来，我们深入探讨Beam模型的细节，从概念上看，我们将数据处理的概念分为四个相关的方面：什么、在哪里、何时和如何。我们还详细研究了在多种情况下处理简单且具体的示例数据集，突出了Beam模型所支持的多种用例，同时提供一些具体的API，以使我们更加接近现实。这些示例将有助于加深本章介绍的事件时间和处理时间的概念，同时探索诸如水位线等新概念。</p>
<hr>
<ol>
<li>为了完整起见，值得一提的是，此定义包括真正的流式处理和微批次实现。对于那些不熟悉微批处理系统的人来说，它们是使用重复执行批处理引擎来处理不受限制的数据的流式处理系统。Spark Streaming是行业中的典型例子。</li>
<li>熟悉我原始的“流式处理101”文章的读者可能会记得，我强烈鼓励在引用数据集时放弃“流”的术语。这从未流行起来，我最初认为是因为它的流行性和普遍使用。然而，回顾一下，我认为我错了。区分两种不同类型的数据集组成：表和流，实际上是很有价值的。事实上，本书的大部分内容都致力于理解这两者之间的关系。</li>
<li>如果您不熟悉我所说的“精确一次”，它是指某些数据处理框架提供的特定一致性保证类型。一致性保证通常分为三个主要类别：最多一次处理，最少一次处理和恰好一次处理。请注意，这里使用的名称是指在管道生成的输出中观察到的有效语义，而不是管道可能处理（或尝试处理）任何给定记录的实际次数。因此，有时会使用“实际上一次”这个术语，而不是“精确一次”，因为它更代表事物的基本本质。Reuven在第5章中更详细地介绍了这些概念。</li>
<li>自“流式处理101”最初发布以来，许多人向我指出，在x轴上放置处理时间并在y轴上放置事件时间更直观。我确实同意，交换两个轴最初会感觉更自然，因为事件时间似乎是因变量，而处理时间是独立变量。然而，由于两个变量都是单调的且密切相关，它们实际上是相互依赖的变量。因此，我认为从技术角度来看，您只需要选择一个轴并坚持使用它即可。数学很令人困惑（尤其是在北美以外的地方，它突然变得复数并对你发起攻击）。</li>
<li>这个结果真的不应该令人惊讶（但对我来说是，因此我要指出来），因为我们在测量两种偏差&#x2F;滞后类型时实际上正在创建一个理想线与其成直角的直角三角形。数学很酷。</li>
<li>我们会在第2章详细介绍对齐的固定窗口，第4章介绍非对齐的固定窗口。</li>
<li>如果您在学术文献或基于SQL的流处理系统中四处寻找，还会发现第三个时间窗口领域：“基于元组的窗口”（即，大小以元素数计算的窗口）。但是，基于元组的窗口本质上是一种处理时间窗口，其中元素在到达系统时分配单调递增的时间戳。因此，我们不会进一步详细讨论基于元组的窗口。</li>
</ol>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-Chapter 2. The What, Where, When, and How of Data Processing[完整]</title>
    <url>/www6vHomeHexo/2000/03/17/streamingSystemChapter2Original/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#roadmap"><strong>Roadmap</strong></a></li>
<li><a href="#batch-foundations-what-and-where"><strong>Batch Foundations: *What* and *Where*</strong></a><ul>
<li><a href="#what-transformations">*<strong>What*: Transformations</strong></a></li>
<li><a href="#where-windowing">*<strong>Where*: Windowing</strong></a></li>
</ul>
</li>
<li><a href="#going-streaming-when-and-how"><strong>Going Streaming: <em>When</em> and <em>How</em></strong></a><ul>
<li><a href="#when-the-wonderful-thing-about-triggers-is-triggers"><strong><em>When</em>: The Wonderful Thing About Triggers Is Triggers</strong></a></li>
<li><a href="#are-wonderful-things"><strong>Are Wonderful Things!</strong></a></li>
<li><a href="#when-watermarks">*<strong>When*: Watermarks</strong></a></li>
<li><a href="#when-earlyon-timelate-triggers-ftw"><strong><em>When</em>: Early&#x2F;On-Time&#x2F;Late Triggers FTW!</strong></a></li>
<li><a href="#when-allowed-lateness-ie-garbage-collection">*<strong>When*: Allowed Lateness (i.e., Garbage Collection)</strong></a></li>
</ul>
</li>
<li><a href="#how-accumulation">*<strong>How*: Accumulation</strong></a></li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 42</p>
<details><summary>点击 原文</summary><p>Okay party people, it’s time to get concrete!</p>
<p>Chapter 1 focused on three main areas: <em>terminology</em>, defining precisely what I</p>
<p>mean when I use overloaded terms like “streaming”; <em>batch versus streaming</em>,</p>
<p>comparing the theoretical capabilities of the two types of systems, and</p>
<p>postulating that only two things are necessary to take streaming systems</p>
<p>beyond their batch counterparts: correctness and tools for reasoning about</p>
<p>time; and <em>data processing patterns</em>, looking at the conceptual approaches</p>
<p>taken with both batch and streaming systems when processing bounded and</p>
<p>unbounded data.</p>
<p>In this chapter, we’re now going to focus further on the data processing</p>
<p>patterns from Chapter 1, but in more detail, and within the context of concrete</p>
<p>examples. By the time we’re finished, we’ll have covered what I consider to</p>
<p>be the core set of principles and concepts required for robust out-of-order data</p>
<p>processing; these are the tools for reasoning about time that truly get you</p>
<p>beyond classic batch processing.</p>
<p>To give you a sense of what things look like in action, I use snippets of</p>
<p>Apache Beam code, coupled with time-lapse diagrams to provide a visual</p>
<p>representation of the concepts. Apache Beam is a unified programming model</p>
<p>and portability layer for batch and stream processing, with a set of concrete</p>
<p>SDKs in various languages (e.g., Java and Python). Pipelines written with</p>
<p>Apache Beam can then be portably run on any of the supported execution</p>
<p>engines (Apache Apex, Apache Flink, Apache Spark, Cloud Dataflow, etc.).</p>
<p>I use Apache Beam here for examples not because this is a Beam book (it’s</p>
<p>not), but because it most completely embodies the concepts described in this</p>
<p>book. Back when “Streaming 102” was originally written (back when it was</p>
<p>still the Dataflow Model from Google Cloud Dataflow and not the Beam</p>
<p>Model from Apache Beam), it was literally the only system in existence that</p>
<p>provided the amount of expressiveness necessary for all the examples we’ll</p>
<p>cover here. A year and a half later, I’m happy to say much has changed, and</p>
<p>most of the major systems out there have moved or are moving toward</p>
<p>supporting a model that looks a lot like the one described in this book. So rest</p>
<p>assured that the concepts we cover here, though informed through the Beam</p>
<p>lens, as it were, will apply equally across most other systems you’ll come</p>
<p>across.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="roadmap"><strong>Roadmap</strong></span><a href="#roadmap" class="header-anchor">#</a></h1><p>To help set the stage for this chapter, I want to lay out the five main concepts</p>
<p>that will underpin all of the discussions therein, and really, for most of the rest</p>
<p>of Part I. We’ve already covered two of them.</p>
<p>In Chapter 1, I first established the critical distinction between event time (the</p>
<p>time that events happen) and processing time (the time they are observed</p>
<p>during processing). This provides the foundation for one of the main theses</p>
<p>put forth in this book: if you care about both correctness and the context</p>
<p>within which events actually occurred, you must analyze data relative to their</p>
<p>inherent event times, not the processing time at which they are encountered</p>
<p>during the analysis itself.</p>
<p>I then introduced the concept of <em>windowing</em> (i.e., partitioning a dataset along</p>
<p>temporal boundaries), which is a common approach used to cope with the fact</p>
<p>that unbounded data sources technically might never end. Some simpler</p>
<p>examples of windowing strategies are <em>fixed</em> and <em>sliding</em> windows, but more</p>
<p>sophisticated types of windowing, such as <em>sessions</em> (in which the windows are</p>
<p>defined by features of the data themselves; for example, capturing a session of</p>
<p>activity per user followed by a gap of inactivity) also see broad usage.</p>
<p>In addition to these two concepts, we’re now going to look closely at three</p>
<p>more:</p>
<ul>
<li>Triggers</li>
</ul>
<p>A trigger is a mechanism for declaring when the output for a window</p>
<p>should be materialized relative to some external signal. Triggers provide</p>
<p>flexibility in choosing when outputs should be emitted. In some sense,</p>
<p>you can think of them as a flow control mechanism for dictating when</p>
<p>results should be materialized. Another way of looking at it is that triggers</p>
<p>are like the shutter-release on a camera, allowing you to declare when to</p>
<p>take a snapshots in time of the results being computed.</p>
<p>Triggers also make it possible to observe the output for a window multiple</p>
<p>times as it evolves. This in turn opens up the door to refining results over</p>
<p>time, which allows for providing speculative results as data arrive, as well</p>
<p>as dealing with changes in upstream data (revisions) over time or data that</p>
<p>arrive late (e.g., mobile scenarios, in which someone’s phone records</p>
<p>various actions and their event times while the person is offline and then</p>
<p>proceeds to upload those events for processing upon regaining</p>
<p>connectivity).</p>
<ul>
<li>Watermarks</li>
</ul>
<p>A watermark is a notion of input completeness with respect to event</p>
<p>times. A watermark with value of time <em>X</em> makes the statement: “all input</p>
<p>data with event times less than <em>X</em> have been observed.” As such,</p>
<p>watermarks act as a metric of progress when observing an unbounded data</p>
<p>source with no known end. We touch upon the basics of watermarks in</p>
<p>this chapter, and then Slava goes super deep on the subject in Chapter 3.</p>
<ul>
<li>Accumulation</li>
</ul>
<p>An accumulation mode specifies the relationship between multiple results</p>
<p>that are observed for the same window. Those results might be completely</p>
<p>disjointed; that is, representing independent deltas over time, or there</p>
<p>might be overlap between them. Different accumulation modes have</p>
<p>different semantics and costs associated with them and thus find</p>
<p>applicability across a variety of use cases.</p>
<p>Also, because I think it makes it easier to understand the relationships</p>
<p>between all of these concepts, we revisit the old and explore the new within</p>
<p>the structure of answering four questions, all of which I propose are critical to</p>
<p>every unbounded data processing problem:</p>
<ul>
<li><em>What</em> results are calculated? This question is answered by the types</li>
</ul>
<p>of transformations within the pipeline. This includes things like</p>
<p>computing sums, building histograms, training machine learning</p>
<p>models, and so on. It’s also essentially the question answered by</p>
<p>classic batch processing</p>
<ul>
<li><em>Where</em> in event time are results calculated? This question is answered</li>
</ul>
<p>by the use of event-time windowing within the pipeline. This</p>
<p>includes the common examples of windowing from Chapter 1 (fixed,</p>
<p>sliding, and sessions); use cases that seem to have no notion of</p>
<p>windowing (e.g., time-agnostic processing; classic batch processing</p>
<p>also generally falls into this category); and other, more complex</p>
<p>types of windowing, such as time-limited auctions. Also note that it</p>
<p>can include processing-time windowing, as well, if you assign</p>
<p>ingress times as event times for records as they arrive at the system.</p>
<ul>
<li><em>When</em> in processing time are results materialized? This question is</li>
</ul>
<p>answered by the use of triggers and (optionally) watermarks. There</p>
<p>are infinite variations on this theme, but the most common patterns</p>
<p>are those involving repeated updates (i.e., materialized view</p>
<p>semantics), those that utilize a watermark to provide a single output</p>
<p>per window only after the corresponding input is believed to be</p>
<p>complete (i.e., classic batch processing semantics applied on a per</p>
<p>window basis), or some combination of the two.</p>
<ul>
<li><em>How</em> do refinements of results relate? This question is answered by</li>
</ul>
<p>the type of accumulation used: discarding (in which results are all</p>
<p>independent and distinct), accumulating (in which later results build</p>
<p>upon prior ones), or accumulating and retracting (in which both the</p>
<p>accumulating value plus a retraction for the previously triggered</p>
<p>value(s) are emitted).</p>
<p>We look at each of these questions in much more detail throughout the rest of</p>
<p>the book. And, yes, I’m going to run this color scheme thing into the ground</p>
<p>in an attempt to make it abundantly clear which concepts relate to which</p>
<p>question in the <em>What</em>&#x2F;<em>Where</em>&#x2F;<em>When</em>&#x2F;<em>How</em> idiom. You’re welcome &lt;winky</p>
<p>smiley&#x2F;&gt;.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="batch-foundations-what-and-where"><strong>Batch Foundations: *What* and *Where*</strong></span><a href="#batch-foundations-what-and-where" class="header-anchor">#</a></h1><p>Okay, let’s get this party started. First stop: batch processing.</p>
<h3><span id="what-transformations">*<strong>What*: Transformations</strong></span><a href="#what-transformations" class="header-anchor">#</a></h3><p>The transformations applied in classic batch processing answer the question:</p>
<p>“<em>What</em> results are calculated?” Even though you are likely already familiar</p>
<p>with classic batch processing, we’re going to start there anyway because it’s</p>
<p>the foundation on top of which we add all of the other concepts.</p>
<p>In the rest of this chapter (and indeed, through much of the book), we look at</p>
<p>a single example: computing keyed integer sums over a simple dataset</p>
<p>consisting of nine values. Let’s imagine that we’ve written a team-based</p>
<p>mobile game and we want to build a pipeline that calculates team scores by</p>
<p>summing up the individual scores reported by users’ phones. If we were to</p>
<p>capture our nine example scores in a SQL table named “UserScores,” it might</p>
<p>look something like this:</p>
<hr>
<p>| Name | Team | Score | EventTime | ProcTime |</p>
<hr>
<p>| Julie | TeamX | 5 | 12:00:26 | 12:05:19 |</p>
<p>| Frank | TeamX | 9 | 12:01:26 | 12:08:19 |</p>
<p>| Ed | TeamX | 7 | 12:02:26 | 12:05:39 |</p>
<p>| Julie | TeamX | 8 | 12:03:06 | 12:07:06 |</p>
<p>| Amy | TeamX | 3 | 12:03:39 | 12:06:13 |</p>
<p>| Fred | TeamX | 4 | 12:04:19 | 12:06:39 |</p>
<p>| Naomi | TeamX | 3 | 12:06:39 | 12:07:19 |</p>
<p>| Becky | TeamX | 8 | 12:07:26 | 12:08:39 |</p>
<p>| Naomi | TeamX | 1 | 12:07:46 | 12:09:00 |</p>
<hr>
<p>Note that all the scores in this example are from users on the same team; this</p>
<p>is to keep the example simple, given that we have a limited number of</p>
<p>dimensions in our diagrams that follow. And because we’re grouping by</p>
<p>team, we really just care about the last three columns:</p>
<ul>
<li>Score</li>
</ul>
<p>The individual user score associated with this event</p>
<ul>
<li>EventTime</li>
</ul>
<p>The event time for the score; that is, the time at which the score occurred</p>
<ul>
<li>ProcTime</li>
</ul>
<p>The processing for the score; that is, the time at which the score was</p>
<p>observed by the pipeline</p>
<p>For each example pipeline, we’ll look at a time-lapse diagram that highlights</p>
<p>how the data evolves over time. Those diagrams plot our nine scores in the</p>
<p>two dimensions of time we care about: event time in the x-axis, and</p>
<p>processing time in the y-axis. Figure 2-1 illustrates what a static plot of the</p>
<p>input data looks like.</p>
<p><em>Figure 2-1. Nine input records, plotted in both event time and processing time</em></p>
<p>Subsequent time-lapse diagrams are either animations (Safari) or a sequence</p>
<p>of frames (print and all other digital formats), allowing you to see how the</p>
<p>data are processed over time (more on this shortly after we get to the first</p>
<p>time-lapse diagram).</p>
<p>Preceding each example is a short snippet of Apache Beam Java SDK</p>
<p>pseudocode to make the definition of the pipeline more concrete. It is</p>
<p>pseudocode in the sense that I sometime bend the rules to make the examples</p>
<p>clearer, elide details (like the use of concrete I&#x2F;O sources), or simplify names</p>
<p>(the trigger names in Beam Java 2.x and earlier are painfully verbose; I use</p>
<p>simpler names for clarity). Beyond minor things like those, it’s otherwise</p>
<p>real-world Beam code (and real code is available on GitHub for all examples</p>
<p>in this chapter).</p>
<p>If you’re already familiar with something like Spark or Flink, you should</p>
<p>have a relatively easy time understanding what the Beam code is doing. But</p>
<p>to give you a crash course in things, there are two basic primitives in Beam:</p>
<ul>
<li>PCollections</li>
</ul>
<p>These represent datasets (possibly massive ones) across which parallel</p>
<p>transformations can be performed (hence the “P” at the beginning of the</p>
<p>name).</p>
<ul>
<li>PTransforms</li>
</ul>
<p>These are applied to PCollections to create new PCollections.</p>
<p>PTransforms may perform element-wise transformations, they may</p>
<p>group&#x2F;aggregate multiple elements together, or they may be a composite</p>
<p>combination of other PTransforms, as depicted in Figure 2-2.</p>
<p><em>Figure 2-2. Types of transformations</em></p>
<p>For the purposes of our examples, we typically assume that we start out with</p>
<p>a pre-loaded PCollection&lt;KV&lt;Team, Integer&gt;&gt; named “input” (that is, a</p>
<p>PCollection composed of key&#x2F;value pairs of Teams and Integers, where</p>
<p>the Teams are just something like Strings representing team names, and the</p>
<p>Integers are scores from any individual on the corresponding team). In a</p>
<p>real-world pipeline, we would’ve acquired input by reading in a</p>
<p>PCollection<string> of raw data (e.g., log records) from an I&#x2F;O source and</string></p>
<p>then transforming it into a PCollection&lt;KV&lt;Team, Integer&gt;&gt; by parsing</p>
<p>the log records into appropriate key&#x2F;value pairs. For the sake of clarity in this</p>
<p>first example, I include pseudocode for all of those steps, but in subsequent</p>
<p>examples, I elide the I&#x2F;O and parsing.</p>
<p>Thus, for a pipeline that simply reads in data from an I&#x2F;O source, parses</p>
<p>team&#x2F;score pairs, and calculates per-team sums of scores, we’d have</p>
<p>something like that shown in Example 2-1.</p>
<p><em>Example 2-1. Summation pipeline</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;String&gt; raw = IO.read(...);</span><br><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; input = raw.apply(new ParseFn());</span><br><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals =</span><br><span class="line">input.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Key&#x2F;value data are read from an I&#x2F;O source, with a Team (e.g., String of the</p>
<p>team name) as the key and an Integer (e.g., individual team member scores)</p>
<p>as the value. The values for each key are then summed together to generate</p>
<p>per-key sums (e.g., total team score) in the output collection.</p>
<p>For all the examples to come, after seeing a code snippet describing the</p>
<p>pipeline that we’re analyzing, we’ll then look at a time-lapse diagram</p>
<p>showing the execution of that pipeline over our concrete dataset for a single</p>
<p>key. In a real pipeline, you can imagine that similar operations would be</p>
<p>happening in parallel across multiple machines, but for the sake of our</p>
<p>examples, it will be clearer to keep things simple.</p>
<p>As noted previously, Safari editions present the complete execution as an</p>
<p>animated movie, whereas print and all other digital formats use a static</p>
<p>sequence of key frames that provide a sense of how the pipeline progresses</p>
<p>over time. In both cases, we also provide a URL to a fully animated version</p>
<p>on <em><a href="http://www.streamingbook.net/">www.streamingbook.net</a></em>.</p>
<p>Each diagram plots the inputs and outputs across two dimensions: event time</p>
<p>(on the x-axis) and processing time (on the y-axis). Thus, real time as</p>
<p>observed by the pipeline progresses from bottom to top, as indicated by the</p>
<p>thick horizontal black line that ascends in the processing-time axis as time</p>
<p>progresses. Inputs are circles, with the number inside the circle representing</p>
<p>the value of that specific record. They start out light gray, and darken as the</p>
<p>pipeline observes them.</p>
<p>As the pipeline observes values, it accumulates them in its intermediate state</p>
<p>and eventually materializes the aggregate results as output. State and output</p>
<p>are represented by rectangles (gray for state, blue for output), with the</p>
<p>aggregate value near the top, and with the area covered by the rectangle</p>
<p>representing the portions of event time and processing time accumulated into</p>
<p>the result. For the pipeline in Example 2-1, it would look something like that</p>
<p>shown in Figure 2-3 when executed on a classic batch engine.</p>
<p><em>Figure 2-3. Classic batch processing</em></p>
<p>Because this is a batch pipeline, it accumulates state until it’s seen all of the</p>
<p>inputs (represented by the dashed green line at the top), at which point it</p>
<p>produces its single output of 48. In this example, we’re calculating a sum over</p>
<p>all of event time because we haven’t applied any specific windowing</p>
<p>transformations; hence the rectangles for state and output cover the entirety of</p>
<p>the x-axis. If we want to process an unbounded data source, however, classic</p>
<p>batch processing won’t be sufficient; we can’t wait for the input to end,</p>
<p>because it effectively never will. One of the concepts we want is windowing,</p>
<p>which we introduced in Chapter 1. Thus, within the context of our second</p>
<p>question—“<em>Where</em> in event time are results calculated?”—we’ll now briefly</p>
<p>revisit windowing.</p>
<h3><span id="where-windowing">*<strong>Where*: Windowing</strong></span><a href="#where-windowing" class="header-anchor">#</a></h3><p>As discussed in Chapter 1, windowing is the process of slicing up a data</p>
<p>source along temporal boundaries. Common windowing strategies include</p>
<p>fixed windows, sliding windows, and sessions windows, as demonstrated in</p>
<p>Figure 2-4.</p>
<p><em>Figure 2-4. Example windowing strategies. Each example is shown for three different keys, highlighting</em></p>
<p><em>the difference between aligned windows (which apply across all the data) and unaligned windows</em></p>
<p><em>(which apply across a subset of the data).</em></p>
<p>To get a better sense of what windowing looks like in practice, let’s take our</p>
<p>integer summation pipeline and window it into fixed, two-minute windows.</p>
<p>With Beam, the change is a simple addition of a Window.into transform,</p>
<p>which you can see highlighted in Example 2-2.</p>
<p><em>Example 2-2. Windowed summation code</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES)))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Recall that Beam provides a unified model that works in both batch and</p>
<p>streaming because semantically batch is really just a subset of streaming. As</p>
<p>such, let’s first execute this pipeline on a batch engine; the mechanics are</p>
<p>more straightforward, and it will give us something to directly compare</p>
<p>against when we switch to a streaming engine. Figure 2-5 presents the result.</p>
<p><em>Figure 2-5. Windowed summation on a batch engine</em></p>
<p>As before, inputs are accumulated in state until they are entirely consumed,</p>
<p>after which output is produced. In this case, however, instead of one output,</p>
<p>we get four: a single output, for each of the four relevant two-minute event</p>
<p>time windows.</p>
<p>At this point we’ve revisited the two main concepts that I introduced in</p>
<p>Chapter 1: the relationship between the event-time and processing-time</p>
<p>domains, and windowing. If we want to go any further, we’ll need to start</p>
<p>adding the new concepts mentioned at the beginning of this section: triggers,</p>
<p>watermarks, and accumulation.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="going-streaming-when-and-how"><strong>Going Streaming: <em>When</em> and <em>How</em></strong></span><a href="#going-streaming-when-and-how" class="header-anchor">#</a></h1><p>We just observed the execution of a windowed pipeline on a batch engine.</p>
<p>But, ideally, we’d like to have lower latency for our results, and we’d also</p>
<p>like to natively handle unbounded data sources. Switching to a streaming</p>
<p>engine is a step in the right direction, but our previous strategy of waiting</p>
<p>until our input has been consumed in its entirety to generate output is no</p>
<p>longer feasible. Enter triggers and watermarks.</p>
<h3><span id="when-the-wonderful-thing-about-triggers-is-triggers"><strong><em>When</em>: The Wonderful Thing About Triggers Is Triggers</strong></span><a href="#when-the-wonderful-thing-about-triggers-is-triggers" class="header-anchor">#</a></h3><h3><span id="are-wonderful-things"><strong>Are Wonderful Things!</strong></span><a href="#are-wonderful-things" class="header-anchor">#</a></h3><p>Triggers provide the answer to the question: “<em>When</em> in processing time are</p>
<p>results materialized?” Triggers declare when output for a window should</p>
<p>happen in processing time (though the triggers themselves might make those</p>
<p>decisions based on things that happen in other time domains, such as</p>
<p>watermarks progressing in the event-time domain, as we’ll see in a few</p>
<p>moments). Each specific output for a window is referred to as a <em>pane</em> of the</p>
<p>window.</p>
<p>Though it’s possible to imagine quite a breadth of possible triggering</p>
<p>semantics, conceptually there are only two generally useful types of triggers,</p>
<p>and practical applications almost always boil down using either one or a</p>
<p>combination of both:</p>
<ul>
<li>Repeated update triggers</li>
</ul>
<p>These periodically generate updated panes for a window as its contents</p>
<p>evolve. These updates can be materialized with every new record, or they</p>
<p>can happen after some processing-time delay, such as once a minute. The</p>
<p>choice of period for a repeated update trigger is primarily an exercise in</p>
<p>balancing latency and cost.</p>
<ul>
<li>Completeness triggers</li>
</ul>
<p>These materialize a pane for a window only after the input for that</p>
<p>window is believed to be complete to some threshold. This type of trigger</p>
<p>is most analogous to what we’re familiar with in batch processing: only</p>
<p>after the input is complete do we provide a result. The difference in the</p>
<p>trigger-based approach is that the notion of completeness is scoped to the</p>
<p>context of a single window, rather than always being bound to the</p>
<p>completeness of the entire input.</p>
<p>Repeated update triggers are the most common type of trigger encountered in</p>
<p>streaming systems. They are simple to implement and simple to understand,</p>
<p>and they provide useful semantics for a specific type of use case: repeated</p>
<p>(and eventually consistent) updates to a materialized dataset, analogous to the</p>
<p>semantics you get with materialized views in the database world.</p>
<p>Completeness triggers are less frequently encountered, but provide streaming</p>
<p>semantics that more closely align with those from the classic batch processing</p>
<p>world. They also provide tools for reasoning about things like missing data</p>
<p>and late data, both of which we discuss shortly (and in the next chapter) as we</p>
<p>explore the underlying primitive that drives completeness triggers:</p>
<p>watermarks.</p>
<p>But first, let’s start simple and look at some basic repeated update triggers in</p>
<p>action. To make the notion of triggers a bit more concrete, let’s go ahead and</p>
<p>add the most straightforward type of trigger to our example pipeline: a trigger</p>
<p>that fires with every new record, as shown in Example 2-3.</p>
<p><em>Example 2-3. Triggering repeatedly with every record</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(AfterCount(1))));</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p>If we were to run this new pipeline on a streaming engine, the results would</p>
<p>look something like that shown in Figure 2-6.</p>
<p><em>Figure 2-6. Per-record triggering on a streaming engine</em></p>
<p>You can see how we now get multiple outputs (panes) for each window: once</p>
<p>per corresponding input. This sort of triggering pattern works well when the</p>
<p>output stream is being written to some sort of table that you can simply poll</p>
<p>for results. Any time you look in the table, you’ll see the most up-to-date</p>
<p>value for a given window, and those values will converge toward correctness</p>
<p>over time.</p>
<p>One downside of per-record triggering is that it’s quite chatty. When</p>
<p>processing large-scale data, aggregations like summation provide a nice</p>
<p>opportunity to reduce the cardinality of the stream without losing information.</p>
<p>This is particularly noticeable for cases in which you have high-volume keys;</p>
<p>for our example, massive teams with lots of active players. Imagine a</p>
<p>massively multiplayer game in which players are split into one of two</p>
<p>factions, and you want to tally stats on a per-faction basis. It’s probably</p>
<p>unnecessary to update your tallies with every new input record for every</p>
<p>player in a given faction. Instead, you might be happy updating them after</p>
<p>some processing-time delay, say every second, or every minute. The nice side</p>
<p>effect of using processing-time delays is that it has an equalizing effect across</p>
<p>high-volume keys or windows: the resulting stream ends up being more</p>
<p>uniform cardinality-wise.</p>
<p>There are two different approaches to processing-time delays in triggers:</p>
<p><em>aligned delays</em> (where the delay slices up processing time into fixed regions</p>
<p>that align across keys and windows) and <em>unaligned delays</em> (where the delay is</p>
<p>relative to the data observed within a given window). A pipeline with</p>
<p>unaligned delays might look like Example 2-4, the results of which are shown</p>
<p>in Figure 2-7.</p>
<p><em>Example 2-4. Triggering on aligned two-minute processing-time boundaries</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(AlignedDelay(TWO_MINUTES)))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-7. Two-minute aligned delay triggers (i.e., microbatching)</em></p>
<p>This sort of aligned delay trigger is effectively what you get from a</p>
<p>microbatch streaming system like Spark Streaming. The nice thing about it is</p>
<p>predictability; you get regular updates across all modified windows at the</p>
<p>same time. That’s also the downside: all updates happen at once, which</p>
<p>results in bursty workloads that often require greater peak provisioning to</p>
<p>properly handle the load. The alternative is to use an unaligned delay. That</p>
<p>would look something Example 2-5 in Beam. Figure 2-8 presents the results.</p>
<p><em>Example 2-5. Triggering on unaligned two-minute processing-time</em></p>
<p><em>boundaries</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(Repeatedly(UnalignedDelay(TWO_MINUTES))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-8. Two-minute unaligned delay triggers</em></p>
<p>Contrasting the unaligned delays in Figure 2-8 to the aligned delays in</p>
<p>Figure 2-6, it’s easy to see how the unaligned delays spread the load out more</p>
<p>evenly across time. The actual latencies involved for any given window differ</p>
<p>between the two, sometimes more and sometimes less, but in the end the</p>
<p>average latency will remain essentially the same. From that perspective,</p>
<p>unaligned delays are typically the better choice for large-scale processing</p>
<p>because they result in a more even load distribution over time.</p>
<p>Repeated update triggers are great for use cases in which we simply want</p>
<p>periodic updates to our results over time and are fine with those updates</p>
<p>converging toward correctness with no clear indication of when correctness is</p>
<p>achieved. However, as we discussed in Chapter 1, the vagaries of distributed</p>
<p>systems often lead to a varying level of skew between the time an event</p>
<p>happens and the time it’s actually observed by your pipeline, which means it</p>
<p>can be difficult to reason about when your output presents an accurate and</p>
<p>complete view of your input data. For cases in which input completeness</p>
<p>matters, it’s important to have some way of reasoning about completeness</p>
<p>rather than blindly trusting the results calculated by whichever subset of data</p>
<p>happen to have found their way to your pipeline. Enter watermarks.</p>
</details>





<details><summary>点击 原文</summary><h3><span id="when-watermarks">*<strong>When*: Watermarks</strong></span><a href="#when-watermarks" class="header-anchor">#</a></h3><p>Watermarks are a supporting aspect of the answer to the question: “<em>When</em> in</p>
<p>processing time are results materialized?” Watermarks are temporal notions</p>
<p>of input completeness in the event-time domain. Worded differently, they are</p>
<p>the way the system measures progress and completeness relative to the event</p>
<p>times of the records being processed in a stream of events (either bounded or</p>
<p>unbounded, though their usefulness is more apparent in the unbounded case).</p>
<p>Recall this diagram from Chapter 1, slightly modified in Figure 2-9, in which</p>
<p>I described the skew between event time and processing time as an ever</p>
<p>changing function of time for most real-world distributed data processing</p>
<p>systems.</p>
<p><em>Figure 2-9. Event-time progress, skew, and watermarks</em></p>
<p>That meandering red line that I claimed represented reality is essentially the</p>
<p>watermark; it captures the progress of event-time completeness as processing</p>
<p>time progresses. Conceptually, you can think of the watermark as a function,</p>
<p><em>F</em>(<em>P</em>) → <em>E</em>, which takes a point in processing time and returns a point in event</p>
<p>time. That point in event time, <em>E</em>, is the point up to which the system believes</p>
<p>all inputs with event times less than <em>E</em> have been observed. In other words,</p>
<p>it’s an assertion that no more data with event times less than <em>E</em> will ever be</p>
<p>seen again. Depending upon the type of watermark, perfect or heuristic, that</p>
<p>assertion can be a strict guarantee or an educated guess, respectively:</p>
<ul>
<li>Perfect watermarks</li>
</ul>
<p>For the case in which we have perfect knowledge of all of the input data,</p>
<p>it’s possible to construct a perfect watermark. In such a case, there is no</p>
<p>such thing as late data; all data are early or on time.</p>
<ul>
<li>Heuristic watermarks</li>
</ul>
<p>For many distributed input sources, perfect knowledge of the input data is</p>
<p>impractical, in which case the next best option is to provide a heuristic</p>
<p>watermark. Heuristic watermarks use whatever information is available</p>
<p>about the inputs (partitions, ordering within partitions if any, growth rates</p>
<p>of files, etc.) to provide an estimate of progress that is as accurate as</p>
<p>possible. In many cases, such watermarks can be remarkably accurate in</p>
<p>their predictions. Even so, the use of a heuristic watermark means that it</p>
<p>might sometimes be wrong, which will lead to late data. We show you</p>
<p>about ways to deal with late data soon.</p>
<p>Because they provide a notion of completeness relative to our inputs,</p>
<p>watermarks form the foundation for the second type of trigger mentioned</p>
<p>previously: <em>completeness triggers</em>. Watermarks themselves are a fascinating</p>
<p>and complex topic, as you’ll see when you get to Slava’s watermarks deep</p>
<p>dive in Chapter 3. But for now, let’s look at them in action by updating our</p>
<p>example pipeline to utilize a completeness trigger built upon watermarks, as</p>
<p>demonstrated in Example 2-6.</p>
<p><em>Example 2-6. Watermark completeness trigger</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(AfterWatermark()))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Now, an interesting quality of watermarks is that they are a class of functions,</p>
<p>meaning there are multiple different functions <em>F</em>(<em>P</em>) → <em>E</em> that satisfy the</p>
<p>properties of a watermark, to varying degrees of success. As I noted earlier,</p>
<p>for situations in which you have perfect knowledge of your input data, it</p>
<p>might be possible to build a perfect watermark, which is the ideal situation.</p>
<p>But for cases in which you lack perfect knowledge of the inputs or for which</p>
<p>it’s simply too computationally expensive to calculate the perfect watermark,</p>
<p>you might instead choose to utilize a heuristic for defining your watermark.</p>
<p>The point I want to make here is that the given watermark algorithm in use is</p>
<p>independent from the pipeline itself. We’re not going to discuss in detail what</p>
<p>it means to implement a watermark here (Slava does that in Chapter 3). For</p>
<p>now, to help drive home this idea that a given input set can have different</p>
<p>watermarks applied to it, let’s take a look at our pipeline in Example 2-6</p>
<p>when executed on the same dataset but using two distinct watermark</p>
<p>implementations (Figure 2-10): on the left, a perfect watermark; on the right,</p>
<p>a heuristic watermark.</p>
<p>In both cases, windows are materialized as the watermark passes the end of</p>
<p>the window. The perfect watermark, as you might expect, perfectly captures</p>
<p>the event-time completeness of the pipeline as time progresses. In contrast,</p>
<p>the specific algorithm used for the heuristic watermark on the right fails to</p>
<p>take the value of 9 into account, which drastically changes the shape of the</p>
<p>materialized outputs, both in terms of output latency and correctness (as seen</p>
<p>by the incorrect answer of 5 that’s provided for the [12:00, 12:02) window).</p>
<p>The big difference between the watermark triggers from Figure 2-9 and the</p>
<p>repeated update triggers we saw in Figures 2-5 through 2-7 is that the</p>
<p><em>watermarks give us a way to reason about the completeness of our input</em>.</p>
<p>Until the system materializes an output for a given window, we know that the</p>
<p>system does not yet believe the inputs to be complete. This is especially</p>
<p>important for use cases in which you want to reason about a <em>lack of data</em> in</p>
<p>the input, or <em>missing data</em>.</p>
<p><em>Figure 2-10. Windowed summation on a streaming engine with perfect (left) and heuristic (right)</em></p>
<p><em>watermarks</em></p>
<p>A great example of a missing-data use case is outer joins. Without a notion of</p>
<p>completeness like watermarks, how do you know when to give up and emit a</p>
<p>partial join rather than continue to wait for that join to complete? You don’t.</p>
<p>And basing that decision on a processing-time delay, which is the common</p>
<p>approach in streaming systems that lack true watermark support, is not a safe</p>
<p>way to go, because of the variable nature of event-time skew we spoke about</p>
<p>in Chapter 1: as long as skew remains smaller than the chosen processing</p>
<p>time delay, your missing-data results will be correct, but any time skew grows</p>
<p>beyond that delay, they will suddenly become <em>in</em>correct. From this</p>
<p>perspective, event-time watermarks are a critical piece of the puzzle for many</p>
<p>real-world streaming use cases which must reason about a lack of data in the</p>
<p>input, such as outer joins, anomaly detection, and so on.</p>
<p>Now, with that said, these watermark examples also highlight two</p>
<p><em>shortcomings</em> of watermarks (and any other notion of completeness),</p>
<p>specifically that they can be one of the following:</p>
<ul>
<li>Too slow</li>
</ul>
<p>When a watermark of any type is correctly delayed due to known</p>
<p>unprocessed data (e.g., slowly growing input logs due to network</p>
<p>bandwidth constraints), that translates directly into delays in output if</p>
<p>advancement of the watermark is the only thing you depend on for</p>
<p>stimulating results.</p>
<p>This is most obvious in the left diagram of Figure 2-10, for which the late</p>
<p>arriving 9 holds back the watermark for all the subsequent windows, even</p>
<p>though the input data for those windows become complete earlier. This is</p>
<p>particularly apparent for the second window, [12:02, 12:04), for which it</p>
<p>takes nearly seven minutes from the time the first value in the window</p>
<p>occurs until we see any results for the window whatsoever. The heuristic</p>
<p>watermark in this example doesn’t suffer the same issue quite so badly</p>
<p>(five minutes until output), but don’t take that to mean heuristic</p>
<p>watermarks never suffer from watermark lag; that’s really just a</p>
<p>consequence of the record I chose to omit from the heuristic watermark in</p>
<p>this specific example.</p>
<p>The important point here is the following: Although watermarks provide a</p>
<p>very useful notion of completeness, depending upon completeness for</p>
<p>producing output is often not ideal from a latency perspective. Imagine a</p>
<p>dashboard that contains valuable metrics, windowed by hour or day. It’s</p>
<p>unlikely you’d want to wait a full hour or day to begin seeing results for</p>
<p>the current window; that’s one of the pain points of using classic batch</p>
<p>systems to power such systems. Instead, it would be much nicer to see the</p>
<p>results for those windows refine over time as the inputs evolve and</p>
<p>eventually become complete.</p>
<ul>
<li>Too fast</li>
</ul>
<p>When a heuristic watermark is incorrectly advanced earlier than it should</p>
<p>be, it’s possible for data with event times before the watermark to arrive</p>
<p>some time later, creating late data. This is what happened in the example</p>
<p>on the right: the watermark advanced past the end of the first window</p>
<p>before all the input data for that window had been observed, resulting in</p>
<p>an incorrect output value of 5 instead of 14. This shortcoming is strictly a</p>
<p>problem with heuristic watermarks; their heuristic nature implies they will</p>
<p>sometimes be wrong. As a result, relying on them alone for determining</p>
<p>when to materialize output is insufficient if you care about correctness.</p>
<p>In Chapter 1, I made some rather emphatic statements about notions of</p>
<p>completeness being insufficient for most use cases requiring robust out-of-</p>
<p>order processing of unbounded data streams. These two shortcomings—</p>
<p>watermarks being too slow or too fast—are the foundations for those</p>
<p>arguments. You simply cannot get both low latency and correctness out of a</p>
<p>system that relies solely on notions of completeness. So, for cases for which</p>
<p>you do want the best of both worlds, what’s a person to do? Well, if repeated</p>
<p>update triggers provide low-latency updates but no way to reason about</p>
<p>completeness, and watermarks provide a notion of completeness but variable</p>
<p>and possible high latency, why not combine their powers together?</p>
</details>





<details><summary>点击 原文</summary><h3><span id="when-earlyx2fon-timex2flate-triggers-ftw"><strong><em>When</em>: Early&#x2F;On-Time&#x2F;Late Triggers FTW!</strong></span><a href="#when-earlyx2fon-timex2flate-triggers-ftw" class="header-anchor">#</a></h3><p>We’ve now looked at the two main types of triggers: repeated update triggers</p>
<p>and completeness&#x2F;watermark triggers. In many case, neither of them alone is</p>
<p>sufficient, but the combination of them together is. Beam recognizes this fact</p>
<p>by providing an extension of the standard watermark trigger that also supports</p>
<p>repeated update triggering on either side of the watermark. This is known as</p>
<p>the early&#x2F;on-time&#x2F;late trigger because it partitions the panes that are</p>
<p>materialized by the compound trigger into three categories:</p>
<ul>
<li>Zero or more <em>early panes</em>, which are the result of a repeated update</li>
</ul>
<p>trigger that periodically fires up until the watermark passes the end of</p>
<p>the window. The panes generated by these firings contain speculative</p>
<p>results, but allow us to observe the evolution of the window over</p>
<p>time as new input data arrive. This compensates for the shortcoming</p>
<p>of watermarks sometimes being <em>too slow</em>.</p>
<ul>
<li>A single <em>on-time pane</em>, which is the result of the</li>
</ul>
<p>completeness&#x2F;watermark trigger firing after the watermark passes the</p>
<p>end of the window. This firing is special because it provides an</p>
<p>assertion that the system now believes the input for this window to</p>
<p>be complete. This means that it is now safe to reason about <em>missing</em></p>
<p><em>data</em>; for example, to emit a partial join when performing an outer</p>
<p>join.</p>
<ul>
<li>Zero or more <em>late panes</em>, which are the result of another (possibly</li>
</ul>
<p>different) repeated update trigger that periodically fires any time late</p>
<p>data arrive after the watermark has passed the end of the window. In</p>
<p>the case of a perfect watermark, there will always be zero late panes.</p>
<p>But in the case of a heuristic watermark, any data the watermark</p>
<p>failed to properly account for will result in a late firing. This</p>
<p>compensates for the shortcoming of watermarks being <em>too fast</em>.</p>
<p>Let’s see how this looks in action. We’ll update our pipeline to use a periodic</p>
<p>processing-time trigger with an aligned delay of one minute for the early</p>
<p>firings, and a per-record trigger for the late firings. That way, the early firings</p>
<p>will give us some amount of batching for high-volume windows (thanks to</p>
<p>the fact that the trigger will fire only once per minute, regardless of the</p>
<p>throughput into the window), but we won’t introduce unnecessary latency for</p>
<p>the late firings, which are hopefully somewhat rare if we’re using a</p>
<p>reasonably accurate heuristic watermark. In Beam, that looks Example 2-7</p>
<p>(Figure 2-11 shows the results).</p>
<p><em>Example 2-7. Early, on-time, and late firings via the early&#x2F;on-time&#x2F;late API</em></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.into(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.triggering(AfterWatermark()</code></p>
<p><code>.withEarlyFirings(AlignedDelay(ONE_MINUTE))</code></p>
<p><code>.withLateFirings(AfterCount(1))))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p><em>Figure 2-11. Windowed summation on a streaming engine with early, on-time, and late firings</em></p>
<p>This version has two clear improvements over Figure 2-9:</p>
<ul>
<li>For the “watermarks too slow” case in the second window, [12:02,</li>
</ul>
<p>12:04): we now provide periodic early updates once per minute. The</p>
<p>difference is most stark in the perfect watermark case, for which</p>
<p>time-to-first-output is reduced from almost seven minutes down to</p>
<p>three and a half; but it’s also clearly improved in the heuristic case,</p>
<p>as well. Both versions now provide steady refinements over time</p>
<p>(panes with values 7, 10, then 18), with relatively minimal latency</p>
<p>between the input becoming complete and materialization of the final</p>
<p>output pane for the window.</p>
<ul>
<li>For the “heuristic watermarks too fast” case in the first window,</li>
</ul>
<p>[12:00, 12:02): when the value of 9 shows up late, we immediately</p>
<p>incorporate it into a new, corrected pane with value of 14.</p>
<p>One interesting side effect of these new triggers is that they effectively</p>
<p>normalize the output pattern between the perfect and heuristic watermark</p>
<p>versions. Whereas the two versions in Figure 2-10 were starkly different, the</p>
<p>two versions here look quite similar. They also look much more similar to the</p>
<p>various repeated update version from Figures 2-6 through 2-8, with one</p>
<p>important difference: thanks to the use of the watermark trigger, we can also</p>
<p>reason about input completeness in the results we generate with the early&#x2F;on</p>
<p>time&#x2F;late trigger. This allows us to better handle use cases that care about</p>
<p><em>missing data</em>, like outer joins, anomaly detection, and so on.</p>
<p>The biggest remaining difference between the perfect and heuristic early&#x2F;on</p>
<p>time&#x2F;late versions at this point is window lifetime bounds. In the perfect</p>
<p>watermark case, we know we’ll never see any more data for a window after</p>
<p>the watermark has passed the end of it, hence we can drop all of our state for</p>
<p>the window at that time. In the heuristic watermark case, we still need to hold</p>
<p>on to the state for a window for some amount of time to account for late data.</p>
<p>But as of yet, our system doesn’t have any good way of knowing just how</p>
<p>long state needs to be kept around for each window. That’s where <em>allowed</em></p>
<p><em>lateness</em> comes in.</p>
</details>



<details><summary>点击 原文</summary><h3><span id="when-allowed-lateness-ie-garbage-collection">*<strong>When*: Allowed Lateness (i.e., Garbage Collection)</strong></span><a href="#when-allowed-lateness-ie-garbage-collection" class="header-anchor">#</a></h3><p>Before moving on to our last question (“<em>How</em> do refinements of results</p>
<p>relate?”), I’d like to touch on a practical necessity within long-lived, out-of</p>
<p>order stream processing systems: garbage collection. In the heuristic</p>
<p>watermarks example in Figure 2-11, the persistent state for each window</p>
<p>lingers around for the entire lifetime of the example; this is necessary to allow</p>
<p>us to appropriately deal with late data when&#x2F;if they arrive. But while it would</p>
<p>be great to be able to keep around all of our persistent state until the end of</p>
<p>time, in reality, when dealing with an unbounded data source, it’s often not</p>
<p>practical to keep state (including metadata) for a given window indefinitely;</p>
<p>we’ll eventually run out of disk space (or at the very least tire of paying for it,</p>
<p>as the value for older data diminishes over time).</p>
<p>As a result, any real-world out-of-order processing system needs to provide</p>
<p>some way to bound the lifetimes of the windows it’s processing. A clean and</p>
<p>concise way of doing this is by defining a horizon on the allowed lateness</p>
<p>within the system; that is, placing a bound on how late any given <em>record</em> may</p>
<p>be (relative to the watermark) for the system to bother processing it; any data</p>
<p>that arrives after this horizon are simply dropped. After you’ve bounded how</p>
<p>late individual data may be, you’ve also established precisely how long the</p>
<p>state for windows must be kept around: until the watermark exceeds the</p>
<p>lateness horizon for the end of the window. But in addition, you’ve also given</p>
<p>the system the liberty to immediately drop any data later than the horizon as</p>
<p>soon as they’re observed, which means the system doesn’t waste resources</p>
<p>processing data that no one cares about.</p>
<p><strong>MEASURING LATENESS</strong></p>
<p>It might seem a little odd to be specifying a horizon for handling late data</p>
<p>using the very metric that resulted in the late data in the first place (i.e., the</p>
<p>heuristic watermark). And in some sense it is. But of the options available,</p>
<p>it’s arguably the best. The only other practical option would be to specify</p>
<p>the horizon in processing time (e.g., keep windows around for 10 minutes</p>
<p>of processing time after the watermark passes the end of the window), but</p>
<p>using processing time would leave the garbage collection policy</p>
<p>vulnerable to issues within the pipeline itself (e.g., workers crashing,</p>
<p>causing the pipeline to stall for a few minutes), which could lead to</p>
<p>windows that didn’t actually have a chance to handle late data that they</p>
<p>otherwise should have. By specifying the horizon in the event-time</p>
<p>domain, garbage collection is directly tied to the actual progress of the</p>
<p>pipeline, which decreases the likelihood that a window will miss its</p>
<p>opportunity to handle late data appropriately.</p>
<p>Note however, that not all watermarks are created equal. When we speak</p>
<p>of watermarks in this book, we generally refer to <em>low</em> watermarks, which</p>
<p>pessimistically attempt to capture the event time of the <em>oldest</em> unprocessed</p>
<p>record the system is aware of. The nice thing about dealing with lateness</p>
<p>via low watermarks is that they are resilient to changes in event-time</p>
<p>skew; no matter how large the skew in a pipeline may grow, the low</p>
<p>watermark will always track the oldest outstanding event known to the</p>
<p>system, providing the best guarantee of correctness possible.</p>
<p>In contrast, some systems may use the term “watermark” to mean other</p>
<p>things. For example, watermarks in Spark Structured Streaming are <em>high</em></p>
<p>watermarks, which optimistically track the event time of the <em>newest</em> record</p>
<p>the system is aware of. When dealing with lateness, the system is free to</p>
<p>garbage collect any window older than the high watermark adjusted by</p>
<p>some user-specified lateness threshold. In other words, the system allows</p>
<p>you to specify the maximum amount of event-time skew you expect to see</p>
<p>in your pipeline, and then throws away any data outside of that skew</p>
<p>window. This can work well if skew within your pipeline remains within</p>
<p>some constant delta, but is more prone to incorrectly discarding data than</p>
<p>low watermarking schemes.</p>
<p>Because the interaction between allowed lateness and the watermark is a little</p>
<p>subtle, it’s worth looking at an example. Let’s take the heuristic watermark</p>
<p>pipeline from Example 2-7&#x2F;Figure 2-11 and add in Example 2-8 a lateness</p>
<p>horizon of one minute (note that this particular horizon has been chosen</p>
<p>strictly because it fits nicely into the diagram; for real-world use cases, a</p>
<p>larger horizon would likely be much more practical):</p>
<p><em>Example 2-8. Early&#x2F;on-time&#x2F;late firings with allowed lateness</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AfterCount(1)))</span><br><span class="line">.withAllowedLateness(ONE_MINUTE))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>The execution of this pipeline would look something like Figure 2-12, in</p>
<p>which I’ve added the following features to highlight the effects of allowed</p>
<p>lateness:</p>
<ul>
<li>The thick black line denoting the current position in processing time</li>
</ul>
<p>is now annotated with ticks indicating the lateness horizon (in event</p>
<p>time) for all active windows.</p>
<ul>
<li>When the watermark passes the lateness horizon for a window, that</li>
</ul>
<p>window is closed, which means that all state for the window is</p>
<p>discarded. I leave around a dotted rectangle showing the extent of</p>
<p>time (in both domains) that the window covered when it was closed,</p>
<p>with a little tail extending to the right to denote the lateness horizon</p>
<p>for the window (for contrasting against the watermark).</p>
<ul>
<li>For this diagram only, I’ve added an additional late datum for the</li>
</ul>
<p>first window with value 6. The 6 is late, but still within the allowed</p>
<p>lateness horizon and thus is incorporated into an updated result with</p>
<p>value 11. The 9, however, arrives beyond the lateness horizon, so it</p>
<p>is simply dropped.</p>
<p><em>Figure 2-12. Allowed lateness with early&#x2F;on-time&#x2F;late firings</em></p>
<p>Two final side notes about lateness horizons:</p>
<ul>
<li>To be absolutely clear, if you happen to be consuming data from</li>
</ul>
<p>sources for which perfect watermarks are available, there’s no need</p>
<p>to deal with late data, and an allowed lateness horizon of zero</p>
<p>seconds will be optimal. This is what we saw in the perfect</p>
<p>watermark portion of Figure 2-10.</p>
<ul>
<li>One noteworthy exception to the rule of needing to specify lateness</li>
</ul>
<p>horizons, even when heuristic watermarks are in use, would be</p>
<p>something like computing global aggregates over all time for a</p>
<p>tractably finite number of keys (e.g., computing the total number of</p>
<p>visits to your site over all time, grouped by web browser family). In</p>
<p>this case, the number of active windows in the system is bounded by</p>
<p>the limited keyspace in use. As long as the number of keys remains</p>
<p>manageably low, there’s no need to worry about limiting the lifetime</p>
<p>of windows via allowed lateness.</p>
<p>Practicality sated, let’s move on to our fourth and final question.</p>
</details>







<details><summary>点击 原文</summary><h1><span id="how-accumulation">*<strong>How*: Accumulation</strong></span><a href="#how-accumulation" class="header-anchor">#</a></h1><p>When triggers are used to produce multiple panes for a single window over</p>
<p>time, we find ourselves confronted with the last question: “<em>How</em> do</p>
<p>refinements of results relate?” In the examples we’ve seen so far, each</p>
<p>successive pane is built upon the one immediately preceding it. However,</p>
<p>there are actually three different modes of accumulation:</p>
<ul>
<li>Discarding</li>
</ul>
<p>Every time a pane is materialized, any stored state is discarded. This</p>
<p>means that each successive pane is independent from any that came</p>
<p>before. Discarding mode is useful when the downstream consumer is</p>
<p>performing some sort of accumulation itself; for example, when sending</p>
<p>integers into a system that expects to receive deltas that it will sum</p>
<p>together to produce a final count.</p>
<ul>
<li>Accumulating</li>
</ul>
<p>As in Figures 2-6 through 2-11, every time a pane is materialized, any</p>
<p>stored state is retained, and future inputs are accumulated into the existing</p>
<p>state. This means that each successive pane builds upon the previous</p>
<p>panes. Accumulating mode is useful when later results can simply</p>
<p>overwrite previous results, such as when storing output in a key&#x2F;value</p>
<p>store like HBase or Bigtable.</p>
<ul>
<li>Accumulating and retracting</li>
</ul>
<p>This is like accumulating mode, but when producing a new pane, it also</p>
<p>produces independent retractions for the previous pane(s). Retractions</p>
<p>(combined with the new accumulated result) are essentially an explicit</p>
<p>way of saying “I previously told you the result was <em>X</em>, but I was wrong.</p>
<p>Get rid of the <em>X</em> I told you last time, and replace it with <em>Y</em>.” There are two</p>
<p>cases for which retractions are particularly helpful:</p>
<ul>
<li>When consumers downstream are <em>regrouping data by a different</em></li>
</ul>
<p><em>dimension</em>, it’s entirely possible the new value may end up keyed</p>
<p>differently from the previous value and thus end up in a different</p>
<p>group. In that case, the new value can’t just overwrite the old value;</p>
<p>you instead need the retraction to remove the old value</p>
<ul>
<li>When <em>dynamic windows</em> (e.g., sessions, which we look at more</li>
</ul>
<p>closely in a few moments) are in use, the new value might be</p>
<p>replacing more than one previous window, due to window merging.</p>
<p>In this case, it can be difficult to determine from the new window</p>
<p>alone which old windows are being replaced. Having explicit</p>
<p>retractions for the old windows makes the task straightforward. We</p>
<p>see an example of this in detail in Chapter 8.</p>
<p>The different semantics for each group are somewhat clearer when seen side</p>
<p>by-side. Consider the two panes for the second window (the one with event</p>
<p>time range [12:06, 12:08)) in Figure 2-11 (the one with early&#x2F;on-time&#x2F;late</p>
<p>triggers). Table 2-1 shows what the values for each pane would look like</p>
<p>across the three accumulation modes (with <em>accumulating</em> mode being the</p>
<p>specific mode used in Figure 2-11 itself).</p>
<p><em>Table 2-1. Comparing accumulation modes using the second</em></p>
<p><em>window from Figure 2-11</em></p>
<p><strong>Discarding</strong></p>
<p><strong>Accumulating</strong></p>
<p><strong>Accumulating &amp; Retracting</strong></p>
<p><strong>Pane 1: inputs&#x3D;[3]</strong></p>
<p>3</p>
<p>3</p>
<p>3</p>
<p><strong>Pane 2: inputs&#x3D;[8, 1]</strong></p>
<p>9</p>
<p>12</p>
<p>12, –3</p>
<p><strong>Value of final normal pane</strong> 9</p>
<p>12</p>
<p>12</p>
<p><strong>Sum of all panes</strong></p>
<p>12</p>
<p>15</p>
<p>12</p>
<p>Let’s take a closer look at what’s happening:</p>
<ul>
<li>Discarding</li>
</ul>
<p>Each pane incorporates only the values that arrived during that specific</p>
<p>pane. As such, the final value observed does not fully capture the total</p>
<p>sum. However, if you were to sum all of the independent panes</p>
<p>themselves, you would arrive at a correct answer of 12. This is why</p>
<p>discarding mode is useful when the downstream consumer itself is</p>
<p>performing some sort of aggregation on the materialized panes.</p>
<ul>
<li>Accumulating</li>
</ul>
<p>As in Figure 2-11, each pane incorporates the values that arrived during</p>
<p>that specific pane, plus all of the values from previous panes. As such, the</p>
<p>final value observed correctly captures the total sum of 12. If you were to</p>
<p>sum up the individual panes themselves, however, you’d effectively be</p>
<p>double-counting the inputs from pane 1, giving you an incorrect total sum</p>
<p>of 15. This is why accumulating mode is most useful when you can</p>
<p>simply overwrite previous values with new values: the new value already</p>
<p>incorporates all of the data seen thus far.</p>
<ul>
<li>Accumulating and retracting</li>
</ul>
<p>Each pane includes both a new accumulating mode value as well as a</p>
<p>retraction of the previous pane’s value. As such, both the last value</p>
<p>observed (excluding retractions) as well as the total sum of all</p>
<p>materialized panes (including retractions) provide you with the correct</p>
<p>answer of 12. This is why retractions are so powerful.</p>
<p>Example 2-9 demonstrates discarding mode in action, illustrating the changes</p>
<p>we would make to Example 2-7:</p>
<p><em>Example 2-9. Discarding mode version of early&#x2F;on-time&#x2F;late firings</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AtCount(1)))</span><br><span class="line">.discardingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Running again on a streaming engine with a heuristic watermark would</p>
<p>produce output like that shown in Figure 2-13.</p>
<p><em>Figure 2-13. Discarding mode version of early&#x2F;on-time&#x2F;late firings on a streaming engine</em></p>
<p>Even though the overall shape of the output is similar to the accumulating</p>
<p>mode version from Figure 2-11, note how none of the panes in this discarding</p>
<p>version overlap. As a result, each output is independent from the others.</p>
<p>If we want to look at retractions in action, the change would be similar, as</p>
<p>shown in Example 2-10. ??? depicts the results.</p>
<p><em>Example 2-10. Accumulating and retracting mode version of early&#x2F;on</em></p>
<p><em>time&#x2F;late firings</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AtCount(1)))</span><br><span class="line">.accumulatingAndRetractingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Accumulating and retracting mode version of early&#x2F;late firings on a streaming</p>
<p>engine</p>
<p>Because the panes for each window all overlap, it’s a little tricky to see the</p>
<p>retractions clearly. The retractions are indicated in red, which combines with</p>
<p>the overlapping blue panes to yield a slightly purplish color. I’ve also</p>
<p>horizontally shifted the values of the two outputs within a given pane slightly</p>
<p>(and separated them with a comma) to make them easier to differentiate.</p>
<p>Figure 2-14 combines the final frames of Figures 2-9, 2-11 (heuristic only),</p>
<p>and side-by-side, providing a nice visual contrast of the three modes.</p>
<p><em>Figure 2-14. Side-by-side comparison of accumulation modes</em></p>
<p>As you can imagine, the modes in the order presented (discarding,</p>
<p>accumulating, accumulating and retracting) are each successively more</p>
<p>expensive in terms of storage and computation costs. To that end, choice of</p>
<p>accumulation mode provides yet another dimension for making trade-offs</p>
<p>along the axes of correctness, latency, and cost.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>With this chapter complete, you now understand the basics of robust stream</p>
<p>processing and are ready to go forth into the world and do amazing things. Of</p>
<p>course, there are eight more chapters anxiously waiting for your attention, so</p>
<p>hopefully you won’t go forth like right now, this very minute. But regardless,</p>
<p>let’s recap what we’ve just covered, lest you forget any of it in your haste to</p>
<p>amble forward. First, the major concepts we touched upon:</p>
<ul>
<li>Event time versus processing time</li>
</ul>
<p>The all-important distinction between when events occurred and when</p>
<p>they are observed by your data processing system.</p>
<ul>
<li>Windowing</li>
</ul>
<p>The commonly utilized approach to managing unbounded data by slicing</p>
<p>it along temporal boundaries (in either processing time or event time,</p>
<p>though we narrow the definition of windowing in the Beam Model to</p>
<p>mean only within event time).</p>
<ul>
<li>Triggers</li>
</ul>
<p>The declarative mechanism for specifying precisely when materialization</p>
<p>of output makes sense for your particular use case.</p>
<ul>
<li>Watermarks</li>
</ul>
<p>The powerful notion of progress in event time that provides a means of</p>
<p>reasoning about completeness (and thus missing data) in an out-of-order</p>
<p>processing system operating on unbounded data.</p>
<ul>
<li>Accumulation</li>
</ul>
<p>The relationship between refinements of results for a single window for</p>
<p>cases in which it’s materialized multiple times as it evolves.</p>
<ul>
<li>Second, the four questions we used to frame our exploration:</li>
<li><em>What</em> results are calculated? &#x3D; transformations.</li>
<li><em>Where</em> in event time are results calculated? &#x3D; windowing.</li>
<li><em>When</em> in processing time are results materialized? &#x3D; triggers plus</li>
</ul>
<p>watermarks.</p>
<ul>
<li><em>How</em> do refinements of results relate? &#x3D; accumulation.</li>
</ul>
<p>Third, to drive home the flexibility afforded by this model of stream</p>
<p>processing (because in the end, that’s really what this is all about: balancing</p>
<p>competing tensions like correctness, latency, and cost), a recap of the major</p>
<p>variations in output we were able to achieve over the same dataset with only a</p>
<p>minimal amount of code change:</p>
<p>Integer summation</p>
<p>Example 2-1 &#x2F; Figure 2-3</p>
<p>Integer summation</p>
<p>Fixed windows batch</p>
<p>Example 2-2 &#x2F; Figure 2-5</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Repeated per-record trigger</p>
<p>Example 2-3 &#x2F; Figure 2-6</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Repeated aligned-delay trigger</p>
<p>Example 2-4 &#x2F; Figure 2-7</p>
<p>Repeated</p>
<p>unaligned-delay</p>
<p>trigger</p>
<p>Example 2-5 &#x2F; Figure 2-8</p>
<p>Fixed windows streaming</p>
<p>Heuristic watermark trigger</p>
<p>Example 2-6 &#x2F; Figure 2-10</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Discarding</p>
<p>Example 2-9 &#x2F; Figure 2-13</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Accumulating</p>
<p>Example 2-7 &#x2F; Figure 2-11</p>
<p>Integer summation</p>
<p>Fixed windows streaming</p>
<p>Early&#x2F;on-time&#x2F;late trigger</p>
<p>Accumulating and Retracting</p>
<p>Example 2-10 &#x2F; ???</p>
<p>All that said, at this point, we’ve really looked at only one type of windowing:</p>
<p>fixed windowing in event time. As we know, there are a number of</p>
<p>dimensions to windowing, and I’d like to touch upon at least two more of</p>
<p>those before we call it day with the Beam Model. First, however, we’re going</p>
<p>to take a slight detour to dive deeper into the world of watermarks, as this</p>
<p>knowledge will help frame future discussions (and be fascinating in and of</p>
<p>itself). Enter Slava, stage right…</p>
</details>







<details><summary>点击 原文</summary><ol>
<li>If you’re fortunate enough to be reading the Safari version of the book, you</li>
</ol>
<p>have full-blown time-lapse animations just like in “Streaming 102”. For print,</p>
<p>Kindle, and other ebook versions, there are static images with a link to</p>
<p>animated versions on the web.</p>
<ol start="2">
<li>Bear with me here. Fine-grained emotional expressions via composite</li>
</ol>
<p>punctuation (i.e., emoticons) are strictly forbidden in O’Reilly publications &lt;</p>
<p>winky-smiley&#x2F;&gt;.</p>
<ol start="3">
<li>And indeed, we did just that with the original triggers feature in Beam. In</li>
</ol>
<p>retrospect, we went a bit overboard. Future iterations will be simpler and</p>
<p>easier to use, and in this book I focus only on the pieces that are likely to</p>
<p>remain in some form or another.</p>
<ol start="4">
<li>More accurately, the input to the function is really the state at time <em>P</em> of</li>
</ol>
<p>everything upstream of the point in the pipeline where the watermark is being</p>
<p>observed: the input source, buffered data, data actively being processed, and</p>
<p>so on; but conceptually it’s simpler to think of it as a mapping from</p>
<p>processing time to event time.</p>
<ol start="5">
<li>Note that I specifically chose to omit the value of 9 from the heuristic</li>
</ol>
<p>watermark because it will help me to make some important points about late</p>
<p>data and watermark lag. In reality, a heuristic watermark might be just as</p>
<p>likely to omit some other value(s) instead, which in turn could have</p>
<p>significantly less drastic effect on the watermark. If winnowing late-arriving</p>
<p>data from the watermark is your goal (which is very valid in some cases, such</p>
<p>as abuse detection, for which you just want to see a significant majority of the</p>
<p>data as quickly as possible), you don’t necessarily want a heuristic watermark</p>
<p>rather than a perfect watermark. What you really want is a percentile</p>
<p>watermark, which explicitly drops some percentile of late-arriving data from</p>
<p>its calculations. See Chapter 3.</p>
<ol start="6">
<li>Which isn’t to say there aren’t use cases that care primarily about</li>
</ol>
<p>correctness and not so much about latency; in those cases, using an accurate</p>
<p>watermark as the sole driver of output from a pipeline is a reasonable</p>
<p>approach.</p>
<ol start="7">
<li>And, as we know from before, this assertion is either guaranteed, in the case</li>
</ol>
<p>of a perfect watermark being used, or an educated guess, in the case of a</p>
<p>heuristic watermark.</p>
<ol start="8">
<li>You might note that there should logically be a fourth mode: discarding and</li>
</ol>
<p>retracting. That mode isn’t terribly useful in most cases, so I don’t discuss it</p>
<p>further here.</p>
<ol start="9">
<li>In retrospect, it probably would have been clearer to choose a different set</li>
</ol>
<p>of names that are more oriented toward the observed nature of data in the</p>
<p>materialized stream (e.g., “output modes”) rather than names describing the</p>
<p>state management semantics that yield those data. Perhaps: discarding mode</p>
<p>→ delta mode, accumulating mode → value mode, accumulating and</p>
<p>retracting mode → value and retraction mode? However, the</p>
<p>discarding&#x2F;accumulating&#x2F;accumulating and retracting names are enshrined in</p>
<p>the 1.x and 2.x lineages of the Beam Model, so I don’t want to introduce</p>
<p>potential confusion in the book by deviating. Also, it’s very likely</p>
<p>accumulating modes will blend into the background more with Beam 3.0 and</p>
<p>the introduction of sink triggers; more on this when we discuss SQL in</p>
<p>Chapter 8.</p>
</details>


]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-第二章： 数据处理的什么、何地、何时以及如何进行[完整]</title>
    <url>/www6vHomeHexo/2000/03/17/streamingSystemChapter2/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%B7%AF%E7%BA%BF%E5%9B%BE">路线图</a></li>
<li><a href="#%E6%89%B9%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%BB%80%E4%B9%88-%E5%92%8C-%E4%BD%95%E5%9C%B0">批处理基础知识：什么 和  何地</a><ul>
<li><a href="#%E4%BB%80%E4%B9%88%E8%BD%AC%E6%8D%A2"><em>什么</em>：转换</a></li>
<li><a href="#u%E4%BD%95%E5%9C%B0where-%E7%AA%97%E5%8F%A3-u"><u><em><strong>何地(Where)</strong></em>: 窗口 </u></a></li>
</ul>
</li>
<li><a href="#u%E6%B5%81%E5%A4%84%E7%90%86%E4%BD%95%E6%97%B6when-%E5%92%8C-%E5%A6%82%E4%BD%95%E8%BD%AC%E6%8D%A2how-u"><u>流处理：何时(When) 和  如何转换(How) </u></a><ul>
<li><a href="#%E4%BD%95%E6%97%B6%E5%85%B3%E4%BA%8E%E8%A7%A6%E5%8F%91%E5%99%A8%E7%9A%84%E7%BE%8E%E5%A6%99%E4%B9%8B%E5%A4%84"><em>何时</em>：关于触发器的美妙之处</a></li>
<li><a href="#%E5%B0%B1%E6%98%AF%E8%A7%A6%E5%8F%91%E5%99%A8%E6%98%AF%E7%BE%8E%E5%A6%99%E7%9A%84%E4%B8%9C%E8%A5%BF">就是触发器是美妙的东西！</a></li>
<li><a href="#%E4%BD%95%E6%97%B6%E6%B0%B4%E4%BD%8D%E7%BA%BF">何时：水位线</a></li>
<li><a href="#%E4%BD%95%E6%97%B6%E8%A7%A6%E5%8F%91%E6%97%A9%E6%9C%9F%E5%87%86%E6%97%B6%E5%BB%B6%E8%BF%9F%E8%A7%A6%E5%8F%91-ftw">何时触发：早期&#x2F;准时&#x2F;延迟触发 FTW！</a></li>
<li><a href="#%E4%BD%95%E6%97%B6-%E5%85%81%E8%AE%B8%E5%BB%B6%E8%BF%9F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6"><em><strong>何时</strong></em>: 允许延迟（垃圾回收）</a></li>
</ul>
</li>
<li><a href="#%E6%96%B9%E6%B3%95%E7%B4%AF%E5%8A%A0"><em><strong>方法</strong></em>：累加</a></li>
<li><a href="#%E6%91%98%E8%A6%81"><strong>摘要</strong></a></li>
<li><a href="#draft-here">Draft Here</a></li>
</ul>
<!-- tocstop -->

</div>


<p>好了，派对人们，是时候开始具体化了！</p>
<p>第一章主要涉及三个方面：<em>术语</em>，准确定义当我使用过载术语“流式处理”时的含义；<em>批处理与流处理</em>，比较两种系统的理论能力，并推断只有两个方面是将流处理系统推向超越批处理系统所必须的：正确性和关于时间的推理工具；以及<em>数据处理模式</em>，研究在处理有界数据和无界数据时，批处理系统和流处理系统采用的概念方法。</p>
<p>在本章中，我们将进一步关注第一章中的数据处理模式，但更详细地在具体示例的背景下进行。到最后，我们将涵盖我认为对于强大的乱序数据处理所必需的核心原则和概念；这些是真正将您推向经典批处理之外的关于时间的推理工具。</p>
<p>为了让您了解实际情况，我使用Apache Beam代码片段，配合时间流逝图表，提供概念的可视化表示。Apache Beam是一种统一的批处理和流处理编程模型和可移植性层，具有各种语言的具体SDK（例如Java和Python）。使用Apache Beam编写的管道可以在任何支持的执行引擎上进行便携式运行（Apache Apex、Apache Flink、Apache Spark、Cloud Dataflow等）。</p>
<p>我在这里使用Apache Beam作为示例，不是因为这是一本Beam书籍（它不是），而是因为它最完全地体现了本书中描述的概念。回想一下，“流处理102”最初编写时（当它仍然是Google Cloud Dataflow的Dataflow模型，而不是Apache Beam的Beam模型时），它是唯一存在的系统，提供了所有我们将在此处涵盖的示例所需的表达能力。一年半后，我很高兴地说，很多事情发生了改变，大多数主要系统已经移动或正在朝向支持类似于本书所描述的模型。因此，请放心，尽管是通过Beam的镜头，我们在这里涵盖的概念同样适用于您将遇到的大多数其他系统。</p>
<h1><span id="路线图">路线图</span><a href="#路线图" class="header-anchor">#</a></h1><p>为了帮助为本章打下基础，我想提出五个主要概念，这些概念将支撑所有讨论，以及大部分第一部分的内容。我们已经讨论了其中两个。</p>
<p>在第一章中，我首先建立了事件时间（事件发生的时间）和处理时间（在处理期间观察到的时间）之间的关键区别。这为本书提出的一个主要论点奠定了基础：如果您关心正确性和事件实际发生的上下文，则必须相对于它们固有的事件时间而不是在分析过程中遇到它们的处理时间来分析数据。</p>
<p>然后，我介绍了<em>窗口化</em>的概念（即沿着时间边界对数据集进行分区），这是一种常用的方法，用于应对无限数据源在技术上可能永远不会结束的事实。窗口化策略的一些更简单的例子是<em>固定</em>和<em>滑动</em>窗口，但更复杂的窗口化类型，例如<em>会话</em>（其中窗口是由数据本身的特征定义的;例如，捕获用户的每个活动会话，然后是不活动的间隙），也广泛使用。</p>
<p>除了这两个概念之外，我们现在将仔细研究另外三个概念：</p>
<ul>
<li><p>触发器 Triggers</p>
<p><u>触发器是一种声明窗口的输出相对于某些外部信号何时被实现的机制。 触发器提供了选择何时发出输出的灵活性。</u>在某种意义上，您可以将它们视为用于指示何时应该实现结果的流量控制机制。<u> 另一种看法是，触发器就像相机的快门释放，允许您声明何时以时间为快照记录正在计算的结果。</u><br><u>触发器还可以观察窗口多次演变时的输出，从而打开了随着时间推移改进结果的大门，这允许在数据到达时提供推测结果，以及处理上游数据（修订）随时间变化或数据到达延迟的情况（例如，移动场景，其中某人的手机在离线时记录各种操作和事件时间，然后在恢复连接时上传这些事件进行处理）。</u></p>
</li>
<li><p>水位线 Watermarks</p>
<p><u>水位线是相对于事件时间的输入完整性的概念。具有时间 <em>X</em> 值的水位线表明：“事件时间小于<em>X</em>的所有输入数据都已被观察到。”因此，水位线在观察没有已知结束的无限数据源时作为进度的指标。我们在本章中介绍了水位线的基础知识，然后Slava在第3章中深入探讨了这个主题。 </u> </p>
</li>
<li><p>累积 Accumulation</p>
<p><u> 累积模式指定了对于同一窗口所观察到的多个结果之间的关系。这些结果可能完全不相关，即代表随时间独立的增量，也可能存在重叠。不同的累积模式具有不同的语义和相关成本，因此适用于各种用例。</u></p>
</li>
</ul>
<p>此外，因为我认为这使得更容易理解所有这些概念之间的关系，我们重新审视了旧的并在回答四个问题的结构内探索了新的，我提出这些问题对于每个无限数据处理问题都至关重要：</p>
<ul>
<li><u>计算出<em><strong>什么</strong></em>结果？这个问题可以通过管道中的转换类型来回答。这包括计算总和、构建直方图、训练机器学习模型等等。这基本上也是经典批处理所回答的问题。</u></li>
<li><u>在事件时间中，结果是<em><strong>在哪里</strong></em>计算的？这个问题通过在管道中使用事件时间窗口来回答。这包括第一章中常见的窗口示例（固定窗口、滑动窗口和会话窗口）；似乎没有窗口概念的用例（例如时间无关的处理；经典的批处理也通常属于这个类别）；以及其他更复杂的窗口类型，例如有时间限制的拍卖。请注意，如果在记录到达系统时将入口时间分配为事件时间，则还可以包括处理时间窗口。</u></li>
<li><u>在处理时间中，结果<em><strong>在何时</strong></em>实现？ 这个问题是通过触发器和（可选的）水位线来回答的。对此主题有无限变化，但最常见的模式是涉及重复更新的模式（即，实现视图语义），利用水位线为每个窗口提供单个输出，仅在相应的输入被认为是完整的后才提供（即，应用于每个窗口的经典批处理语义），或两者的某种组合。</u></li>
<li><em><strong>如何</strong></em>处理结果的细化？这个问题是由使用的累积类型回答的：丢弃（其中结果是所有独立和独特的），累积（其中后来的结果建立在先前的结果之上），或累积和撤回（其中发出累积值加上先前触发的值的撤回）。</li>
</ul>
<p>我们在本书的其余部分中更详细地研究了这些问题。是的，我将尝试将这个颜色方案运用到底，以尝试清晰地表明哪些概念与<em>What&#x2F;Where&#x2F;When&#x2F;How</em>习语中的哪些问题有关系。你是受欢迎的 <code> &lt;winky smiley/&gt;</code>。 </p>
<h1><span id="批处理基础知识什么-和-何地">批处理基础知识：什么  和  何地</span><a href="#批处理基础知识什么-和-何地" class="header-anchor">#</a></h1><p>好的，让我们开始吧。首先，是批处理。</p>
<h3><span id="什么转换"><em>什么</em>：转换</span><a href="#什么转换" class="header-anchor">#</a></h3><p><u>应用在经典批处理中的转换回答了这个问题：“计算出了<em><strong>什么</strong></em>结果？”尽管你可能已经熟悉经典批处理，但我们还是要从这里开始，因为它是我们添加所有其他概念的基础。</u></p>
<p><u>在本章的其余部分（事实上，在本书的很大一部分中），我们将看到一个单一的示例：在由九个值组成的简单数据集上计算有键[keyed]整数总和。假设我们编写了一个团队游戏，并且我们想要构建一个管道，通过总结用户手机报告的个人分数来计算团队分数。如果我们将我们的九个示例得分捕获在名为“UserScores”的SQL表中，它可能看起来像这样：</u></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">----------------------------------</span><br><span class="line">|名称|团队|分数|事件时间|ProcTime|</span><br><span class="line">----------------------------------</span><br><span class="line">|朱莉|TeamX|5|12:00:26|12:05:19|</span><br><span class="line">|弗兰克|TeamX|9|12:01:26|12:08:19|</span><br><span class="line">|爱德华|TeamX|7|12:02:26|12:05:39|</span><br><span class="line">|朱莉|TeamX|8|12:03:06|12:07:06|</span><br><span class="line">|艾米|TeamX|3|12:03:39|12:06:13|</span><br><span class="line">|弗雷德|TeamX|4|12:04:19|12:06:39|</span><br><span class="line">|纳奥米|TeamX|3|12:06:39|12:07:19|</span><br><span class="line">|贝基|TeamX|8|12:07:26|12:08:39|</span><br><span class="line">|纳奥米|TeamX|1|12:07:46|12:09:00|</span><br><span class="line">----------------------------------</span><br></pre></td></tr></table></figure>

<p><u>请注意，此示例中的所有分数均来自同一团队的用户；这是为了让示例保持简单，因为我们的图表中的维度数量有限。而且，因为我们按团队分组，所以我们只关心最后三列：</u></p>
<ul>
<li>分数<br>与此事件相关联的单个用户分数</li>
<li>事件时间[EventTime]<br>  得分的事件时间；即得分发生的时间</li>
<li>处理时间[ProcTime]<br>  得分的处理时间；即管道观察到得分的时间</li>
</ul>
<p>对于每个示例管道，我们将查看一个时间演变图，该图突出显示数据随时间的演变方式。这些图将我们的九个分数绘制在我们关心的两个时间维度上：x轴上的事件时间和y轴上的处理时间。图2-1说明了输入数据的静态绘图效果。</p>
<img src="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0201.png" class>
<p><em>图2-1.在事件时间和处理时间中绘制的九个输入记录</em></p>
<p>随后的时间演变图是动画（Safari）或一系列帧（打印和所有其他数字格式），可以让您看到数据随时间的处理方式（在我们到达第一个时间演变图之后不久，我们会详细介绍这一点）。</p>
<p>在每个示例之前，都有一个Apache Beam Java SDK伪代码的简短片段，以使管道定义更具体。它是伪代码，因为我有时会弯曲规则以使示例更清晰，省略细节（如使用具体I &#x2F; O源），或简化名称（Beam Java 2.x及更早版本的触发器名称过长；我使用更简单的名称来清晰起见）。除了像这样的小事情之外，它是真实的Beam代码（本章中所有示例的真实代码都可在GitHub上找到）。</p>
<p>如果您已经熟悉像Spark或Flink这样的东西，您应该很容易理解Beam代码正在做什么。但是，为了让您快速了解，Beam有两个基本原语：</p>
<ul>
<li><p>PCollections</p>
<p>这些表示可以执行并行转换的数据集（因此名称的开头有“P”）。</p>
</li>
<li><p>PTransforms</p>
<p>这些应用于PCollections以创建新的PCollections。</p>
<p>PTransforms可以执行逐个元素的转换，它们可以将多个元素分组&#x2F;聚合在一起，或者它们可以是其他PTransforms的组合，如图2-2所示。</p>
</li>
</ul>
<img src="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0202.png" class>
<p><em>图2-2.转换类型</em></p>
<p>对于我们的示例，我们通常假设我们从预加载的PCollection&lt;KV&lt;Team，Integer&gt;&gt;“input”（即由Team和Integers组成的键&#x2F;值对的PCollection，其中Teams仅是表示团队名称的字符串，而Integers是来自相应团队的任何个人的分数）开始。在实际的管道中，我们将通过从I &#x2F; O源读取原始数据（例如，日志记录）的PCollection<string>并将其转换为PCollection&lt;KV&lt;Team，Integer&gt;&gt;来获取输入。通过将日志记录解析为适当的键&#x2F;值对。为了在这个第一个示例中清楚起见，我包括了所有这些步骤的伪代码，但在接下来的示例中，我省略了I &#x2F; O和解析。</string></p>
<p>因此，对于一个简单地从I &#x2F; O源读取数据，解析团队&#x2F;分数对并计算每个团队的总分的管道，我们将有像示例2-1中所示的那样的东西。</p>
<p><em>示例2-1.总和管道</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;String&gt; raw = IO.read（...）;</span><br><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; input = raw.apply（new ParseFn（））;</span><br><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals =</span><br><span class="line">  input.apply（Sum.integersPerKey（））;</span><br></pre></td></tr></table></figure>
<p>从I &#x2F; O源读取键&#x2F;值数据，其中Team（例如，团队名称的字符串）作为键，Integer（例如，对应团队成员的分数）作为值。然后将每个键的值相加以生成每个键的总和（例如，团队总得分）在输出集合中。</p>
<p>在接下来的所有示例中，看到描述我们正在分析的管道的代码片段后，我们将查看一个时间演变图，显示该管道在单个键的具体数据集上的执行。在实际管道中，您可以想象类似的操作会在多台机器上并行发生，但是为了简单起见，在我们的示例中将更清晰。</p>
<p>如前所述，Safari版本将完整执行作为动画电影呈现，而打印和所有其他数字格式使用一系列静态关键帧序列，这些关键帧提供了管道如何随时间推移的进展的感觉。在这两种情况下，我们还提供指向*<a href="http://www.streamingbook.net/">www.streamingbook.net</a>*上完全动画版本的URL。</p>
<p>每个图表在两个维度上绘制输入和输出：事件时间（x轴）和处理时间（y轴）。因此，由管道观察到的实时时间从底部向上推进，如处理时间轴中上升的粗黑线所示。输入是圆圈，圆圈内的数字表示特定记录的值。它们开始是浅灰色的，并随着管道观察到它们而变暗。</p>
<p>当管道观察到值时，它会在其中间状态中累积它们，并最终将聚合结果实现为输出。状态和输出由矩形表示（灰色为状态，蓝色为输出），聚合值靠近顶部，并且由矩形覆盖的区域表示积累到结果的事件时间和处理时间的部分。对于示例2-1中的管道，在经典批处理引擎上执行时，它看起来像图2-3所示。</p>
<div id="dplayer0" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0203.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图2-3.经典批处理</em></p>
<p>因为这是批处理管道，所以它会累积状态，直到看到所有输入为止（由顶部的虚线绿线表示），此时它会产生其单个输出为48。在这个示例中，我们正在计算事件时间的所有总和，因为我们没有应用任何特定的窗口转换；因此，状态和输出的矩形覆盖了整个x轴。但是，如果我们想要处理无限数据源，则经典批处理将不足够；我们无法等待输入结束，因为它实际上永远不会结束。我们想要的概念之一是窗口，这是我们在第1章中介绍的。<del>因此，在我们的第二个问题“事件时间的<em><strong>哪里</strong></em>计算结果？”的上下文中，我们现在将简要重新访问窗口。</del> 因此，在我们的第二个问题“事件时间<strong>在哪里</strong>计算结果？”的背景下，我们现在将简要重温窗口化。</p>
<h3><span id="何地where-窗口"><u><em><strong>何地(Where)</strong></em>: 窗口 </u></span><a href="#何地where-窗口" class="header-anchor">#</a></h3><p><u>如第1章所述，窗口是沿时间边界切分数据源的过程。常见的窗口策略包括固定窗口、滑动窗口和会话窗口，如图2-4所示。</u></p>
<img src="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0204.png" class>
<p><em>图2-4.示例窗口策略。每个示例均显示为三个不同的键，突出显示对齐窗口（适用于所有数据）和不对齐窗口（适用于数据子集）之间的差异。</em></p>
<p><u>为了更好地了解实践中的窗口处理，让我们将整数求和管道分隔成固定的两分钟窗口。<br>使用Beam，改变很简单，只需添加一个Window.into转换，您可以在示例2-2中看到高亮显示。</u></p>
<p><u><em>示例2-2.窗口化的求和代码</em></u></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">  .apply（Window.into（FixedWindows.of（TWO_MINUTES）））</span><br><span class="line">  .apply（Sum.integersPerKey（））;</span><br></pre></td></tr></table></figure>
<p><u>请记住，Beam提供了一个统一的模型，可以在批处理和流处理中使用，因为从语义上讲，批处理实际上只是流处理的一个子集。因此，让我们首先在批处理引擎上执行此管道；机制更加简单，当我们切换到流引擎时，它将为我们提供直接比较的内容。图2-5呈现了结果。</u></p>
<div id="dplayer1" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0205.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图2-5.在批处理引擎上的分段求和</em></p>
<p>与以前一样，输入会在状态中累积，直到完全消耗，此后才会产生输出。但是，在这种情况下，我们不仅仅得到一个输出，而是得到四个输出：每个相关的两分钟事件时间窗口的单个输出。</p>
<p>到目前为止，我们已经重新访问了我在第1章中介绍的两个主要概念：事件时间和处理时间域之间的关系以及窗口。如果我们想继续，我们将需要开始添加本节开头提到的新概念：触发器、水位线和累积。</p>
<h1><span id="流处理何时when-和-如何转换how"><u>流处理：何时(When)  和  如何转换(How) </u></span><a href="#流处理何时when-和-如何转换how" class="header-anchor">#</a></h1><p><u>我们刚刚观察了在批处理引擎上执行窗口化管道的过程。但是，理想情况下，我们希望结果具有更低的延迟，并且我们还希望本地处理无界的数据源。切换到流处理引擎是朝着正确方向迈出的一步，但是我们之前等待输入完全被消耗后才生成输出的策略已经不再可行。于是，我们引入了触发器和水印的概念。</u></p>
<h3><span id="何时关于触发器的美妙之处"><em>何时</em>：关于触发器的美妙之处</span><a href="#何时关于触发器的美妙之处" class="header-anchor">#</a></h3><h3><span id="就是触发器是美妙的东西">就是触发器是美妙的东西！</span><a href="#就是触发器是美妙的东西" class="header-anchor">#</a></h3><p>触发器提供了答案：在处理时间中“<em><strong>何时</strong></em>” 来实现结果材料化。触发器声明何时在处理时间中应该发生窗口的输出（尽管触发器本身可能基于发生在其他时间域（例如事件时间域中进行的水位线进展）的事情做出这些决定，正如我们将在接下来的几个时刻中看到的）。每个窗口的特定输出称为窗格。</p>
<p>虽然可以想象出相当广泛的可能触发语义，但在概念上只有两种通常有用的触发器类型，实际应用几乎总是使用其中一种或两种的组合：</p>
<ul>
<li><p><u>重复更新触发器</u><br><u>这些触发器会定期生成窗口的更新窗格，随着其内容的变化而变化。这些更新可以在每个新记录到来时进行实现，也可以在一定的处理时间延迟之后进行，例如每分钟一次。选择重复更新触发器的周期主要是在平衡延迟和成本之间进行权衡。</u></p>
</li>
<li><p><u>完整性触发器</u><br><u>这些触发器仅在相信窗口的输入已经完整达到某个阈值时才为窗口创建一个窗格。这种类型的触发器最类似于我们在批处理中熟悉的处理方式：只有在输入完成之后才会提供结果。触发器方法中的区别在于完整性的概念仅限于单个窗口的上下文范围，而不总是绑定到整个输入的完整性。</u></p>
</li>
</ul>
<p>重复更新触发器是流式系统中最常见的触发器类型。它们易于实现，易于理解，并为特定类型的用例提供有用的语义：对材料化数据集的重复（并最终一致）更新，类似于数据库世界中材料化视图所得到的语义。</p>
<p>完整性触发器很少遇到，但提供与经典批处理世界中更相似的流式语义。它们还提供了用于推理诸如缺失数据和延迟数据之类的事物的工具，我们很快就会讨论（并在下一章中）当我们探索驱动完整性触发器的基础原语时：水位线。</p>
<p>但首先，让我们从简单的方面入手，看看一些基本的重复更新触发器的操作。为了使触发器的概念更加具体化，让我们继续向我们的示例管道添加最简单的触发器类型：每次新记录都会触发的触发器，如示例2-3所示。</p>
<p><em>示例2-3。每个记录都会重复触发</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(Repeatedly(AfterCount(1))));</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>如果我们在流式引擎上运行这个新管道，结果会像图2-6所示。</p>
<div id="dplayer2" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer2"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0206.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><u><em>图2-6.在流处理引擎上的单个记录触发</em> </u></p>
<p>您可以看到我们现在为每个窗口获得多个输出（窗格）：每个对应输入的一次。当输出流被写入某种表以便您可以简单地轮询结果时，这种触发模式工作得很好。每当您查看表时，您将看到给定窗口的最新值，这些值随着时间的推移会趋于正确。</p>
<p>单个记录触发的一个缺点是它非常繁琐。在处理大规模数据时，聚合（例如求和）提供了一个很好的机会，可以在不丢失信息的情况下减少流的基数。这对于具有高交易量密钥的情况特别明显；对于我们的示例，具有许多活动玩家的大型团队。想象一下一个大规模的多人游戏，其中玩家被分为两个阵营，并且您想按阵营基础上统计统计数据。对于给定阵营中的每个玩家的每个新输入记录更新您的统计数据可能是不必要的。相反，您可能会满意地在一些处理时间延迟之后进行更新，例如每秒或每分钟。使用处理时间延迟的好处是，它对高交易量键或窗口具有均等化效应：由此产生的流在基数方面最终会更加统一。</p>
<p><u>触发器中的处理时间延迟有两种不同的方法：<em>对齐延迟</em>（其中延迟将处理时间切片为在键和窗口之间对齐的固定区域）和<em>不对齐延迟</em>（其中延迟相对于在给定窗口中观察到的数据）。具有不对齐延迟的管道可能类似于Beam中的示例2-4，其结果如图2-7所示。</u></p>
<p><u><em>示例 2-4。 在对齐的两分钟处理时间边界上触发</em></u></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(Repeatedly(AlignedDelay(TWO_MINUTES)))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>


<div id="dplayer3" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer3"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0207.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><u><em>图2-7。两分钟对齐的延迟触发器（即，微批处理）</em></u></p>
<p><u>这种对齐延迟触发器实际上类似于Spark Streaming之类的微批处理流系统。它的好处在于可预测性;您在相同时间内获得所有修改的窗口的定期更新。这也是缺点：所有更新都同时发生，这会导致爆发式的工作负载，通常需要更大的峰值配额来正确处理负载。另一种选择是使用不对齐的延迟。这在Beam中可能类似于示例2-5，其结果如图2-8所示。</u></p>
<p><u><em>示例2-5。在不对齐的两分钟处理时间边界上触发</em></u></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(Repeatedly(UnalignedDelay(TWO_MINUTES))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>



<div id="dplayer4" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer4"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0208.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><u><em>图2-8。两分钟不对齐的延迟触发器</em></u></p>
<p>将图2-8中的不对齐延迟与图2-6中的对齐延迟进行对比，可以很容易地看出不对齐延迟如何更均匀地分布负载。对于任何给定窗口涉及的实际延迟在两者之间有所不同，有时更多，有时更少，但最终平均延迟将基本保持不变。从这个角度来看，对于大规模处理，不对齐的延迟通常是更好的选择，因为它们会导致时间上更均匀的负载分布。</p>
<p>重复更新触发器非常适合我们只想要随着时间的推移定期更新结果的用例，并且对于这些更新收敛到正确性并没有明确的指示的情况感到满意。但是，如我们在第1章讨论的那样，分布式系统的不确定性通常会导致事件发生的时间与您的管道实际观察到该事件的时间之间的变化程度不同，这意味着很难理解您的输出何时提供了输入数据的准确和完整视图。对于重视输入完整性的情况，重要的是有某种方法来推理完整性，而不是盲目地信任哪个子集的数据已经流到您的管道中。然后进入水位线。</p>
<h3><span id="何时水位线">何时：水位线</span><a href="#何时水位线" class="header-anchor">#</a></h3><p>水位线是回答“处理时间何时实现结果”的问题的一个支持方面。水位线是输入完整性在事件时间域中的时间概念。换句话说，它们是系统相对于正在处理的事件流记录的事件时间测量进度和完整性的方式（有界或无界的情况下都很有用，但在无界的情况下它们更明显）。</p>
<p>回想一下第一章中的这个图表，在图2-9中稍作修改，我将事件时间和处理时间之间的偏差描述为大多数实际分布式数据处理系统中时间的不断变化的函数。</p>
<img src="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0209.png" class>

<p><em>图2-9. 事件时间进度，偏差和水位线</em></p>
<p>我声称代表现实的那条蜿蜒的红线本质上是水位线；它捕获事件时间完整性随着处理时间的推进的进展。在概念上，您可以将水位线视为一个函数，<em>F</em>(<em>P</em>)→<em>E</em>，它将处理时间中的一个点返回到事件时间中的一个点。事件时间中的那个点，<em>E</em>，是系统认为已经观察到了所有事件时间小于<em>E</em>的输入的点。换句话说，它是一个断言，即不会再看到事件时间小于<em>E</em>的更多数据。根据水位线的类型，完美或启发式，该断言可以是严格保证或有教养的猜测。</p>
<ul>
<li><p>完美水位线</p>
<p>对于我们拥有所有输入数据的情况，可以构建完美的水位线。在这种情况下，不存在延迟数据；所有数据都是提前或准时的。</p>
</li>
<li><p>启发式水位线</p>
<p>对于许多分布式输入源，实际上不可能完全了解输入数据，因此提供启发式水位线是下一个最佳选择。启发式水位线使用有关输入的任何信息（分区、如果有的话，分区内的排序、文件的增长率等），以提供尽可能准确的进度估计。在许多情况下，这样的水位线可以令人惊讶地准确预测。即便如此，使用启发式水位线意味着它有时可能是错误的，这将导致延迟数据。我们很快就会向您展示处理延迟数据的方法。</p>
</li>
</ul>
<p>因为它们提供了相对于我们的输入的完整性概念，水位线是前面提到的第二种触发器的基础：完整性触发器。水位线本身是一个非常有趣和复杂的话题，当您到达第3章的Slava的水位线深度探索时，您就会看到。但是，现在让我们通过更新我们的示例管道以利用基于水位线的完整性触发器来看看它们的实际应用，如示例2-6所示。</p>
<p><em>示例2-6. 水位线完整性触发器</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">              .triggering(AfterWatermark()))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>水位线的一个有趣的特点是它们是一类函数，这意味着有多个不同的函数<em>F</em>(<em>P</em>)→<em>E</em>满足水位线的属性，成功程度不同。正如我之前所指出的，对于您拥有输入数据的情况，可能可以构建完美的水位线，这是理想的情况。但是对于缺乏输入的完美知识或计算完美水位线的计算成本太高的情况，您可能会选择使用启发式来定义您的水位线。我想在这里强调的一点是，使用的水位线算法独立于管道本身。我们不会在这里详细讨论实施水位线的含义（Slava在第3章中介绍），但是为了帮助加强这个想法，即一组给定的输入可以应用不同的水位线，让我们看一下在相同的数据集上运行示例2-6的管道，但使用两个不同的水位线实现（图2-10）：在左侧，完美的水位线；在右侧，启发式水位线。</p>
<p>在两种情况下，窗口在水位线通过窗口结束时实现。如您所预期的，完美水位线完美地捕获了管道随着时间的推移的事件时间完整性。相比之下，右侧启发式水位线使用的特定算法未考虑9的值，这大大改变了材料化输出的形状，无论是在输出延迟还是正确性方面（如为[12:00,12:02)窗口提供的不正确答案5所示）。水位线触发器与图2-9中所示的重复更新触发器之间的最大区别是，水位线使我们有一种方法来推断我们的输入的完整性。在系统为给定窗口实现输出之前，我们知道系统尚未认为输入已完成。这对于希望推断输入中缺少数据或缺少数据的用例尤其重要。</p>
<div id="dplayer5" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer5"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0210.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图2-10. 具有完美（左）和启发式（右）水位线的流式引擎上的窗口求和</em></p>
<p>缺少数据用例的一个很好的例子是外连接。没有像水位线这样的完整性概念，您如何知道何时放弃发出部分连接而不是继续等待该连接完成？你不知道。在缺乏真正的水位线支持的流式系统中，将该决定基于处理时间延迟并不安全，因为我们在第1章中所说的事件时间偏差的可变性：只要偏差保持小于所选处理时间延迟，缺少数据的结果将是正确的，但是任何时候偏差超过该延迟，它们将突然变得不正确。从这个角度来看，事件时间水位线是许多必须推断输入中缺少数据（例如外连接，异常检测等）的实际流用例的关键部分。</p>
<p>现在，话虽如此，这些水位线示例也突出了水位线（和任何其他完整性概念）的两个缺点，具体而言，它们可能是以下两者之一：</p>
<ul>
<li><p>太慢</p>
<p>当任何类型的水位线由于已知的未处理数据（例如由于网络带宽限制而慢慢增长的输入日志）而被正确延迟时，如果仅依赖于水位线推进来刺激结果，则会直接导致输出延迟。</p>
<p>这在图2-10的左图中最为明显，由于晚到达的9为所有后续窗口的水位线保留，即使这些窗口的输入数据更早地变得完整。这对于第二个窗口[12:02,12:04)尤其明显，因为从窗口第一个值发生的时间到我们看到窗口的任何结果为止，需要将近七分钟的时间。在这个例子中使用的启发式水位线没有如此严重的问题（五分钟的输出），但是不要认为启发式水位线永远不会受到水位线滞后的影响；这实际上只是我选择从这个特定示例中省略启发式水位线的记录的结果。</p>
<p>这里的重要观点如下：尽管水位线提供了非常有用的完整性概念，但是从延迟的角度来看，仅依赖于完整性来产生输出通常并不理想。想象一下包含按小时或按天分组的有价值指标的仪表板。您不太可能想要等待整个小时或天才开始看到当前窗口的结果；这是使用经典批处理系统来为这些系统提供动力的痛点之一。相反，随着输入的演变和最终变得完整，看到这些窗口的结果逐渐完善会更好。</p>
</li>
<li><p>太快</p>
<p>当启发式水位线被错误地提前时，事件时间早于水位线的数据可能会在一段时间后到达，从而创建延迟数据。这就是右侧示例中发生的事情：水位线在观察到该窗口的所有输入数据之前就超过了第一个窗口的结束，导致输出值不正确，而是5而不是14。这个缺点严格来说是启发式水位线的问题；它们的启发式性质意味着它们有时会出错。因此，仅依靠它们来确定何时实现输出是不足的，如果您关心正确性。</p>
</li>
</ul>
<p>在第1章中，我对大多数需要处理无界数据流的强有力的乱序处理用例的完整性概念提出了一些非常强烈的声明。这两个缺点——水位线太慢或太快——是这些论点的基础。您只能从依赖于完整性概念的系统中获得低延迟或正确性的最佳效果。因此，对于那些想要同时拥有最佳效果的情况，一个人该怎么办？如果重复的更新触发器提供低延迟更新但没有关于完整性的推理方式，而水位线提供关于完整性的概念但具有可变且可能具有高延迟的特性，那么为什么不将它们的力量结合起来呢？</p>
<h3><span id="何时触发早期x2f准时x2f延迟触发-ftw">何时触发：早期&#x2F;准时&#x2F;延迟触发 FTW！</span><a href="#何时触发早期x2f准时x2f延迟触发-ftw" class="header-anchor">#</a></h3><p>我们现在已经看过了两种主要类型的触发器：重复更新触发器和完整性&#x2F;水位线触发器。在许多情况下，它们单独使用都不足够，但将它们结合在一起就可以了。Beam通过提供标准水位线触发器的扩展来识别这一点，该扩展还支持水位线两侧的重复更新触发。这被称为早期&#x2F;准时&#x2F;延迟触发器，因为它将由复合触发器实现的窗格分为三类：</p>
<ul>
<li>零个或多个<em>早期窗格</em>，这是由重复更新触发器产生的结果，该触发器会定期触发，直到水位线通过窗口的末端为止。这些触发生成的窗格包含推测结果，但允许我们观察随着新的输入数据到达窗口而窗口的演变。这弥补了水位线有时过于缓慢的缺点。</li>
<li>一个<em>准时窗格</em>，这是由完整性&#x2F;水位线触发器在水位线通过窗口的末端之后触发的结果。这个触发很特别，因为它提供了系统现在相信该窗口的输入是完整的断言。这意味着现在可以推断<em>缺少的数据</em>，例如在执行外部连接时发出部分连接。</li>
<li>零个或多个<em>延迟窗格</em>，这是由另一个（可能不同的）重复更新触发器产生的结果，该触发器会在水位线通过窗口的末端之后定期触发任何延迟数据。对于完美的水位线，将始终没有延迟窗格。但是，在启发式水位线的情况下，水位线未能正确计算的任何数据都将导致延迟触发。这弥补了水位线过于快的缺点。</li>
</ul>
<p>让我们看看这在实际中是什么样子。我们将更新我们的管道，使用具有一分钟对齐延迟的周期处理时间触发器进行早期触发，以及针对延迟触发的每个记录触发器。这样，早期触发将为我们的高容量窗口提供一定量的批处理（由于触发器每分钟只触发一次，无论窗口的吞吐量如何），但我们不会为延迟触发引入不必要的延迟，如果我们使用一个相对准确的启发式水位线，那么延迟触发应该是相对较少的。在Beam中，它看起来像Example 2-7（Figure 2-11显示结果）。</p>
<p><em>Example 2-7.通过早期&#x2F;准时&#x2F;延迟API进行早期、准时和延迟触发</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">   .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(AfterWatermark()</span><br><span class="line">                         .withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">                         .withLateFirings(AfterCount(1))))</span><br><span class="line">   .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<div id="dplayer6" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer6"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0211.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>Figure 2-11.带有早期、准时和延迟触发的流式引擎的窗口求和</em></p>
<p>这个版本比Figure 2-9有两个明显的改进：</p>
<ul>
<li>对于第二个窗口中“水位线过慢”的情况[12:02，12:04）：我们现在每分钟提供定期的早期更新。最大的差异在完美的水位线情况下，从几乎七分钟的时间到第一次输出减少到三分半钟；但在启发式情况下，它也显然得到了改善。两个版本现在都提供随着时间的推移而稳定的改进（带有值为7、10、18的窗格），在输入变为完整和窗格的最终输出材料化之间的延迟相对较小。</li>
<li>对于第一个窗口中“启发式水位线过快”的情况，[12:00，12:02）：当值为9的数据出现延迟时，我们立即将其合并到新的校正窗格中，其值为14。</li>
</ul>
<p>这些新触发器的一个有趣的副作用是它们有效地将完美水位线和启发式水位线版本之间的输出模式规范化。在Figure 2-10中的两个版本有着鲜明的不同之处，而在这里的两个版本看起来非常相似。它们看起来也更类似于Figures 2-6到2-8中的各种重复更新版本，但有一个重要的区别：由于使用水位线触发器，我们还可以在使用早期&#x2F;准时&#x2F;延迟触发器生成的结果中推断输入的完整性，这使我们可以更好地处理关心<em><strong>缺少的数据</strong></em>的用例，例如外部连接、异常检测等。</p>
<p>此时完美和启发式早期&#x2F;准时&#x2F;延迟版本之间最大的差异是窗口生命周期边界。在完美水位线的情况下，我们知道在水位线通过窗口的末端之后将不会再看到窗口的任何数据，因此我们可以在那个时间丢弃窗口的所有状态。在启发式水位线的情况下，我们仍然需要保留窗口的状态一段时间来解决延迟数据的问题。但是，到目前为止，我们的系统没有任何好的方法来知道需要为每个窗口保留状态的时间有多长。这就是<em><strong>允许延迟</strong></em>的作用所在。</p>
<h3><span id="何时-允许延迟垃圾回收"><em><strong>何时</strong></em>: 允许延迟（垃圾回收）</span><a href="#何时-允许延迟垃圾回收" class="header-anchor">#</a></h3><p>在我们进入最后一个问题（“<em>如何</em>进行结果的改进？”）之前，我想谈一下长期、无序流处理系统中的一个实际必要性：垃圾回收。在图2-11中的启发式水位线示例中，每个窗口的持久状态在整个示例的生命周期内一直存在；这是必要的，以便在数据到达时&#x2F;如果到达时可以适当地处理它们。但是，虽然保留所有持久状态直到永远可能很好，但实际上，在处理无界数据源时，通常不实用保留给定窗口的状态（包括元数据）无限期；我们最终会用尽磁盘空间（或至少会厌倦为其付款，因为较旧数据的价值随时间降低）。</p>
<p>因此，任何现实世界的无序处理系统都需要提供一种方法来限制它正在处理的窗口的生命周期。一个干净而简洁的方法是通过定义系统内允许延迟的时间范围来实现；也就是说，为系统在处理时要费心处理的任何给定<em>记录</em>设置一个延迟限制；超出此时间范围的任何数据都会被简单地丢弃。在您限制了单个数据可以被延迟的时间之后，您也确定了窗口状态必须保留多长时间：直到水位线超过窗口结束时的延迟时间范围。但是，此外，您还赋予了系统立即丢弃任何超过延迟限制的数据的自由，这意味着系统不会浪费资源处理任何人都不关心的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            **测量延迟**</span><br><span class="line"></span><br><span class="line">使用导致第一次出现迟到数据的度量标准（即启发式水位线）来指定处理迟到数据的时间范围可能有些奇怪。在某种意义上，它确实是这样的。但是在可用选项中，它可能是最好的选择。唯一的其他实际选项将是按处理时间指定时间范围（例如，水位线通过窗口结束后的10分钟保留窗口），但是使用处理时间将使垃圾回收策略容易受到管道本身的问题（例如，工作人员崩溃，导致管道停滞几分钟），这可能会导致实际上没有机会处理迟到数据的窗口。通过在事件时间域中指定时间范围，垃圾回收与管道的实际进度直接相关联，从而降低了窗口错过适当处理迟到数据的机会的可能性。</span><br><span class="line"></span><br><span class="line">但是请注意，并非所有水位线都是相同的。当我们在本书中谈论水位线时，通常是指*低*水位线，它们悲观地尝试捕获系统已知的*最旧*未处理记录的事件时间。使用低水位线处理迟到数据的好处是它们对事件时间偏差的变化具有弹性；无论管道中的偏差增长多大，低水位线始终会跟踪系统已知的最旧未完成事件，提供可能的正确性保证。</span><br><span class="line"></span><br><span class="line">相反，某些系统可能会将“水位线”一词用于其他目的。例如，Spark Structured Streaming中的水位线是*高*水位线，它们乐观地跟踪系统已知的*最新*记录的事件时间。在处理迟到数据时，系统可以自由地丢弃早于高水位线调整了某些用户指定的延迟阈值的任何旧窗口。换句话说，系统允许您指定预计在管道中看到的最大事件时间偏差，然后丢弃任何超出该偏差窗口的数据。如果管道内的偏差保持在某个恒定的增量内，这可能很有效，但比低水位线方案更容易错误地丢弃数据。</span><br></pre></td></tr></table></figure>

<p>由于允许延迟和水位线之间的交互有些微妙，因此很值得看一下示例。让我们把示例2-7&#x2F;图2-11中的启发式水位线管道添加一个1分钟的延迟时间范围（请注意，这个特定延迟时间范围之所以被选择是严格因为它能很好地适合图表；对于实际用例，一个较大的延迟时间范围可能会更实用）：</p>
<p><em>示例2-8。允许迟到的早期&#x2F;按时&#x2F;晚期点燃</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">   .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">              .triggering(</span><br><span class="line">                AfterWatermark()</span><br><span class="line">                  .withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">                  .withLateFirings(AfterCount(1)))</span><br><span class="line">              .withAllowedLateness(ONE_MINUTE))</span><br><span class="line">    .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>这个管道的执行看起来像图2-12，我添加了以下特征以突出允许延迟的影响：</p>
<ul>
<li>当前处理时间的粗黑线现在带有标记，表示所有活动窗口的事件时间的延迟范围。</li>
<li>当水位线通过窗口的延迟范围时，该窗口关闭，这意味着窗口的所有状态都会被丢弃。我保留了一个虚线矩形，显示窗口关闭时它覆盖的时间范围（在两个域中），并带有一个小尾巴向右延伸，以与水位线进行对比。</li>
<li>仅在此图表中，我添加了一个额外的晚到数据（第一个值为6的窗口）。6是晚到的，但仍然在允许的延迟时间范围内，因此与值11的更新结果合并。然而，9在延迟时间范围之外到达，因此被简单地丢弃。</li>
</ul>
<div id="dplayer7" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer7"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0212.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图2-12。允许早期&#x2F;按时&#x2F;晚期触发</em></p>
<p>关于延迟时间范围，最后有两个侧面需要注意：</p>
<ul>
<li>为了绝对清楚，如果您恰好从可用完美水位线的来源中获取数据，则无需处理迟到数据，允许的延迟时间范围为零秒将是最佳的。这就是我们在图2-10的完美水位线部分中看到的。</li>
<li>即使在使用启发式水位线时，需要指定延迟时间范围，也有一个值得注意的例外情况，那就是对于针对有限数量的键的所有时间的全局聚合（例如，按网页浏览器系列分组计算您网站上的总访问次数）。在这种情况下，系统中活动窗口的数量受使用的有限键空间的限制。只要键的数量保持在可管理的低水平，就不需要担心通过允许延迟来限制窗口的生命周期。</li>
</ul>
<p>现在，让我们满足实际需求，继续第4个也是最后一个问题。</p>
<h1><span id="方法累加"><em><strong>方法</strong></em>：累加</span><a href="#方法累加" class="header-anchor">#</a></h1><p>当使用触发器在单个窗口上生成多个窗格时，我们面临的最后一个问题是：“<em>如何</em>关联结果的细化？在我们迄今看到的示例中，每个连续的窗格都建立在紧接着的窗格之上。但是，实际上有三种不同的累加模式：</p>
<ul>
<li><p>丢弃</p>
<p>每次实现窗格时，任何存储的状态都被丢弃。这意味着每个连续的窗格都独立于之前的任何窗格。当下游消费者执行某种聚合时，例如将整数发送到一个希望接收将它们相加以生成最终计数的增量的系统时，丢弃模式是有用的。</p>
</li>
<li><p>累加</p>
<p>与图2-6到2-11中的情况一样，每次实现窗格时，任何存储的状态都被保留，并且将来的输入被累加到现有状态中。这意味着每个连续的窗格都建立在以前的窗格之上。当稍后的结果可以简单地覆盖以前的结果时，例如在像HBase或Bigtable这样的键&#x2F;值存储中存储输出时，累加模式是有用的。</p>
</li>
<li><p>累加和撤回</p>
<p>这就像累加模式，但在生成新窗格时，它还会为先前的窗格生成独立的撤回。撤回（与新的累积结果相结合）本质上是明确地说“我先前告诉过你结果是<em><strong>X</strong></em>，但我错了。去掉上次告诉你的<em><strong>X</strong></em>，并用<em><strong>Y</strong></em>替换它。”有两种情况特别有用：</p>
<ul>
<li>当下游消费者通过不同的维度重新分组数据时，新值可能会与以前的值有不同的键，并因此进入不同的组。在这种情况下，新值不能仅覆盖旧值；您需要撤回以删除旧值。</li>
<li>当使用<em><strong>动态窗口</strong></em>（例如会话，在接下来的几分钟中我们将更详细地了解）时，由于窗口合并，新值可能会替换多个先前的窗口。在这种情况下，仅从新窗口中确定被替换的旧窗口是困难的。为旧窗口提供明确的撤回使任务变得简单。我们在第8章中详细介绍了这个例子。</li>
</ul>
</li>
</ul>
<p>每个组的不同语义在并排看起来会更加清晰。考虑图2-11中第二个窗口（事件时间范围为[12:06,12:08)）的两个窗格。表2-1显示了每个窗格在三种累加模式（其中<em>累加</em>模式是图2-11本身使用的特定模式）中的值是什么。</p>
<p><em>表2-1。使用图2-11中的第二个窗口比较累加模式</em></p>
<table>
<thead>
<tr>
<th></th>
<th><strong>丢弃</strong></th>
<th><strong>累加</strong></th>
<th><strong>累加和撤回</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>窗格1：输入&#x3D;[3]</strong></td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td><strong>窗格2：输入&#x3D;[8, 1]</strong></td>
<td>9</td>
<td>12</td>
<td>12,–3</td>
</tr>
<tr>
<td><strong>最终正常窗格的值</strong></td>
<td>9</td>
<td>12</td>
<td>12</td>
</tr>
<tr>
<td><strong>所有窗格的总和</strong></td>
<td>12</td>
<td>15</td>
<td>12</td>
</tr>
</tbody></table>
<p>让我们更仔细地看看发生了什么：</p>
<ul>
<li><p>丢弃</p>
<p>每个窗格仅包含在该特定窗格期间到达的值。因此，观察到的最终值并未完全捕获总和。但是，如果您将所有独立的窗格本身相加，您将得到正确的答案12。这就是为什么当下游消费者本身执行材料化窗格的某种聚合时，丢弃模式很有用的原因。</p>
</li>
<li><p>累加</p>
<p>与图2-11相同，每个窗格包含该特定窗格到达的值以及以前窗格的所有值。因此，观察到的最终值正确地捕获了总和12。但是，如果您总结单独的窗格本身，您将有效地重复计算来自第1个窗格的输入，从而给出错误的总和15。这就是为什么当您可以简单地使用新值覆盖先前的值时，累加模式最有用：新值已经包含了迄今为止看到的所有数据。</p>
</li>
<li><p>累加和撤回</p>
<p>每个窗格都包括一个新的累加模式值和上一个窗格的撤回。因此，最后一个观察到的值（不包括撤回）以及所有材料化窗格（包括撤回）的总和都为12，这就是为什么撤回如此强大的原因。</p>
</li>
</ul>
<p>示例2-9演示了丢弃模式的工作原理，说明了我们将对示例2-7进行的更改：</p>
<p><em>示例2-9.早期&#x2F;按时&#x2F;迟到触发的丢弃模式版本</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">   .apply（Window.into（FixedWindows.of（TWO_MINUTES））</span><br><span class="line">                .triggering（</span><br><span class="line">                  AfterWatermark（）</span><br><span class="line">                   .withEarlyFirings（AlignedDelay（ONE_MINUTE））</span><br><span class="line">                   .withLateFirings（AtCount（1）））</span><br><span class="line">                .discardingFiredPanes（））</span><br><span class="line">    .apply（Sum.integersPerKey（））;</span><br></pre></td></tr></table></figure>
<p>在启用启发式水位线的流引擎上重新运行会产生如图2-13所示的输出。</p>
<div id="dplayer8" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer8"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0213.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图2-13。流引擎上的早期&#x2F;按时&#x2F;迟到触发的丢弃模式版本</em></p>
<p>即使整体输出的形状与图2-11中的累加模式版本相似，但请注意，此丢弃版本中的窗格都不重叠。因此，每个输出都与其他输出无关。<br>如果我们想查看撤回的操作，更改将类似，如示例2-10所示。 ???描述了结果。</p>
<p><em>示例2-10.早期&#x2F;按时&#x2F;迟到触发的累加和撤回模式版本</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt; totals = input</span><br><span class="line">  .apply（Window.into（FixedWindows.of（TWO_MINUTES））</span><br><span class="line">               .triggering（</span><br><span class="line">                 AfterWatermark（）</span><br><span class="line">                  .withEarlyFirings（AlignedDelay（ONE_MINUTE））</span><br><span class="line">                  .withLateFirings（AtCount（1）））</span><br><span class="line">               .accumulatingAndRetractingFiredPanes（））</span><br><span class="line">  .apply（Sum.integersPerKey（））;</span><br></pre></td></tr></table></figure>

<div id="dplayer9" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer9"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0214.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p>在流引擎上运行早期&#x2F;按时&#x2F;迟到触发的累加和撤回模式版本</p>
<p>由于每个窗口的窗格都重叠，因此很难清晰地看到撤回。撤回以红色表示，与重叠的蓝色窗格相结合，形成了略带紫色的颜色。我还略微水平移了给定窗格内两个输出的值（并用逗号分隔它们），以使它们更容易区分。</p>
<p>图2-14组合了图2-9、2-11（仅启发式）和并排的最终帧，提供了三种模式的良好视觉对比。</p>
<img src="/www6vHomeHexo/2000/03/17/streamingSystemChapter2/stsy_0215.png" class>
<p><em>图2-14。累积模式的并排比较</em></p>
<p>正如您可以想象的那样，按照呈现的顺序的模式（丢弃，累加，累加和撤回）在存储和计算成本方面都是越来越昂贵的。为此，累加模式的选择为沿着正确性、延迟和成本的轴进行权衡提供了另一个维度。</p>
<h1><span id="摘要"><strong>摘要</strong></span><a href="#摘要" class="header-anchor">#</a></h1><p>通过本章的学习，您现在已经了解了鲁棒流处理的基础知识，已经准备好在世界上做出惊人的事情了。当然，还有八个更多的章节等待您的关注，所以希望您不会在此刻非常急切地前进。但无论如何，让我们回顾一下我们刚刚讨论的内容，以防止您在匆忙前进时忘记了它。首先，我们触及的主要概念：</p>
<ul>
<li><p>事件时间与处理时间</p>
<p>重要的区别是事件发生的时间和数据处理系统观察到它们的时间。</p>
</li>
<li><p>窗口化</p>
<p>通过沿着时间边界（在处理时间或事件时间中）切片管理无界数据的常用方法，尽管在Beam模型中我们将窗口化的定义限制为仅在事件时间内。</p>
</li>
<li><p>触发器</p>
<p>用于指定在特定用例中何时对输出进行实体化的声明性机制。</p>
</li>
<li><p>水位线</p>
<p>在事件时间中提供进度的强大概念，为在无界数据上运行的乱序处理系统中推理完整性（因此缺少数据）提供手段。</p>
</li>
<li><p>积累</p>
<p>为单个窗口的结果的细化之间的关系，对于多次演化时它的实体化情况。</p>
</li>
<li><p>第二，我们用来构建我们的探索的四个问题：</p>
<ul>
<li>计算<em><strong>什么</strong></em>结果？&#x3D; 变换。</li>
<li>在事件时间中的<em><strong>哪个位置</strong></em>计算结果？&#x3D; 窗口化。</li>
<li>结果在处理时间中的<em><strong>何时</strong></em>实体化？&#x3D; 触发器加水位线。</li>
<li>结果的细化<em><strong>如何</strong></em>关联？&#x3D; 积累。</li>
</ul>
</li>
</ul>
<p>第三，为了凸显这种流处理模型所提供的灵活性（因为最终，这就是所有事情的平衡：正确性、延迟和成本等竞争张力），我们仅仅通过最少的代码更改就能在相同的数据集上实现各种主要输出的回顾：</p>
<p>整数求和</p>
<p>示例2-1 &#x2F; 图2-3</p>
<p>整数求和</p>
<p>固定窗口批处理</p>
<p>示例2-2 &#x2F; 图2-5</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>每个记录触发器的重复</p>
<p>示例2-3 &#x2F; 图2-6</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>重复对齐延迟触发器</p>
<p>示例2-4 &#x2F; 图2-7</p>
<p>重复的</p>
<p>未对齐延迟</p>
<p>触发器</p>
<p>示例2-5 &#x2F; 图2-8</p>
<p>固定窗口流式处理</p>
<p>启发式水位线触发器</p>
<p>示例2-6 &#x2F; 图2-10</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>早期&#x2F;准时&#x2F;延迟触发器</p>
<p>丢弃</p>
<p>示例2-9 &#x2F; 图2-13</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>早期&#x2F;准时&#x2F;延迟触发器</p>
<p>积累</p>
<p>示例2-7 &#x2F; 图2-11</p>
<p>整数求和</p>
<p>固定窗口流处理</p>
<p>早期&#x2F;准时&#x2F;延迟触发器</p>
<p>累积和撤回</p>
<p>示例2-10 &#x2F; ???</p>
<p>话虽如此，到目前为止，我们只看了一种窗口化类型：事件时间的固定窗口化。正如我们所知，窗口化有许多维度，我想在我们结束Beam模型之前至少再涉及其中两个。不过，首先，我们将稍微偏离主题，深入探讨水位线的世界，因为这些知识将有助于框架未来的讨论（并且本身也很有趣）。请欢迎Slava，右台……</p>
<hr>
<ol>
<li>如果你足够幸运能读到 Safari 版本的书籍，你会看到完整的类似“流式媒体 102”中的时间-lapse 动画。对于印刷、Kindle 和其他电子书版本，这些都是静态图像，带有到网络动画版本的链接。</li>
<li>请耐心等待。复合标点符号（即表情符号）的细粒度情感表达在 O’Reilly 出版物中是严格禁止的 <winky-smiley>。</winky-smiley></li>
<li>的确，我们在 Beam 的原始触发器功能中就是这样做的。回想起来，我们有点过头了。未来的迭代将更简单、更易于使用，在本书中我只关注那些可能以某种形式保留的部分。</li>
<li>更准确地说，函数的输入实际上是上游管道中水位线被观察到的点上游的所有内容在时间 <em>P</em> 的状态:输入源、缓冲数据、正在处理的数据等等;但从概念上来讲，把它看作是从处理时间到事件时间的映射更简单。</li>
<li>请注意，我选择明确省略启发式水位线的 9 值，因为它将帮助我就延迟数据和水位线滞后提出一些重要观点。实际上，一个启发式水位线可能会同样有可能省略其他某些值，这反过来可能会对水位线产生显著不那么严重的影响。如果你的目标是从水位线中筛选迟到的数据（在某些情况下非常有效，例如在滥用检测中，你只想尽快看到大多数数据），你不一定要选择启发式水位线而不是完美水位线。你真正想要的是百分位水位线，它明确地从其计算中删除一些迟到的数据百分位。见第三章。</li>
<li>这并不是说没有那些主要关心正确性而不是延迟的使用情况；在这些情况下，使用准确的水位线作为管道输出的唯一驱动器是一种合理的方法。</li>
<li>正如我们之前所知道的，这个断言要么是在使用完美水位线的情况下保证的，要么是在使用启发式水位线的情况下的一个经过教育的猜测。</li>
<li>你可能会注意到应该有第四种模式：丢弃和撤销。在大多数情况下，这种模式并不是非常有用，所以我在这里不再讨论它。</li>
<li>回想起来，选择一组更加面向材料化流中数据的观察性名称（例如“输出模式”）而不是描述产生这些数据的状态管理语义的名称可能会更清晰。例如：丢弃模式 → Δ 模式，累积模式 → 值模式，累积和撤回模式 → 值和撤回模式？然而，丢弃&#x2F;累积&#x2F;累积和撤回名称已经在 Beam Model 的 1.x 和 2.x 衍生版本中得到了确认，因此我不想通过偏离来引入潜在的困惑。此外，随着 Beam 3.0 的推出和汇合触发器的引入，累积模式很可能会更加淡化。在第 8 章中讨论 SQL 时会更详细地讨论这个问题。</li>
</ol>
<h1><span id="draft-here">Draft Here</span><a href="#draft-here" class="header-anchor">#</a></h1>]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-Chapter 5. Exactly-Once and Side Effects [完整]</title>
    <url>/www6vHomeHexo/2000/03/16/streamingSystemChapter5Original/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#why-exactly-once-matters"><strong>Why Exactly Once Matters</strong></a></li>
<li><a href="#accuracy-versus-completeness"><strong>Accuracy Versus Completeness</strong></a><ul>
<li><a href="#side-effects"><strong>Side Effects</strong></a></li>
<li><a href="#problem-definition"><strong>Problem Definition</strong></a></li>
</ul>
</li>
<li><a href="#ensuring-exactly-once-in-shuffle"><strong>Ensuring Exactly Once in Shuffle</strong></a></li>
<li><a href="#addressing-determinism"><strong>Addressing Determinism</strong></a></li>
<li><a href="#performance"><strong>Performance</strong></a><ul>
<li><a href="#graph-optimization"><strong>Graph Optimization</strong></a></li>
<li><a href="#bloom-filters"><strong>Bloom Filters</strong></a></li>
<li><a href="#garbage-collection"><strong>Garbage Collection</strong></a></li>
</ul>
</li>
<li><a href="#exactly-once-in-sources"><strong>Exactly Once in Sources</strong></a></li>
<li><a href="#exactly-once-in-sinks"><strong>Exactly Once in Sinks</strong></a></li>
<li><a href="#use-cases"><strong>Use Cases</strong></a><ul>
<li><a href="#example-source-cloud-pubsub"><strong>Example Source: Cloud Pub&#x2F;Sub</strong></a></li>
<li><a href="#example-sink-files"><strong>Example Sink: Files</strong></a></li>
<li><a href="#example-sink-google-bigquery"><strong>Example Sink: Google BigQuery</strong></a></li>
</ul>
</li>
<li><a href="#other-systems"><strong>Other Systems</strong></a><ul>
<li><a href="#apache-spark-streaming"><strong>Apache Spark Streaming</strong></a></li>
<li><a href="#apache-flink"><strong>Apache Flink</strong></a></li>
</ul>
</li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 127</p>
<details><summary>点击 原文</summary><p>We now shift from discussing programming models and APIs to the systems</p>
<p>that implement them. A model and API allows users to describe what they</p>
<p>want to compute. Actually running the computation accurately at scale</p>
<p>requires a system—usually a distributed system.</p>
<p>In this chapter, we focus on how an implementing system can correctly</p>
<p>implement the Beam Model to produce accurate results. Streaming systems</p>
<p>often talk about <em>exactly-once processing</em>; that is, ensuring that every record is</p>
<p>processed exactly one time. We will explain what we mean by this, and how</p>
<p>it might be implemented.</p>
<p>As a motivating example, this chapter focuses on techniques used by Google</p>
<p>Cloud Dataflow to efficiently guarantee exactly-once processing of records.</p>
<p>Toward the end of the chapter, we also look at techniques used by some other</p>
<p>popular streaming systems to guarantee exactly once.</p>
</details>


<details><summary>点击 原文</summary><h1><span id="why-exactly-once-matters"><strong>Why Exactly Once Matters</strong></span><a href="#why-exactly-once-matters" class="header-anchor">#</a></h1><p>It almost goes without saying that for many users, any risk of dropped records</p>
<p>or data loss in their data processing pipelines is unacceptable. Even so,</p>
<p>historically many general-purpose streaming systems made no guarantees</p>
<p>about record processing—all processing was “best effort” only. Other systems</p>
<p>provided at-least-once guarantees, ensuring that records were always</p>
<p>processed at least once, but records might be duplicated (and thus result in</p>
<p>inaccurate aggregations); in practice, many such at-least-once systems</p>
<p>performed aggregations in memory, and thus their aggregations could still be</p>
<p>lost when machines crashed. These systems were used for low-latency,</p>
<p>speculative results but generally could guarantee nothing about the veracity of</p>
<p>these results.</p>
<p>As Chapter 1 points out, this led to a strategy that was coined the <em>Lambda</em></p>
<p><em>Architecture</em>—run a streaming system to get fast, but inaccurate results.</p>
<p>Sometime later (often after end of day), a batch system runs to the correct</p>
<p>answer. This works only if the data stream is replayable; however, this was</p>
<p>true for enough data sources that this strategy proved viable. Nonetheless,</p>
<p>many people who tried this experienced a number of issues with the Lambda</p>
<p>Architecture:</p>
<ul>
<li>Inaccuracy</li>
</ul>
<p>Users tend to underestimate the impact of failures. They often assume that</p>
<p>a small percentage of records will be lost or duplicated (often based on</p>
<p>experiments they ran), and are shocked on that one bad day when 10% (or</p>
<p>more!) of records are lost or are duplicated. In a sense, such systems</p>
<p>provide only “half” a guarantee—and without a full one, anything is</p>
<p>possible.</p>
<ul>
<li>Inconsistency</li>
</ul>
<p>The batch system used for the end-of-day calculation often has different</p>
<p>data semantics than the streaming system. Getting the two pipelines to</p>
<p>produce comparable results proved more difficult than initially thought.</p>
<ul>
<li>Complexity</li>
</ul>
<p>By definition, Lambda requires you to write and maintain two different</p>
<p>codebases. You also must run and maintain two complex distributed</p>
<p>systems, each with different failure modes. For anything but the simplest</p>
<p>of pipelines, this quickly becomes overwhelming.</p>
<ul>
<li>Unpredictability</li>
</ul>
<p>In many use cases, end users will see streaming results that differ from the</p>
<p>daily results by an uncertain amount, which can change randomly. In</p>
<p>these cases, users will stop trusting the streaming data and wait for daily</p>
<p>batch results instead, thus destroying the value of getting low-latency</p>
<p>results in the first place.</p>
<ul>
<li>Latency</li>
</ul>
<p>Some business use cases <em>require</em> low-latency correct results, which the</p>
<p>Lambda Architecture does not provide by design.</p>
<p>Fortunately, many Beam runners can do much better. In this chapter, we</p>
<p>explain how exactly-once stream processing helps users count on accurate</p>
<p>results and avoid the risk of data loss while relying on a single codebase and</p>
<p>API. Because a variety of issues that can affect a pipeline’s output are often</p>
<p>erroneously conflated with exactly-once guarantees, we first explain precisely</p>
<p>which issues are in and out of scope when we refer to “exactly once” in the</p>
<p>context of Beam and data processing.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="accuracy-versus-completeness"><strong>Accuracy Versus Completeness</strong></span><a href="#accuracy-versus-completeness" class="header-anchor">#</a></h1><p>Whenever a Beam pipeline processes a record for a pipeline, we want to</p>
<p>ensure that the record is never dropped or duplicated. However, the nature of</p>
<p>streaming pipelines is such that records sometimes show up late, after</p>
<p>aggregates for their time windows have already been processed. The Beam</p>
<p>SDK allows the user to configure how long the system should wait for late</p>
<p>data to arrive; any (and only) records arriving later than this deadline are</p>
<p>dropped. This feature contributes to <em>completeness</em>, not to accuracy: all records</p>
<p>that showed up in time for processing are accurately processed exactly once,</p>
<p>whereas these late records are explicitly dropped.</p>
<p>Although late records are usually discussed in the context of streaming</p>
<p>systems, it’s worth noting that batch pipelines have similar completeness</p>
<p>issues. For example, a common batch paradigm is to run a job at 2 AM over</p>
<p>all the previous day’s data. However, if some of yesterday’s data wasn’t</p>
<p>collected until after 2 AM, it won’t be processed by the batch job! Thus, batch</p>
<p>pipelines also provide accurate but not always complete results.</p>
<h3><span id="side-effects"><strong>Side Effects</strong></span><a href="#side-effects" class="header-anchor">#</a></h3><p>One characteristic of Beam and Dataflow is that users inject custom code that</p>
<p>is executed as part of their pipeline graph. Dataflow does <em>not</em> guarantee that</p>
<p>this code is run only once per record,  whether by the streaming or batch</p>
<p>runner. It might run a given record through a user transform multiple times, or</p>
<p>it might even run the same record simultaneously on multiple workers; this is</p>
<p>necessary to guarantee at-least-once processing in the face of worker failures.</p>
<p>Only one of these invocations can “win” and produce output further down the</p>
<p>pipeline.</p>
<p>As a result, nonidempotent side effects are not guaranteed to execute exactly</p>
<p>once; if you write code that has side effects external to the pipeline, such as</p>
<p>contacting an outside service, these effects might be executed more than once</p>
<p>for a given record. This situation is usually unavoidable because there is no</p>
<p>way to atomically commit Dataflow’s processing with the side effect on the</p>
<p>external service. Pipelines do need to eventually send results to the outside</p>
<p>world, and such calls might not be idempotent. As you will see later in the</p>
<p>chapter, often such sinks are able to add an extra stage to restructure the call</p>
<p>into an idempotent operation first.</p>
<h3><span id="problem-definition"><strong>Problem Definition</strong></span><a href="#problem-definition" class="header-anchor">#</a></h3><p>So, we’ve given a couple of examples of what we’re <em>not</em> talking about. What</p>
<p>do we mean then by exactly-once processing? To motivate this, let’s begin</p>
<p>with a simple streaming pipeline,  shown in Example 5-1.</p>
<p><em>Example 5-1. A simple streaming pipeline</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Pipeline p = Pipeline.create(options);</span><br><span class="line">// Calculate 1-minute counts of events per user.</span><br><span class="line">PCollection&lt;..&gt; perUserCounts =</span><br><span class="line">p.apply(ReadFromUnboundedSource.read())</span><br><span class="line">.apply(new KeyByUser())</span><br><span class="line">.Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1)))</span><br><span class="line">.apply(Count.perKey());</span><br><span class="line">// Process these per-user counts, and write the output somewhere.</span><br><span class="line">perUserCounts.apply(new ProcessPerUserCountsAndWriteToSink());</span><br><span class="line">// Add up all these per-user counts to get 1-minute counts of all events.</span><br><span class="line">perUserCounts.apply(Values.&lt;..&gt;create())</span><br><span class="line">.apply(Count.globally())</span><br><span class="line">.apply(new ProcessGlobalCountAndWriteToSink());</span><br><span class="line">p.run();</span><br></pre></td></tr></table></figure>

<p>This pipeline computes two different windowed aggregations. The first counts</p>
<p>how many events came from each individual user over the course of a minute,</p>
<p>and the second counts how many total events came in each minute. Both</p>
<p>aggregations are written to unspecified streaming sinks.</p>
<p>Remember that Dataflow executes pipelines on many different workers in</p>
<p>parallel. After each GroupByKey (the Count operations use GroupByKey under</p>
<p>the covers), all records with the same key are processed on the same machine</p>
<p>following a process called <em>shuffle</em>. The Dataflow workers shuffle data</p>
<p>between themselves using Remote Procedure Calls (RPCs), ensuring that</p>
<p>records for a given key all end up on the same machine.</p>
<p>Figure 5-1 shows the shuffles that Dataflow creates for the pipeline in</p>
<p>Example 5-1.  The Count.perKey shuffles all the data for each user onto a</p>
<p>given worker, whereas the Count.globally shuffles all these partial counts</p>
<p>to a single worker to calculate the global sum.</p>
<p><em>Figure 5-1. Shuffles in a pipeline</em></p>
<p>For Dataflow to accurately process data, this shuffle process must ensure that</p>
<p>every record is shuffled exactly once. As you will see in a moment, the</p>
<p>distributed nature of shuffle makes this a challenging problem.</p>
<p>This pipeline also both reads and writes data from and to the outside world, so</p>
<p>Dataflow must ensure that this interaction does not introduce any</p>
<p>inaccuracies. Dataflow has always supported this task—what Apache Spark</p>
<p>and Apache Flink call <em>end-to-end exactly once</em>—for sources and sinks</p>
<p>whenever technically feasible.</p>
<p>The focus of this chapter will be on three things:</p>
<ul>
<li>Shuffle</li>
</ul>
<p>How Dataflow guarantees that every record is shuffled exactly once.</p>
<ul>
<li>Sources</li>
</ul>
<p>How Dataflow guarantees that every source record is processed exactly</p>
<p>once.</p>
<ul>
<li>Sinks</li>
</ul>
<p>How Dataflow guarantees that every sink produces accurate output.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="ensuring-exactly-once-in-shuffle"><strong>Ensuring Exactly Once in Shuffle</strong></span><a href="#ensuring-exactly-once-in-shuffle" class="header-anchor">#</a></h1><p>As just explained, Dataflow’s streaming shuffle uses RPCs. Now, any time</p>
<p>you have two machines communicating via RPC, you should think long and</p>
<p>hard about data integrity. First of all, RPCs can fail for many reasons. The</p>
<p>network might be interrupted, the RPC might time out before completing, or</p>
<p>the receiving server might decide to fail the call. To guarantee that records are</p>
<p>not lost in shuffle, Dataflow employs <em>upstream backup</em>. This simply means</p>
<p>that the sender will retry RPCs until it receives positive acknowledgment of</p>
<p>receipt. Dataflow also ensures that it will continue retrying these RPCs even if</p>
<p>the sender crashes. This guarantees that every record is delivered <em>at least</em></p>
<p><em>once</em>.</p>
<p>Now, the problem is that these retries might themselves create duplicates.</p>
<p>Most RPC frameworks, including the one Dataflow uses, provide the sender</p>
<p>with a status indicating success or failure. In a distributed system, you need to</p>
<p>be aware that RPCs can sometimes succeed even when they have appeared to</p>
<p>fail. There are many reasons for this: race conditions with the RPC timeout,</p>
<p>positive acknowledgment from the server failing to transfer even though the</p>
<p>RPC succeeded, and so on. The only status that a sender can really trust is a</p>
<p>successful one.</p>
<p>An RPC returning a failure status generally indicates that the call might or</p>
<p>might not have succeeded. Although specific error codes can communicate</p>
<p>unambiguous failure, many common RPC failures, such as Deadline</p>
<p>Exceeded, are ambiguous. In the case of streaming shuffle, retrying an RPC</p>
<p>that really succeeded means delivering a record twice! Dataflow needs some</p>
<p>way of detecting and removing these duplicates.</p>
<p>At a high level, the algorithm for this task is quite simple (see Figure 5-2):</p>
<p>every message sent is tagged with a unique identifier. Each receiver stores a</p>
<p>catalog of all identifiers that have already been seen and processed. Every</p>
<p>time a record is received, its identifier is looked up in this catalog. If it is</p>
<p>found, the record is dropped as a duplicate. Because Dataflow is built on top</p>
<p>of a scalable key&#x2F;value store, this store is used to hold the deduplication</p>
<p>catalog.</p>
<p><em>Figure 5-2. Detecting duplicates in shuffle</em></p>
<h1><span id="addressing-determinism"><strong>Addressing Determinism</strong></span><a href="#addressing-determinism" class="header-anchor">#</a></h1><p>Making this strategy work in the real world requires a lot of care, however.</p>
<p>One immediate wrinkle is that the Beam Model allows for user code to</p>
<p>produce nondeterministic output. This means that a ParDo can execute twice</p>
<p>on the same input record (due to a retry), yet produce different output on each</p>
<p>retry. The desired behavior is that only one of those outputs will commit into</p>
<p>the pipeline; however, the nondeterminism involved makes it difficult to</p>
<p>guarantee that both outputs have the same deterministic ID. Even trickier, a</p>
<p>ParDo can output multiple records, so each of these retries might produce a</p>
<p>different number of outputs!</p>
<p>So, why don’t we simply require that all user processing be deterministic?</p>
<p>Our experience is that in practice, many pipelines require nondeterministic</p>
<p>transforms And all too often, pipeline authors do not realize that the code they</p>
<p>wrote is nondeterministic. For example, consider a transform that looks up</p>
<p>supplemental data in Cloud Bigtable in order to enrich its input data. This is a</p>
<p>nondeterministic task, as the external value might change in between retries</p>
<p>of the transform. Any code that relies on current time is likewise not</p>
<p>deterministic. We have also seen transforms that need to rely on random</p>
<p>number generators. And even if the user code is purely deterministic, any</p>
<p>event-time aggregation that allows for late data might have nondeterministic</p>
<p>inputs.</p>
<p>Dataflow addresses this issue by using checkpointing to make</p>
<p>nondeterministic processing effectively deterministic. Each output from a</p>
<p>transform is checkpointed, together with its unique ID, to stable storage</p>
<p><em>before</em> being delivered to the next stage.  Any retries in the shuffle delivery</p>
<p>simply replay the output that has been checkpointed—the user’s</p>
<p>nondeterministic code is not run again on retry. To put it another way, the</p>
<p>user’s code may be run multiple times but only one of those runs can “win.”</p>
<p>Furthermore, Dataflow uses a consistent store that allows it to prevent</p>
<p>duplicates from being written to stable storage.</p>
</details>




<details><summary>点击 原文</summary><h1><span id="performance"><strong>Performance</strong></span><a href="#performance" class="header-anchor">#</a></h1><p>To implement exactly-once shuffle delivery, a catalog of record IDs is stored</p>
<p>in each receiver key. For every record that arrives, Dataflow looks up the</p>
<p>catalog of IDs already seen to determine whether this record is a duplicate.</p>
<p>Every output from step to step is checkpointed to storage to ensure that the</p>
<p>generated record IDs are stable.</p>
<p>However, unless implemented carefully, this process would significantly</p>
<p>degrade pipeline performance for customers by creating a huge increase in</p>
<p>reads and writes. Thus, for exactly-once processing to be viable for Dataflow</p>
<p>users, that I&#x2F;O has to be reduced, in particular by preventing I&#x2F;O on every</p>
<p>record.</p>
<p>Dataflow achieves this goal via two key techniques: <em>graph optimization</em> and</p>
<p><em>Bloom filters</em>.</p>
<h3><span id="graph-optimization"><strong>Graph Optimization</strong></span><a href="#graph-optimization" class="header-anchor">#</a></h3><p>The Dataflow service runs a series of optimizations on the pipeline graph</p>
<p>before executing it. One such optimization is <em>fusion</em>, in which the service</p>
<p>fuses many logical steps into a single execution stage. Figure 5-3 shows some</p>
<p>simple examples.</p>
<p><em>Figure 5-3. Example optimizations: fusion</em></p>
<p>All fused steps are run as an in-process unit, so there’s no need to store</p>
<p>exactly-once data for each of them. In many cases, fusion reduces the entire</p>
<p>graph down to a few physical steps, greatly reducing the amount of data</p>
<p>transfer needed (and saving on state usage, as well).</p>
<p>Dataflow also optimizes associative and commutative Combine operations</p>
<p>(such as Count and Sum) by performing partial combining locally before</p>
<p>sending the data to the main grouping operation, as illustrated in Figure 5-4.</p>
<p>This approach can greatly reduce the number of messages for delivery,</p>
<p>consequently also reducing the number of reads and writes.</p>
<p><em>Figure 5-4. Example optimizations: combiner lifting</em></p>
<h3><span id="bloom-filters"><strong>Bloom Filters</strong></span><a href="#bloom-filters" class="header-anchor">#</a></h3><p>The aforementioned optimizations are general techniques that improve</p>
<p>exactly-once performance as a byproduct. For an optimization aimed strictly</p>
<p>at improving exactly-once processing, we turn to <em>Bloom filters</em>.</p>
<p>In a healthy pipeline, most arriving records will not be duplicates. We can use</p>
<p>that fact to greatly improve performance via Bloom filters, which are compact</p>
<p>data structures that allow for quick set-membership checks. Bloom filters</p>
<p>have a very interesting property: they can return false positives but never false</p>
<p>negatives. If the filter says “Yes, the element is in the set,” we know that the</p>
<p>element is <em>probably</em> in the set (with a probability that can be calculated).</p>
<p>However, if the filter says an element is <em>not</em> in the set, it definitely isn’t. This</p>
<p>function is a perfect fit for the task at hand.</p>
<p>The implementation in Dataflow works like this: each worker keeps a Bloom</p>
<p>filter of every ID it has seen. Whenever a new record ID shows up, it looks it</p>
<p>up in the filter. If the filter returns false, this record is not a duplicate and the</p>
<p>worker can skip the more expensive lookup from stable storage. It needs to do</p>
<p>that second lookup only if the Bloom filter returns true, but as long as the</p>
<p>filter’s false-positive rate is low, that step is rarely needed.</p>
<p>Bloom filters tend to fill up over time, however, and as that happens, the</p>
<p>false-positive rate increases. We also need to construct this Bloom filter anew</p>
<p>any time a worker restarts by scanning the ID catalog stored in state.</p>
<p>Helpfully, Dataflow attaches a system timestamp to each record.  Thus,</p>
<p>instead of creating a single Bloom filter, the service creates a separate one for</p>
<p>every 10-minute range. When a record arrives, Dataflow queries the</p>
<p>appropriate filter based on the system timestamp. This step prevents the</p>
<p>Bloom filters from saturating because filters are garbage-collected over time,</p>
<p>and it also bounds the amount of data that needs to be scanned at startup.</p>
<p>Figure 5-5 illustrates this process: records arrive in the system and are</p>
<p>delegated to a Bloom filter based on their arrival time. None of the records</p>
<p>hitting the first filter are duplicates, and all of their catalog lookups are</p>
<p>filtered. Record r1 is delivered a second time, so a catalog lookup is needed</p>
<p>to verify that it is indeed a duplicate; the same is true for records r4 and r6.</p>
<p>Record r8 is not a duplicate; however, due to a false positive in its Bloom</p>
<p>filter, a catalog lookup is generated (which will determine that r8 is not a</p>
<p>duplicate and should be processed).</p>
<p><em>Figure 5-5. Exactly-once Bloom filters</em></p>
<h3><span id="garbage-collection"><strong>Garbage Collection</strong></span><a href="#garbage-collection" class="header-anchor">#</a></h3><p>Every Dataflow worker persistently stores a catalog of unique record IDs it</p>
<p>has seen. As Dataflow’s state and consistency model is per-key, in reality</p>
<p>each key stores a catalog of records that have been delivered to that key. We</p>
<p>can’t store these identifiers forever, or all available storage will eventually fill</p>
<p>up. To avoid that issue, you need garbage collection of acknowledged record</p>
<p>IDs.</p>
<p>One strategy for accomplishing this goal would be for senders to tag each</p>
<p>record with a strictly increasing sequence number in order to track the earliest</p>
<p>sequence number still in flight (corresponding to an unacknowledged record</p>
<p>delivery). Any identifier in the catalog with an earlier sequence number could</p>
<p>then be garbage-collected because all earlier records have already been</p>
<p>acknowledged.</p>
<p>There is a better alternative, however. As previously mentioned, Dataflow</p>
<p>already tags each record with a system timestamp that is used for bucketing</p>
<p>exactly-once Bloom filters. Consequently, instead of using sequence numbers</p>
<p>to garbage-collect the exactly-once catalog, Dataflow calculates a garbage</p>
<p>collection watermark based on these system timestamps (this is the</p>
<p>processing-time watermark discussed in Chapter 3). A nice side benefit of this</p>
<p>approach is that because this watermark is based on the amount of physical</p>
<p>time spent waiting in a given stage (unlike the data watermark, which is based</p>
<p>on custom event times), it provides intuition on what parts of the pipeline are</p>
<p>slow. This metadata is the basis for the System Lag metric shown in the</p>
<p>Dataflow WebUI.</p>
<p>What happens if a record arrives with an old timestamp and we’ve already</p>
<p>garbage-collected identifiers for this point in time? This can happen due to an</p>
<p>effect we call <em>network remnants</em>, in which an old message becomes stuck for</p>
<p>an indefinite period of time inside the network and then suddenly shows up.</p>
<p>Well, the low watermark that triggers garbage collection won’t advance until</p>
<p>record deliveries have been acknowledged, so we know that this record has</p>
<p>already been successfully processed. Such network remnants are clearly</p>
<p>duplicates and are ignored.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="exactly-once-in-sources"><strong>Exactly Once in Sources</strong></span><a href="#exactly-once-in-sources" class="header-anchor">#</a></h1><p>Beam provides a source API for reading data into a Dataflow pipeline. Dataflow might retry reads from a source if processing fails and needs to</p>
<p>ensure that every unique record produced by a source is processed exactly</p>
<p>once.</p>
<p>For most sources Dataflow handles this process transparently; such sources</p>
<p>are <em>deterministic</em>. For example, consider a source that reads data out of files.</p>
<p>The records in a file will always be in a deterministic order and at</p>
<p>deterministic byte locations, no matter how many times the file is read. The</p>
<p>filename and byte location uniquely identify each record, so the service can</p>
<p>automatically generate unique IDs for each record. Another source that</p>
<p>provides similar determinism guarantees is Apache Kafka; each Kafka topic</p>
<p>is divided into a static set of partitions, and records in a partition always have</p>
<p>a deterministic order. Such deterministic sources will work seamlessly in</p>
<p>Dataflow with no duplicates.</p>
<p>However, not all sources are so simple. For example, one common source for</p>
<p>Dataflow pipelines is Google Cloud Pub&#x2F;Sub. Pub&#x2F;Sub is a <em>nondeterministic</em></p>
<p>source: multiple subscribers can pull from a Pub&#x2F;Sub topic, but which</p>
<p>subscribers receive a given message is unpredictable. If processing fails</p>
<p>Pub&#x2F;Sub will redeliver messages but the messages might be delivered to</p>
<p>different workers than those that processed them originally, and in a different</p>
<p>order. This nondeterministic behavior means that Dataflow needs assistance</p>
<p>for detecting duplicates because there is no way for the service to</p>
<p>deterministically assign record IDs that will be stable upon retry. (We dive</p>
<p>into a more detailed case study of Pub&#x2F;Sub later in this chapter.)</p>
<p>Because Dataflow cannot automatically assign record IDs, nondeterministic</p>
<p>sources are required to inform the system what the record IDs should be.</p>
<p>Beam’s Source API provides the UnboundedReader.getCurrentRecordId method. If a source provides unique IDs per record and notifies Dataflow that</p>
<p>it requires deduplication, records with the same ID will be filtered out.</p>
<h1><span id="exactly-once-in-sinks"><strong>Exactly Once in Sinks</strong></span><a href="#exactly-once-in-sinks" class="header-anchor">#</a></h1><p>At some point, every pipeline needs to output data to the outside world, and a</p>
<p>sink is simply a transform that does exactly that. Keep in mind that delivering</p>
<p>data externally is a side effect, and we have already mentioned that Dataflow</p>
<p>does not guarantee exactly-once application of side effects. So, how can a</p>
<p>sink guarantee that outputs are delivered exactly once?</p>
<p>The simplest answer is that a number of built-in sinks are provided as part of</p>
<p>the Beam SDK. These sinks are carefully designed to ensure that they do not</p>
<p>produce duplicates, even if executed multiple times. Whenever possible,</p>
<p>pipeline authors are encouraged to use one of these built-in sinks.</p>
<p>However, sometimes the built-ins are insufficient and you need to write your</p>
<p>own. The best approach is to ensure that your side-effect operation is</p>
<p>idempotent and therefore robust in the face of replay. However, often some</p>
<p>component of a side-effect DoFn is nondeterministic and thus might change</p>
<p>on replay. For example, in a windowed aggregation, the set of records in the</p>
<p>window can also be nondeterministic!</p>
<p>Specifically, the window might attempt to fire with elements e0, e1, e2, but</p>
<p>the worker crashes before committing the window processing (but not before</p>
<p>those elements are sent as a side effect). When the worker restarts, the</p>
<p>window will fire again, but now a late element e3 shows up. Because this</p>
<p>element shows up before the window is committed, it’s not counted as late</p>
<p>data, so the DoFn is called again with elements e0, e1, e2, e3. These are then</p>
<p>sent to the side-effect operation. Idempotency does not help here, because</p>
<p>different logical record sets were sent each time.</p>
<p>There are other ways nondeterminism can be introduced. The standard way to</p>
<p>address this risk is to rely on the fact that Dataflow currently guarantees that</p>
<p>only one version of a DoFn’s output can make it past a shuffle boundary.</p>
<p>A simple way of using this guarantee is via the built-in Reshuffle transform.</p>
<p>The pattern presented in Example 5-2 ensures that the side-effect operation</p>
<p>always receives a deterministic record to output.</p>
<p><em>Example 5-2. Reshuffle example</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c.apply(Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1))))</span><br><span class="line">.apply(GroupByKey.&lt;..&gt;.create())</span><br><span class="line">.apply(new PrepareOutputData())</span><br><span class="line">.apply(Reshuffle.&lt;..&gt;of())</span><br><span class="line">.apply(WriteToSideEffect());</span><br></pre></td></tr></table></figure>

<p>The preceding pipeline splits the sink into two steps: PrepareOutputData</p>
<p>and WriteToSideEffect.  PrepareOutputData outputs  records</p>
<p>corresponding to idempotent writes. If we simply ran one after the other, the</p>
<p>entire process might be replayed on failure, PrepareOutputData might</p>
<p>produce a different result, and both would be written as side effects. When we</p>
<p>add the Reshuffle in between the two, Dataflow guarantees this can’t</p>
<p>happen.</p>
<p>Of course, Dataflow might still run the WriteToSideEffect operation</p>
<p>multiple times. The side effects themselves still need to be idempotent, or the</p>
<p>sink will receive duplicates. For example, an operation that sets or overwrites</p>
<p>a value in a data store is idempotent, and will generate correct output even if</p>
<p>it’s run several times. An operation that appends to a list is not idempotent; if</p>
<p>the operation is run multiple times, the same value will be appended each</p>
<p>time.</p>
<p>While Reshuffle provides a simple way of achieving stable input to a DoFn,</p>
<p>a GroupByKey works just as well. However, there is currently a proposal that</p>
<p>removes the need to add a GroupByKey to achieve stable input into a DoFn.</p>
<p>Instead, the user could annotate WriteToSideEffect with a special</p>
<p>annotation, @RequiresStableInput, and the system would then ensure stable</p>
<p>input to that transform.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="use-cases"><strong>Use Cases</strong></span><a href="#use-cases" class="header-anchor">#</a></h1><p>To illustrate, let’s examine some built-in sources and sinks to see how they</p>
<p>implement the aforementioned patterns.</p>
<h3><span id="example-source-cloud-pubx2fsub"><strong>Example Source: Cloud Pub&#x2F;Sub</strong></span><a href="#example-source-cloud-pubx2fsub" class="header-anchor">#</a></h3><p>Cloud Pub&#x2F;Sub is a fully managed, scalable, reliable, and low-latency system</p>
<p>for delivering messages from publishers to subscribers. Publishers publish</p>
<p>data on named topics, and subscribers create named subscriptions to pull data</p>
<p>from these topics. Multiple subscriptions can be created for a single topic, in</p>
<p>which case each subscription receives a full copy of all data published on the</p>
<p>topic from the time of the subscription’s creation. Pub&#x2F;Sub guarantees that</p>
<p>records will continue to be delivered until they are acknowledged; however, a</p>
<p>record might be delivered multiple times.</p>
<p>Pub&#x2F;Sub is intended for distributed use, so many publishing processes can</p>
<p>publish to the same topic and many subscribing processes can pull from the</p>
<p>same subscription. After a record has been pulled, the subscriber must</p>
<p>acknowledge it within a certain amount of time, or that pull expires and</p>
<p>Pub&#x2F;Sub will redeliver that record to another of the subscribing processes.</p>
<p>Although these characteristics make Pub&#x2F;Sub highly scalable, they also make</p>
<p>it a challenging source for a system like Dataflow. It’s impossible to know</p>
<p>which record will be delivered to which worker, and in which order. What’s</p>
<p>more, in the case of failure, redelivery might send the records to different</p>
<p>workers in different orders!</p>
<p>Pub&#x2F;Sub provides a stable message ID with each message, and this ID will be</p>
<p>the same upon redelivery. The Dataflow Pub&#x2F;Sub source will default to using</p>
<p>this ID for removing duplicates from Pub&#x2F;Sub. (The records are shuffled</p>
<p>based on a hash of the ID, so that repeated deliveries are always processed on</p>
<p>the same worker.) In some cases, however, this is not quite enough. The</p>
<p>user’s publishing process might retry publishes, and as a result introduce</p>
<p>duplicates into Pub&#x2F;Sub. From that service’s perspective these are unique</p>
<p>records, so they will get unique record IDs. Dataflow’s Pub&#x2F;Sub source</p>
<p>allows the user to provide their own record IDs as a custom attribute. As long</p>
<p>as the publisher sends the same ID when retrying, Dataflow will be able to</p>
<p>detect these duplicates.</p>
<p>Beam (and therefore Dataflow) provides a reference source implementation</p>
<p>for Pub&#x2F;Sub. However, keep in mind that this is <em>not</em> what Dataflow uses but</p>
<p>rather an implementation used only by non-Dataflow runners (such as Apache</p>
<p>Spark, Apache Flink, and the DirectRunner). For a variety of reasons,</p>
<p>Dataflow handles Pub&#x2F;Sub internally and does not use the public Pub&#x2F;Sub</p>
<p>source.</p>
<h3><span id="example-sink-files"><strong>Example Sink: Files</strong></span><a href="#example-sink-files" class="header-anchor">#</a></h3><p>The streaming runner can use Beam’s file sinks (TextIO, AvroIO, and any</p>
<p>other sink that implements FileBasedSink) to continuously output records to</p>
<p>files. Example 5-3 provides an example use case.</p>
<p><em>Example 5-3. Windowed file writes</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c.apply(Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1))))</span><br><span class="line">.apply(TextIO.writeStrings().to(new MyNamePolicy()).withWindowedWrites());</span><br></pre></td></tr></table></figure>

<p>The snippet in Example 5-3 writes 10 new files each minute, containing data</p>
<p>from that window. MyNamePolicy is a user-written function that determines</p>
<p>output filenames based on the shard and the window. You can also use</p>
<p>triggers, in which case each trigger pane will be output as a new file.</p>
<p>This process is implemented using a variant on the pattern in Example 5-3.</p>
<p>Files are written out to temporary locations, and these temporary filenames</p>
<p>are sent to a subsequent transform through a GroupByKey. After the</p>
<p>GroupByKey is a finalize transform that atomically moves the temporary files</p>
<p>into their final location. The pseudocode in Example 5-4 provides a sketch of</p>
<p>how a consistent streaming file sink is implemented in Beam. (For more</p>
<p>details, see FileBasedSink and WriteFiles in the Beam codebase.)</p>
<p><em>Example 5-4. File sink</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c</span><br><span class="line">// Tag each record with a random shard id.</span><br><span class="line">.apply(&quot;AttachShard&quot;, WithKeys.of(new RandomShardingKey(getNumShards())))</span><br><span class="line">// Group all records with the same shard.</span><br><span class="line">.apply(&quot;GroupByShard&quot;, GroupByKey.&lt;..&gt;())</span><br><span class="line">// For each window, write per-shard elements to a temporary file. This is the</span><br><span class="line">// non-deterministic side effect. If this DoFn is executed multiple times, it</span><br><span class="line">will</span><br><span class="line">// simply write multiple temporary files; only one of these will pass on through</span><br><span class="line">// to the Finalize stage.</span><br><span class="line">.apply(&quot;WriteTempFile&quot;, ParDo.of(new DoFn&lt;..&gt; &#123;</span><br><span class="line">@ProcessElement</span><br><span class="line">public void processElement(ProcessContext c, BoundedWindow window) &#123;</span><br><span class="line">// Write the contents of c.element() to a temporary file.</span><br><span class="line">// User-provided name policy used to generate a final filename.</span><br><span class="line">c.output(new FileResult()).</span><br><span class="line">&#125;</span><br><span class="line">&#125;))</span><br><span class="line">// Group the list of files onto a singleton key.</span><br><span class="line">.apply(&quot;AttachSingletonKey&quot;, WithKeys.&lt;..&gt;of((Void)null))</span><br><span class="line">.apply(&quot;FinalizeGroupByKey&quot;, GroupByKey.&lt;..&gt;create())</span><br><span class="line">// Finalize the files by atomically renaming them. This operation is idempotent.</span><br><span class="line">// Once this DoFn has executed once for a given FileResult, the temporary file</span><br><span class="line">// is gone, so any further executions will have no effect.</span><br><span class="line">.apply(&quot;Finalize&quot;, ParDo.of(new DoFn&lt;..&gt;, Void&gt; &#123;</span><br><span class="line">@ProcessElement</span><br><span class="line">public void processElement(ProcessContext c) &#123;</span><br><span class="line">for (FileResult result : c.element()) &#123;</span><br><span class="line">rename(result.getTemporaryFileName(), result.getFinalFilename());</span><br><span class="line">&#125;</span><br><span class="line">&#125;&#125;));</span><br></pre></td></tr></table></figure>

<p>You can see how the nonidempotent work is done in WriteTempFile. After</p>
<p>the GroupByKey completes, the Finalize step will always see the same</p>
<p>bundles across retries. Because file rename is idempotent, this give us an</p>
<p>exactly-once sink.</p>
<h3><span id="example-sink-google-bigquery"><strong>Example Sink: Google BigQuery</strong></span><a href="#example-sink-google-bigquery" class="header-anchor">#</a></h3><p>Google BigQuery is a fully managed, cloud-native data warehouse. Beam</p>
<p>provides a BigQuery sink, and BigQuery provides a streaming insert API that</p>
<p>supports extremely low-latency inserts. This streaming insert API allows</p>
<p>allows you to tag inserts with a unique ID, and BigQuery will attempt to filter</p>
<p>duplicate inserts with the same ID. To use this capability, the BigQuery sink</p>
<p>must generate statistically unique IDs for each record. It does this by using</p>
<p>the java.util.UUID package, which generates statistically unique 128-bit</p>
<p>IDs.</p>
<p>Generating a random universally unique identifier (UUID) is a</p>
<p>nondeterministic operation, so we must add a Reshuffle before we insert</p>
<p>into BigQuery. After we do this, any retries by Dataflow will always use the</p>
<p>same UUID that was shuffled. Duplicate attempts to insert into BigQuery will</p>
<p>always have the same insert ID, so BigQuery is able to filter them. The</p>
<p>pseudocode shown in Example 5-5 illustrates how the BigQuery sink is</p>
<p>implemented.</p>
<p><em>Example 5-5. BigQuery sink</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// Apply a unique identifier to each record</span><br><span class="line">c</span><br><span class="line">.apply(new DoFn&lt;&gt; &#123;</span><br><span class="line">@ProcessElement</span><br><span class="line">public void processElement(ProcessContext context) &#123;</span><br><span class="line">String uniqueId = UUID.randomUUID().toString();</span><br><span class="line">context.output(KV.of(ThreadLocalRandom.current().nextInt(0, 50),</span><br><span class="line">new RecordWithId(context.element(),</span><br><span class="line">uniqueId)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">// Reshuffle the data so that the applied identifiers are stable and will not</span><br><span class="line">change.</span><br><span class="line">.apply(Reshuffle.&lt;Integer, RecordWithId&gt;of())</span><br><span class="line">// Stream records into BigQuery with unique ids for deduplication.</span><br><span class="line">.apply(ParDo.of(new DoFn&lt;..&gt; &#123;</span><br><span class="line">@ProcessElement</span><br><span class="line">public void processElement(ProcessContext context) &#123;</span><br><span class="line">insertIntoBigQuery(context.element().record(), context.element.id());</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>Again we split the sink into a nonidempotent step (generating a random</p>
<p>number), followed by a step that is idempotent.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="other-systems"><strong>Other Systems</strong></span><a href="#other-systems" class="header-anchor">#</a></h1><p>Now that we have explained Dataflow’s exactly once in detail, let us contrast</p>
<p>this with some brief overviews of other popular streaming systems. Each</p>
<p>implements exactly-once guarantees in a different way and makes different</p>
<p>trade-offs as a result.</p>
<h3><span id="apache-spark-streaming"><strong>Apache Spark Streaming</strong></span><a href="#apache-spark-streaming" class="header-anchor">#</a></h3><p>Spark Streaming uses a microbatch architecture for continuous data</p>
<p>processing. Users logically deal with a stream object; however, under the</p>
<p>covers, Spark represents this stream as a continuous series of RDDs. Each</p>
<p>RDD is processed as a batch, and Spark relies on the exactly-once nature of</p>
<p>batch processing to ensure correctness; as mentioned previously, techniques</p>
<p>for correct batch shuffles have been known for some time. This approach can</p>
<p>cause increased latency to output—especially for deep pipelines and high</p>
<p>input volumes—and often careful tuning is required to achieve desired</p>
<p>latency.</p>
<p>Spark does assume that operations are all idempotent and might replay the</p>
<p>chain of operations up the current point in the graph. A checkpoint primitive</p>
<p>is provided, however, that causes an RDD to be materialized, guaranteeing</p>
<p>that history prior to that RDD will not be replayed. This checkpoint feature is</p>
<p>intended for performance reasons (e.g., to prevent replaying an expensive</p>
<p>operation); however, you can also use it to implement nonidempotent side</p>
<p>effects.</p>
<h3><span id="apache-flink"><strong>Apache Flink</strong></span><a href="#apache-flink" class="header-anchor">#</a></h3><p>Apache Flink also provides exactly-once processing for streaming pipelines</p>
<p>but does so in a manner different than either Dataflow or Spark. Flink</p>
<p>streaming pipelines periodically compute consistent snapshots, each</p>
<p>representing the consistent point-in-time state of an entire pipeline. Flink</p>
<p>snapshots are computed progressively, so there is no need to halt all</p>
<p>processing while computing a snapshot. This allows records to continue</p>
<p>flowing through the system while taking a snapshot, alleviating some of the</p>
<p>latency issues with the Spark Streaming approach.</p>
<p>Flink implements these snapshots by inserting special numbered snapshot</p>
<p>markers into the data streams flowing from sources. As each operator receives</p>
<p>a snapshot marker, it executes a specific algorithm allowing it to copy its state</p>
<p>to an external location and propagate the snapshot marker to downstream</p>
<p>operators. After all operators have executed this snapshot algorithm, a</p>
<p>complete snapshot is made available. Any worker failures will cause the</p>
<p>entire pipeline to roll back its state from the last complete snapshot. In-flight</p>
<p>messages do not need to be included in the snapshot. All message delivery in</p>
<p>Flink is done via an ordered TCP-based channel. Any connection failures can</p>
<p>be handled by resuming the connection from the last good sequence</p>
<p>number; unlike Dataflow, Flink tasks are statically allocated to workers, so</p>
<p>it can assume that the connection will resume from the same sender and</p>
<p>replay the same payloads.</p>
<p>Because Flink might roll back to the previous snapshot at any time, any state</p>
<p>modifications not yet in a snapshot must be considered tentative. A sink that</p>
<p>sends data to the world outside the Flink pipeline must wait until a snapshot</p>
<p>has completed, and then send only the data that is included in that snapshot.</p>
<p>Flink provides a notifySnapshotComplete callback that allows sinks to</p>
<p>know when each snapshot is completed, and send the data onward. Even</p>
<p>though this does affect the output latency of Flink pipelines, this latency is</p>
<p>introduced only at sinks. In practice, this allows Flink to have lower end-to</p>
<p>end latency than Spark for deep pipelines because Spark introduces batch</p>
<p>latency at each stage in the pipeline.</p>
<p>Flink’s distributed snapshots are an elegant way of dealing with consistency</p>
<p>in a streaming pipeline; however, a number of assumptions are made about</p>
<p>the pipeline. Failures are assumed to be rare, as the impact of a failure</p>
<p>(rolling back to the previous snapshot) is substantial. To maintain low-latency</p>
<p>output, it is also assumed that snapshots can complete quickly. It remains to</p>
<p>be seen whether this causes issues on very large clusters where the failure rate</p>
<p>will likely increase, as will the time needed to complete a snapshot.</p>
<p>Implementation is also simplified by assuming that tasks are statically</p>
<p>allocated to workers (at least within a single snapshot epoch). This</p>
<p>assumption allows Flink to provide a simple exactly-once transport between</p>
<p>workers because it knows that if a connection fails, the same data can be</p>
<p>pulled in order from the same worker. In contrast, tasks in Dataflow are</p>
<p>constantly load balanced between workers (and the set of workers is</p>
<p>constantly growing and shrinking), so Dataflow is unable to make this</p>
<p>assumption. This forces Dataflow to implement a much more complex</p>
<p>transport layer in order to provide exactly-once processing.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>In summary, exactly-once data processing, which was once thought to be</p>
<p>incompatible with low-latency results, is quite possible—Dataflow does it</p>
<p>efficiently without sacrificing latency. This enables far richer uses for stream</p>
<p>processing.</p>
<p>Although this chapter has focused on Dataflow-specific techniques, other</p>
<p>streaming systems also provide exactly-once guarantees. Apache Spark</p>
<p>Streaming runs streaming pipelines as a series of small batch jobs, relying on</p>
<p>exactly-once guarantees in the Spark batch runner. Apache Flink uses a</p>
<p>variation on Chandy Lamport distributed snapshots to get a running consistent</p>
<p>state and can use these snapshots to ensure exactly-once processing. We</p>
<p>encourage you to learn about these other systems, as well, for a broad</p>
<p>understanding of how different stream-processing systems work!</p>
<ol>
<li>In fact, no system we are aware of that provides at-least once (or better) is</li>
</ol>
<p>able to guarantee this, including all other Beam runners.</p>
<ol>
<li>Dataflow also provides an accurate batch runner; however, in this context</li>
</ol>
<p>we are focused on the streaming runner.</p>
<ol>
<li>The Dataflow optimizer groups many steps together and adds shuffles only</li>
</ol>
<p>where they are needed.</p>
<ol>
<li>Batch pipelines also need to guard against duplicates in shuffle. However</li>
</ol>
<p>the problem is much easier to solve in batch, which is why historical batch</p>
<p>systems did do this and streaming systems did not. Streaming runtimes that</p>
<p>use a microbatch architecture, such as Spark Streaming, delegate duplicate</p>
<p>detection to a batch shuffler.</p>
<ol>
<li>A lot of care is taken to make sure this checkpointing is efficient; for</li>
</ol>
<p>example, schema and access pattern optimizations that are intimately tied to</p>
<p>the characteristics of the underlying key&#x2F;value store.</p>
<ol>
<li>This is not the custom user-supplied timestamp used for windowing. Rather</li>
</ol>
<p>this is a deterministic processing-time timestamp that is assigned by the</p>
<p>145sending worker.</p>
<ol>
<li>Some care needs to be taken to ensure that this algorithm works. Each</li>
</ol>
<p>sender must guarantee that the system timestamps it generates are strictly</p>
<p>increasing, and this guarantee must be maintained across worker restarts.</p>
<ol>
<li>In theory, we could dispense with startup scans entirely by lazily building</li>
</ol>
<p>the Bloom filter for a bucket only when a threshold number of records show</p>
<p>up with timestamps in that bucket.</p>
<ol>
<li>At the time of this writing, a new, more-flexible API called SplittableDoFn</li>
</ol>
<p>is available for Apache Beam.</p>
<ol>
<li>We assume that nobody is maliciously modifying the bytes in the file while</li>
</ol>
<p>we are reading it.</p>
<ol>
<li>Again note that the SplittableDoFn API has different methods for this.</li>
<li>Using the requiresDedupping override.</li>
<li>Note that these determinism boundaries might become more explicit in the</li>
</ol>
<p>Beam Model at some point. Other Beam runners vary in their ability to handle</p>
<p>nondeterministic user code.</p>
<ol>
<li>As long as you properly handle the failure when the source file no longer</li>
</ol>
<p>exists.</p>
<ol>
<li>Due to the global nature of the service, BigQuery does not guarantee that</li>
</ol>
<p>all duplicates are removed. Users can periodically run a query over their</p>
<p>tables to remove any duplicates that were not caught by the streaming insert</p>
<p>API. See the BigQuery documentation for more information.</p>
<ol>
<li>Resilient Distributed Datasets; Spark’s abstraction of a distributed dataset,</li>
</ol>
<p>similar to PCollection in Beam.</p>
<ol>
<li>These sequence numbers are per connection and are unrelated to the</li>
</ol>
<p>snapshot epoch number.</p>
<ol>
<li>Only for nonidempotent sinks. Completely idempotent sinks do not need to</li>
</ol>
<p>wait for the snapshot to complete.</p>
<ol>
<li>Specifically, Flink assumes that the mean time to worker failure is less than</li>
</ol>
<p>the time to snapshot; otherwise, the pipeline would be unable to make</p>
<p>progress.</p>
</details>]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-第五章：精确一次和副作用 [完整]</title>
    <url>/www6vHomeHexo/2000/03/16/streamingSystemChapter5/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1%E5%BE%88%E9%87%8D%E8%A6%81">为什么”精确一次”很重要</a></li>
<li><a href="#%E5%87%86%E7%A1%AE%E6%80%A7-vs-%E5%AE%8C%E6%95%B4%E6%80%A7">准确性 vs. 完整性</a><ul>
<li><a href="#%E5%89%AF%E4%BD%9C%E7%94%A8">副作用</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89">问题定义</a></li>
</ul>
</li>
<li><a href="#%E7%A1%AE%E4%BF%9D%E6%B4%97%E7%89%8C%E6%93%8D%E4%BD%9C%E7%9A%84%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1">确保洗牌操作的精确一次</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E7%A1%AE%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98">解决确定性问题</a></li>
<li><a href="#%E6%80%A7%E8%83%BD">性能</a><ul>
<li><a href="#%E5%9B%BE%E4%BC%98%E5%8C%96">图优化</a></li>
<li><a href="#bloom-%E8%BF%87%E6%BB%A4%E5%99%A8">Bloom 过滤器</a></li>
<li><a href="#%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6">垃圾回收</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1">数据源的精确一次</a></li>
<li><a href="#sinks%E7%9A%84%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1">Sinks的精确一次</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B">使用案例</a><ul>
<li><a href="#%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E6%BA%90cloud-pubsub">示例数据源：Cloud Pub&#x2F;Sub</a></li>
<li><a href="#%E7%A4%BA%E4%BE%8B%E6%8E%A5%E6%94%B6%E5%99%A8%E6%96%87%E4%BB%B6">示例接收器：文件</a></li>
<li><a href="#%E7%A4%BA%E4%BE%8B%E6%8E%A5%E6%94%B6%E5%99%A8google-bigquery">示例接收器：Google BigQuery</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96%E7%B3%BB%E7%BB%9F">其他系统</a><ul>
<li><a href="#apache-spark-streaming">Apache Spark Streaming</a></li>
<li><a href="#apache-flink">Apache Flink</a></li>
</ul>
</li>
<li><a href="#%E6%A6%82%E8%A6%81">概要</a></li>
</ul>
<!-- tocstop -->

</div>

<p>我们现在从讨论编程模型和API转向实现它们的系统。模型和API允许用户描述他们想要计算什么。在大规模准确地运行计算需要一个系统，通常是分布式系统。</p>
<p>在本章中，我们重点介绍如何正确实现Beam模型以产生准确的结果。流系统经常谈论<em><strong>精确一次处理</strong></em>;也就是确保每个记录仅被处理一次。我们将解释这是什么意思，以及如何实现。</p>
<p>作为一个激励性的例子，本章重点介绍Google Cloud Dataflow使用的技术，以有效地保证记录的精确一次处理。在本章末尾，我们还将了解一些其他流行流系统用于保证精确一次处理的技术。</p>
<h1><span id="为什么精确一次很重要">为什么”精确一次”很重要</span><a href="#为什么精确一次很重要" class="header-anchor">#</a></h1><p>对于许多用户而言，数据处理管道中任何丢失记录或数据丢失的风险都是无法接受的。即使如此，历史上许多通用流处理系统也没有提供关于记录处理的任何保证——所有处理都只是“尽力而为”。其他系统提供了至少一次的保证，确保记录至少被处理一次，但记录可能会重复（从而导致不准确的聚合）；在实践中，许多此类至少一次的系统在内存中执行聚合，因此它们的聚合仍然可能会丢失，当机器崩溃时会出现这种情况。这些系统用于低延迟、推测性结果，但通常不能保证这些结果的准确性。</p>
<p>正如第1章所指出的那样，这导致了一种被称为“Lambda架构”的策略——运行一个流处理系统以获得快速但不准确的结果。稍后（通常是在一天结束后），批处理系统运行以得出正确的答案。这仅在数据流是可重放的情况下才有效；然而，对于足够多的数据源来说，这种策略是可行的。尽管如此，许多尝试使用这种策略的人都遇到了一些问题：</p>
<ul>
<li>不准确</li>
</ul>
<p>用户往往低估失败的影响。他们通常假设只有少量的记录会丢失或重复（通常基于他们运行的实验），并且在某一天10%（甚至更多！）的记录丢失或重复时感到震惊。在某种意义上，这些系统只提供了“一半”保证——没有完全的保证，任何事情都有可能发生。</p>
<ul>
<li>不一致</li>
</ul>
<p>用于每日计算的批处理系统通常具有与流处理系统不同的数据语义。让这两个管道产生可比较的结果比最初想象的更困难。</p>
<ul>
<li>复杂</li>
</ul>
<p>根据定义，Lambda要求您编写和维护两个不同的代码库。您还必须运行和维护两个具有不同故障模式的复杂分布式系统。对于除了最简单的管道之外的任何东西，这很快就变得不可承受。</p>
<ul>
<li>不可预测性</li>
</ul>
<p>在许多用例中，最终用户将看到与每日结果相差不确定量的流处理结果，这可能会随机变化。在这些情况下，用户将停止信任流处理数据，并等待每日批处理结果，从而破坏了首次获得低延迟结果的价值。</p>
<ul>
<li>延迟</li>
</ul>
<p>某些业务用例<em>需要</em>低延迟的正确结果，而Lambda架构本质上不提供这种结果。</p>
<p>幸运的是，许多Beam运行器可以做得更好。在本章中，我们将解释如何进行“精确一次”流处理，以帮助用户依赖单个代码库和API来保证准确的结果并避免数据丢失的风险。由于可能会影响管道输出的各种问题通常会错误地与“精确一次”保证混淆，因此我们首先解释了在Beam和数据处理的上下文中引用“精确一次”时，哪些问题是在范围内，哪些问题是在范围外。</p>
<h1><span id="准确性-vs-完整性">准确性 vs. 完整性</span><a href="#准确性-vs-完整性" class="header-anchor">#</a></h1><p>每当 Beam 管道处理管道的记录时，我们希望确保记录从未被丢弃或重复。然而，流水线的本质是，有时记录会在晚些时候出现，此时它们的时间窗口聚合已经被处理完毕。Beam SDK 允许用户配置系统应该等待迟到的数据多长时间；任何晚于此截止日期到达的记录都会被丢弃。这个特性有助于<em>完整性</em>，而不是准确性：所有及时出现的记录都被准确地处理一次，而这些迟到的记录则被明确地删除。</p>
<p>虽然迟到的记录通常在流系统的上下文中讨论，但值得注意的是，批处理管道也存在类似的完整性问题。例如，通常的批次范例是在凌晨 2 点运行作业，处理前一天的所有数据。但是，如果昨天的某些数据直到凌晨 2 点后才被收集，那么批处理作业就不会处理这些数据！因此，批处理管道也提供了准确但不总是完整的结果。</p>
<h3><span id="副作用">副作用</span><a href="#副作用" class="header-anchor">#</a></h3><p>Beam 和 Dataflow 的一个特点是，用户注入自定义代码作为其管道图的一部分。Dataflow 不保证该代码仅在流或批处理运行器中每个记录中运行一次。它可能会多次将给定记录通过用户转换运行，或者甚至可能同时在多个工作程序上运行同一记录；这是为了确保在工作程序出现故障时至少处理一次。这些调用中只有一个可以“获胜”，并在管道中进一步产生输出。</p>
<p>因此，不可重复的副作用不能保证仅执行一次；如果您编写具有外部副作用的代码（例如与外部服务联系），那么这些副作用可能会针对给定记录执行多次。这种情况通常是不可避免的，因为无法原子地提交 Dataflow 处理与外部服务上的副作用。管道确实需要最终将结果发送到外部世界，并且这些调用可能不是幂等的。正如本章后面将要介绍的那样，这样的终点通常可以添加一个额外的阶段，将调用重构为幂等操作。</p>
<h3><span id="问题定义">问题定义</span><a href="#问题定义" class="header-anchor">#</a></h3><p>那么，我们已经给出了几个我们 <em>不</em> 讨论的例子。那么，我们究竟是什么意思呢？为了说明这一点，让我们从一个简单的流水线开始，如示例 5-1 所示。</p>
<p><em>示例 5-1. 一个简单的流水线</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Pipeline p = Pipeline.create(options);</span><br><span class="line">// 计算每个用户每分钟事件的计数。</span><br><span class="line">PCollection&lt;&gt; perUserCounts =</span><br><span class="line">       p.apply(ReadFromUnboundedSource.read())</span><br><span class="line">        .apply(new KeyByUser())</span><br><span class="line">        .Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1)))</span><br><span class="line">        .apply(Count.perKey());</span><br><span class="line">// 处理这些每个用户的计数，并将输出写到某个地方。</span><br><span class="line">perUserCounts.apply(new ProcessPerUserCountsAndWriteToSink());</span><br><span class="line">// 将所有这些每个用户的计数相加，以获取所有事件的 1 分钟计数。</span><br><span class="line">perUserCounts.apply(Values.&lt;..&gt;create())</span><br><span class="line">             .apply(Count.globally())</span><br><span class="line">             .apply(new ProcessGlobalCountAndWriteToSink());</span><br><span class="line">p.run();</span><br></pre></td></tr></table></figure>

<p>此流水线计算两个不同的窗口聚合。第一个计算了每个用户在一分钟内发生了多少事件，第二个计算了每分钟总共有多少事件。两个聚合都写入了未指定的流式传输终点。</p>
<p>请记住，Dataflow 并行地在许多不同的工作程序上执行管道。在每个 GroupByKey（Count 操作在底层使用 GroupByKey）之后，具有相同键的所有记录都在同一台机器上进行处理，这是一种称为 <em>shuffle</em> 的过程。Dataflow 工作程序使用远程过程调用（RPC）在它们之间进行数据洗牌，确保给定键的记录最终都在同一台机器上。</p>
<p>图 5-1 显示了 Dataflow 在示例 5-1 的管道中创建的洗牌过程。Count.perKey 将每个用户的所有数据都洗牌到给定的工作程序上，而 Count.globally 将所有这些部分计数洗牌到单个工作程序上以计算全局总和。</p>
<img src="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/stsy_0501.png" class>
<p><em>图 5-1. 管道中的洗牌</em></p>
<p>为了准确地处理数据，此洗牌过程必须确保每个记录仅洗牌一次。正如您很快就会看到的那样，洗牌的分布式性质使这成为了一个具有挑战性的问题。</p>
<p>此管道还从外部世界读取和写入数据，因此 Dataflow 必须确保此交互不会引入任何不准确性。Dataflow 一直支持此任务 - 在技术可行的情况下，对于来源和终点，它称之为 Apache Spark 和 Apache Flink 的 <em>端到端完全一次</em>。</p>
<p>本章的重点将是以下三个方面：</p>
<ul>
<li><p>洗牌 Shuffle<br>Dataflow 如何保证每个记录仅洗牌一次。</p>
</li>
<li><p>来源 Sources<br>Dataflow 如何保证每个来源记录仅处理一次。</p>
</li>
<li><p>终点 Sinks<br>Dataflow 如何保证每个终点产生准确的输出。</p>
</li>
</ul>
<h1><span id="确保洗牌操作的精确一次">确保洗牌操作的精确一次</span><a href="#确保洗牌操作的精确一次" class="header-anchor">#</a></h1><p>正如刚才所解释的那样，Dataflow 的流式洗牌使用 RPC。现在，每当你有两台机器通过 RPC 通信时，你都应该认真考虑数据完整性。首先，RPC 可能因为许多原因而失败。网络可能会中断，RPC 可能在完成之前超时，或者接收服务器可能决定失败调用。为了保证记录在洗牌过程中不会丢失，Dataflow 使用了 <em>上游备份</em>。这意味着发送方将重试 RPC，直到它收到接收肯定收到的确认。Dataflow 还确保即使发送方崩溃，它也将继续重试这些 RPC。这保证了每条记录都至少被传递了一次。</p>
<p>现在，问题是这些重试可能会创建重复。大多数 RPC 框架（包括 Dataflow 使用的框架）提供了一个状态，表示成功或失败。在分布式系统中，您需要意识到，即使 RPC 看起来失败，它有时也可能会成功。有许多原因：与 RPC 超时的竞争条件，服务器的肯定确认尽管 RPC 成功但传输失败等等。发送方真正可以信任的唯一状态是成功状态。</p>
<p>返回失败状态的 RPC 通常表示调用可能成功或可能失败。尽管特定的错误代码可以传达明确的失败，但许多常见的 RPC 失败（如超过截止时间）是模棱两可的。在流式洗牌的情况下，重试真正成功的 RPC 意味着将记录传递两次！Dataflow 需要某种方式来检测和删除这些重复项。</p>
<p>在高层次上，该任务的算法非常简单（见图 5-2）：每个发送的消息都带有唯一的标识符。每个接收器存储已经被看到和处理的所有标识符的目录。每次接收到记录时，它的标识符将在此目录中查找。如果找到了，则将记录作为重复项丢弃。因为 Dataflow 是建立在可扩展的键&#x2F;值存储之上的，所以该存储用于保存去重目录。</p>
<img src="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/stsy_0502.png" class>
<p><em>图 5-2. 检测洗牌中的重复项</em></p>
<h1><span id="解决确定性问题">解决确定性问题</span><a href="#解决确定性问题" class="header-anchor">#</a></h1><p>但是，在现实世界中使此策略起作用需要非常小心。一种即时的纹理是 Beam 模型允许用户代码产生不确定的输出。这意味着 ParDo 可以在相同的输入记录上执行两次（由于重试），但每次重试都会产生不同的输出。期望的行为是只有其中一个输出将提交到管道中；但是，所涉及的不确定性使得难以保证两个输出具有相同的确定性 ID。更棘手的是，ParDo 可以输出多条记录，因此这些重试可能会产生不同数量的输出！</p>
<p>那么，为什么我们不简单地要求所有的用户处理都是确定性的呢？我们的经验是，在实践中，许多管道需要不确定的转换。而且，管道作者往往没有意识到他们编写的代码是不确定的。例如，考虑一个转换，它在 Cloud Bigtable 中查找补充数据，以便丰富其输入数据。这是一个不确定的任务，因为外部值可能在转换的重试之间发生变化。任何依赖于当前时间的代码也是不确定的。我们还看到需要依赖随机数生成器的转换。即使用户代码是纯确定性的，任何允许晚数据的事件时间聚合都可能具有不确定性的输入。</p>
<p>Dataflow 通过使用检查点使非确定性处理实际上变得确定性。每个转换的输出都与其唯一 ID 一起检查点到稳定存储中，然后再交付给下一个阶段。在洗牌传递中的任何重试都只是重放已检查点的输出 - 用户的不确定性代码不会在重试时再次运行。换句话说，用户的代码可能会运行多次，但只有其中一个运行可以“赢”。此外，Dataflow 使用一致存储，使其可以防止将重复项写入稳定存储。</p>
<h1><span id="性能">性能</span><a href="#性能" class="header-anchor">#</a></h1><p>为了实现精确一次的洗牌交付，每个接收器键中都存储了一个记录 ID 目录，以便于每个到达的记录都可以查找到已经看到的 ID 目录，以确定是否为重复记录。每个步骤的每个输出都会被检查点到存储器中，以确保生成的记录 ID 是稳定的。然而，除非小心实现，否则这个过程会大大降低管道的性能，因为会创建大量的读写。因此，为了使 Dataflow 用户的精确一次处理可行，必须减少 I&#x2F;O，特别是防止对每个记录进行 I&#x2F;O。</p>
<p>Dataflow 通过两种关键技术实现了这个目标：图优化和 Bloom 过滤器。</p>
<h3><span id="图优化">图优化</span><a href="#图优化" class="header-anchor">#</a></h3><p>在执行管道图之前，Dataflow 服务对管道图运行一系列优化。其中一项优化是“融合”，即服务将许多逻辑步骤融合成一个单独的执行阶段。图 5-3 显示了一些简单的示例。</p>
<img src="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/stsy_0503.png" class>
<p><em>图 5-3. 示例优化：融合</em></p>
<p>所有融合的步骤都作为一个进程单元运行，因此不需要为它们中的每一个存储精确一次数据。在许多情况下，融合会将整个图缩减到几个物理步骤，大大减少了数据传输所需的数据量（并节省了状态使用）。</p>
<p>Dataflow 还通过在发送数据到主分组操作之前在本地执行部分组合操作（例如 Count 和 Sum）来优化关联和可交换的组合操作，如图 5-4 所示。这种方法可以大大减少交付的消息数量，因此也减少了读写的数量。</p>
<img src="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/stsy_0504.png" class>
<p><em>图 5-4. 示例优化：组合器提升</em></p>
<h3><span id="bloom-过滤器">Bloom 过滤器</span><a href="#bloom-过滤器" class="header-anchor">#</a></h3><p>上述优化是改善精确一次性能的一般技术。针对严格改善精确一次处理的优化，我们转向 Bloom 过滤器。</p>
<p>在健康的管道中，大多数到达的记录不是重复的。我们可以利用这个事实通过 Bloom 过滤器大大提高性能，Bloom 过滤器是一种紧凑的数据结构，允许快速的集合成员检查。Bloom 过滤器具有非常有趣的属性：它们可以返回错误的正结果，但永远不会返回错误的负结果。如果过滤器说“是的，元素在集合中”，我们知道元素很可能在集合中（可以计算出概率）。但是，如果过滤器说一个元素不在集合中，那就一定不在。这个功能非常适合这个任务。</p>
<p>Dataflow 的实现方式是这样的：每个 worker 都会保留它看到的每个 ID 的 Bloom 过滤器。每当出现一个新的记录 ID，它就在过滤器中查找。如果过滤器返回 false，则此记录不是重复的，worker 可以跳过更昂贵的稳定存储查找。只有当 Bloom 过滤器返回 true 时，才需要进行第二次查找，但只要过滤器的假阳性率低，这一步很少需要。</p>
<p>然而，Bloom 过滤器往往会随着时间的推移而填满，随着这种情况的发生，假阳性率会增加。此外，我们还需要通过扫描存储的 ID 目录来构建每次 worker 重新启动时的 Bloom 过滤器。</p>
<p>有助于解决这个问题的是，Dataflow 为每个 10 分钟范围创建一个单独的 Bloom 过滤器，而不是创建一个单一的 Bloom 过滤器。当记录到达时，Dataflow 基于系统时间戳查询相应的过滤器。这一步可以防止 Bloom 过滤器饱和，因为随着时间的推移，过滤器会被垃圾回收，同时它也限制了启动时需要扫描的数据量。图 5-5 说明了这个过程：记录到达系统并根据到达时间委派到 Bloom 过滤器。没有任何记录命中第一个过滤器是重复的，所有它们的目录查找都被过滤了。记录 r1 重复，因此需要进行目录查找以验证它确实是重复的；r4 和 r6 的情况也是如此。记录 r8 不是重复的，但由于其 Bloom 过滤器中的假阳性，会生成一个目录查找（该查找将确定 r8 不是重复的，并应该被处理）。</p>
<img src="/www6vHomeHexo/2000/03/16/streamingSystemChapter5/stsy_0505.png" class>
<p><em>图 5-5. 精确一次的 Bloom 过滤器</em></p>
<h3><span id="垃圾回收">垃圾回收</span><a href="#垃圾回收" class="header-anchor">#</a></h3><p>每个 Dataflow worker 持久存储它看到的唯一记录 ID 目录。由于 Dataflow 的状态和一致性模型是按键分的，因此每个键存储传递到该键的记录目录。我们不能永远存储这些标识符，否则所有可用存储将最终填满。为了避免这个问题，需要对已确认的记录 ID 进行垃圾回收。</p>
<p>实现此目标的一种策略是，发送方为跟踪仍在传输中的最早序列号（对应于未确认的记录传递）标记每个记录，并为目录中具有早期序列号的任何标识符进行垃圾回收，因为所有早期记录都已经被确认。</p>
<p>然而，有一个更好的选择。如前所述，Dataflow 已经标记每个记录的系统时间戳，用于分桶精确一次的 Bloom 过滤器。因此，Dataflow 不是使用序列号来垃圾回收精确一次目录，而是基于这些系统时间戳计算垃圾回收水位线（这是第 3 章讨论的处理时间水位线）。这种方法的一个好处是，因为此水位线基于在给定阶段中等待的物理时间量（不像数据水位线，它基于自定义事件时间），因此它提供了哪些管道部分是慢的直觉。这些元数据是 Dataflow WebUI 中显示的系统滞后度指标的基础。</p>
<p>如果一条记录到达并且带有旧的时间戳，并且我们已经为此时刻进行了垃圾回收标识符会怎么样？这可能是由于我们称为“网络残留”的影响，其中旧消息在网络内部停留了无限期的时间，然后突然出现。垃圾回收的低水位线不会提前，直到记录传递已被确认，因此我们知道这条记录已经被成功处理。这些网络残留显然是重复的，并被忽略。</p>
<h1><span id="数据源的精确一次">数据源的精确一次</span><a href="#数据源的精确一次" class="header-anchor">#</a></h1><p>Beam 提供了一个用于将数据读入 Dataflow 管道的源 API。如果处理失败并且需要确保源产生的每个唯一记录仅被处理一次，则 Dataflow 可能会从源中重试读取。</p>
<p>对于大多数源，Dataflow 会在透明的情况下处理此过程；这些源是<em>确定性的</em>。例如，考虑一个从文件中读取数据的源。文件中的记录始终会按照确定性顺序出现在确定性字节位置上，无论该文件读取多少次。文件名和字节位置唯一标识每个记录，因此服务可以自动生成每个记录的唯一 ID。提供类似确定性保证的另一个源是 Apache Kafka；每个 Kafka 主题都被分成一组固定的分区，分区中的记录始终具有确定性顺序。这些确定性源将在 Dataflow 中无缝工作，没有重复项。</p>
<p>然而，并非所有的源都如此简单。例如，Dataflow 管道的一个常见源是 Google Cloud Pub&#x2F;Sub。Pub&#x2F;Sub 是一个<em>不确定性源</em>：多个订阅者可以从 Pub&#x2F;Sub 主题中拉取，但接收给定消息的订阅者是不可预测的。如果处理失败，Pub&#x2F;Sub 将重新发送消息，但消息可能会被发送到与最初处理它们的不同工作程序以及以不同的顺序。这种不确定性行为意味着 Dataflow 需要协助检测重复项，因为服务无法确定地分配记录 ID 来稳定重试。 （我们稍后在本章中将深入研究 Pub&#x2F;Sub 的一个更详细的案例研究）。</p>
<p>由于 Dataflow 无法自动分配记录 ID，因此需要非确定性源通知系统记录 ID 应该是什么。Beam 的 Source API 提供了 UnboundedReader.getCurrentRecordId 方法。如果源为每个记录提供唯一 ID 并通知 Dataflow 它需要去重，则具有相同 ID 的记录将被过滤掉。</p>
<h1><span id="sinks的精确一次">Sinks的精确一次</span><a href="#sinks的精确一次" class="header-anchor">#</a></h1><p>在某个时候，每个管道都需要将数据输出到外部世界，而下沉只是执行此操作的变换。请记住，将数据传送到外部是副作用，我们已经提到过 Dataflow 不保证副作用的精确一次应用。那么，下沉如何保证输出仅被传递一次呢？</p>
<p>最简单的答案是，Beam SDK 的一些内置下沉作为一部分提供。即使多次执行，这些下沉也经过精心设计以确保它们不会产生重复项。在可能的情况下，鼓励管道作者使用其中一个内置下沉。</p>
<p>但是，有时内置功能是不足的，您需要编写自己的功能。最好的方法是确保您的副作用操作是幂等的，并因此能够在重放时具有鲁棒性。但是，副作用 DoFn 的某些组件经常是不确定性的，因此可能会在重放时发生更改。例如，在窗口聚合中，窗口中的记录集也可能是不确定性的！</p>
<p>具体而言，窗口可能会尝试使用元素 e0、e1、e2 触发，但是在提交窗口处理之前（但在将这些元素作为副作用发送之前），工作程序会崩溃。当工作程序重新启动时，窗口将再次触发，但现在会出现延迟元素 e3。因为此元素出现在提交窗口之前，所以它不被计算为延迟数据，因此 DoFn 再次使用元素 e0、e1、e2、e3 调用。然后将它们发送到副作用操作。在这种情况下，幂等性无济于事，因为每次发送了不同的逻辑记录集。</p>
<p>还有其他方法可以引入不确定性。解决此风险的标准方法是依赖于 Dataflow 目前保证 DoFn 的输出版本只有一个可以通过洗牌边界。通过内置的 Reshuffle 变换使用此保证的简单方法。示例 5-2 中呈现的模式确保副作用操作始终接收确定性记录以进行输出。</p>
<p><em>示例5-2.Reshuffle 示例</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c.apply(Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1))))</span><br><span class="line"> .apply(GroupByKey.&lt;..&gt;.create())</span><br><span class="line"> .apply(new PrepareOutputData())</span><br><span class="line"> .apply(Reshuffle.&lt;..&gt;of())</span><br><span class="line"> .apply(WriteToSideEffect());</span><br></pre></td></tr></table></figure>

<p>前面的管道将下沉分为两个步骤：PrepareOutputData 和 WriteToSideEffect。PrepareOutputData 输出对应于幂等写入的记录。如果我们只是一个接一个地运行它们，整个过程可能会在失败时重播，PrepareOutputData 可能会产生不同的结果，并且两者都将作为副作用写入。当我们在两者之间添加 Reshuffle 时，Dataflow 保证不会发生这种情况。</p>
<p>当然，Dataflow 仍然可能多次运行 WriteToSideEffect 操作。副作用本身仍然需要是幂等的，否则下沉将接收到重复项。例如，在数据存储中设置或覆盖值的操作是幂等的，并且即使多次运行，也会生成正确的输出。将值附加到列表的操作不是幂等的；如果多次运行操作，则每次都会附加相同的值。</p>
<p>虽然 Reshuffle 提供了一种实现对 DoFn 的稳定输入的简单方法，但 GroupByKey 同样有效。但是，目前有一个提案，可以消除添加 GroupByKey 以实现 DoFn 稳定输入的需要。相反，用户可以使用特殊注释@RequiresStableInput注释WriteToSideEffect，系统将确保该变换的稳定输入。</p>
<hr>
<h1><span id="使用案例">使用案例</span><a href="#使用案例" class="header-anchor">#</a></h1><p>为了说明问题，让我们来看一些内置的数据源和数据接收器，以了解它们如何实现上述模式。</p>
<h3><span id="示例数据源cloud-pubx2fsub">示例数据源：Cloud Pub&#x2F;Sub</span><a href="#示例数据源cloud-pubx2fsub" class="header-anchor">#</a></h3><p>Cloud Pub&#x2F;Sub是一个完全托管、可扩展、可靠且低延迟的系统，用于将发布者的消息传递给订阅者。发布者在命名主题上发布数据，而订阅者创建命名订阅以从这些主题中获取数据。可以为单个主题创建多个订阅，如果是这种情况，则每个订阅将从其创建时间起接收到的主题上发布的所有数据的完整副本。 Pub&#x2F;Sub保证记录将继续传递，直到它们被确认；但是，可能会多次传递记录。</p>
<p>Pub &#x2F; Sub旨在进行分布式使用，因此许多发布进程可以发布到同一主题，许多订阅进程可以从同一订阅中拉取。在拉取记录后，订阅者必须在一定时间内进行确认，否则该拉取将过期，并且Pub &#x2F; Sub将将该记录重新传递给其他订阅进程。</p>
<p>尽管这些特征使Pub &#x2F; Sub高度可扩展，但也使其成为像Dataflow这样的系统的具有挑战性的源。无法知道将向哪个工作程序员传递哪个记录以及传递的顺序。此外，在出现故障的情况下，重新传递可能会将记录发送到不同的工作程序员以不同的顺序！</p>
<p>Pub &#x2F; Sub为每条消息提供稳定的消息ID，并且此ID将在重新传递时保持不变。 Dataflow Pub &#x2F; Sub源默认使用此ID从Pub &#x2F; Sub中删除重复项。 （记录基于ID的哈希进行洗牌，以便始终在同一工作程序上处理重复的传递。）但是，在某些情况下，这还不够。用户的发布过程可能会重试发布，从而将重复项引入Pub &#x2F; Sub。从该服务的角度来看，这些都是唯一的记录，因此它们将获得唯一的记录ID。 Dataflow的Pub &#x2F; Sub源允许用户提供自己的记录ID作为自定义属性。只要发布者在重试时发送相同的ID，Dataflow就能够检测到这些重复项。</p>
<p>Beam（因此Dataflow）为Pub &#x2F; Sub提供了参考源实现。但是，请记住，这不是Dataflow使用的内容，而是仅由非Dataflow运行器（例如Apache Spark、Apache Flink和DirectRunner）使用的实现。由于各种原因，Dataflow在内部处理Pub &#x2F; Sub并且不使用公共Pub &#x2F; Sub源。</p>
<h3><span id="示例接收器文件">示例接收器：文件</span><a href="#示例接收器文件" class="header-anchor">#</a></h3><p>流式处理运行器可以使用Beam的文件接收器（TextIO、AvroIO和实现FileBasedSink的任何其他接收器）不断将记录输出到文件中。示例5-3提供了一个使用示例。</p>
<p><em>示例5-3。窗口化文件写入</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c.apply(Window.&lt;..&gt;into(FixedWindows.of(Duration.standardMinutes(1))))</span><br><span class="line">.apply(TextIO.writeStrings().to(new MyNamePolicy()).withWindowedWrites());</span><br></pre></td></tr></table></figure>

<p>示例5-3中的片段每分钟写入10个新文件，其中包含该窗口的数据。MyNamePolicy是一个用户编写的函数，它基于分片和窗口确定输出文件名。您还可以使用触发器，在这种情况下，每个触发器窗格都将作为新文件输出。</p>
<p>该过程是使用示例5-3中的模式的变体实现的。文件写入到临时位置，这些临时文件名通过GroupByKey发送到后续转换。在GroupByKey之后是一个最终转换，它将临时文件原子地移动到其最终位置。示例5-4中的伪代码提供了Beam中实现一致的流式文件接收器的草图。（有关更多详细信息，请参见Beam代码库中的FileBasedSink和WriteFiles。）</p>
<p><em>示例5-4。文件接收器</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c</span><br><span class="line">  //使用随机分片ID为每个记录打标记。</span><br><span class="line">  .apply(&quot;AttachShard&quot;, WithKeys.of(new RandomShardingKey(getNumShards())))</span><br><span class="line">  //将所有具有相同分片的记录分组。</span><br><span class="line">  .apply(&quot;GroupByShard&quot;, GroupByKey.&lt;..&gt;())</span><br><span class="line">  //对于每个窗口，将每个分片元素写入临时文件。这是</span><br><span class="line">  //非确定性的副作用。如果此DoFn执行多次，则会</span><br><span class="line">  //仅写入多个临时文件；其中的一个将通过</span><br><span class="line">  //到最终化阶段。</span><br><span class="line">  .apply(&quot;WriteTempFile&quot;, ParDo.of(new DoFn&lt;..&gt; &#123;</span><br><span class="line">    @ProcessElement</span><br><span class="line">    public void processElement(ProcessContext c, BoundedWindow window) &#123;</span><br><span class="line">      //将c.element()的内容写入临时文件。</span><br><span class="line">      //用户提供的名称策略用于生成最终文件名。</span><br><span class="line">      c.output(new FileResult()).</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;))</span><br><span class="line">   //将文件列表分组到单个键上。</span><br><span class="line">   .apply(&quot;AttachSingletonKey&quot;, WithKeys.&lt;..&gt;of((Void)null))</span><br><span class="line">   .apply(&quot;FinalizeGroupByKey&quot;, GroupByKey.&lt;..&gt;create())</span><br><span class="line">   //通过原子重命名它们来完成文件的最终化。此操作是幂等的。</span><br><span class="line">   //一旦此DoFn为给定的FileResult执行一次，则临时文件</span><br><span class="line">   //消失了，因此任何进一步的执行都没有影响。</span><br><span class="line">   .apply(&quot;Finalize&quot;, ParDo.of(new DoFn&lt;..&gt;, Void&gt; &#123;</span><br><span class="line">      @ProcessElement</span><br><span class="line">       public void processElement(ProcessContext c) &#123;</span><br><span class="line">         for (FileResult result : c.element()) &#123;</span><br><span class="line">           rename(result.getTemporaryFileName(), result.getFinalFilename());</span><br><span class="line">         &#125;</span><br><span class="line">&#125;&#125;));</span><br></pre></td></tr></table></figure>

<p>您可以看到在WriteTempFile中完成非幂等工作。在GroupByKey完成之后，Finalize步骤始终会看到相同的束再次尝试。因为文件重命名是幂等的，所以这使我们得到了一个精确一次的接收器。</p>
<h3><span id="示例接收器google-bigquery">示例接收器：Google BigQuery</span><a href="#示例接收器google-bigquery" class="header-anchor">#</a></h3><p>Google BigQuery是一个完全托管的云原生数据仓库。Beam提供了BigQuery接收器，而BigQuery提供了支持极低延迟插入的流式插入API。此流式插入API允许您使用唯一ID标记插入，并且BigQuery将尝试过滤具有相同ID的重复插入。为了使用此功能，BigQuery接收器必须为每个记录生成统计唯一的ID。它通过使用java.util.UUID包生成统计唯一的128位ID来实现此目的。</p>
<p>生成随机通用唯一标识符（UUID）是一种非确定性操作，因此我们必须在插入到BigQuery之前添加一个Reshuffle。在我们这样做之后，Dataflow的任何重试都将始终使用被洗牌的相同UUID。尝试重复插入到BigQuery将始终具有相同的插入ID，因此BigQuery能够过滤它们。示例5-5中显示的伪代码说明了如何实现BigQuery接收器。</p>
<p><em>示例5-5. BigQuery接收器</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//为每个记录应用唯一标识符</span><br><span class="line">c</span><br><span class="line">  .apply(new DoFn&lt;&gt; &#123;</span><br><span class="line">    @ProcessElement</span><br><span class="line">    public void processElement(ProcessContext context) &#123;</span><br><span class="line">      String uniqueId = UUID.randomUUID().toString();</span><br><span class="line">      context.output(KV.of(ThreadLocalRandom.current().nextInt(0, 50),</span><br><span class="line">                                   new RecordWithId(context.element(),</span><br><span class="line">uniqueId)));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line">   //重洗数据，以便应用的标识符是稳定的并且不会更改。</span><br><span class="line">  .apply(Reshuffle.&lt;Integer, RecordWithId&gt;of())</span><br><span class="line">  //使用唯一ID将记录流式传输到BigQuery以进行重复项检测。</span><br><span class="line">  .apply(ParDo.of(new DoFn&lt;..&gt; &#123;</span><br><span class="line">    @ProcessElement</span><br><span class="line">     public void processElement(ProcessContext context) &#123;</span><br><span class="line">       insertIntoBigQuery(context.element().record(), context.element.id());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>同样，我们将接收器拆分为不幂等步骤（生成随机数），然后是幂等步骤。</p>
<hr>
<h1><span id="其他系统">其他系统</span><a href="#其他系统" class="header-anchor">#</a></h1><p>现在我们已经详细解释了Dataflow的Exactly Once，让我们来对比一下其他流行的流处理系统的简要概述。每个系统都以不同的方式实现Exactly Once保证，并因此做出不同的权衡。</p>
<h3><span id="apache-spark-streaming">Apache Spark Streaming</span><a href="#apache-spark-streaming" class="header-anchor">#</a></h3><p>Spark Streaming使用微批次体系结构进行连续数据处理。用户逻辑上处理一个流对象；但是，在内部，Spark将此流表示为RDD的连续系列。每个RDD作为批处理进行处理，并且Spark依赖于批处理的Exactly Once属性来确保正确性；就像先前提到过的，正确批处理洗牌技术已经被知道一段时间了。这种方法可能会导致输出的延迟增加 - 尤其是对于深度管道和高输入量，通常需要仔细调整才能实现所需的延迟。</p>
<p>Spark假设所有操作都是幂等的，可能会重播到当前图中的点的操作链。但是，提供了一个检查点原语，导致RDD被实现，保证在该RDD之前的历史记录不会被重播。该检查点功能旨在提高性能（例如，防止重新播放昂贵的操作）；但是，您也可以使用它来实现不可幂等的副作用。</p>
<h3><span id="apache-flink">Apache Flink</span><a href="#apache-flink" class="header-anchor">#</a></h3><p>Apache Flink也为流水线提供Exactly Once处理，但是采用与Dataflow或Spark不同的方式。Flink流水线定期计算一致的快照，每个快照表示整个流水线的一致时间点状态。Flink快照是逐步计算的，因此无需在计算快照时停止所有处理。这使记录可以在系统中继续流动而进行快照，缓解了Spark Streaming方法的一些延迟问题。</p>
<p>Flink通过将特殊编号的快照标记插入从源流出的数据流中来实现这些快照。当每个运算符接收到快照标记时，它执行特定的算法，使其可以将其状态复制到外部位置并将快照标记传播到下游运算符。在所有运算符都执行完此快照算法后，将可用完整快照。任何工作器失败都将导致整个管道从上一个完整快照中回滚其状态。在快照中不需要包含正在进行的消息。Flink中的所有消息传递都是通过有序的基于TCP的通道完成的。任何连接故障都可以通过从最后一个良好序列号恢复连接来处理；与Dataflow不同，Flink任务静态分配给工作器，因此可以假定连接将从相同的发送者恢复，并重新播放相同的有效负载。</p>
<p>因为Flink随时可能回滚到以前的快照，所以尚未在快照中的任何状态修改都必须被视为暂时的。将数据发送到Flink管道外部的接收器必须等待快照完成，然后仅发送包含在该快照中的数据。Flink提供了一个notifySnapshotComplete回调，允许接收器知道每个快照何时完成，并将数据发送到下一个节点。即使这会影响Flink管道的输出延迟，但这种延迟仅在接收器中引入。实际上，这使Flink在深层管道的端到端延迟方面比Spark具有更低的延迟，因为Spark在管道的每个阶段引入了批处理延迟。</p>
<p>Flink的分布式快照是处理流水线一致性的一种优雅方法；但是，对管道做出了许多假设。假设故障很少，因为故障的影响（从上一个快照回滚）很大。为了保持低延迟输出，还假设快照可以快速完成。尚不清楚这是否会在非常大的集群上引起问题，因为故障率可能会增加，完成快照所需的时间也会增加。</p>
<p>还通过假设任务静态分配给工作器（至少在单个快照时期内）来简化实现。该假设允许Flink在工作器之间提供简单的Exactly Once传输，因为它知道如果连接失败，则可以按顺序从同一工作器中提取相同的数据。相反，Dataflow中的任务不断在工作器之间负载平衡（并且工作器集合不断增加和缩小），因此Dataflow无法做出这种假设。这迫使Dataflow实现更复杂的传输层以提供Exactly Once处理。</p>
<h1><span id="概要">概要</span><a href="#概要" class="header-anchor">#</a></h1><p>总的来说，曾经被认为与低延迟结果不兼容的精确一次数据处理是完全可能的，Dataflow可以在不牺牲延迟的情况下有效地实现。这使得流处理有更丰富的用途。</p>
<p>虽然本章重点介绍了Dataflow特定的技术，但其他流处理系统也提供精确一次的保证。Apache Spark Streaming将流水线作为一系列小批量作业运行，依赖于Spark批处理运行程序的精确一次保证。Apache Flink使用Chandy Lamport分布式快照的变化来获得运行一致的状态，并可以使用这些快照来确保精确一次处理。我们也鼓励您学习这些其他系统，以便广泛了解不同流处理系统的工作方式！</p>
<hr>
<ol>
<li>实际上，我们所知道的提供至少一次保证（或更好）的系统都无法保证这一点，包括所有其他Beam运行程序。</li>
<li>Dataflow也提供了准确的批处理运行程序；但在这个上下文中，我们专注于流运行程序。</li>
<li>Dataflow优化器将许多步骤分组在一起，并仅在需要时添加洗牌。</li>
<li>批处理管道也需要防止洗牌中的重复项。但是，批处理中的问题要容易得多，这就是为什么历史批处理系统这样做而流处理系统没有这样做的原因。使用微批处理架构的流处理运行时，例如Spark Streaming，将重复项检测委托给批量混洗器。</li>
<li>为确保检查点效率，需要采取许多谨慎的措施；例如，与底层键&#x2F;值存储的特性密切相关的模式和访问模式优化。</li>
<li>这不是用于窗口处理的自定义用户提供的时间戳。而是由发送工作器分配的确定性处理时间戳。</li>
<li>需要采取一些措施确保该算法有效。每个发送者必须保证其生成的系统时间戳严格递增，并且此保证必须在工作器重新启动时维护。</li>
<li>从理论上讲，我们可以通过在桶中出现时间戳的阈值记录时才惰性构建桶的布隆过滤器，从而完全放弃启动扫描。</li>
<li>在撰写本文时，Apache Beam提供了一种新的、更灵活的API，称为SplittableDoFn。</li>
<li>我们假设在我们读取文件时，没有人恶意修改文件中的字节。</li>
<li>再次注意，SplittableDoFn API具有不同的方法。</li>
<li>使用requiresDedupping覆盖。</li>
<li>请注意，这些确定性边界可能在Beam模型的某个时候变得更加明确。其他Beam运行程序在处理非确定性用户代码方面有所不同。</li>
<li>只要在源文件不再存在时正确处理故障。</li>
<li>由于该服务的全局性质，BigQuery不能保证所有重复项都被删除。用户可以定期对他们的表运行查询，以删除未被流式插入API捕获的任何重复项。有关更多信息，请参阅BigQuery文档。</li>
<li>弹性分布式数据集；Spark的分布式数据集抽象，类似于Beam中的PCollection。</li>
<li>这些序列号是每个连接的，与快照时期号码无关。</li>
<li>仅适用于非幂等的汇聚。完全幂等的聚合不需要等待快照完成。</li>
<li>具体而言，Flink假定工作器故障的平均时间小于快照时间；否则，管道将无法取得进展。</li>
</ol>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-Chapter 3. Watermarks[完整]</title>
    <url>/www6vHomeHexo/2000/03/15/streamingSystemChapter3Original/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#definition"><strong>Definition</strong></a></li>
<li><a href="#source-watermark-creation"><strong>Source Watermark Creation</strong></a><ul>
<li><a href="#perfect-watermark-creation"><strong>Perfect Watermark Creation</strong></a></li>
<li><a href="#heuristic-watermark-creation"><strong>Heuristic Watermark Creation</strong></a></li>
</ul>
</li>
<li><a href="#watermark-propagation"><strong>Watermark Propagation</strong></a><ul>
<li><a href="#understanding-watermark-propagation"><strong>Understanding Watermark Propagation</strong></a></li>
<li><a href="#watermark-propagation-and-output-timestamps"><strong>Watermark Propagation and Output Timestamps</strong></a></li>
<li><a href="#the-tricky-case-of-overlapping-windows"><strong>The Tricky Case of Overlapping Windows</strong></a></li>
</ul>
</li>
<li><a href="#percentile-watermarks"><strong>Percentile Watermarks</strong></a></li>
<li><a href="#processing-time-watermarks"><strong>Processing-Time Watermarks</strong></a></li>
<li><a href="#case-studies"><strong>Case Studies</strong></a><ul>
<li><a href="#case-study-watermarks-in-google-cloud-dataflow"><strong>Case Study: Watermarks in Google Cloud Dataflow</strong></a></li>
<li><a href="#case-study-watermarks-in-apache-flink"><strong>Case Study: Watermarks in Apache Flink</strong></a></li>
<li><a href="#case-study-source-watermarks-for-google-cloud-pubsub"><strong>Case Study: Source Watermarks for Google Cloud Pub&#x2F;Sub</strong></a></li>
</ul>
</li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 73</p>
<details><summary>点击 原文</summary><p>So far, we have been looking at stream processing from the perspective of the</p>
<p>pipeline author or data scientist. Chapter 2 introduced watermarks as part of</p>
<p>the answer to the fundamental questions of <em>where</em> in event-time processing is</p>
<p>taking place and <em>when</em> in processing time results are materialized. In this</p>
<p>chapter, we approach the same questions, but instead from the perspective of</p>
<p>the underlying mechanics of the stream processing system. Looking at these</p>
<p>mechanics will help us motivate, understand, and apply the concepts around</p>
<p>watermarks. We discuss how watermarks are created at the point of data</p>
<p>ingress, how they propagate through a data processing pipeline, and how they</p>
<p>affect output timestamps. We also demonstrate how watermarks preserve the</p>
<p>guarantees that are necessary for answering the questions of <em>where</em> in event</p>
<p>time data are processed and <em>when</em> it is materialized, while dealing with</p>
<p>unbounded data.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="definition"><strong>Definition</strong></span><a href="#definition" class="header-anchor">#</a></h1><p>Consider any pipeline that ingests data and outputs results continuously. We</p>
<p>wish to solve the general problem of when it is safe to call an event-time</p>
<p>window closed, meaning that the window does not expect any more data. To</p>
<p>do so we would like to characterize the progress that the pipeline is making</p>
<p>relative to its unbounded input.</p>
<p>One naive approach for solving the event-time windowing problem would be</p>
<p>to simply base our event-time windows on the current processing time. As we</p>
<p>saw in Chapter 1, we quickly run into trouble—data processing and transport</p>
<p>is not instantaneous, so processing and event times are almost never equal.</p>
<p>Any hiccup or spike in our pipeline might cause us to incorrectly assign</p>
<p>messages to windows. Ultimately, this strategy fails because we have no</p>
<p>robust way to make any guarantees about such windows.</p>
<p>Another intuitive, but ultimately incorrect, approach would be to consider the</p>
<p>rate of messages processed by the pipeline. Although this is an interesting</p>
<p>metric, the rate may vary arbitrarily with changes in input, variability of</p>
<p>expected results, resources available for processing, and so on. Even more</p>
<p>important, rate does not help answer the fundamental questions of</p>
<p>completeness. Specifically, rate does not tell us when we have seen all of the</p>
<p>messages for a particular time interval. In a real-world system, there will be</p>
<p>situations in which messages are not making progress through the system.</p>
<p>This could be the result of transient errors (such as crashes, network failures,</p>
<p>machine downtime), or the result of persistent errors such as application-level</p>
<p>failures that require changes to the application logic or other manual</p>
<p>intervention to resolve. Of course, if lots of failures are occurring, a rate-of</p>
<p>processing metric might be a good proxy for detecting this. However a rate</p>
<p>metric could never tell us that a single message is failing to make progress</p>
<p>through our pipeline. Even a single such message, however, can arbitrarily</p>
<p>affect the correctness of the output results.</p>
<p>We require a more robust measure of progress. To arrive there, we make one</p>
<p>fundamental assumption about our streaming data: <em>each message has an</em></p>
<p><em>associated logical event timestamp</em>. This assumption is reasonable in the</p>
<p>context of continuously arriving unbounded data because this implies the</p>
<p>continuous generation of input data. In most cases, we can take the time of the</p>
<p>original event’s occurrence as its logical event timestamp. With all input</p>
<p>messages containing an event timestamp, we can then examine the</p>
<p>distribution of such timestamps in any pipeline. Such a pipeline might be</p>
<p>distributed to process in parallel over many agents and consuming input</p>
<p>messages with no guarantee of ordering between individual shards. Thus, the</p>
<p>set of event timestamps for active in-flight messages in this pipeline will form</p>
<p>a distribution, as illustrated in Figure 3-1.</p>
<p>Messages are ingested by the pipeline, processed, and eventually marked</p>
<p>completed. Each message is either “in-flight,” meaning that it has been</p>
<p>received but not yet completed, or “completed,” meaning that no more</p>
<p>processing on behalf of this message is required. If we examine the</p>
<p>distribution of messages by event time, it will look something like Figure 3-1.</p>
<p>As time advances, more messages will be added to the “in-flight” distribution</p>
<p>on the right, and more of those messages from the “in-flight” part of the</p>
<p>distribution will be completed and moved into the “completed” distribution.</p>
<p><em>Figure 3-1. Distribution of in-flight and completed message event times within a streaming pipeline.</em></p>
<p><em>New messages arrive as input and remain “in-flight” until processing for them completes. The leftmost</em></p>
<p><em>edge of the “in-flight” distribution corresponds to the oldest unprocessed element at any given moment.</em></p>
<p>There is a key point on this distribution, located at the leftmost edge of the</p>
<p>“in-flight” distribution, corresponding to the oldest event timestamp of any</p>
<p>unprocessed message of our pipeline. We use this value to define the</p>
<p>watermark:</p>
<p><em>The watermark is a monotonically increasing timestamp of the oldest work</em></p>
<p><em>not yet completed.</em></p>
<p>There are two fundamental properties that are provided by this definition that</p>
<p>make it useful:</p>
<ul>
<li>Completeness</li>
</ul>
<p>If the watermark has advanced past some timestamp <em>T</em>, we are guaranteed</p>
<p>by its monotonic property that no more processing will occur for on-time</p>
<p>(nonlate data) events at or before <em>T</em>. Therefore, we can correctly emit any</p>
<p>aggregations at or before <em>T</em>. In other words, the watermark allows us to</p>
<p>know when it is correct to close a window.</p>
<ul>
<li>Visibility</li>
</ul>
<p>If a message is stuck in our pipeline for any reason, the watermark cannot</p>
<p>advance. Furthermore, we will be able to find the source of the problem</p>
<p>by examining the message that is preventing the watermark from</p>
<p>advancing.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="source-watermark-creation"><strong>Source Watermark Creation</strong></span><a href="#source-watermark-creation" class="header-anchor">#</a></h1><p>Where do these watermarks come from? To establish a watermark for a data</p>
<p>source, we must assign a logical event timestamp to every message entering</p>
<p>the pipeline from that source. As Chapter 2 informs us, all watermark creation</p>
<p>falls into one of two broad categories: <em>perfect</em> or <em>heuristic</em>. To remind</p>
<p>ourselves about the difference between perfect and heuristic watermarks, let’s</p>
<p>look at Figure 3-2, which presents the windowed summation example from</p>
<p>Chapter 2.</p>
<p><em>Figure 3-2. Windowed summation with perfect (left) and heuristic (right) watermarks</em></p>
<p>Notice that the distinguishing feature is that perfect watermarks ensure that</p>
<p>the watermark accounts for <em>all</em> data, whereas heuristic watermarks admit</p>
<p>some late-data elements.</p>
<p>After the watermark is created as either perfect or heuristic, watermarks</p>
<p>remain so throughout the rest of the pipeline. As to what makes watermark</p>
<p>creation perfect or heuristic, it depends a great deal on the nature of the source</p>
<p>that’s being consumed. To see why, let’s look at a few examples of each type</p>
<p>of watermark creation.</p>
<h3><span id="perfect-watermark-creation"><strong>Perfect Watermark Creation</strong></span><a href="#perfect-watermark-creation" class="header-anchor">#</a></h3><p>Perfect watermark creation assigns timestamps to incoming messages in such</p>
<p>a way that the resulting watermark is a <em>strict guarantee</em> that no data with</p>
<p>event times less than the watermark will ever be seen again from this source.</p>
<p>Pipelines using perfect watermark creation never have to deal with late data;</p>
<p>that is, data that arrive after the watermark has advanced past the event times</p>
<p>of newly arriving messages. However, perfect watermark creation requires</p>
<p>perfect knowledge of the input, and thus is impractical for many real-world</p>
<p>distributed input sources. Here are a couple of examples of use cases that can</p>
<p>create perfect watermarks:</p>
<ul>
<li>Ingress timestamping</li>
</ul>
<p>A source that assigns ingress times as the event times for data entering the</p>
<p>system can create a perfect watermark. In this case, the source watermark</p>
<p>simply tracks the current processing time as observed by the pipeline.</p>
<p>This is essentially the method that nearly all streaming systems supporting</p>
<p>windowing prior to 2016 used.</p>
<p>Because event times are assigned from a single, monotonically increasing</p>
<p>source (actual processing time), the system thus has perfect knowledge</p>
<p>about which timestamps will come next in the stream of data. As a result,</p>
<p>event-time progress and windowing semantics become vastly easier to</p>
<p>reason about. The downside, of course, is that the watermark has no</p>
<p>correlation to the event times of the data themselves; those event times</p>
<p>were effectively discarded, and the watermark instead merely tracks the</p>
<p>progress of data relative to its arrival in the system.</p>
<ul>
<li>Static sets of time-ordered logs</li>
</ul>
<p>A statically sized input source of time-ordered logs (e.g., an Apache</p>
<p>Kafka topic with a static set of partitions, where each partition of the</p>
<p>source contains monotonically increasing event times) would be relatively</p>
<p>straightforward source atop which to create a perfect watermark. To do so,</p>
<p>the source would simply track the minimum event time of unprocessed</p>
<p>data across the known and static set of source partitions (i.e., the</p>
<p>minimum of the event times of the most recently read record in each of</p>
<p>the partitions).</p>
<p>Similar to the aforementioned ingress timestamps, the system has perfect</p>
<p>knowledge about which timestamps will come next, thanks to the fact that</p>
<p>event times across the static set of partitions are known to increase</p>
<p>monotonically. This is effectively a form of bounded out-of-order</p>
<p>processing; the amount of disorder across the known set of partitions is</p>
<p>bounded by the minimum observed event time among those partitions.</p>
<p>Typically, the only way you can guarantee monotonically increasing</p>
<p>timestamps within partitions is if the timestamps within those partitions</p>
<p>are assigned as data are written to it; for example, by web frontends</p>
<p>logging events directly into Kafka. Though still a limited use case, this is</p>
<p>definitely a much more useful one than ingress timestamping upon arrival</p>
<p>at the data processing system because the watermark tracks meaningful</p>
<p>event times of the underlying data.</p>
<h3><span id="heuristic-watermark-creation"><strong>Heuristic Watermark Creation</strong></span><a href="#heuristic-watermark-creation" class="header-anchor">#</a></h3><p>Heuristic watermark creation, on the other hand, creates a watermark that is</p>
<p>merely an <em>estimate</em> that no data with event times less than the watermark will</p>
<p>ever be seen again. Pipelines using heuristic watermark creation might need</p>
<p>to deal with some amount of <em>late data</em>. Late data is any data that arrives after</p>
<p>the watermark has advanced past the event time of this data. Late data is only</p>
<p>possible with heuristic watermark creation. If the heuristic is a reasonably</p>
<p>good one, the amount of late data might be very small, and the watermark</p>
<p>remains useful as a completion estimate. The system still needs to provide a</p>
<p>way for the user to cope with late data if it’s to support use cases requiring</p>
<p>correctness (e.g., things like billing).</p>
<p>For many real-world, distributed input sources, it’s computationally or</p>
<p>operationally impractical to construct a perfect watermark, but still possible to</p>
<p>build a highly accurate heuristic watermark by taking advantage of structural</p>
<p>features of the input data source. Following are two example for which</p>
<p>heuristic watermarks (of varying quality) are possible:</p>
<ul>
<li>Dynamic sets of time-ordered logs</li>
</ul>
<p>Consider a dynamic set of structured log files (each individual file</p>
<p>containing records with monotonically increasing event times relative to</p>
<p>other records in the same file but with no fixed relationship of event times</p>
<p>between files), where the full set of expected log files (i.e., partitions, in</p>
<p>Kafka parlance) is not known at runtime. Such inputs are often found in</p>
<p>global-scale services constructed and managed by a number of</p>
<p>independent teams. In such a use case, creating a perfect watermark over</p>
<p>the input is intractable, but creating an accurate heuristic watermark is</p>
<p>quite possible.</p>
<p>By tracking the minimum event times of unprocessed data in the existing</p>
<p>set of log files, monitoring growth rates, and utilizing external information</p>
<p>like network topology and bandwidth availability, you can create a</p>
<p>remarkably accurate watermark, even given the lack of perfect knowledge</p>
<p>of all the inputs. This type of input source is one of the most common</p>
<p>types of unbounded datasets found at Google, so we have extensive</p>
<p>experience with creating and analyzing watermark quality for such</p>
<p>scenarios and have seen them used to good effect across a number of use</p>
<p>cases.</p>
<ul>
<li>Google Cloud Pub&#x2F;Sub</li>
</ul>
<p>Cloud Pub&#x2F;Sub is an interesting use case. Pub&#x2F;Sub currently makes no</p>
<p>guarantees on in-order delivery; even if a single publisher publishes two</p>
<p>messages in order, there’s a chance (usually small) that they might be</p>
<p>delivered out of order (this is due to the dynamic nature of the underlying</p>
<p>architecture, which allows for transparent scaling up to very high levels of</p>
<p>throughput with zero user intervention). As a result, there’s no way to</p>
<p>guarantee a perfect watermark for Cloud Pub&#x2F;Sub. The Cloud Dataflow</p>
<p>team has, however, built a reasonably accurate heuristic watermark by</p>
<p>taking advantage of what knowledge <em>is</em> available about the data in Cloud</p>
<p>Pub&#x2F;Sub. The implementation of this heuristic is discussed at length as a</p>
<p>case study later in this chapter.</p>
<p>Consider an example where users play a mobile game, and their scores are</p>
<p>sent to our pipeline for processing: you can generally assume that for any</p>
<p>source utilizing mobile devices for input it will be generally impossible to</p>
<p>provide a perfect watermark. Due to the problem of devices that go offline for</p>
<p>extended periods of time, there’s just no way to provide any sort of</p>
<p>reasonable estimate of absolute completeness for such a data source. You can,</p>
<p>however, imagine building a watermark that accurately tracks input</p>
<p>completeness for devices that are currently online, similar to the Google</p>
<p>Pub&#x2F;Sub watermark described a moment ago. Users who are actively online</p>
<p>are likely the most relevant subset of users from the perspective of providing</p>
<p>low-latency results anyway, so this often isn’t as much of a shortcoming as</p>
<p>you might initially think.</p>
<p>With heuristic watermark creation, broadly speaking, the more that is known</p>
<p>about the source, the better the heuristic, and the fewer late data items will be</p>
<p>seen. There is no one-size-fits-all solution, given that the types of sources,</p>
<p>distributions of events, and usage patterns will vary greatly. But in either case</p>
<p>(perfect or heuristic), after a watermark is created at the input source, the</p>
<p>system can propagate the watermark through the pipeline perfectly. This</p>
<p>means perfect watermarks will remain perfect downstream, and heuristic</p>
<p>watermarks will remain strictly as heuristic as they were when established.</p>
<p>This is the benefit of the watermark approach: you can reduce the complexity</p>
<p>of tracking completeness in a pipeline entirely to the problem of creating a</p>
<p>watermark at the source.</p>
</details>





<details><summary>点击 原文</summary><h1><span id="watermark-propagation"><strong>Watermark Propagation</strong></span><a href="#watermark-propagation" class="header-anchor">#</a></h1><p>So far, we have considered only the watermark for the inputs within the</p>
<p>context of a single operation or stage. However, most real-world pipelines</p>
<p>consist of multiple stages. Understanding how watermarks propagate across</p>
<p>independent stages is important in understanding how they affect the pipeline</p>
<p>as a whole and the observed latency of its results.</p>
<p><strong>PIPELINE STAGES</strong></p>
<p>Different stages are typically necessary every time your pipeline groups</p>
<p>data together by some new dimension. For example, if you had a pipeline</p>
<p>that consumed raw data, computed some per-user aggregates, and then</p>
<p>used those per-user aggregates to compute some per-team aggregates,</p>
<p>you’d likely end up with a three-stage pipeline:</p>
<p>One consuming the raw, ungrouped data</p>
<p>One grouping the data by user and computing per-user aggregates</p>
<p>One grouping the data by team and computing per-team</p>
<p>aggregates</p>
<p>We learn more about the effects of grouping on pipeline shapes in</p>
<p>Chapter 6.</p>
<p>Watermarks are created at input sources, as discussed in the preceding</p>
<p>section. They then conceptually flow through the system as data progress</p>
<p>through it. You can track watermarks at varying levels of granularity. For</p>
<p>pipelines comprising multiple distinct stages, each stage likely tracks its own</p>
<p>watermark, whose value is a function of all the inputs and stages that come</p>
<p>before it. Therefore, stages that come later in the pipeline will have</p>
<p>watermarks that are further in the past (because they’ve seen less of the</p>
<p>overall input).</p>
<p>We can define watermarks at the boundaries of any single operation, or stage,</p>
<p>in the pipeline. This is useful not only in understanding the relative progress</p>
<p>that each stage in the pipeline is making, but for dispatching timely results</p>
<p>independently and as soon as possible for each individual stage. We give the</p>
<p>following definitions for the watermarks at the boundaries of stages:</p>
<ul>
<li>An <em>input watermark</em>, which captures the progress of everything</li>
</ul>
<p>upstream of that stage (i.e., how complete the input is for that stage).</p>
<p>For sources, the input watermark is a source-specific function</p>
<p>creating the watermark for the input data. For nonsource stages, the</p>
<p>input watermark is defined as the minimum of the output watermarks</p>
<p>of all shards&#x2F;partitions&#x2F;instances of all of its upstream sources and</p>
<p>stages.</p>
<ul>
<li>An <em>output watermark</em>, which captures the progress of the stage itself,</li>
</ul>
<p>and is essentially defined as the minimum of the stage’s input</p>
<p>watermark and the event times of all nonlate data active messages</p>
<p>within the stage. Exactly what “active” encompasses is somewhat</p>
<p>dependent upon the operations a given stage actually performs, and</p>
<p>the implementation of the stream processing system. It typically</p>
<p>includes data buffered for aggregation but not yet materialized</p>
<p>downstream, pending output data in flight to downstream stages, and</p>
<p>so on.</p>
<p>One nice feature of defining an input and output watermark for a specific</p>
<p>stage is that we can use these to calculate the amount of event-time latency</p>
<p>introduced by a stage. Subtracting the value of a stage’s output watermark</p>
<p>from the value of its input watermark gives the amount of event-time latency</p>
<p>or <em>lag</em> introduced by the stage. This lag is the notion of how far delayed</p>
<p>behind real time the output of each stage will be. As an example, a stage</p>
<p>performing 10-second windowed aggregations will have a lag of 10 seconds</p>
<p>or more, meaning that the output of the stage will be at least that much</p>
<p>delayed behind the input and real time. Definitions of input and output</p>
<p>watermarks provide a recursive relationship of watermarks throughout a</p>
<p>pipeline. Each subsequent stage in a pipeline delays the watermark as</p>
<p>necessary, based on event-time lag of the stage.</p>
<p>Processing within each stage is also not monolithic. We can segment the</p>
<p>processing within one stage into a flow with several conceptual components,</p>
<p>each of which contributes to the output watermark. As mentioned previously,</p>
<p>the exact nature of these components depends on the operations the stage</p>
<p>performs and the implementation of the system. Conceptually, each such</p>
<p>component serves as a buffer where active messages can reside until some</p>
<p>operation has completed. For example, as data arrives, it is buffered for</p>
<p>processing. Processing might then write the data to state for later delayed</p>
<p>aggregation. Delayed aggregation, when triggered, might write the results to</p>
<p>an output buffer awaiting consumption from a downstream stage, as shown in</p>
<p>Figure 3-3.</p>
<p><em>Figure 3-3. Example system components of a streaming system stage, containing buffers of in-flight</em></p>
<p><em>data. Each will have associated watermark tracking, and the overall output watermark of the stage will be the minimum of the watermarks across all such buffers.</em></p>
<p>We can track each such buffer with its own watermark. The minimum of the</p>
<p>watermarks across the buffers of each stage forms the output watermark of</p>
<p>the stage. Thus the output watermark could be the minimum of the following:</p>
<ul>
<li><em>Per-source</em> watermark—for each sending stage.</li>
<li><em>Per-external input</em> watermark—for sources external to the pipeline</li>
<li><em>Per-state component</em> watermark—for each type of state that can be</li>
</ul>
<p>written</p>
<ul>
<li><em>Per-output buffer</em> watermark—for each receiving stage</li>
</ul>
<p>Making watermarks available at this level of granularity also provides better</p>
<p>visibility into the behavior of the system. The watermarks track locations of</p>
<p>messages across various buffers in the system, allowing for easier diagnosis</p>
<p>of stuckness.</p>
<h3><span id="understanding-watermark-propagation"><strong>Understanding Watermark Propagation</strong></span><a href="#understanding-watermark-propagation" class="header-anchor">#</a></h3><p>To get a better sense for the relationship between input and output</p>
<p>watermarks and how they affect watermark propagation, let’s look at an</p>
<p>example. Let’s consider gaming scores, but instead of computing sums of</p>
<p>team scores, we’re going to take a stab at measuring user engagement levels.</p>
<p>We’ll do this by first calculating per-user session lengths, under the</p>
<p>assumption that the amount of time a user stays engaged with the game is a</p>
<p>reasonable proxy for how much they’re enjoying it. After answering our four</p>
<p>questions once to calculate sessions lengths, we’ll then answer them a second</p>
<p>time to calculate average session lengths within fixed periods of time.</p>
<p>To make our example even more interesting, lets say that we are working</p>
<p>with two datasets, one for Mobile Scores and one for Console Scores. We</p>
<p>would like to perform identical score calculations via integer summation in</p>
<p>parallel over these two independant datasets. One pipeline is calculating</p>
<p>scores for users playing on mobile devices, whereas the other is for users</p>
<p>playing on home gaming consoles, perhaps due to different data collection</p>
<p>strategies employed for the different platforms. The important point is that</p>
<p>these two stages are performing the same operation but over different data,</p>
<p>and thus with very different output watermarks.</p>
<p>To begin, let’s take a look at Example 3-1 to see what the abbreviated code</p>
<p>for what the first section of this pipeline might be like.</p>
<p><em>Example 3-1. Calculating session lengths</em></p>
<p><code>PCollection&lt;Double&gt; mobileSessions = IO.read(new MobileInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p><code>PCollection&lt;Double&gt; consoleSessions = IO.read(new ConsoleInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p>Here, we read in each of our inputs independently, and whereas previously we</p>
<p>were keying our collections by team, in this example we key by user. After</p>
<p>that, for the first stage of each pipeline, we window into sessions and then call</p>
<p>a custom PTransform named CalculateWindowLength. This PTransform</p>
<p>simply groups by key (i.e., User) and then computes the per-user session</p>
<p>length by treating the size of the current window as the value for that window.</p>
<p>In this case, we’re fine with the default trigger (AtWatermark) and</p>
<p>accumulation mode (discardingFiredPanes) settings, but I’ve listed them</p>
<p>explicitly for completeness. The output for each pipeline for two particular</p>
<p>users might look something like Figure 3-4.</p>
<p><em>Figure 3-4. Per-user session lengths across two different input pipelines</em></p>
<p>Because we need to track data across multiple stages, we track everything</p>
<p>related to Mobile Scores in red, everything related to Console Scores in blue,</p>
<p>while the watermark and output for Average Session Lengths in Figure 3-5</p>
<p>are yellow.</p>
<p>We have answered the four questions of <em>what</em>, <em>where</em>, <em>when</em>, and <em>how</em> to</p>
<p>compute individual session lengths. Next we’ll answer them a second time to</p>
<p>transform those session lengths into global session-length averages within</p>
<p>fixed windows of time. This requires us to first flatten our two data sources</p>
<p>into one, and then re-window into fixed windows; we’ve already captured the</p>
<p>important essence of the session in the session-length value we computed, and</p>
<p>we now want to compute a global average of those sessions within consistent</p>
<p>windows of time over the course of the day. Example 3-2 shows the code for</p>
<p>this.</p>
<p><em>Example 3-2. Calculating session lengths</em></p>
<p><code>PCollection&lt;Double&gt; mobileSessions = IO.read(new MobileInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p><code>PCollection&lt;Double&gt; consoleSessions = IO.read(new ConsoleInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p><code>PCollection&lt;Float&gt; averageSessionLengths = PCollectionList</code></p>
<p><code>.of(mobileSessions).and(consoleSessions)</code></p>
<p><code>.apply(Flatten.pCollections())</code></p>
<p><code>.apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.apply(Mean.globally());</code></p>
<p>If we were to see this pipeline in action, it would look something like</p>
<p>Figure 3-5. As before, the two input pipelines are computing individual</p>
<p>session lengths for mobile and console players. Those session lengths then</p>
<p>feed into the second stage of the pipeline, where global session-length</p>
<p>averages are computed in fixed windows.</p>
<p><em>Figure 3-5. Average session lengths of mobile and console gaming sessions</em></p>
<p>Let’s walk through some of this example, given that there’s a lot going on.</p>
<p>The two important points here are:</p>
<ul>
<li>The <em>output watermark</em> for each of the Mobile Sessions and Console</li>
</ul>
<p>Sessions stages is at least as old as the corresponding input</p>
<p>watermark of each, and in reality a little bit older. This is because in</p>
<p>a real system computing answers takes time, and we don’t allow the</p>
<p>output watermark to advance until processing for a given input has</p>
<p>completed.</p>
<ul>
<li>The <em>input watermark</em> for the Average Session Lengths stage is the</li>
</ul>
<p>minimum of the output watermarks for the two stages directly</p>
<p>upstream.</p>
<p>The result is that the downstream input watermark is an alias for the minimum</p>
<p>composition of the upstream output watermarks. Note that this matches the</p>
<p>definitions for those two types of watermarks earlier in the chapter. Also</p>
<p>notice how watermarks further downstream are further in the past, capturing</p>
<p>the intuitive notion that upstream stages are going to be further ahead in time</p>
<p>than the stages that follow them.</p>
<p>One observation worth making here is just how cleanly we were able to ask</p>
<p>the questions again in Example 3-1 to substantially alter the results of the</p>
<p>pipeline. Whereas before we simply computed per-user session lengths, we</p>
<p>now compute two-minute global session-length averages. This provides a</p>
<p>much more insightful look into the overall behaviors of the users playing our</p>
<p>games and gives you a tiny glimpse of the difference between simple data</p>
<p>transformations and real data science.</p>
<p>Even better, now that we understand the basics of how this pipeline operates,</p>
<p>we can look more closely at one of the more subtle issues related to asking the</p>
<p>four questions over again: <em>output timestamps</em>.</p>
<h3><span id="watermark-propagation-and-output-timestamps"><strong>Watermark Propagation and Output Timestamps</strong></span><a href="#watermark-propagation-and-output-timestamps" class="header-anchor">#</a></h3><p>In Figure 3-5, I glossed over some of the details of output timestamps. But if</p>
<p>you look closely at the second stage in the diagram, you can see that each of</p>
<p>the outputs from the first stage was assigned a timestamp that matched the</p>
<p>end of its window. Although that’s a fairly natural choice for output</p>
<p>timestamps, it’s not the only valid choice. As you know from earlier in this</p>
<p>chapter, watermarks are never allowed to move backward. Given that</p>
<p>restriction, you can infer that the range of valid timestamps for a given</p>
<p>window begins with the timestamp of the earliest nonlate record in the</p>
<p>window (because only nonlate records are guaranteed to hold a watermark up)</p>
<p>and extends all the way to positive infinity. That’s quite a lot of options. In</p>
<p>practice, however, there tend to be only a few choices that make sense in most</p>
<p>circumstances:</p>
<ul>
<li>End of the window</li>
</ul>
<p>Using the end of the window is the only safe choice if you want the output</p>
<p>timestamp to be representative of the window bounds. As we’ll see in a</p>
<p>moment, it also allows the smoothest watermark progression out of all of</p>
<p>the options.</p>
<ul>
<li>Timestamp of first nonlate element</li>
</ul>
<p>Using the timestamp of the first nonlate element is a good choice when</p>
<p>you want to keep your watermarks as conservative as possible. The trade</p>
<p>off, however, is that watermark progress will likely be more hindered, as</p>
<p>we’ll also see shortly.</p>
<ul>
<li>Timestamp of a specific element</li>
</ul>
<p>For certain use cases, the timestamp of some other arbitrary (from the</p>
<p>system’s perspective) element is the right choice. Imagine a use case in</p>
<p>which you’re joining a stream of queries to a stream of clicks on results</p>
<p>for that query. After performing the join, some systems will find the</p>
<p>timestamp of the query to be more useful; others will prefer the timestamp</p>
<p>of the click. Any such timestamp is valid from a watermark correctness</p>
<p>perspective, as long as it corresponded to an element that did not arrive</p>
<p>late.</p>
<p>Having thought a bit about some alternate options for output timestamps, let’s</p>
<p>look at what effects the choice of output timestamp can have on the overall</p>
<p>pipeline. To make the changes as dramatic as possible, in Example 3-3 and</p>
<p>Figure 3-6, we’ll switch to using the earliest timestamp possible for the</p>
<p>window: the timestamp of the first nonlate element as the timestamp for the</p>
<p>window.</p>
<p><em>Example 3-3. Average session lengths pipeline, that output timestamps for</em></p>
<p><em>session windows set at earliest element</em></p>
<p><code>PCollection&lt;Double&gt; mobileSessions = IO.read(new MobileInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.withTimestampCombiner(EARLIEST)</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p><code>PCollection&lt;Double&gt; consoleSessions = IO.read(new ConsoleInputSource())</code></p>
<p><code>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.withTimestampCombiner(EARLIEST)</code></p>
<p><code>.discardingFiredPanes())</code></p>
<p><code>.apply(CalculateWindowLength());</code></p>
<p><code>PCollection&lt;Float&gt; averageSessionLengths = PCollectionList</code></p>
<p><code>.of(mobileSessions).and(consoleSessions)</code></p>
<p><code>.apply(Flatten.pCollections())</code></p>
<p><code>.apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))</code></p>
<p><code>.triggering(AtWatermark())</code></p>
<p><code>.apply(Mean.globally());</code></p>
<p><em>Figure 3-6. Average session lengths for sessions that are output at the timestamp of the earliest element</em></p>
<p>To help call out the effect of the output timestamp choice, look at the dashed</p>
<p>lines in the first stages showing what the output watermark for each stage is</p>
<p>being held to. The output watermark is delayed by our choice of timestamp,</p>
<p>as compared to Figures 3-7 and 3-8, in which the output timestamp was</p>
<p>chosen to be the end of the window. You can see from this diagram that the</p>
<p>input watermark of the second stage is thus subsequently also delayed.</p>
<p><em>Figure 3-7. Comparison of watermarks and results with different choice of window outout timestamps.</em></p>
<p><em>The watermarks in this figure correspond to output timestamps at the end of the session windows (i.e.,</em></p>
<p><em>Figure 3-5).</em></p>
<p><em>Figure 3-8. In this figure, the watermarks are at the beginning of the session windows (i.e., Figure 3-6).</em></p>
<p><em>We can see that the watermark line in this figure is more delayed, and the resulting average session</em></p>
<p><em>lengths are different.</em></p>
<p>As far as differences in this version compared to Figure 3-7, two are worth</p>
<p>noting:</p>
<ul>
<li>Watermark delay</li>
</ul>
<p>Compared to Figure 3-5, the watermark proceeds much more slowly in</p>
<p>Figure 3-6. This is because the output watermark for the first stage is held</p>
<p>back to the timestamp of the first element in every window until the input</p>
<p>for that window becomes complete. Only after a given window has been</p>
<p>materialized is the output watermark (and thus the downstream input</p>
<p>watermark) allowed to advance.</p>
<ul>
<li>Semantic differences</li>
</ul>
<p>Because the session timestamps are now assigned to match the earliest</p>
<p>nonlate element in the session, the individual sessions often end up in</p>
<p>different fixed window buckets when we then calculate the session-length</p>
<p>averages in the next stage. There’s nothing inherently right or wrong</p>
<p>about either of the two options we’ve seen so far; they’re just different.</p>
<p>But it’s important to understand that they <em>will</em> be different as well as have</p>
<p>an intuition for the way in which they’ll be different so that you can make</p>
<p>the correct choice for your specific use case when the time comes.</p>
<h3><span id="the-tricky-case-of-overlapping-windows"><strong>The Tricky Case of Overlapping Windows</strong></span><a href="#the-tricky-case-of-overlapping-windows" class="header-anchor">#</a></h3><p>One additional subtle but important issue regarding output timestamps is how</p>
<p>to handle sliding windows. The naive approach of setting the output</p>
<p>timestamp to the earliest element can very easily lead to delays downstream</p>
<p>due to watermarks being (correctly) held back. To see why, consider an</p>
<p>example pipeline with two stages, each using the same type of sliding</p>
<p>windows. Suppose that each element ends up in three successive windows. As</p>
<p>the input watermark advances, the desired semantics for sliding windows in</p>
<p>this case would be as follows:</p>
<ul>
<li>The first window completes in the first stage and is emitted</li>
</ul>
<p>downstream.</p>
<ul>
<li>The first window then completes in the second stage and can also be</li>
</ul>
<p>emitted downstream.</p>
<ul>
<li>Some time later, the second window completes in the first stage…</li>
</ul>
<p>and so on.</p>
<p>However, if output timestamps are chosen to be the timestamp of the first</p>
<p>nonlate element in the pane, what actually happens is the following:</p>
<ul>
<li>The first window completes in the first stage and is emitted</li>
</ul>
<p>downstream.</p>
<ul>
<li>The first window in the second stage remains unable to complete</li>
</ul>
<p>because its input watermark is being held up by the output</p>
<p>watermark of the second and third windows upstream. Those</p>
<p>watermarks are rightly being held back because the earliest element</p>
<p>timestamp is being used as the output timestamp for those windows.</p>
<ul>
<li>The second window completes in the first stage and is emitted</li>
</ul>
<p>downstream.</p>
<ul>
<li>The first and second windows in the second stage remain unable to</li>
</ul>
<p>complete, held up by the third window upstream.</p>
<ul>
<li>The third window completes in the first stage and is emitted</li>
</ul>
<p>downstream.</p>
<ul>
<li>The first, second, and third windows in the second stage are now all</li>
</ul>
<p>able to complete, finally emitting all three in one swoop.</p>
<p>Although the results of this windowing are correct, this leads to the results</p>
<p>being materialized in an unnecessarily delayed way. Because of this, Beam</p>
<p>has special logic for overlapping windows that ensures the output timestamp</p>
<p>for window <em>N</em>+1 is always greater than the end of window <em>N</em>.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="percentile-watermarks"><strong>Percentile Watermarks</strong></span><a href="#percentile-watermarks" class="header-anchor">#</a></h1><p>So far, we have concerned ourselves with watermarks as measured by the</p>
<p>minimum event time of active messages in a stage. Tracking the minimum</p>
<p>allows the system to know when all earlier timestamps have been accounted</p>
<p>for. On the other hand, we could consider the entire distribution of event</p>
<p>timestamps for active messages and make use of it to create finer-grained</p>
<p>triggering conditions.</p>
<p>Instead of considering the minimum point of the distribution, we could take</p>
<p>any percentile of the distribution and say that we are guaranteed to have</p>
<p>processed this percentage of all events with earlier timestamps.</p>
<p>What is the advantage of this scheme? If for the business logic “mostly”</p>
<p>correct is sufficient, percentile watermarks provide a mechanism by which the</p>
<p>watermark can advance more quickly and more smoothly than if we were</p>
<p>tracking the minimum event time by discarding outliers in the long tail of the</p>
<p>distribution from the watermark. Figure 3-9 shows a compact distribution of</p>
<p>event times where the 90th percentile watermark is close to the 100th</p>
<p>percentile. Figure 3-10 demonstrates a case where the outlier is further</p>
<p>behind, so the 90th percentile watermark is significantly ahead of the 100th</p>
<p>percentile. By discarding the outlier data from the watermark, the percentile</p>
<p>watermark can still keep track of the bulk of the distribution without being</p>
<p>delayed by the outliers.</p>
<p><em>Figure 3-9. Normal-looking watermark histogram</em></p>
<p><em>Figure 3-10. Watermark histogram with outliers</em></p>
<p>Figure 3-11 shows an example of percentile watermarks used to draw window</p>
<p>boundaries for two-minute fixed windows. We can draw early boundaries</p>
<p>based on the percentile of timestamps of arrived data as tracked by the</p>
<p>percentile watermark.</p>
<p><em>Figure 3-11. Effects of varying watermark percentiles. As the percentile increases, more events are</em></p>
<p><em>included in the window: however, the processing time delay to materialize the window also increases.</em></p>
<p>Figure 3-11 shows the 33rd percentile, 66th percentile, and 100th percentile</p>
<p>(full) watermark, tracking the respective timestamp percentiles in the data</p>
<p>distribution. As expected, these allow boundaries to be drawn earlier than</p>
<p>tracking the full 100th percentile watermark. Notice that the 33rd and 66th</p>
<p>percentile watermarks each allow earlier triggering of windows but with the</p>
<p>trade-off of marking more data as late. For example, for the first window,</p>
<p>[12:00, 12:02), a window closed based on the 33rd percentile watermark</p>
<p>would include only four events and materialize the result at 12:06 processing</p>
<p>time. If we use the 66th percentile watermark, the same event-time window</p>
<p>would include seven events, and materialize at 12:07 processing time. Using</p>
<p>the 100th percentile watermark includes all ten events and delays</p>
<p>materializing the results until 12:08 processing time. Thus, percentile</p>
<p>watermarks provide a way to tune the trade-off between latency of</p>
<p>materializing results and precision of the results.</p>
<h1><span id="processing-time-watermarks"><strong>Processing-Time Watermarks</strong></span><a href="#processing-time-watermarks" class="header-anchor">#</a></h1><p>Until now, we have been looking at watermarks as they relate to the data</p>
<p>flowing through our system. We have seen how looking at the watermark can</p>
<p>help us identify the overall delay between our oldest data and real time.</p>
<p>However, this is not enough to distinguish between old data and a delayed</p>
<p>system. In other words, by only examining the event-time watermark as we</p>
<p>have defined it up until now, we cannot distinguish between a system that is</p>
<p>processing data from an hour ago quickly and without delay, and a system</p>
<p>that is attempting to process real-time data and has been delayed for an hour</p>
<p>while doing so.</p>
<p>To make this distinction, we need something more: processing-time</p>
<p>watermarks. We have already seen that there are two time domains in a</p>
<p>streaming system: processing time and event time. Until now, we have</p>
<p>defined the watermark entirely in the event-time domain, as a function of</p>
<p>timestamps of the data flowing through the system. This is an event-time</p>
<p>watermark. We will now apply the same model to the processing-time domain</p>
<p>to define a processing-time watermark.</p>
<p>Our stream processing system is constantly performing operations such as</p>
<p>shuffling messages between stages, reading or writing messages to persistent</p>
<p>state, or triggering delayed aggregations based on watermark progress. All of</p>
<p>these operations are performed in response to previous operations done at the</p>
<p>current or upstream stage of the pipeline. Thus, just as data elements “flow”</p>
<p>through the system, a cascade of operations involved in processing these</p>
<p>elements also “flows” through the system.</p>
<p>We define the processing-time watermark in the exact same way as we have</p>
<p>defined the event-time watermark, except instead of using the event-time</p>
<p>timestamp of oldest work not yet completed, we use the processing-time</p>
<p>timestamp of the oldest operation not yet completed. An example of delay to</p>
<p>the processing-time watermark could be a stuck message delivery from one</p>
<p>stage to another, a stuck I&#x2F;O call to read state or external data, or an exception</p>
<p>while processing that prevents processing from completing.</p>
<p>The processing-time watermark, therefore, provides a notion of processing</p>
<p>delay separate from the data delay. To understand the value of this distinction,</p>
<p>consider the graph in Figure 3-12 where we look at the event-time watermark</p>
<p>delay.</p>
<p>We see that the data delay is monotonically increasing, but there is not</p>
<p>enough information to distinguish between the cases of a stuck system and</p>
<p>stuck data. Only by looking at the processing-time watermark, shown in</p>
<p>Figure 3-13, can we distinguish the cases.</p>
<p><em>Figure 3-12. Event-time watermark increasing. It is not possible to know from this information whether this is due to data buffering or system processing delay.</em></p>
<p><em>Figure 3-13. Processing-time watermark also increasing. This indicates that the system processing is</em></p>
<p><em>delayed.</em></p>
<p>In the first case (Figure 3-12), when we examine the processing-time</p>
<p>watermark delay we see that it too is increasing. This tells us that an operation</p>
<p>in our system is stuck, and the stuckness is also causing the data delay to fall</p>
<p>behind. Some real-world examples of situations in which this might occur are</p>
<p>when there is a network issue preventing message delivery between stages of</p>
<p>a pipeline or if a failure has occurred and is being retried. In general, a</p>
<p>growing processing-time watermark indicates a problem that is preventing</p>
<p>operations from completing that are necessary to the system’s function, and</p>
<p>often involves user or administrator intervention to resolve.</p>
<p>In this second case, as seen in Figure 3-14, the processing-time watermark</p>
<p>delay is small. This tells us that there are no stuck operations. The event-time</p>
<p>watermark delay is still increasing, which indicates that we have some</p>
<p>buffered state that we are waiting to drain. This is possible, for example, if we</p>
<p>are buffering some state while waiting for a window boundary to emit an</p>
<p>aggregation, and corresponds to a normal operation of the pipeline, as in</p>
<p>Figure 3-15.</p>
<p><em>Figure 3-14. Event-time watermark delay increasing, processing-time watermark stable. This is an</em></p>
<p><em>indication that data are buffered in the system and waiting to be processed, rather than an indication</em></p>
<p><em>that a system operation is preventing data processing from completing.</em></p>
<p><em>Figure 3-15. Watermark delay for fixed windows. The event-time watermark delay increases as</em></p>
<p><em>elements are buffered for each window, and decreases as each window’s aggregate is emitted via an</em></p>
<p><em>on-time trigger, whereas the processing-time watermark simply tracks system-level delays (which</em></p>
<p><em>remain relatively steady in a healthy pipeline).</em></p>
<p>Therefore, the processing-time watermark is a useful tool in distinguishing</p>
<p>system latency from data latency. In addition to visibility, we can use the</p>
<p>processing-time watermark at the system-implementation level for tasks such</p>
<p>as garbage collection of temporary state (Reuven talks more about an example</p>
<p>of this in Chapter 5).</p>
</details>



<details><summary>点击 原文</summary><h1><span id="case-studies"><strong>Case Studies</strong></span><a href="#case-studies" class="header-anchor">#</a></h1><p>Now that we’ve laid the groundwork for how watermarks ought to behave,</p>
<p>it’s time to take a look at some real systems to understand how different</p>
<p>mechanisms of the watermark are implemented. We hope that these shed</p>
<p>some light on the trade-offs that are possible between latency and correctness</p>
<p>as well as scalability and availability for watermarks in real-world systems.</p>
<h3><span id="case-study-watermarks-in-google-cloud-dataflow"><strong>Case Study: Watermarks in Google Cloud Dataflow</strong></span><a href="#case-study-watermarks-in-google-cloud-dataflow" class="header-anchor">#</a></h3><p>There are many possible approaches to implementing watermarks in a stream</p>
<p>processing system. Here, we present a quick survey of the implementation in</p>
<p>Google Cloud Dataflow, a fully managed service for executing Apache Beam</p>
<p>pipelines. Dataflow includes SDKs for defining data processing workflows,</p>
<p>and a Cloud Platform managed service to run those workflows on Google</p>
<p>Cloud Platform resources.</p>
<p>Dataflow stripes (shards) each of the data processing steps in its data</p>
<p>processing graph across multiple physical workers by splitting the available</p>
<p>keyspace of each worker into key ranges and assigning each range to a</p>
<p>worker. Whenever a GroupByKey operation with distinct keys is encountered,</p>
<p>data must be shuffled to corresponding keys.</p>
<p>Figure 3-16 depicts a logical representation of the processing graph with a</p>
<p>GroupByKey.</p>
<p><em>Figure 3-16. A GroupByKey step consumes data from another DoFn. This means that there is a data</em></p>
<p><em>shuffle between the keys of the first step and the keys of the second step.</em></p>
<p>Whereas the physical assignment of key ranges to workers might look</p>
<p>Figure 3-17.</p>
<p><em>Figure 3-17. Key ranges of both steps are assigned (striped) across the available workers.</em></p>
<p>In the watermark propagation section, we discussed that the watermark is</p>
<p>maintained for multiple subcomponents of each step. Dataflow keeps track of</p>
<p>the per-range watermarks of each of these components. Watermark</p>
<p>aggregation then involves computing the minimum of each watermark across</p>
<p>all ranges, ensuring that the following guarantees are met:</p>
<ul>
<li>All ranges must be reporting a watermark. If a watermark is not</li>
</ul>
<p>present for a range, we cannot advance the watermark, because a</p>
<p>range not reporting must be treated as unknown.</p>
<ul>
<li>Ensure that the watermark is monotonically increasing. Because late</li>
</ul>
<p>data is possible, we must not update the watermark if it would cause</p>
<p>the watermark to move backward.</p>
<p>Google Cloud Dataflow performs aggregation via a centralized aggregator</p>
<p>agent. We can shard this agent for efficiency. From a correctness standpoint,</p>
<p>the watermark aggregator serves as a “single source of truth” about the</p>
<p>watermark.</p>
<p>Ensuring correctness in distributed watermark aggregation poses certain</p>
<p>challenges. It is paramount that watermarks are not advanced prematurely</p>
<p>because advancing the watermark prematurely will turn on-time data into late</p>
<p>data. Specifically, as physical assignments are actuated to workers, the</p>
<p>workers maintain leases on the persistent state attached to the key ranges,</p>
<p>ensuring that only a single worker may mutate the persistent state for a key.</p>
<p>To guarantee watermark correctness, we must ensure that each watermark</p>
<p>update from a worker process is admitted into the aggregate only if the</p>
<p>worker process still maintains a lease on its persistent state; therefore, the</p>
<p>watermark update protocol must take state ownership lease validation into</p>
<p>account.</p>
</details>





<details><summary>点击 原文</summary><h3><span id="case-study-watermarks-in-apache-flink"><strong>Case Study: Watermarks in Apache Flink</strong></span><a href="#case-study-watermarks-in-apache-flink" class="header-anchor">#</a></h3><p>Apache Flink is an open source stream processing framework for distributed,</p>
<p>high-performing, always-available, and accurate data streaming applications.</p>
<p>It is possible to run Beam programs using a Flink runner. In doing so, Beam</p>
<p>relies on the implementation of stream processing concepts such as</p>
<p>watermarks within Flink. Unlike Google Cloud Dataflow, which implements</p>
<p>watermark aggregation via a centralized watermark aggregator agent, Flink</p>
<p>performs watermark tracking and aggregation in-band.</p>
<p>To understand how this works, let’s look at a Flink pipeline, as shown in</p>
<p>Figure 3-18.</p>
<p><em>Figure 3-18. A Flink pipeline with two sources and event-time watermarks propagating in-band</em></p>
<p>In this pipeline data is generated at two sources. These sources also both</p>
<p>generate watermark “checkpoints” that are sent synchronously in-band with</p>
<p>the data stream. This means that when a watermark checkpoint from source A</p>
<p>for timestamp “53” is emitted, it guarantees that no nonlate data messages</p>
<p>will be emitted from source A with timestamp behind “53”. The downstream</p>
<p>“keyBy” operators consume the input data and the watermark checkpoints. As</p>
<p>new watermark checkpoints are consumed, the downstream operators’ view</p>
<p>of the watermark is advanced, and a new watermark checkpoint for</p>
<p>downstream operators can be emitted.</p>
<p>This choice to send watermark checkpoints in-band with the data stream</p>
<p>differs from the Cloud Dataflow approach that relies on central aggregation</p>
<p>and leads to a few interesting trade-offs.</p>
<p>Following are some advantages of in-band watermarks:</p>
<ul>
<li>Reduced watermark propagation latency, and very low-latency watermarks</li>
</ul>
<p>Because it is not necessary to have watermark data traverse multiple hops</p>
<p>and await central aggregation, it is possible to achieve very low latency</p>
<p>more easily with the in-band approach.</p>
<ul>
<li>No single point of failure for watermark aggregation</li>
</ul>
<p>Unavailability in the central watermark aggregation agent will lead to a</p>
<p>delay in watermarks across the entire pipeline. With the in-band approach,</p>
<p>unavailability of part of the pipeline cannot cause watermark delay to the</p>
<p>entire pipeline.</p>
<ul>
<li>Inherent scalability</li>
</ul>
<p>Although Cloud Dataflow scales well in practice, more complexity is</p>
<p>needed to achieve scalability with a centralized watermark aggregation</p>
<p>service versus implicit scalability with in-band watermarks.</p>
<p>Here are some advantages of out-of-band watermark aggregation:</p>
<ul>
<li>Single source of “truth”</li>
</ul>
<p>For debuggability, monitoring, and other applications such as throttling</p>
<p>inputs based on pipeline progress, it is advantageous to have a service that</p>
<p>can vend the values of watermarks rather than having watermarks implicit</p>
<p>in the streams, with each component of the system having its own partial</p>
<p>view.</p>
<ul>
<li>Source watermark creation</li>
</ul>
<p>Some source watermarks require global information. For example,</p>
<p>sources might be temprarily idle, have low data rates, or require out-of</p>
<p>band information about the source or other system components to</p>
<p>generate the watermarks. This is easier to achieve in a central service. For</p>
<p>an example see the case study that follows on source watermarks for</p>
<p>Google Cloud Pub&#x2F;Sub.</p>
</details>





<details><summary>点击 原文</summary><h3><span id="case-study-source-watermarks-for-google-cloud-pubx2fsub"><strong>Case Study: Source Watermarks for Google Cloud Pub&#x2F;Sub</strong></span><a href="#case-study-source-watermarks-for-google-cloud-pubx2fsub" class="header-anchor">#</a></h3><p>Google Cloud Pub&#x2F;Sub is a fully managed real-time messaging service that</p>
<p>allows you to send and receive messages between independent applications.</p>
<p>Here, we discuss how to create a reasonable heuristic watermark for data sent</p>
<p>into a pipeline via Cloud Pub&#x2F;Sub.</p>
<p>First, we need to describe a little about how Pub&#x2F;Sub works. Messages are</p>
<p>published on Pub&#x2F;Sub <em>topics</em>. A particular topic can be subscribed to by any</p>
<p>number of Pub&#x2F;Sub <em>subscriptions</em>. The same messages are delivered on all</p>
<p>subscriptions subscribed to a given topic. The method of delivery is for</p>
<p>clients to <em>pull</em> messages off the subscription, and to ack the receipt of</p>
<p>particular messages via provided IDs. Clients do not get to choose which</p>
<p>messages are pulled, although Pub&#x2F;Sub does attempt to provide oldest</p>
<p>messages first, with no hard guarantees around this.</p>
<p>To build a heuristic, we make some assumptions about the source that is</p>
<p>sending data into Pub&#x2F;Sub. Specifically, we assume that the timestamps of the</p>
<p>original data are “well behaved”; in other words, we expect a bounded</p>
<p>amount of out-of-order timestamps on the source data, before it is sent to</p>
<p>Pub&#x2F;Sub. Any data that are sent with timestamps outside the allowed out-of</p>
<p>order bounds will be considered late data. In our current implementation, this</p>
<p>bound is at least 10 seconds, meaning reordering of timestamps up to 10</p>
<p>seconds before sending to Pub&#x2F;Sub will not create late data. We call this value</p>
<p>the <em>estimation band</em>. Another way to look at this is that when the pipepline is</p>
<p>perfectly caught up with the input, the watermark will be 10 seconds behind</p>
<p>real time to allow for possible reorderings from the source. If the pipeline is</p>
<p>backlogged, all of the backlog (not just the 10-second band) is used for</p>
<p>estimating the watermark.</p>
<p>What are the challenges we face with Pub&#x2F;Sub? Because Pub&#x2F;Sub does not</p>
<p>guarantee ordering, we must have some kind of additional metadata to know</p>
<p>enough about the backlog. Luckily, Pub&#x2F;Sub provides a measurement of</p>
<p>backlog in terms of the “oldest unacknowledged publish timestamp.” This is</p>
<p>not the same as the event timestamp of our message, because Pub&#x2F;Sub is</p>
<p>agnostic to the application-level metadata being sent through it; instead, this</p>
<p>is the timestamp of when the message was ingested by Pub&#x2F;Sub.</p>
<p>This measurement is not the same as an event-time watermark. It is in fact the</p>
<p>processing-time watermark for Pub&#x2F;Sub message delivery. The Pub&#x2F;Sub</p>
<p>publish timestamps are not equal to the event timestamps, and in the case that</p>
<p>historical (past) data are being sent, it might be arbitrarily far away. The</p>
<p>ordering on these timestamps might also be different because, as mentioned</p>
<p>earlier, we allow a limited amount of reordering.</p>
<p>However, we can use this as a measure of backlog to learn enough</p>
<p>information about the event timestamps present in the backlog so that we can</p>
<p>create a reasonable watermark as follows.</p>
<p>We create two subscriptions to the topic containing the input messages: a</p>
<p><em>base subscription</em> that the pipeline will actually use to read the data to be</p>
<p>processed, and a <em>tracking subscription</em>, which is used for metadata only, to</p>
<p>perform the watermark estimation.</p>
<p>Taking a look at our base subscription in Figure 3-19, we see that messages</p>
<p>might arrive out of order. We label each message with its Pub&#x2F;Sub publish</p>
<p>timestamp “pt” and its event-time timestamp “et.” Note that the two time</p>
<p>domains can be unrelated.</p>
<p><em>Figure 3-19. Processing-time and event-time timestamps of messages arriving on a Pub&#x2F;Sub</em></p>
<p><em>subscription</em></p>
<p>Some messages on the base subscription are unacknowledged forming a</p>
<p>backlog. This might be due to them not yet being delivered or they might</p>
<p>have been delivered but not yet processed. Remember also that pulls from this</p>
<p>subscription are distributed across multiple shards. Thus, it is not possible to</p>
<p>say just by looking at the base subscription what our watermark should be.</p>
<p>The tracking subscription, seen in Figure 3-20, is used to effectively inspect</p>
<p>the backlog of the base subscription and take the minimum of the event</p>
<p>timestamps in the backlog. By maintaining little or no backlog on the tracking</p>
<p>subscription, we can inspect the messages ahead of the base subsciption’s</p>
<p>oldest unacknowledged message.</p>
<p><em>Figure 3-20. An additional “tracking” subscription receiving the same messages as the “base”</em></p>
<p><em>subscription</em></p>
<p>We stay caught up on the tracking subscription by ensuring that pulling from</p>
<p>this subscription is computationally inexpensive. Conversely, if we fall</p>
<p>sufficiently behind on the tracking subscription, we will stop advancing the</p>
<p>watermark. To do so, we ensure that at least one of the following conditions is</p>
<p>met:</p>
<ul>
<li>The tracking subscription is sufficiently ahead of the base</li>
</ul>
<p>subscription. Sufficiently ahead means that the tracking subscription</p>
<p>is ahead by at least the estimation band. This ensures that any</p>
<p>bounded reorder within the estimation band is taken into account.</p>
<ul>
<li>The tracking subscription is sufficiently close to real time. In other</li>
</ul>
<p>words, there is no backlog on the tracking subscription.</p>
<p>We acknowledge the messages on the tracking subscription as soon as</p>
<p>possible, after we have durably saved metadata about the publish and event</p>
<p>timestamps of the messages. We store this metadata in a sparse histogram</p>
<p>format to minimize the amount of space used and the size of the durable</p>
<p>writes.</p>
<p>Finally, we ensure that we have enough data to make a reasonable watermark</p>
<p>estimate. We take a band of event timestamps we’ve read from our tracking</p>
<p>subscription with publish timestamps newer than the oldest unacknowledged</p>
<p>of the base subscription, or the width of the estimation band. This ensures that</p>
<p>we consider all event timestamps in the backlog, or if the backlog is small, the</p>
<p>most recent estimation band, to make a watermark estimate.</p>
<p>Finally, the watermark value is computed to be the minimum event time in</p>
<p>the band.</p>
<p>This method is correct in the sense that all timestamps within the reordering</p>
<p>limit of 10 seconds at the input will be accounted for by the watermark and</p>
<p>not appear as late data. However, it produces possibly an overly conservative</p>
<p>watermark, one that advances “too slowly” in the sense described in</p>
<p>Chapter 2. Because we consider all messages ahead of the base subscription’s</p>
<p>oldest unacknowledged message on the tracking subscription, we can include</p>
<p>event timestamps in the watermark estimate for messages that have already</p>
<p>been acknowledged.</p>
<p>Additionally, there are a few heuristics to ensure progress. This method works</p>
<p>well in the case of dense, frequently arriving data. In the case of sparse or</p>
<p>infrequent data, there might not be enough recent messages to build a</p>
<p>reasonable estimate. In the case that we have not seen data on the subscription</p>
<p>in more than two minutes (and there’s no backlog), we advance the</p>
<p>watermark to near real time. This ensures that the watermark and the pipeline</p>
<p>continue to make progress even if no more messages are forthcoming.</p>
<p>All of the above ensures that as long as source data-event timestamp</p>
<p>reordering is within the estimation band, there will be no additional late data.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>At this point, we have explored how we can use the event times of messages</p>
<p>to give a robust definition of progress in a stream processing system. We saw</p>
<p>how this notion of progress can subsequently help us answer the question of</p>
<p><em>where</em> in event time processing is taking place and <em>when</em> in processing time</p>
<p>results are materialized. Specifically, we looked at how watermarks are</p>
<p>created at the sources, the points of data ingestion into a pipeline, and then</p>
<p>propagated throughout the pipeline to preserve the essential guarantees that</p>
<p>allow the questions of <em>where</em> and <em>when</em> to be answered. We also looked at the</p>
<p>implications of changing the output window timestamps on watermarks.</p>
<p>Finally, we explored some real-world system considerations when building</p>
<p>watermarks at scale.</p>
<p>Now that we have a firm footing in how watermarks work under the covers,</p>
<p>we can take a dive into what they can do for us as we use windowing and</p>
<p>triggering to answer more complex queries in Chapter 4.</p>
</details>





<details><summary>点击 原文</summary><ol>
<li>Note the additional mention of monotonicity; we have not yet discussed</li>
</ol>
<p>how to achieve this. Indeed the discussion thus far makes no mention of</p>
<p>monotonicity. If we considered exclusively the oldest in-flight event time, the</p>
<p>watermark would not always be monotonic, as we have made no assumptions</p>
<p>about our input. We return to this discussion later on.</p>
<ol start="2">
<li>To be precise, it’s not so much that the number of logs need be static as it is</li>
</ol>
<p>that the number of logs at any given time be known a priori by the system. A</p>
<p>more sophisticated input source composed of a dynamically chosen number</p>
<p>of inputs logs, such as Pravega, could just as well be used for constructing a</p>
<p>perfect watermark. It’s only when the number of logs that exist in the</p>
<p>dynamic set at any given time is unknown (as in the example in the next</p>
<p>section) that one must fall back on a heuristic watermark.</p>
<ol start="3">
<li>Note that by saying “flow through the system,” I don’t necessarily imply</li>
</ol>
<p>they flow along the same path as normal data. They might (as in Apache</p>
<p>Flink), but they might also be transmitted out-of-band (as in MillWheel&#x2F;Cloud</p>
<p>Dataflow).</p>
<ol start="4">
<li>The <em>start</em> of the window is not a safe choice from a watermark correctness</li>
</ol>
<p>perspective because the first element in the window often comes <em>after</em> the</p>
<p>beginning of the window itself, which means that the watermark is not</p>
<p>guaranteed to have been held back as far as the start of the window.</p>
<ol start="5">
<li>The percentile watermark triggering scheme described here is not currently</li>
</ol>
<p>implemented by Beam; however, other systems such as MillWheel implement</p>
<p>this.</p>
<ol start="6">
<li>For more information on Flink watermarks, see the Flink documentation on</li>
</ol>
<p>the subject.</p>
</details>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-第三章：水位线[完整]</title>
    <url>/www6vHomeHexo/2000/03/15/streamingSystemChapter3/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E6%BA%90%E6%B0%B4%E4%BD%8D%E7%BA%BF%E7%9A%84%E5%88%9B%E5%BB%BA">源水位线的创建</a><ul>
<li><a href="#%E5%AE%8C%E7%BE%8E%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%88%9B%E5%BB%BA">完美水位线创建</a></li>
<li><a href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%88%9B%E5%BB%BA">启发式水位线创建</a></li>
</ul>
</li>
<li><a href="#%E6%B0%B4%E4%BD%8D%E7%BA%BF%E4%BC%A0%E6%92%AD">水位线传播</a><ul>
<li><a href="#%E4%BA%86%E8%A7%A3%E6%B0%B4%E4%BD%8D%E7%BA%BF%E4%BC%A0%E6%92%AD">了解水位线传播</a></li>
<li><a href="#%E6%B0%B4%E4%BD%8D%E7%BA%BF%E4%BC%A0%E6%92%AD%E5%92%8C%E8%BE%93%E5%87%BA%E6%97%B6%E9%97%B4%E6%88%B3">水位线传播和输出时间戳</a></li>
<li><a href="#%E9%87%8D%E5%8F%A0%E7%AA%97%E5%8F%A3%E7%9A%84%E6%A3%98%E6%89%8B%E6%83%85%E5%86%B5">重叠窗口的棘手情况</a></li>
</ul>
</li>
<li><a href="#%E7%99%BE%E5%88%86%E4%BD%8D%E6%95%B0%E6%B0%B4%E4%BD%8D%E7%BA%BF">百分位数水位线</a></li>
<li><a href="#%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E6%B0%B4%E4%BD%8D%E7%BA%BF">处理时间水位线</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6">案例研究</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6google-cloud-dataflow%E4%B8%AD%E7%9A%84%E6%B0%B4%E4%BD%8D%E7%BA%BF">案例研究：Google Cloud Dataflow中的水位线</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6apache-flink%E4%B8%AD%E7%9A%84%E6%B0%B4%E4%BD%8D%E7%BA%BF">案例研究：Apache Flink中的水位线</a></li>
<li><a href="#%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6google-cloud-pubsub-%E7%9A%84%E6%BA%90%E6%B0%B4%E4%BD%8D%E7%BA%BF">案例研究：Google Cloud Pub&#x2F;Sub 的源水位线</a></li>
</ul>
</li>
<li><a href="#%E6%91%98%E8%A6%81">摘要</a></li>
</ul>
<!-- tocstop -->

</div>

<p>到目前为止，我们已经从流处理的角度来看待管道作者或数据科学家。第2章将水位线引入作为回答事件时间处理正在进行的<em><strong>位置</strong></em>和处理时间结果<em><strong>何时</strong></em>实现的基本问题的一部分。在本章中，我们从流处理系统的基本机制的角度来看待相同的问题。观察这些机制将有助于我们激发、理解和应用水位线的概念。我们讨论了水位线如何在数据输入点创建、如何通过数据处理管道传播以及如何影响输出时间戳。我们还演示了水位线如何保留必要的保证，以回答事件时间数据在<em><strong>何处</strong></em>处理以及<em><strong>何时</strong></em>它被实现，同时处理无界数据。</p>
<h1><span id="定义">定义</span><a href="#定义" class="header-anchor">#</a></h1><p>考虑任何连续摄取数据并输出结果的管道。我们希望解决何时可以安全地关闭事件时间窗口的一般问题，即窗口不再期望有任何数据。为了解决这个问题，我们希望相对于无限输入特征化管道正在做的进展。</p>
<p>解决事件时间窗口问题的一种天真的方法是只基于当前处理时间来设置事件时间窗口。如第一章所述，我们很快就会遇到麻烦，因为数据处理和传输不是瞬间完成的，因此处理和事件时间几乎永远不会相等。我们的管道中的任何故障或峰值可能会导致我们错误地将消息分配到窗口中。最终，这种策略失败了，因为我们没有可靠的方法来保证这样的窗口。</p>
<p>另一种直观但最终不正确的方法是考虑管道处理的消息速率。虽然这是一个有趣的指标，但速率可能会因输入变化、预期结果的变化、可用于处理的资源等因素而任意变化。更重要的是，速率无法帮助回答完整性的根本问题。具体而言，速率无法告诉我们何时看到了特定时间间隔的所有消息。在现实世界的系统中，会有消息无法通过系统进行进展的情况。这可能是暂时性错误（例如崩溃、网络故障、机器停机）的结果，也可能是需要更改应用逻辑或其他手动干预的应用级故障的结果。当然，如果发生大量故障，则处理速率指标可能是检测到这种情况的良好代理。但是，速率指标永远无法告诉我们单个消息未能通过管道进行进展。然而，即使单个这样的消息，也可能会任意影响输出结果的正确性。</p>
<p>我们需要更可靠的进展度量。为了到达那里，我们对我们的流数据做出一个基本假设：每个消息都有一个关联的逻辑事件时间戳。在不断到达的无界数据的上下文中，这种假设是合理的，因为这意味着输入数据的持续生成。在大多数情况下，我们可以将原始事件发生的时间作为其逻辑事件时间戳。由于所有输入消息都包含事件时间戳，因此我们可以检查任何管道中这些时间戳的分布。这样的管道可能被分布在许多代理上并以并行方式处理，并且没有保证单个分片之间的顺序。因此，此管道中正在进行的活动消息的事件时间戳集将形成分布，如图3-1所示。</p>
<p>消息被管道摄取、处理，最终被标记为已完成。每个消息都是“运行中”的，表示已经接收但尚未完成，还是“已完成”的，表示不需要在此消息上执行更多处理。如果我们按事件时间检查消息的分布，它将看起来像图3-1。随着时间的推移，“运行中”分布的右侧将添加更多的消息，“运行中”部分的消息将被完成并移动到“已完成”分布。</p>
<div id="dplayer10" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer10"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0301.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><em>图3-1.流水线中的运行中和完成的消息事件时间分布。新消息作为输入到达并保持“运行中”，直到为它们完成处理为止。在任何给定时刻，“运行中”分布的最左侧边缘对应于最旧的未处理元素。</em></p>
<p>在此分布中有一个关键点，位于“运行中”分布的最左边缘，对应于我们管道中任何未处理消息的最旧事件时间戳。我们使用此值来定义水位线：</p>
<p><strong>水位线是最旧未完成的工作的单调递增时间戳。</strong></p>
<p>此定义提供了两个基本属性，使其有用：</p>
<ul>
<li><p>完整性<br>如果水位线已超过某个时间戳<em>T</em>，则由其单调属性保证，不会再发生处理，因此我们可以正确地发出任何<em>T</em>时间以及<em>T</em>之前的时段的聚合结果。换句话说，水位线使我们知道何时正确地关闭窗口。</p>
</li>
<li><p>可见性<br>如果消息因任何原因在管道中卡住，则水位线无法前进。此外，我们将能够通过检查阻止水位线前进的消息来找到问题的来源。</p>
</li>
</ul>
<h1><span id="源水位线的创建">源水位线的创建</span><a href="#源水位线的创建" class="header-anchor">#</a></h1><p>这些水位线从哪里来？为了为数据源建立水位线，我们必须为从该源进入管道的每个消息分配逻辑事件时间戳。正如第2章所述，所有水位线创建都属于以下两个广泛类别之一：<em>完美</em>或<em>启发式</em>。为了提醒我们完美和启发式水位线之间的区别，让我们看一下第2章中的窗口求和示例的图3-2。</p>
<div id="dplayer11" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer11"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0302.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图3-2。完美（左）和启发式（右）水位线的窗口求和</em></p>
<p>请注意，区别的特点在于完美的水位线确保水位线考虑到<em>所有</em>数据，而启发式水位线则允许一些后到数据元素。</p>
<p>水位线创建为完美或启发式后，水位线将在整个管道中保持不变。至于是什么使水位线创建完美还是启发式，这在很大程度上取决于正在使用的源的性质。为了看到原因，让我们看一下每种类型的水位线创建的一些示例。</p>
<h3><span id="完美水位线创建">完美水位线创建</span><a href="#完美水位线创建" class="header-anchor">#</a></h3><p>完美的水位线创建分配时间戳给进入的消息，以使得产生的水位线是<em>严格保证</em>，即从此源再也看不到事件时间小于水位线的数据了。使用完美水位线创建的管道永远不必处理后到数据；也就是说，在水位线超过新到达消息的事件时间之后到达的数据。然而，完美的水位线创建需要对输入有完美的了解，因此对于许多真实世界的分布式输入源来说是不切实际的。以下是可以创建完美水位线的用例的一些示例：</p>
<ul>
<li><p>进入时间戳</p>
<p>将进入时间分配为进入系统的数据的事件时间的源可以创建完美的水位线。在这种情况下，源水位线仅跟踪流水线观察到的当前处理时间。这本质上是近年来支持窗口的几乎所有流处理系统使用的方法。</p>
<p>因为事件时间是从一个单一的、单调递增的源（实际处理时间）分配的，所以系统对于数据流中接下来的哪些时间戳将到来具有完美的了解。因此，事件时间进度和窗口语义变得容易推理。当然，缺点是水位线与数据本身的事件时间没有关联；这些事件时间被有效地丢弃，水位线只是跟踪数据相对于其到达系统的进度。</p>
</li>
<li><p>时间有序日志的静态集</p>
<p>时间有序日志的静态大小输入源（例如，具有静态分区集的Apache Kafka主题，其中源的每个分区包含单调递增的事件时间）将是一个相对简单的源，可以在其上创建完美的水位线。为此，源将简单地跟踪未处理数据的已知和静态源分区集的最小事件时间（即，每个分区中最近读取的记录的事件时间的最小值）。</p>
<p>类似于前面提到的进入时间戳，由于静态分区集中的事件时间已知单调递增，因此系统对于接下来哪些时间戳将到来具有完美的了解。这实际上是一种有界的无序处理形式；已知分区集中的无序程度由这些分区中观察到的最小事件时间所限制。</p>
<p>通常，您只能在分区内保证单调递增的时间戳，如果在写入数据时为这些分区分配时间戳，例如通过Web前端直接记录事件到Kafka中。尽管仍然是有限的用例，但这绝对比到达数据处理系统时的进入时间戳有用得多，因为水位线跟踪底层数据的有意义的事件时间。</p>
</li>
</ul>
<h3><span id="启发式水位线创建">启发式水位线创建</span><a href="#启发式水位线创建" class="header-anchor">#</a></h3><p>另一方面，启发式水位线创建创建的水位线仅是估计，即事件时间小于水位线的数据将不会再次出现。使用启发式水位线创建的管道可能需要处理一些<em>后到数据</em>。后到数据是任何在水位线超过此数据的事件时间之后到达的数据。后到数据仅在使用启发式水位线创建时可能出现。如果启发式是一个合理好的启发式，那么后到数据的数量可能非常小，水位线仍然有用作为完成估计。如果系统要支持需要正确性的用例（例如，像计费这样的东西），则仍然需要提供一种处理后到数据的方法。</p>
<p>对于许多真实世界的分布式输入源来说，构建完美的水位线在计算或操作上是不切实际的，但是利用输入数据源的结构特征仍然可以构建高度准确的启发式水位线。以下是两个示例，其中可以使用启发式水位线（具有不同质量）：</p>
<ul>
<li><p>动态时间有序日志集</p>
<p>考虑一组动态结构化日志文件（每个单独的文件包含相对于同一文件中的其他记录单调递增的事件时间，但事件时间之间没有固定的关系），其中预期的完整日志文件集（即，在Kafka术语中的分区）在运行时未知。这样的输入通常在由多个独立团队构建和管理的全球规模服务中发现。在这种情况下，创建输入的完美水位线是棘手的，但创建准确的启发式水位线是完全可能的。</p>
<p>通过跟踪现有日志文件中未处理数据的最小事件时间、监视增长率以及利用外部信息（如网络拓扑和带宽可用性），即使缺乏有关所有输入的完美知识，也可以创建一个非常准确的水位线。这种类型的输入源是在Google发现的最常见的无界数据集之一，因此我们在这些情况下有着丰富的创建和分析水位线质量的经验，并看到它们在许多用例中被很好地使用。</p>
</li>
<li><p>Google Cloud Pub&#x2F;Sub</p>
<p>Cloud Pub&#x2F;Sub是一个有趣的用例。Pub&#x2F;Sub目前不保证按顺序传递；即使单个发布者按顺序发布两个消息，它们也有可能（通常很小）按顺序传递（这是由于底层架构的动态性，允许在零用户干预下扩展到非常高的吞吐量）。因此，无法保证Cloud Pub&#x2F;Sub的完美水位线。然而，Cloud Dataflow团队通过利用有关Cloud Pub&#x2F;Sub中数据的可用信息构建了一个相当准确的启发式水位线。本章后面将详细讨论此启发式的实现作为案例研究。</p>
</li>
</ul>
<p>考虑一个用户玩手机游戏的示例，他们的分数被发送到我们的管道进行处理：通常可以假设对于任何使用移动设备作为输入的源，提供完美的水位线基本上是不可能的。由于设备长时间离线的问题，没有办法为这种数据源提供任何合理的完整性估计。但是，您可以想象构建一个准确地跟踪当前在线设备的输入完整性的水位线，类似于上面描述的Google Pub&#x2F;Sub水位线。从提供低延迟结果的角度来看，处于活动状态的用户可能是最相关的用户子集，因此这通常不是您最初想到的缺点。</p>
<p>总的来说，启发式水位线的创建越了解源，启发式就越好，就越少见到后到数据项。鉴于源类型、事件分布和使用模式会有很大的变化，因此没有一种万能的解决方案。但是无论哪种情况（完美还是启发式），在输入源处创建水位线后，系统可以完美地传播水位线。这意味着完美水位线将在下游保持完美，启发式水位线将严格保持与创建时一样的启发式。这是水位线方法的好处：您可以将管道中的完整性跟踪复杂性完全减少到在源处创建水位线的问题上。</p>
<h1><span id="水位线传播">水位线传播</span><a href="#水位线传播" class="header-anchor">#</a></h1><p>到目前为止，我们仅考虑了单个操作或阶段内输入数据的水位线。然而，大多数实际的流水线都由多个阶段组成。了解水位线在独立阶段之间如何传播对于理解它们如何影响整个流水线以及其结果的观察延迟非常重要。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">**流水线阶段**</span><br><span class="line"></span><br><span class="line">每次通过某个新的维度将数据分组到流水线中时，通常需要不同的阶段。例如，如果您有一个流水线，它消耗原始数据，计算一些按用户统计的数据，然后使用这些按用户统计的数据计算一些按团队统计的数据，那么您很可能会得到一个三阶段的流水线：</span><br><span class="line"></span><br><span class="line">- 消耗原始数据的阶段</span><br><span class="line">- 将数据按用户分组并计算每个用户的统计数据的阶段</span><br><span class="line">- 将数据按团队分组并计算每个团队的统计数据的阶段</span><br><span class="line"></span><br><span class="line">我们将在第6章中更多地了解有关数据分组对流水线形状的影响。</span><br></pre></td></tr></table></figure>



<p>水位线是在输入源处创建的，就像前面讨论的那样。然后在数据通过系统时，它们在概念上通过系统流动。您可以在不同粒度级别上跟踪水位线。对于由多个独立阶段组成的流水线，每个阶段可能都跟踪其自己的水位线，其值是其前面的所有输入和阶段的函数。因此，流水线后面的阶段将具有更过去的水位线（因为它们看到的整体输入要少）。</p>
<p>我们可以在流水线中任何单个操作或阶段的边界上定义水位线。这不仅有助于了解流水线中每个阶段的相对进度，而且对于独立地且尽快为每个单独的阶段分派及时结果非常有用。我们为阶段边界处的水位线给出以下定义：</p>
<ul>
<li>输入水位线（input watermark）：捕获该阶段上游的所有进度（即该阶段的输入数据完整程度）。对于源，输入水位线是创建输入数据水位线的源特定函数。对于非源阶段，输入水位线定义为所有上游源和阶段的所有分片&#x2F;分区&#x2F;实例的输出水位线的最小值。</li>
<li>输出水位线（output watermark）：捕获阶段本身的进度，本质上定义为阶段的输入水位线和所有非延迟数据激活消息的事件时间的最小值。 “活动”确切包含的内容在很大程度上取决于给定阶段实际执行的操作以及流处理系统的实现。它通常包括缓冲以进行聚合但尚未向下游实现的数据，正在等待输出数据传输到下游阶段等。</li>
</ul>
<p>定义特定阶段的输入和输出水位线的一个好处是我们可以使用它们计算阶段引入的事件时间延迟量。将阶段的输出水位线值减去其输入水位线值可得出阶段引入的事件时间延迟或滞后。此滞后是每个阶段输出相对于实际时间的延迟程度的概念。例如，执行10秒窗口聚合的阶段将具有10秒或更多的滞后，这意味着阶段的输出至少要比输入和实际时间延迟那么多。输入和输出水位线的定义提供了整个流水线中水位线的递归关系。流水线中的每个后续阶段都会根据阶段的事件时间滞后必要地延迟水位线。</p>
<p>每个阶段内的处理也不是单一的。我们可以将一个阶段内的处理划分为具有多个概念组件的流，每个组件都有助于生成输出水位线。如前所述，这些组件的确切性质取决于阶段执行的操作和系统的实现。从概念上讲，每个这样的组件都充当缓冲区，其中活动消息可以驻留，直到某个操作完成。例如，数据到达时，它会被缓冲以进行处理。然后，处理可能会将数据写入状态以进行延迟聚合。当触发延迟聚合时，它可能会将结果写入等待下游阶段消耗的输出缓冲区，如图3-3所示。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0303.png" class>

<p><em>图3-3。流系统阶段的示例系统组件，包含正在进行的数据的缓冲区。每个都将有关联的水位线跟踪，阶段的整体输出水位线将是所有此类缓冲区的水位线的最小值。</em></p>
<p>我们可以使用自己的水位线跟踪每个这样的缓冲区。每个阶段缓冲区中的水位线的最小值形成该阶段的输出水位线。因此，输出水位线可以是以下内容的最小值：</p>
<ul>
<li>每个发送阶段的<em>每个源</em>水位线。</li>
<li><em>每个外部输入</em>水位线——用于流水线外部的源</li>
<li><em>每种类型的状态</em>组件水位线——可以写入每种状态</li>
<li><em>每个接收阶段</em>的水位线</li>
</ul>
<p>在这个粒度级别上提供水位线也可以更好地了解系统的行为。水位线跟踪系统中各个缓冲区中的消息位置，从而更容易诊断卡住的问题。</p>
<h3><span id="了解水位线传播">了解水位线传播</span><a href="#了解水位线传播" class="header-anchor">#</a></h3><p>为了更好地了解输入和输出水位线之间的关系以及它们如何影响水位线传播，让我们看一个例子。我们考虑游戏得分，但不是计算团队得分的总和，而是试图衡量用户参与水平。我们将首先通过计算每个用户会话的长度来实现这一点，假设用户参与游戏的时间是他们享受游戏程度的合理代理。在回答我们的四个问题以计算会话长度一次后，我们将再次回答这些问题，以在固定时间段内计算平均会话长度。</p>
<p>为了使我们的示例更加有趣，假设我们正在使用两个数据集，一个用于移动得分，另一个用于控制台得分。我们希望通过整数求和在这两个独立数据集上并行执行相同的得分计算。一个管道正在计算在移动设备上玩游戏的用户的得分，而另一个管道则是针对在家庭游戏机上玩游戏的用户，可能是由于针对不同平台采用了不同的数据收集策略。重要的是，这两个阶段执行相同的操作，但是在不同的数据上执行，因此输出的水位线非常不同。</p>
<p>首先，让我们看一下示例3-1，以了解该管道的第一部分的缩写代码是什么样子的。</p>
<p><em>示例3-1。计算会话长度</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PCollection&lt;Double&gt; mobileSessions = IO.read(<span class="keyword">new</span> <span class="title class_">MobileInputSource</span>())</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(<span class="number">1</span>)))            .triggering(AtWatermark())</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br><span class="line"></span><br><span class="line">PCollection&lt;Double&gt; consoleSessions = IO.read(<span class="keyword">new</span> <span class="title class_">ConsoleInputSource</span>())</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(<span class="number">1</span>)))            .triggering(AtWatermark())</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br></pre></td></tr></table></figure>

<p>在这里，我们独立读取每个输入，而不是之前按团队键入我们的集合，在这个示例中，我们按用户键入。之后，对于每个管道的第一阶段，我们窗口进入会话，然后调用名为CalculateWindowLength的自定义PTransform。这个PTransform仅通过键（即用户）进行分组，然后通过将当前窗口的大小视为该窗口的值来计算每个用户会话长度。在这种情况下，我们对默认触发器（AtWatermark）和累积模式（discardingFiredPanes）设置感到满意，但是为了完整起见，我已经列出了它们。两个特定用户的每个管道的输出可能看起来像图3-4。</p>
<div id="dplayer12" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer12"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0304.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script>
<p><em>图3-4。两个不同输入管道中的每个用户会话长度</em></p>
<p>因为我们需要跨多个阶段跟踪数据，所以我们将与Mobile Scores相关的一切都记录为红色，与Console Scores相关的一切都记录为蓝色，而Figure 3-5中的平均会话长度的水位线和输出为黄色。</p>
<p>我们已经回答了计算单个会话长度的“什么”，“何地”，“何时”和“如何”这四个问题。接下来，我们将再次回答这些问题，将这些会话长度转换为固定时间窗口内的全局会话长度平均值。这需要我们首先将两个数据源展平为一个，然后重新窗口进入固定窗口；我们已经捕捉到会话中重要的本质，在我们计算的会话长度值中，我们现在想在一天中的一致时间窗口内计算这些会话的全局平均值。示例3-2显示了此代码。</p>
<p><em>示例3-2。计算会话长度</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;Double&gt; mobileSessions = IO.read(new MobileInputSource())</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))            .triggering(AtWatermark())</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br><span class="line"></span><br><span class="line">PCollection&lt;Double&gt; consoleSessions = IO.read(new ConsoleInputSource())</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))            .triggering(AtWatermark())</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br><span class="line"></span><br><span class="line">PCollection&lt;Float&gt; averageSessionLengths = PCollectionList</span><br><span class="line">  .of(mobileSessions).and(consoleSessions)</span><br><span class="line">  .apply(Flatten.pCollections())</span><br><span class="line">  .apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))</span><br><span class="line">               .triggering(AtWatermark())</span><br><span class="line">  .apply(Mean.globally());</span><br></pre></td></tr></table></figure>

<p>如果我们看到这个管道在运行，它看起来会像图3-5。与以前一样，两个输入管道正在计算移动和控制台玩家的单个会话长度。然后，这些会话长度进入管道的第二个阶段，在其中计算了固定窗口内的全局会话长度平均值。</p>
<div id="dplayer13" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer13"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0305.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图3-5。移动和控制台游戏会话的平均会话长度</em></p>
<p>鉴于这里发生了很多事情，让我们走一遍这个例子。这里的两个重要点是：</p>
<ul>
<li>每个Mobile Sessions和Console Sessions阶段的<em>输出水位线</em>至少与每个对应的输入水位线一样旧，并且实际上要旧一些。这是因为在真实系统中，计算答案需要时间，我们不允许输出水位线在给定输入的处理完成之前向前推进。</li>
<li>平均会话长度阶段的<em>输入水位线</em>是两个直接上游阶段的输出水位线的最小值。</li>
</ul>
<p>结果是下游输入水位线是上游输出水位线的最小组合的别名。请注意，这与本章前面对这两种类型的水位线的定义相匹配。还要注意，下游的水位线进一步落后，捕捉到上游阶段将比后续阶段超前一些时间的直观概念。</p>
<p>这里值得注意的一个观察是，我们能够在示例3-1中再次询问这些问题，以从根本上改变管道的结果。以前，我们只是计算单个用户会话长度，现在我们计算了两分钟的全局会话长度平均值。这提供了对我们的游戏玩家的整体行为更深入的了解，让您微小地了解了简单数据转换和真正的数据科学之间的差异。</p>
<p>更好的是，既然我们了解了这个管道运作的基础知识，我们现在可以更仔细地查看与再次询问这四个问题相关的更微妙的问题：<em>输出时间戳</em>。</p>
<h3><span id="水位线传播和输出时间戳">水位线传播和输出时间戳</span><a href="#水位线传播和输出时间戳" class="header-anchor">#</a></h3><p>在图3-5中，我忽略了一些输出时间戳的细节。但是，如果您仔细观察图中的第二个阶段，您会发现从第一个阶段输出的每个数据都被赋予了一个时间戳，该时间戳与其窗口的结束时间相匹配。虽然这是一个相当自然的输出时间戳选择，但它并不是唯一有效的选择。正如您在本章早期所知道的，水位线永远不允许后移。在这种限制下，您可以推断出对于给定窗口的有效时间戳范围始于窗口中最早的非延迟记录的时间戳（因为只有非延迟记录保证保持水位线），并延伸到正无穷。这是相当多的选择。然而，在实践中，通常只有几个选择在大多数情况下是有意义的：</p>
<ul>
<li><p>窗口结束时间</p>
<p>如果您希望输出时间戳代表窗口边界，则使用窗口结束时间是唯一安全的选择。正如我们很快就会看到的那样，这也允许水位线的平稳进展，比其他选项更加平稳。</p>
</li>
<li><p>第一个非延迟元素的时间戳</p>
<p>在您希望尽可能保守地保持水位线时，使用第一个非延迟元素的时间戳是一个不错的选择。然而，这种权衡的代价是水位线的进展可能会更加受到阻碍，我们很快也会看到。</p>
</li>
<li><p>特定元素的时间戳</p>
<p>对于某些用例，某些其他任意时间戳（从系统的角度来看）的时间戳是正确的选择。想象一下这样一个用例，您正在将查询流连接到该查询的结果流的点击流上。在执行连接之后，一些系统会发现查询的时间戳更有用；其他人则更喜欢单击的时间戳。只要它对应于没有延迟到达的元素，任何这样的时间戳都是从水位线正确性的角度来看是有效的。</p>
</li>
</ul>
<p>考虑了一些备选项之后，让我们看看输出时间戳选择对整个管道的影响。为了使变化尽可能显著，在示例3-3和图3-6中，我们将使用最早的时间戳作为窗口的时间戳：第一个非延迟元素的时间戳。</p>
<p><em>示例3-3。平均会话长度管道，设置为最早元素的会话窗口输出时间戳</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;Double&gt; mobileSessions = IO.read(new MobileInputSource())`</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))             .triggering(AtWatermark())</span><br><span class="line">              .withTimestampCombiner(EARLIEST)</span><br><span class="line">              .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br><span class="line"></span><br><span class="line">PCollection&lt;Double&gt; consoleSessions = IO.read(new ConsoleInputSource())</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))            .triggering(AtWatermark())</span><br><span class="line">             .withTimestampCombiner(EARLIEST)</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(CalculateWindowLength());</span><br><span class="line"></span><br><span class="line">PCollection&lt;Float&gt; averageSessionLengths = PCollectionList</span><br><span class="line">  .of(mobileSessions).and(consoleSessions)</span><br><span class="line">  .apply(Flatten.pCollections())</span><br><span class="line">  .apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))</span><br><span class="line">               .triggering(AtWatermark())</span><br><span class="line">  .apply(Mean.globally());</span><br></pre></td></tr></table></figure>

<div id="dplayer14" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer14"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0306.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图3-6。以最早元素的时间戳为输出时间戳的会话平均长度</em></p>
<p>为了突出输出时间戳选择的影响，请查看第一阶段中的虚线，显示每个阶段的输出水位线被保持的时间。由于我们选择了时间戳，输出水位线后移，与图3-7和3-8相比，在这些示例中，输出时间戳被选择为窗口结束时间。从这个图表中，您可以看出第二个阶段的输入水位线因此也被推迟了。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0307.png" class>
<p><em>图3-7。具有不同窗口输出时间戳的水位线和结果的比较。</em><br><em>本图中的水位线对应于会话窗口的结束时间戳（即图3-5）。</em></p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0308.png" class>
<p><em>图3-8。在这个图中，水位线位于会话窗口的开始处（即图3-6）。</em><br><em>我们可以看到，这个图中的水位线线被推迟了，导致平均会话长度不同。</em></p>
<p>就与图3-7相比这个版本的区别，值得注意的有两点：</p>
<ul>
<li><p>水位线延迟</p>
<p>与图3-5相比，图3-6中的水位线进展要慢得多。这是因为第一个阶段的输出水位线被保留到每个窗口中的第一个元素的时间戳，直到该窗口的输入变得完整。只有在给定的窗口已经成为物化后，输出水位线（以及下游输入水位线）才被允许向前推进。</p>
</li>
<li><p>语义差异</p>
<p>因为现在会话时间戳被分配为与会话中最早的非延迟元素相匹配，所以当我们在下一个阶段计算会话长度平均值时，单个会话经常会落入不同的固定窗口桶中。两种选择都没有本质上的对错；它们只是不同的。但是，重要的是要理解它们将是不同的，并且具有它们将不同的直觉，以便在需要时为您特定的用例做出正确的选择。</p>
</li>
</ul>
<h3><span id="重叠窗口的棘手情况">重叠窗口的棘手情况</span><a href="#重叠窗口的棘手情况" class="header-anchor">#</a></h3><p>关于输出时间戳的另一个微妙但重要的问题是如何处理滑动窗口。将输出时间戳设置为最早元素的幼稚方法很容易导致下游的延迟，因为水位线被（正确地）阻止。要了解原因，请考虑具有两个阶段的示例管道，每个阶段都使用相同类型的滑动窗口。假设每个元素都在三个连续的窗口中结束。随着输入水位线的推进，滑动窗口的期望语义如下：</p>
<ul>
<li>第一个窗口在第一阶段完成并向下游发出。</li>
<li>然后在第二阶段中，第一个窗口完成并也可以向下游发出。</li>
<li>一段时间后，第二个窗口在第一阶段完成…等等。</li>
</ul>
<p>然而，如果将输出时间戳选择为面板中第一个非延迟元素的时间戳，实际发生的是以下情况：</p>
<ul>
<li>第一个窗口在第一阶段完成并向下游发出。</li>
<li>第二阶段的第一个窗口仍无法完成，因为其输入水位线被上游第二个和第三个窗口的输出水位线所阻止。这些水位线被正确地阻止，因为这些窗口的输出时间戳使用最早元素的时间戳。</li>
<li>第二个窗口在第一阶段完成并向下游发出。</li>
<li>在上游第三个窗口的带领下，第二阶段的第一个和第二个窗口仍无法完成。</li>
<li>第三个窗口在第一阶段完成并向下游发出。</li>
<li>第二阶段的第一个，第二个和第三个窗口现在都能够完成，最终一次性发出所有三个窗口。</li>
</ul>
<p>尽管这种窗口处理的结果是正确的，但这导致了结果以不必要的延迟方式被实现。因此，Beam对于重叠窗口具有特殊逻辑，以确保窗口N+1的输出时间戳始终大于窗口N的结束时间戳。</p>
<h1><span id="百分位数水位线">百分位数水位线</span><a href="#百分位数水位线" class="header-anchor">#</a></h1><p>到目前为止，我们一直关注以活动信息在阶段中的最小事件时间为度量的水位线。跟踪最小值允许系统知道所有先前时间戳都已经被考虑。另一方面，我们可以考虑活动消息的事件时间分布，并利用它来创建更细粒度的触发条件。</p>
<p>我们可以选择考虑分布的任何百分位数，并且说我们保证处理了这个百分比的所有具有较早时间戳的事件。</p>
<p>这个方案的优点是什么？如果对于业务逻辑来说，“主要正确”已经足够，那么百分位数水位线提供了一种机制，可以使水位线比通过从水位线中剔除长尾的异常数据来跟踪最小事件时间更快、更平稳。图3-9展示了一个紧凑的事件时间分布，其中90百分位数水位线接近100百分位数。图3-10展示了一个离群点更远的情况，因此90百分位数水位线显着领先于100百分位数水位线。通过从水位线中剔除异常数据，百分位数水位线仍然可以跟踪分布的大部分，而不会被异常数据拖延。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0309.png" class>
<p><em>图3-9. 看起来正常的水位线直方图</em></p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0310.png" class>
<p><em>图3-10. 含有异常值的水位线直方图</em></p>
<p>图3-11展示了使用分位数水位线绘制两分钟固定窗口边界的示例。我们可以基于到达数据的时间戳的分位数，通过百分位数水位线绘制早期边界。</p>
<div id="dplayer15" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer15"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0311.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图3-11. 百分位数水位线变化的影响。随着百分位数的增加，窗口中包含更多的事件：但是，材料化窗口的处理时间延迟也会增加。</em></p>
<p>图3-11展示了33百分位数、66百分位数和100百分位数（全）水位线，跟踪数据分布中的相应时间戳百分位数。如预期的那样，这些允许绘制较早的边界，而不是跟踪完整的100百分位数水位线。请注意，33百分位数和66百分位数水位线都允许较早的窗口触发，但以更多的数据被标记为延迟为代价。例如，对于第一个窗口[12:00，12:02)，基于33百分位数水位线关闭的窗口只包括四个事件，并在12:06的处理时间材料化结果。如果我们使用66百分位数水位线，则相同的事件时间窗口将包括七个事件，并在12:07的处理时间材料化。使用100百分位数水位线包括所有十个事件，并推迟材料化结果，直到12:08的处理时间。因此，百分位数水位线提供了一种调整材料化结果的延迟和精度之间权衡的方法。</p>
<h1><span id="处理时间水位线">处理时间水位线</span><a href="#处理时间水位线" class="header-anchor">#</a></h1><p>到目前为止，我们一直在看水位线与流经我们系统的数据有关的情况。我们已经看到了查看水位线如何帮助我们识别最旧数据和实时数据之间的总延迟。然而，这还不足以区分旧数据和延迟的系统。换句话说，仅仅通过我们迄今为止定义的事件时间水位线来检查，我们无法区分正在快速处理一小时前的数据和被延迟一个小时的实时数据的系统。</p>
<p>为了区分这一点，我们需要更多的东西：处理时间水位线。我们已经看到，在流处理系统中有两个时间域：处理时间和事件时间。到目前为止，我们完全在事件时间域中定义了水位线，作为流经系统的数据时间戳的函数。这是一个事件时间水位线。我们现在将同样的模型应用于处理时间域，定义一个处理时间水位线。</p>
<p>我们的流处理系统不断执行操作，例如在管道的当前或上游阶段之前执行的操作之间移动消息、读取或写入消息到持久状态或触发基于水位线进度的延迟聚合。因此，正如数据元素“流”通过系统一样，处理这些元素涉及的操作的级联也“流”通过系统。</p>
<p>我们以完全相同的方式定义处理时间水位线，就像我们迄今为止定义的事件时间水位线一样，只不过我们使用最旧尚未完成的操作的处理时间戳，而不是使用最旧尚未完成的工作的事件时间戳。延迟处理时间水位线的例子可能是从一个阶段到另一个阶段的消息传递被阻塞、读取状态或外部数据的I&#x2F;O调用被阻塞，或者在处理过程中发生的阻止处理完成的异常。</p>
<p>因此，处理时间水位线提供了一个单独于数据延迟的处理延迟概念。为了理解这个区别的价值，考虑图3-12中查看事件时间水位线延迟的情况。</p>
<p>我们看到数据延迟单调递增，但是没有足够的信息来区分系统卡住和数据卡住的情况。只有查看处理时间水位线，如图3-13所示，我们才能区分这些情况。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0312.png" class> 
<p><em>图3-12. 事件时间水位线增加。从这些信息中无法知道这是由于数据缓冲还是系统处理延迟引起的。</em></p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0313.png" class> 
<p><em>图3-13. 处理时间水位线也在增加。这表明系统处理被延迟了。</em></p>
<p>在第一种情况（图3-12）中，当我们检查处理时间水位线延迟时，我们发现它也在增加。这告诉我们，系统中的一个操作被卡住了，卡住也导致数据延迟落后。这可能发生在网络问题阻止管道阶段之间的消息传递或发生故障并正在进行重试的情况下。一般来说，增长的处理时间水位线表示一个问题，阻止了必要于系统功能的操作完成，通常需要用户或管理员干预来解决。</p>
<p>在第二种情况下，如图3-14所示，处理时间水位线延迟很小。这告诉我们，没有卡住的操作。事件时间水位线延迟仍在增加，这表明我们有一些缓冲状态正在等待排空。例如，如果我们正在等待窗口边界来通过及时触发器发出聚合，那么我们可能会缓冲一些状态，这对应于管道的正常操作，如图3-15所示。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0314.png" class> 
<p><em>图3-14. 事件时间水位线延迟增加，处理时间水位线稳定。这表明数据在系统中被缓冲并等待处理，而不是系统操作阻止了数据处理的迹象。</em></p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0315.png" class> 
<p><em>图3-15. 固定窗口的水位线延迟。随着元素为每个窗口缓冲，事件时间水位线延迟增加，随着每个窗口的聚合通过及时触发器发出，事件时间水位线延迟减少，而处理时间水位线仅跟踪系统级延迟（在健康的管道中仍保持相对稳定）。</em></p>
<p>因此，处理时间水位线是区分系统延迟和数据延迟的有用工具。除了可见性外，我们可以在系统实现级别使用处理时间水位线来执行任务，例如临时状态的垃圾回收（Reuven在第5章中对一个示例进行了更多的讨论）。</p>
<h1><span id="案例研究">案例研究</span><a href="#案例研究" class="header-anchor">#</a></h1><p>既然我们已经奠定了水位线应该如何行为的基础，现在是时候看一下一些真实的系统，以了解不同<br>实现水位线机制的方式。我们希望这些能够为现实世界中的水位线在延迟和正确性以及可扩展性和可用性之间的权衡提供一些启示。</p>
<h3><span id="案例研究google-cloud-dataflow中的水位线">案例研究：Google Cloud Dataflow中的水位线</span><a href="#案例研究google-cloud-dataflow中的水位线" class="header-anchor">#</a></h3><p>在流处理系统中实现水位线有许多可能的方法。在这里，我们简要介绍在Google Cloud Dataflow中的实现情况，这是一个完全托管的服务，用于执行Apache Beam管道。Dataflow包括用于定义数据处理工作流程的SDK，以及在Google Cloud Platform资源上运行这些工作流程的Cloud Platform托管服务。</p>
<p>数据流通过将每个数据处理图中的数据处理步骤条带（划分）到多个物理工作器中，将每个工作器的可用键空间分割为键范围，并将每个范围分配给一个工作器。每当遇到具有不同键的GroupByKey操作时，数据必须被洗牌到相应的键。</p>
<p>图3-16描述了具有GroupByKey的处理图的逻辑表示。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0316.png" class>
<p><em>图3-16。 GroupByKey步骤从另一个DoFn消耗数据。这意味着第一步的键和第二步的键之间存在数据洗牌。</em></p>
<p>虽然将键范围物理分配给工作器的方式可能如图3-17所示。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0317.png" class>
<p><em>图3-17。两个步骤的键范围分配（条带）在可用工作器上。</em></p>
<p>在水位线传播部分，我们讨论了为每个步骤维护水位线的多个子组件。Dataflow跟踪每个组件的每个范围的水位线。然后，水位线聚合涉及计算所有范围中每个水位线的最小值，以确保满足以下保证：</p>
<ul>
<li>所有范围都必须报告水位线。如果没有范围的水位线，我们无法推进水位线，因为必须将不报告的范围视为未知。</li>
<li>确保水位线单调递增。因为可能存在延迟数据，所以如果更新水位线会导致水位线后移，我们不能更新水位线。</li>
</ul>
<p>Google Cloud Dataflow通过一个集中的聚合代理进行聚合。我们可以为了效率而分片这个代理。从正确性的角度来看，水位线聚合器作为水位线的“真正的数据源”。</p>
<p>确保分布式水位线聚合的正确性带来了某些挑战。重要的是不要过早地推进水位线，因为过早地推进水位线会将准时数据转换为延迟数据。具体而言，当物理分配被激活到工作器时，工作器对附加到键范围的持久状态维护租约，确保只有一个工作器可以对一个键的持久状态进行修改。为了保证水位线的正确性，必须确保仅当工作器进程仍然在其持久状态上保持租约时，才会将来自工作器进程的每个水位线更新纳入聚合;因此，水位线更新协议必须考虑状态所有权租约验证。</p>
<h3><span id="案例研究apache-flink中的水位线">案例研究：Apache Flink中的水位线</span><a href="#案例研究apache-flink中的水位线" class="header-anchor">#</a></h3><p>Apache Flink是一个开源的流处理框架，用于分布式、高性能、始终可用和准确的数据流应用程序。可以使用Flink运行Beam程序。在这样做时，Beam依赖于Flink中实现的流处理概念，例如水位线。与实现集中式水位线聚合的Google Cloud Dataflow不同，Flink在带内执行水位线跟踪和聚合。</p>
<p>为了理解这是如何工作的，让我们看看Flink管道，如图3-18所示。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0318.png" class>

<p><em>图3-18。带有两个源和带内传播事件时间水位线的Flink管道</em></p>
<p>在此管道中，数据在两个源处生成。这些源还都生成与数据流同步带内发送的水位线“检查点”。这意味着当来自时间戳“53”的源A的水位线检查点被发出时，它保证不会从源A发出时间戳在“53”之后的非延迟数据消息。下游的“keyBy”操作员消耗输入数据和水位线检查点。随着新的水位线检查点被消耗，下游操作员的水位线视图被推进，可以发出下游操作员的新水位线检查点。</p>
<p>与依赖于集中聚合的Cloud Dataflow方法不同，选择在数据流中带内发送水位线检查点会导致一些有趣的权衡。</p>
<p>以下是带内水位线的一些优点：</p>
<ul>
<li><p>减少水位线传播延迟，非常低延迟的水位线<br>因为不需要水位线数据穿过多个跳跃并等待集中聚合，所以使用带内方法更容易实现非常低延迟。</p>
</li>
<li><p>水位线聚合没有单点故障<br>集中的水位线聚合代理不可用将导致整个管道中的水位线延迟。使用带内方法，管道的一部分不可用也不能导致整个管道的水位线延迟。</p>
</li>
<li><p>内在可扩展性<br>尽管Cloud Dataflow在实践中具有良好的可扩展性，但是使用集中式水位线聚合服务需要更多的复杂性，而使用带内水位线具有隐式的可扩展性。</p>
</li>
</ul>
<p>以下是带外水位线聚合的一些优点：</p>
<ul>
<li><p>唯一的“真相”来源<br>为了可调试性、监视和其他应用程序（例如基于管道进度限制输入），拥有一个能够提供水位线值的服务比将水位线隐式地放在流中更有优势，因为系统的每个组件都有自己的部分视图。</p>
</li>
<li><p>源水位线创建<br>某些源水位线需要全局信息。例如，源可能处于暂时闲置状态、数据速率较低或需要有关源或其他系统组件的带外信息来生成水位线。这在中央服务中更容易实现。有关示例，请参见下面针对Google Cloud Pub&#x2F;Sub的源水位线案例研究。</p>
</li>
</ul>
<h3><span id="案例研究google-cloud-pubx2fsub-的源水位线">案例研究：Google Cloud Pub&#x2F;Sub 的源水位线</span><a href="#案例研究google-cloud-pubx2fsub-的源水位线" class="header-anchor">#</a></h3><p>Google Cloud Pub&#x2F;Sub 是一种完全托管的实时消息传递服务，允许您在独立应用程序之间发送和接收消息。在这里，我们讨论如何为通过 Cloud Pub&#x2F;Sub 发送到管道的数据创建一个合理的启发式水位线。</p>
<p>首先，我们需要描述一下 Pub&#x2F;Sub 的工作原理。消息发布在 Pub&#x2F;Sub <em>主题</em> 上。任何数量的 Pub&#x2F;Sub <em>订阅</em> 可以订阅特定主题。相同的消息被发送到订阅给定主题的所有订阅。客户端通过提供的 ID 从订阅中<em>拉取</em>消息，并确认接收到特定消息。客户端无法选择拉取哪些消息，尽管 Pub&#x2F;Sub 尽力提供最旧的消息，但没有硬性保证。</p>
<p>为了构建一个启发式，我们对将数据发送到 Pub&#x2F;Sub 的源进行了一些假设。具体来说，我们假设原始数据的时间戳是“良好的”，换句话说，我们期望在将数据发送到 Pub&#x2F;Sub 之前，源数据出现有限数量的时间戳乱序。任何以时间戳超出允许的乱序范围发送的数据都将被视为延迟数据。在我们当前的实现中，此边界至少为 10 秒，这意味着在发送到 Pub&#x2F;Sub 之前重新排序时间戳最多可达到 10 秒。我们称这个值为<em>估计范围</em>。另一个看待这个的方式是，当管道完全赶上输入时，水位线将落后于实时 10 秒，以允许源可能的重新排序。如果管道积压，所有积压（而不仅仅是 10 秒的范围）都用于估计水位线。</p>
<p>我们在 Pub&#x2F;Sub 中面临的挑战是什么？因为 Pub&#x2F;Sub 不保证顺序，我们必须具有某种额外的元数据以了解有关积压的足够信息。幸运的是，Pub&#x2F;Sub 提供了一个“最旧的未确认发布时间戳”的积压度量。这与我们的消息的事件时间戳不同，因为 Pub&#x2F;Sub 对通过它发送的应用程序级元数据是不可知的；相反，这是消息被 Pub&#x2F;Sub 摄取的时间戳。</p>
<p>这个测量不同于事件时间水位线。实际上，它是 Pub&#x2F;Sub 消息传递的处理时间水位线。Pub&#x2F;Sub 发布时间戳不等于事件时间戳，在发送历史（过去）数据的情况下，它可能会远离。这些时间戳的排序也可能是不同的，因为我们允许有限数量的重新排序，如前面所述。</p>
<p>但是，我们可以将其用作积压的度量，以了解有关积压中存在的事件时间戳的足够信息，从而可以创建如下合理的水位线。</p>
<p>我们在包含输入消息的主题中创建两个订阅：<em>基本订阅</em>，管道将实际使用它来读取要处理的数据，以及<em>跟踪订阅</em>，仅用于元数据，以执行水位线估计。</p>
<p>在图3-19中查看我们的基本订阅，我们可以看到消息可能乱序到达。我们使用 Pub&#x2F;Sub 发布时间戳“pt”和事件时间戳“et”标记每条消息。请注意，这两个时间域可能是无关的。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0319.png" class>
<p><em>图3-19。Pub&#x2F;Sub订阅上到达的处理时间和事件时间时间戳</em></p>
<p>基本订阅中的一些消息未被确认，形成了积压。这可能是因为它们尚未被传递，或者它们可能已经被传递但尚未被处理。请记住，从此订阅拉取是分布在多个碎片上的。因此，仅仅通过查看基本订阅，无法确定我们的水位线应该是什么。</p>
<p>跟踪订阅（如图3-20所示）用于有效地检查基本订阅的积压，并获取积压中事件时间戳的最小值。通过在跟踪订阅上保持很少或没有积压，我们可以检查基本订阅的最旧未确认消息之前的消息。</p>
<img src="/www6vHomeHexo/2000/03/15/streamingSystemChapter3/stsy_0320.png" class>
<p><em>图3-20。接收相同消息的附加“跟踪”订阅和“基本”订阅</em></p>
<p>我们通过确保从此订阅中拉取是计算廉价的来保持在跟踪订阅上。相反，如果我们在跟踪订阅上落后足够多，我们将停止推进水位线。为此，我们确保至少满足以下条件之一：</p>
<ul>
<li>跟踪订阅足够领先于基本订阅。足够领先意味着跟踪订阅领先至少估计范围。这确保考虑了估计范围内的任何有界重新排序。</li>
<li>跟踪订阅足够接近实时。换句话说，跟踪订阅上没有积压。</li>
</ul>
<p>我们尽快确认跟踪订阅上的消息，之后我们会持久保存有关消息的发布和事件时间戳的元数据。我们将此元数据以稀疏直方图格式存储，以最小化使用空间和持久写入的大小。</p>
<p>最后，我们确保具有足够的数据来进行合理的水位线估计。我们从我们的跟踪订阅中读取的事件时间戳带宽有最新的发布时间戳，这些时间戳比基本订阅的最旧未确认时间戳新，或者估计范围的宽度。这确保我们考虑了积压中的所有事件时间戳，或者如果积压很小，则考虑了最近的估计范围，以进行水位线估计。</p>
<p>最后，水位线值计算为带宽中的最小事件时间。</p>
<p>这种方法的正确性在于，输入中在重新排序限制范围内的所有时间戳将由水位线考虑，不会出现延迟数据。然而，它可能会产生过度保守的水位线，一种“进展过慢”的方式，如第2章所述。因为我们在跟踪订阅上考虑了基本订阅的最旧未确认消息之前的所有消息，所以我们可以将已经确认的消息的事件时间戳包括在水位线估计中。</p>
<p>此外，还有一些启发式方法以确保进展。在密集、频繁到达数据的情况下，此方法效果很好。在数据稀疏或不频繁的情况下，可能没有足够的最近消息来构建合理的估计。如果我们在订阅上没有看到数据超过两分钟（并且没有积压），我们将将水位线推进到接近实时。这确保即使没有更多的消息，水位线和管道也会继续前进。</p>
<p>以上所有内容都确保只要源数据事件时间戳的重新排序在估计范围内，就不会有额外的延迟数据。</p>
<h1><span id="摘要">摘要</span><a href="#摘要" class="header-anchor">#</a></h1><p>到目前为止，我们已经探讨了如何利用消息的事件时间来为流处理系统提供强大的进度定义。我们看到这种进度的概念随后如何帮助我们回答处理事件时间发生的位置以及处理时间产生的结果的时间。具体来说，我们看到了如何在源头创建水位线，也就是将数据注入到管道中的数据摄取点，然后在整个管道中传播水位线，以保留允许回答“何时”和“何处”的关键保证。我们还探讨了更改输出窗口时间戳对水位线的影响。最后，我们探讨了在大规模构建水位线时考虑的一些实际系统问题。</p>
<p>现在我们已经对水位线在幕后的工作方式有了牢固的基础，我们可以深入研究它们在使用窗口和触发器回答更复杂的查询时可以为我们做些什么。请参阅第4章。</p>
<hr>
<ol>
<li>注意到了单调性的提及；我们尚未讨论如何实现这一点。实际上，到目前为止的讨论中没有提到单调性。如果我们仅考虑最旧的飞行事件时间，水位线不总是单调的，因为我们对输入没有做出任何假设。我们稍后回到这个讨论。</li>
<li>要精确，不仅需要日志数量是静态的，而且系统必须事先知道任何给定时间的日志数量。一个更复杂的输入源，由动态选择的输入日志组成，例如Pravega，也可以用于构建完美的水位线。只有在任何给定时间动态集合中存在的日志数量是未知的（如下一节中的示例）时，才必须回退到启发式水位线。</li>
<li>请注意，当我说“流经系统”时，并不一定意味着它们沿着正常数据的相同路径流动。它们可能（如Apache Flink中），但它们也可能以带外方式传输（如MillWheel &#x2F; Cloud Dataflow）。</li>
<li>从水位线正确性的角度来看，“窗口”的<em>开始</em>不是一个安全的选择，因为窗口中的第一个元素通常在窗口本身的开始之后出现，这意味着水位线不保证被拖延到窗口的开始位置。</li>
<li>这里描述的分位数水位线触发方案目前尚未由Beam实现；然而，其他系统（如MillWheel）实现了这一点。</li>
<li>有关Flink水位线的更多信息，请参阅Flink文档。</li>
</ol>
]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-Chapter4. Advanced Windowing [完整]</title>
    <url>/www6vHomeHexo/2000/03/14/streamingSystemChapter4Original/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#whenwhere-processing-time-windows">*<strong>When*&#x2F;*Where*: Processing-Time Windows</strong></a><ul>
<li><a href="#event-time-windowing"><strong>Event-Time Windowing</strong></a></li>
<li><a href="#processing-time-windowing-via-triggers"><strong>Processing-Time Windowing via Triggers</strong></a></li>
<li><a href="#processing-time-windowing-via-ingress-time"><strong>Processing-Time Windowing via Ingress Time</strong></a></li>
</ul>
</li>
<li><a href="#where-session-windows">*<strong>Where*: Session Windows</strong></a></li>
<li><a href="#where-custom-windowing"><strong><em>Where</em>: Custom Windowing</strong></a><ul>
<li><a href="#variations-on-fixed-windows"><strong>Variations on Fixed Windows</strong></a></li>
<li><a href="#variations-on-session-windows"><strong>Variations on Session Windows</strong></a></li>
<li><a href="#one-size-does-not-fit-all"><strong>One Size Does Not Fit All</strong></a></li>
</ul>
</li>
<li><a href="#summary"><strong>Summary</strong></a></li>
</ul>
<!-- tocstop -->

</div>

<p>Page 106</p>
<details><summary>点击 原文</summary><p>Hello again! I hope you enjoyed Chapter 3 as much as I did. Watermarks are</p>
<p>a fascinating topic, and Slava knows them better than anyone on the planet.</p>
<p>Now that we have a deeper understanding of watermarks under our belts, I’d</p>
<p>like to dive into some more advanced topics related to the <em>what</em>, <em>where</em>, <em>when</em>,</p>
<p>and <em>how</em> questions.</p>
<p>We first look at <em>processing-time windowing</em>, which is an interesting mix of</p>
<p>both where and when, to understand better how it relates to event-time</p>
<p>windowing and get a sense for times when it’s actually the right approach to</p>
<p>take. We then dive into some more advanced event-time windowing concepts,</p>
<p>looking at <em>session windows</em> in detail, and finally making a case for why</p>
<p>generalized <em>custom windowing</em> is a useful (and surprisingly straightforward)</p>
<p>concept by exploring three different types of custom windows: <em>unaligned</em></p>
<p>fixed windows, <em>per-key</em> fixed windows, and <em>bounded</em> sessions windows.</p>
</details>


<details><summary>点击 原文</summary><h1><span id="whenx2fwhere-processing-time-windows">*<strong>When*&#x2F;*Where*: Processing-Time Windows</strong></span><a href="#whenx2fwhere-processing-time-windows" class="header-anchor">#</a></h1><p>Processing-time windowing is important for two reasons:</p>
<ul>
<li>For certain use cases, such as usage monitoring (e.g., web service</li>
</ul>
<p>traffic QPS), for which you want to analyze an incoming stream of</p>
<p>data as it’s observed, processing-time windowing is absolutely the</p>
<p>appropriate approach to take.</p>
<ul>
<li>For use cases for which the time that events happened is important</li>
</ul>
<p>(e.g., analyzing user behavior trends, billing, scoring, etc.),</p>
<p>processing-time windowing is absolutely the wrong approach to take,</p>
<p>and being able to recognize these cases is critical.</p>
<p>As such, it’s worth gaining a solid understanding of the differences between</p>
<p>processing-time windowing and event-time windowing, particularly given the</p>
<p>prevalence of processing-time windowing in many streaming systems today. —</p>
<p>When working within a model for which windowing as a first-class notion is</p>
<p>strictly event-time based, such as the one presented in this book, there are two</p>
<p>methods that you can use to achieve processing-time windowing: —</p>
<p>Triggers</p>
<p>​	Ignore event time (i.e., use a global window spanning all of event time)</p>
<p>​	and use triggers to provide snapshots of that window in the processing</p>
<p>​	time axis.</p>
<p>Ingress time</p>
<p>​	Assign ingress times as the event times for data as they arrive, and use</p>
<p>​	normal event-time windowing from there on. This is essentially what</p>
<p>​	something like Spark Streaming 1.x does.</p>
<p>Note that the two methods are more or less equivalent, although they differ</p>
<p>slightly in the case of multistage pipelines: in the triggers version, a</p>
<p>multistage pipeline will slice the processing-time “windows” independently at</p>
<p>each stage, so, for example, data in window <em>N</em> for one stage might instead end</p>
<p>up in window <em>N</em>–1 or <em>N</em>+1 in the following stage; in the ingress-time version,</p>
<p>after a datum is incorporated into window <em>N</em>, it will remain in window <em>N</em> for</p>
<p>the duration of the pipeline due to synchronization of progress between stages</p>
<p>via watermarks (in the Cloud Dataflow case), microbatch boundaries (in the</p>
<p>Spark Streaming case), or whatever other coordinating factor is involved at</p>
<p>the engine level. —</p>
<p>As I’ve noted to death, the big downside of processing-time windowing is</p>
<p>that the contents of the windows change when the observation order of the</p>
<p>inputs changes. To drive this point home in a more concrete manner, we’re</p>
<p>going to look at these three use cases: <em>event-time</em> windowing, <em>processing-time</em></p>
<p>windowing via triggers, and <em>processing-time</em> windowing via ingress time. —</p>
<p>Each will be applied to two different input sets (so six variations total). The</p>
<p>two inputs sets will be for the exact same events (i.e., same values, occurring</p>
<p>at the same event times), but with different observation orders. The first set</p>
<p>will be the observation order we’ve seen all along, colored white; the second</p>
<p>one will have all the values shifted in the processing-time axis as in Figure 4-</p>
<p>1, colored purple. You can simply imagine that the purple example is another</p>
<p>way reality could have happened if the winds had been blowing in from the</p>
<p>east instead of the west (i.e., the underlying set of complex distributed</p>
<p>systems had played things out in a slightly different order). —</p>
<p><em>Figure 4-1. Shifting input observation order in processing time, holding values, and event-times</em></p>
<p><em>constant</em></p>
</details>





<details><summary>点击 原文</summary><h3><span id="event-time-windowing"><strong>Event-Time Windowing</strong></span><a href="#event-time-windowing" class="header-anchor">#</a></h3><p>To establish a baseline, let’s first compare fixed windowing in event time</p>
<p>with a heuristic watermark over these two observation orderings. We’ll reuse</p>
<p>the early&#x2F;late code from Example 2-7&#x2F;Figure 2-10 to get the results shown in</p>
<p>Figure 4-2. The lefthand side is essentially what we saw before; the righthand</p>
<p>side is the results over the second observation order. The important thing to</p>
<p>note here is that even though the overall shape of the outputs differs (due to</p>
<p>the different orders of observation in processing time), <em>the final results for the</em></p>
<p><em>four windows remain the same</em>: 14, 18, 3, and 12.</p>
<p><em>Figure 4-2. Event-time windowing over two different processing-time orderings of the same inputs</em></p>
</details>









<details><summary>点击 原文</summary><h3><span id="processing-time-windowing-via-triggers"><strong>Processing-Time Windowing via Triggers</strong></span><a href="#processing-time-windowing-via-triggers" class="header-anchor">#</a></h3><p>Let’s now compare this to the two processing-time methods just described.</p>
<p>First, we’ll try the triggers method. There are three aspects to making</p>
<p>processing-time “windowing” work in this manner:</p>
<p><em>Windowing</em></p>
<p>We use the global event-time window because we’re essentially emulating</p>
<p>processing-time windows with event-time panes.</p>
<p><em>Triggering</em></p>
<p>We trigger periodically in the processing-time domain based on the</p>
<p>desired size of the processing-time windows.</p>
<p><em>Accumulation</em></p>
<p>We use discarding mode to keep the panes independent from one another,</p>
<p>thus letting each of them act like an independent processing-time</p>
<p>“window.”</p>
<p>The corresponding code looks something like Example 4-1; note that global</p>
<p>windowing is the default in Beam, hence there is no specific override of the</p>
<p>108windowing strategy.</p>
<p><em>Example 4-1. Processing-time windowing via repeated, discarding panes of a</em></p>
<p><em>global event-time window</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.triggering(Repeatedly(AlignedDelay(ONE_MINUTE)))</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>When executed on a streaming runner against our two different orderings of</p>
<p>the input data, the results look like Figure 4-3. Here are some interesting notes</p>
<p>about this figure:</p>
<ul>
<li>Because we’re emulating processing-time windows via event-time</li>
</ul>
<p>panes, the “windows” are delineated in the processing-time axis,</p>
<p>which means their effective width is measured on the y-axis instead</p>
<p>of the x-axis.</p>
<ul>
<li>Because processing-time windowing is sensitive to the order that</li>
</ul>
<p>input data are encountered, the results for each of the “windows”</p>
<p>differs for each of the two observation orders, even though the events</p>
<p>themselves technically happened at the same times in each version.</p>
<p>On the left we get 12, 18, 18, whereas on the right we get 7, 36, 5.</p>
<p><em>Figure 4-3. Processing-time “windowing” via triggers, over two different processing-time orderings of</em></p>
<p><em>the same inputs</em></p>
</details>







<details><summary>点击 原文</summary><h3><span id="processing-time-windowing-via-ingress-time"><strong>Processing-Time Windowing via Ingress Time</strong></span><a href="#processing-time-windowing-via-ingress-time" class="header-anchor">#</a></h3><p>Lastly, let’s look at processing-time windowing achieved by mapping the</p>
<p>event times of input data to be their ingress times. Code-wise, there are four</p>
<p>aspects worth mentioning here:</p>
<ul>
<li><em>Time-shifting</em></li>
</ul>
<p>When elements arrive, their event times need to be overwritten with the</p>
<p>time of ingress. We can do this in Beam by providing a new DoFn that sets</p>
<p>the timestamp of the element to the current time via the</p>
<p>outputWithTimestamp method.</p>
<ul>
<li><em>Windowing</em></li>
</ul>
<p>Return to using standard event-time fixed windowing.</p>
<ul>
<li><em>Triggering</em></li>
</ul>
<p>Because ingress time affords the ability to calculate a perfect watermark,</p>
<p>we can use the default trigger, which in this case implicitly fires exactly</p>
<p>once when the watermark passes the end of the window.</p>
<ul>
<li><em>Accumulation mode</em></li>
</ul>
<p>Because we only ever have one output per window, the accumulation</p>
<p>mode is irrelevant.</p>
<p>The actual code might thus look something like that in Example 4-2.</p>
<p><em>Example 4-2. Processing-time windowing via repeated, discarding panes of a</em></p>
<p><em>global event-time window</em></p>
<p><code>PCollection&lt;String&gt; raw = IO.read().apply(ParDo.of(</code></p>
<p><code>new DoFn&lt;String, String&gt;() {</code></p>
<p><code>public void processElement(ProcessContext c) {</code></p>
<p><code>c.outputWithTimestmap(new Instant());</code></p>
<p><code>}</code></p>
<p><code>});</code></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; input =</code></p>
<p><code>raw.apply(ParDo.of(new ParseFn());</code></p>
<p><code>PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</code></p>
<p><code>.apply(Window.info(FixedWindows.of(TWO_MINUTES))</code></p>
<p><code>.apply(Sum.integersPerKey());</code></p>
<p>Execution on a streaming engine would look like Figure 4-4. As data arrive,</p>
<p>their event times are updated to match their ingress times (i.e., the processing</p>
<p>times at arrival), resulting in a rightward horizontal shift onto the ideal</p>
<p>watermark line. Here are some interesting notes about this figure:</p>
<ul>
<li>As with the other processing-time windowing example, we get</li>
</ul>
<p>different results when the ordering of inputs changes, even though</p>
<p>the values and event times for the input stay constant.</p>
<ul>
<li>Unlike the other example, the windows are once again delineated in</li>
</ul>
<p>the event-time domain (and thus along the x-axis). Despite this, they</p>
<p>aren’t bonafide event-time windows; we’ve simply mapped</p>
<p>processing time onto the event-time domain, erasing the original</p>
<p>record of occurrence for each input and replacing it with a new one</p>
<p>that instead represents the time the datum was first observed by the</p>
<p>pipeline.</p>
<ul>
<li>Despite this, thanks to the watermark, trigger firings still happen at</li>
</ul>
<p>exactly the same time as in the previous processing-time example.</p>
<p>Furthermore, the output values produced are identical to that</p>
<p>example, as predicted: 12, 18, 18 on the left, and 7, 36, 5 on the right.</p>
<ul>
<li>Because perfect watermarks are possible when using ingress time,</li>
</ul>
<p>the actual watermark matches the ideal watermark, ascending up and</p>
<p>to the right with a slope of one.</p>
<p><em>Figure 4-4. Processing-time windowing via the use of ingress time, over two different processing-time</em></p>
<p><em>orderings of the same inputs</em></p>
<p>Although it’s interesting to see the different ways you can implement</p>
<p>processing-time windowing, the big takeaway here is the one I’ve been</p>
<p>harping on since the first chapter: event-time windowing is order-agnostic, at</p>
<p>least in the limit (actual panes along the way might differ until the input</p>
<p>becomes complete); processing-time windowing is not. <em>If you care about the</em></p>
<p><em>times at which your events actually happened, you must use event-time</em></p>
<p><em>windowing or your results will be meaningless.</em> I will get off my soapbox</p>
<p>now.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="where-session-windows">*<strong>Where*: Session Windows</strong></span><a href="#where-session-windows" class="header-anchor">#</a></h1><p>Enough with processing-time windowing. Let’s now go back to tried-and-true</p>
<p>event-time windowing, but now we’re going to look at one of my favorite</p>
<p>features: the dynamic, data-driven windows called <em>sessions</em>.</p>
<p>Sessions are a special type of window that captures a period of activity in the</p>
<p>data that is terminated by a gap of inactivity. They’re particularly useful in</p>
<p>data analysis because they can provide a view of the activities for a specific</p>
<p>user over a specific period of time during which they were engaged in some</p>
<p>activity. This allows for the correlation of activities within the session,</p>
<p>drawing inferences about levels of engagement based on the lengths of the</p>
<p>sessions, and so on.</p>
<p>From a windowing perspective, sessions are particularly interesting in two</p>
<p>ways:</p>
<ul>
<li>They are an example of a <em>data-driven window</em>: the location and sizes</li>
</ul>
<p>of the windows are a direct consequence of the input data</p>
<p>themselves, rather than being based on some predefined pattern</p>
<p>within time, as are fixed and sliding windows.</p>
<ul>
<li>They are also an example of an <em>unaligned window</em>; that is, a window</li>
</ul>
<p>that does not apply uniformly across the data, but instead only to a</p>
<p>specific subset of the data (e.g., per user). This is in contrast to</p>
<p>aligned windows like fixed and sliding windows, which typically</p>
<p>apply uniformly across the data.</p>
<p>For some use cases, it’s possible to tag the data within a single session with a</p>
<p>common identifier ahead of time (e.g., a video player that emits heartbeat</p>
<p>pings with quality-of-service information; for any given viewing, all of the</p>
<p>pings can be tagged ahead of time with a single session ID). In this case,</p>
<p>sessions are much easier to construct because it’s basically just a form of</p>
<p>grouping by key.</p>
<p>However, in the more general case (i.e., where the actual session itself is not</p>
<p>known ahead of time), the sessions must be constructed from the locations of</p>
<p>the data within time alone. When dealing with out-of-order data, this becomes</p>
<p>particularly tricky.</p>
<p>Figure 4-5 shows an example of this, with five independent records grouped</p>
<p>together into session windows with a gap timeout of 60 minutes. Each record</p>
<p>starts out in a 60-minute window of its own (a proto-session). Merging</p>
<p>together overlapping proto-sessions yields the two larger session windows</p>
<p>containing three and two records, respectively.</p>
<p><em>Figure 4-5. Unmerged proto-session windows, and the resultant merged sessions</em></p>
<p>They key insight in providing general session support is that a complete</p>
<p>session window is, by definition, a composition of a set of smaller,</p>
<p>overlapping windows, each containing a single record, with each record in the</p>
<p>sequence separated from the next by a gap of inactivity no larger than a</p>
<p>predefined timeout. Thus, even if we observe the data in the session out of</p>
<p>order, we can build up the final session simply by merging together any</p>
<p>overlapping windows for individual data as they arrive.</p>
<p>To look at this another way, consider the example we’ve been using so far. If</p>
<p>we specify a session timeout of one minute, we would expect to identify two</p>
<p>sessions in the data, delineated in Figure 4-6 by the dashed black lines. Each</p>
<p>of those sessions captures a burst of activity from the user, with each event in</p>
<p>the session separate by less than one minute from at least one other event in</p>
<p>the session.</p>
<p><em>Figure 4-6. Sessions we want to compute</em></p>
<p>To see how the window merging works to build up these sessions over time</p>
<p>as events are encountered, let’s look at it in action. We’ll take the early&#x2F;late</p>
<p>code with retractions enabled from Example 2-10 and update the windowing</p>
<p>to build sessions with a one-minute gap duration timeout instead. Example 4-</p>
<p>3 illustrates what this looks like.</p>
<p><em>Example 4-3. Early&#x2F;on-time&#x2F;late firings with session windows and retractions</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(Sessions.withGapDuration(ONE_MINUTE))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AfterCount(1))))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>Executed on a streaming engine, you’d get something like that shown in</p>
<p>Figure 4-7 (note that I’ve left in the dashed black lines annotating the</p>
<p>expected final sessions for reference).</p>
<p><em>Figure 4-7. Early and late firings with session windows and retractions on a streaming engine</em></p>
<p>There’s quite a lot going on here, so I’ll walk you through some of it:</p>
<ul>
<li>When the first record with value 5 is encountered, it’s placed into a</li>
</ul>
<p>single proto-session window that begins at that record’s event time</p>
<p>and spans the width of the session gap duration; for example, one</p>
<p>minute beyond the point at which that datum occurred. Any windows</p>
<p>we encounter in the future that overlap this window should be part of</p>
<p>the same session and will be merged into it as such.</p>
<ul>
<li>The second record to arrive is the 7, which similarly is placed into its</li>
</ul>
<p>own proto-session window, given that it doesn’t overlap with the</p>
<p>window for the 5.</p>
<ul>
<li>In the meantime, the watermark has passed the end of the first</li>
</ul>
<p>window, so the value of 5 is materialized as an on-time result just</p>
<p>before 12:06. Shortly thereafter, the second window is also</p>
<p>materialized as a speculative result with value 7, right as processing</p>
<p>time hits 12:06.</p>
<ul>
<li>We next observe a pair of records 3 and 4, the proto-sessions for</li>
</ul>
<p>which overlap. As a result, they are merged together, and by the time</p>
<p>the early trigger for 12:07 fires, a single window with value 7 is</p>
<p>emitted.</p>
<ul>
<li>When the 8 arrives shortly thereafter, it overlaps with both of the</li>
</ul>
<p>windows with value 7. All three are thus merged together, forming a</p>
<p>new combined session with value 22. When the watermark then</p>
<p>passes the end of this session, it materializes both the new session</p>
<p>with value 22 as well as retractions for the two windows of value 7</p>
<p>that were previously emitted, but later incorporated into it.</p>
<ul>
<li>A similar dance occurs when the 9 arrives late, joining the proto</li>
</ul>
<p>session with value 5 and session with value 22 into a single larger</p>
<p>session of value 36. The 36 and the retractions for the 5 and 22</p>
<p>windows are all emitted immediately by the late data trigger.</p>
<p>This is some pretty powerful stuff. And what’s really awesome is how easy it</p>
<p>is to describe something like this within a model that breaks apart the</p>
<p>dimensions of stream processing into distinct, composable pieces. In the end,</p>
<p>you can focus more on the interesting business logic at hand, and less on the</p>
<p>minutiae of shaping the data into some usable form.</p>
<p>If you don’t believe me, check out this blog post describing how to manually</p>
<p>build up sessions on Spark Streaming 1.x (note that this is not done to point</p>
<p>fingers at them; the Spark folks had just done a good enough job with</p>
<p>everything else that someone actually bothered to go to the trouble of</p>
<p>documenting what it takes to build a specific variety of sessions support on</p>
<p>top of Spark 1.x; you can’t say the same for most other systems out there).</p>
<p>It’s quite involved, and they’re not even doing proper event-time sessions, or</p>
<p>providing speculative or late firings, or retractions.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="where-custom-windowing"><strong><em>Where</em>: Custom Windowing</strong></span><a href="#where-custom-windowing" class="header-anchor">#</a></h1><p>Up until now, we’ve talked primarily about predefined types of windowing</p>
<p>strategies: fixed, sliding, and sessions. You can get a lot of mileage out of</p>
<p>standard types of windows, but there are plenty of real-world use cases for</p>
<p>which being able to define a custom windowing strategy can really save the</p>
<p>day (three of which we’re about to see now).</p>
<p>Most systems today don’t support custom windowing to the degree that it’s</p>
<p>supported in Beam, so we focus on the Beam approach. In Beam, a custom</p>
<p>windowing strategy consists of two things:</p>
<ul>
<li>Window assignment</li>
</ul>
<p>This places each element into an initial window. At the limit, this allows</p>
<p>every element to be placed within a unique window, which is very</p>
<p>powerful.</p>
<ul>
<li>(Optional) window merging</li>
</ul>
<p>This allows windows to merge at grouping times, which makes it possible</p>
<p>for windows to evolve over time, which we saw in action earlier with</p>
<p>session windows.</p>
<p>To give you a sense for how simple windowing strategies really are, and also</p>
<p>how useful custom windows support can be, we’re going to look in detail at</p>
<p>the stock implementations of fixed windows and sessions in Beam and then</p>
<p>consider a few real-world use cases that require custom variations on those</p>
<p>themes. In the process, we’ll see both how easy it is to create a custom</p>
<p>windowing strategy, and how limiting the lack of custom windowing support</p>
<p>can be when your use case doesn’t quite fit into the stock approaches.</p>
<h3><span id="variations-on-fixed-windows"><strong>Variations on Fixed Windows</strong></span><a href="#variations-on-fixed-windows" class="header-anchor">#</a></h3><p>To begin, let’s look at the relatively simple strategy of fixed windows. The</p>
<p>stock fixed-windows implementation is as straightforward as you might</p>
<p>imagine, and consists of the following logic:</p>
<ul>
<li>Assignment</li>
</ul>
<p>The element is placed into the appropriate fixed-window based on its</p>
<p>timestamp and the window’s size and offset parameters.</p>
<ul>
<li>Merging</li>
</ul>
<p>None.</p>
<p>An abbreviated version of the code looks like Example 4-4.</p>
<p><em>Example 4-4. Abbreviated FixedWindows implementation</em></p>
<p><code>public class FixedWindows extends WindowFn&lt;Object, IntervalWindow&gt; {</code></p>
<p><code>private final Duration size;</code></p>
<p><code>private final Duration offset;</code></p>
<p><code>public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) {</code></p>
<p><code>long start = c.timestamp().getMillis() - c.timestamp()</code></p>
<p><code>.plus(size)</code></p>
<p><code>.minus(offset)</code></p>
<p><code>.getMillis() % size.getMillis();</code></p>
<p><code>return Arrays.asList(IntervalWindow(new Instant(start), size));</code></p>
<p><code>}</code></p>
<p><code>}</code></p>
<p>Keep in mind that the point of showing you the code here isn’t so much to</p>
<p>teach you how to write windowing strategies (although it’s nice to demystify</p>
<p>them and call out how simple they are). It’s really to help contrast the</p>
<p>comparative ease and difficulty of supporting some relatively basic use cases,</p>
<p>both with and without custom windowing, respectively. Let’s consider two</p>
<p>such use cases that are variations on the fixed-windows theme now.</p>
</details>



<details><summary>点击 原文</summary><p><strong>Unaligned fixed windows</strong></p>
<p>One characteristic of the default fixed-windows implementation that we</p>
<p>alluded to previously is that windows are aligned across all of the data. In our</p>
<p>running example, the window from noon to 1 PM for any given team aligns</p>
<p>with the corresponding windows for all other teams, which also extend from</p>
<p>noon to 1 PM. And in use cases for which you want to compare like windows</p>
<p>across another dimension, such as between teams, this alignment is very</p>
<p>useful. However, it comes at a somewhat subtle cost. All of the active</p>
<p>windows from noon to 1 PM become complete at around the same time,</p>
<p>which means that once an hour the system is hit with a massive load of</p>
<p>windows to materialize.</p>
<p>To see what I mean, let’s look at a concrete example (Example 4-5). We’ll</p>
<p>begin with a score summation pipeline as we’ve used in most examples, with</p>
<p>fixed two-minute windows, and a single watermark trigger.</p>
<p><em>Example 4-5. Watermark completeness trigger (same as Example 2-6)</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(AfterWatermark()))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>But in this instance, we’ll look at two different keys (see Figure 4-8) from the</p>
<p>same dataset in parallel. What we’ll see is that the outputs for those two keys</p>
<p>are all aligned, on account of the windows being aligned across all of the</p>
<p>keys. As a result, we end up with <em>N</em> panes being materialized every time the</p>
<p>watermark passes the end of a window, where <em>N</em> is the number of keys with</p>
<p>updates in that window. In this example, where <em>N</em> is 2, that’s maybe not too</p>
<p>painful. But when <em>N</em> starts to order in the thousands, millions, or more, that</p>
<p>synchronized burstiness can become problematic.</p>
<p><em>Figure 4-8. Aligned fixed windows</em></p>
<p>In circumstances for which comparing across windows is unnecessary, it’s</p>
<p>often more desirable to spread window completion load out evenly across</p>
<p>time. This makes system load more predictable, which can reduce the</p>
<p>provisioning requirements for handling peak load. In most systems, however,</p>
<p>unaligned fixed windows are only available if the system provides support for</p>
<p>them out of the box.  But with custom-windowing support, it’s a relatively</p>
<p>trivial modification to the default fixed-windows implementation to provide</p>
<p>unaligned fixed-windows support. What we want to do is continue</p>
<p>guaranteeing that the windows for all elements being grouped together (i.e.,</p>
<p>the ones with the same key) have the same alignment, while relaxing the</p>
<p>alignment restriction across different keys. The code changes to the default</p>
<p>fixed-windowing strategy and looks something like Example 4-6.</p>
<p><em>Example 4-6. Abbreviated UnalignedFixedWindows implementation</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class UnalignedFixedWindows</span><br><span class="line">extends WindowFn&lt;KV&lt;K, V&gt;, IntervalWindow&gt; &#123;</span><br><span class="line">private final Duration size;</span><br><span class="line">private final Duration offset;</span><br><span class="line">public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) &#123;</span><br><span class="line">long perKeyShift = hash(c.element().key()) % size;</span><br><span class="line">long start = perKeyShift + c.timestamp().getMillis()</span><br><span class="line">-c.timestamp()</span><br><span class="line">.plus(size)</span><br><span class="line">.minus(offset)</span><br><span class="line">return Arrays.asList(IntervalWindow(new Instant(start), size));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>With this change, the windows for all elements <em>with the same key</em> are</p>
<p>aligned, but the windows for elements <em>with different keys</em> will (typically) be</p>
<p>unaligned, thus spreading window completion load out at the cost of also</p>
<p>making comparisons across keys somewhat less meaningful. We can switch</p>
<p>our pipeline to use our new windowing strategy, illustrated in Example 4-7.</p>
<p><em>Example 4-7. Unaligned fixed windows with a single watermark trigger</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(UnalignedFixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(AfterWatermark()))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>And then you can see what this looks like in Figure 4-9 by comparing</p>
<p>different fixed-window alignments across the same dataset as before (in this</p>
<p>case, I’ve chosen a maximal phase shift between the two alignments to most</p>
<p>clearly call out the benefits, given that randomly chosen phases across a large</p>
<p>number of keys will result in similar effects).</p>
<p><em>Figure 4-9. Unaligned fixed windows</em></p>
<p>Note how there are no instances where we emit multiple panes for multiple</p>
<p>keys simultaneously. Instead, the panes arrive individually at a much more</p>
<p>even cadence. This is another example of being able to make trade-offs in one</p>
<p>dimension (ability to compare across keys) in exchange for benefits in another</p>
<p>dimension (reduced peak resource provisioning requirements) when the use</p>
<p>case allows. Such flexibility is critical when you’re trying to process massive</p>
<p>quantities of data as efficiently as possible.</p>
<p>Let’s now look at a second variation on fixed windows, one which is more</p>
<p>intrinsically tied to the data being processed.</p>
</details>



<details><summary>点击 原文</summary><p><strong>Per-element&#x2F;key fixed windows</strong></p>
<p>Our second example comes courtesy of one of the early adopters of Cloud</p>
<p>Dataflow. This company generates analytics data for its customers, but each</p>
<p>customer is allowed to configure the window size over which it wants to</p>
<p>aggregate its metrics. In other words, each customer gets to define the specific</p>
<p>size of its fixed windows.</p>
<p>Supporting a use case like this isn’t too difficult as long the number of</p>
<p>available window sizes is itself fixed. For example, you could imagine</p>
<p>offering the option of choosing 30-minute, 60-minute, and 90-minute fixed</p>
<p>windows and then running a separate pipeline (or fork of the pipeline) for</p>
<p>each of those options. Not ideal, but not too horrible. However, that rapidly</p>
<p>becomes intractable as the number of options increases, and in the limit of</p>
<p>providing support for truly arbitrary window sizes (which is what this</p>
<p>customer’s use case required) is entirely impractical.</p>
<p>Fortunately, because each record the customer processes is already annotated</p>
<p>with metadata describing the desired size of window for aggregation,</p>
<p>supporting arbitrary, per-user fixed-window size was as simple as changing a</p>
<p>couple of lines from the stock fixed-windows implementation, as</p>
<p>demonstrated in Example 4-8.</p>
<p><em>Example 4-8. Modified (and abbreviated) FixedWindows implementation that</em></p>
<p><em>supports per-element window sizes</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class PerElementFixedWindows&lt;T extends HasWindowSize%gt;</span><br><span class="line">extends WindowFn&lt;T, IntervalWindow&gt; &#123;</span><br><span class="line">private final Duration offset;</span><br><span class="line">public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) &#123;</span><br><span class="line">long perElementSize = c.element().getWindowSize();</span><br><span class="line">long start = perKeyShift + c.timestamp().getMillis()</span><br></pre></td></tr></table></figure>

<ul>
<li><code>c.timestamp()</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.plus(size)</span><br><span class="line">.minus(offset)</span><br><span class="line">.getMillis() % size.getMillis();</span><br><span class="line">return Arrays.asList(IntervalWindow(</span><br><span class="line">new Instant(start), perElementSize));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>With this change, each element is assigned to a fixed window with the</p>
<p>appropriate size, as dictated by metadata carried around in the element itself. Changing the pipeline code to use this new strategy is again trivial, as shown</p>
<p>in Example 4-9.</p>
<p><em>Example 4-9. Per-element fixed-window sizes with a single watermark trigger</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(PerElementFixedWindows.of(TWO_MINUTES))</span><br><span class="line">.triggering(AfterWatermark()))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>And then looking at an this pipeline in action (Figure 4-10), it’s easy to see</p>
<p>that the elements for Key A all have two minutes as their window size,</p>
<p>whereas the elements for Key B have one-minute window sizes.</p>
<p><em>Figure 4-10. Per-key custom-sized fixed windows</em></p>
<p>This really isn’t something you would ever reasonably expect a system to</p>
<p>provide to you; the nature of where window size preferences are stored is too</p>
<p>use-case specific for it to make sense to try to build into a standard API.</p>
<p>Nevertheless, as exhibited by this customer’s needs, use cases like this do</p>
<p>exist. That’s why the flexibility provided by custom windowing is so</p>
<p>powerful.</p>
</details>



<details><summary>点击 原文</summary><h3><span id="variations-on-session-windows"><strong>Variations on Session Windows</strong></span><a href="#variations-on-session-windows" class="header-anchor">#</a></h3><p>To really drive home the usefulness of custom windowing, let’s look at one</p>
<p>final example, which is a variation on sessions. Session windowing is</p>
<p>understandably a bit more complex than fixed windows. Its implementation</p>
<p>consists of the following:</p>
<ul>
<li>Assignment</li>
</ul>
<p>Each element is initially placed into a proto-session window that begins at</p>
<p>the element’s timestamp and extends for the gap duration.</p>
<ul>
<li>Merging</li>
</ul>
<p>At grouping time, all eligible windows are sorted, after which any</p>
<p>overlapping windows are merged together.</p>
<p>An abbreviated version of the sessions code (hand merged together from a</p>
<p>number of helper classes) looks something like that shown in Example 4-10.</p>
<p><em>Example 4-10. Abbreviated Sessions implementation</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Sessions extends WindowFn&lt;Object, IntervalWindow&gt; &#123;</span><br><span class="line">private final Duration gapDuration;</span><br><span class="line">public Collection&lt;IntervalWindow&gt; assignWindows(AssignContext c) &#123;</span><br><span class="line">return Arrays.asList(</span><br><span class="line">new IntervalWindow(c.timestamp(), gapDuration));</span><br><span class="line">&#125;</span><br><span class="line">public void mergeWindows(MergeContext c) throws Exception &#123;</span><br><span class="line">List&lt;IntervalWindow&gt; sortedWindows = new ArrayList&lt;&gt;();</span><br><span class="line">for (IntervalWindow window : c.windows()) &#123;</span><br><span class="line">sortedWindows.add(window);</span><br><span class="line">&#125;</span><br><span class="line">Collections.sort(sortedWindows);</span><br><span class="line">List&lt;MergeCandidate&gt; merges = new ArrayList&lt;&gt;();</span><br><span class="line">MergeCandidate current = new MergeCandidate();</span><br><span class="line">for (IntervalWindow window : sortedWindows) &#123;</span><br><span class="line">if (current.intersects(window)) &#123;</span><br><span class="line">current.add(window);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">merges.add(current);</span><br><span class="line">current = new MergeCandidate(window);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">merges.add(current);</span><br><span class="line">for (MergeCandidate merge : merges) &#123;</span><br><span class="line">merge.apply(c);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>As before, the point of seeing the code isn’t so much to teach you how custom</p>
<p>windowing functions are implemented, or even what the implementation of</p>
<p>sessions looks like; it’s really to show the ease with which you can support</p>
<p>new use via custom windowing.</p>
</details>



<details><summary>点击 原文</summary><p><strong>Bounded sessions</strong></p>
<p>One such custom use case I’ve come across multiple times is bounded</p>
<p>sessions: sessions that are not allowed to grow beyond a certain size, either in</p>
<p>time, element count, or some other dimension. This can be for semantic</p>
<p>reasons, or it can simply be an exercise in spam protection. However, given</p>
<p>the variations in types of limits (some use cases care about total session size</p>
<p>in event time, some care about total element count, some care about element</p>
<p>density, etc.), it’s difficult to provide a clean and concise API for bounded</p>
<p>sessions. Much more practical is allowing users to implement their own</p>
<p>custom windowing logic, tailored to their specific use case. An example of</p>
<p>one such use case, in which session windows are time-limited, might look</p>
<p>something like Example 4-11 (eliding some of the builder boilerplate we’ll</p>
<p>utilize here).</p>
<p><em>Example 4-11. Abbreviated Sessions implementation</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class BoundedSessions extends WindowFn&lt;Object, IntervalWindow&gt; &#123;</span><br><span class="line">private final Duration gapDuration;</span><br><span class="line">private final Duration maxSize;</span><br><span class="line">public Collection&lt;IntervalWindow&gt; assignWindows(AssignContext c) &#123;</span><br><span class="line">return Arrays.asList(</span><br><span class="line">new IntervalWindow(c.timestamp(), gapDuration));</span><br><span class="line">&#125;</span><br><span class="line">private Duration windowSize(IntervalWindow window) &#123;</span><br><span class="line">return window == null</span><br><span class="line">? new Duration(0)</span><br><span class="line">: new Duration(window.start(), window.end());</span><br><span class="line">&#125;</span><br><span class="line">public static void mergeWindows(</span><br><span class="line">WindowFn&lt;?, IntervalWindow&gt;.MergeContext c) throws Exception &#123;</span><br><span class="line">List&lt;IntervalWindow&gt; sortedWindows = new ArrayList&lt;&gt;();</span><br><span class="line">for (IntervalWindow window : c.windows()) &#123;</span><br><span class="line">sortedWindows.add(window);</span><br><span class="line">&#125;</span><br><span class="line">Collections.sort(sortedWindows);</span><br><span class="line">List&lt;MergeCandidate&gt; merges = new ArrayList&lt;&gt;();</span><br><span class="line">MergeCandidate current = new MergeCandidate();</span><br><span class="line">for (IntervalWindow window : sortedWindows) &#123;</span><br><span class="line">MergeCandidate next = new MergeCandidate(window);</span><br><span class="line">if (current.intersects(window)) &#123;</span><br><span class="line">current.add(window);</span><br><span class="line">if (windowSize(current.union) &lt;= (maxSize - gapDuration))</span><br><span class="line">continue;</span><br><span class="line">// Current window exceeds bounds, so flush and move to next</span><br><span class="line">next = new MergeCandidate();</span><br><span class="line">&#125;</span><br><span class="line">merges.add(current);</span><br><span class="line">current = next;</span><br><span class="line">&#125;</span><br><span class="line">merges.add(current);</span><br><span class="line">for (MergeCandidate merge : merges) &#123;</span><br><span class="line">merge.apply(c);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>As always, updating our pipeline (the early&#x2F;on-time&#x2F;late version of it, from</p>
<p>Example 2-7, in this case) to use this custom windowing strategy is trivial, as</p>
<p>you can see in Example 4-12.</p>
<p><em>Example 4-12. Early, on-time, and late firings via the early&#x2F;on-time&#x2F;late API</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.into(BoundedSessions</span><br><span class="line">.withGapDuration(ONE_MINUTE)</span><br><span class="line">.withMaxSize(THREE_MINUTES))</span><br><span class="line">.triggering(</span><br><span class="line">AfterWatermark()</span><br><span class="line">.withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">.withLateFirings(AfterCount(1))))</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>And executed over our running example, it might then look something like</p>
<p>Figure 4-11.</p>
<p><em>Figure 4-11. Per-key custom-sized fixed windows</em></p>
<p>Note how the large session with value 36 that spanned [12:00.26, 12:05.20),</p>
<p>or nearly five minutes of time, in the unbounded sessions implementation</p>
<p>from Figure 2-7 now ends up broken apart into two shorter sessions of length</p>
<p>2 minutes and 2 minutes 53 seconds.</p>
<p>Given how few systems provide custom windowing support today, it’s worth</p>
<p>pointing out how much more effort would be required to implement such a</p>
<p>thing using a system that supported only an unbounded sessions</p>
<p>implementation. Your only real recourse would be to write code downstream</p>
<p>of the session grouping logic that looked at the generated sessions and</p>
<p>chopped them up if they exceed the length limit. This would require the</p>
<p>ability to decompose a session after the fact, which would obviate the benefits</p>
<p>of incremental aggregation (something we look at in more detail in</p>
<p>Chapter 7), increasing cost. It would also eliminate any spam protection</p>
<p>benefits one might hope to gain by limiting session lengths, because the</p>
<p>sessions would first need to grow to their full sizes before being chopped or</p>
<p>truncated.</p>
<h3><span id="one-size-does-not-fit-all"><strong>One Size Does Not Fit All</strong></span><a href="#one-size-does-not-fit-all" class="header-anchor">#</a></h3><p>We’ve now looked at three real-world use cases, each of which was a subtle</p>
<p>variation on the stock types of windowing typically provided by data</p>
<p>processing systems: unaligned fixed windows, per-element fixed windows,</p>
<p>and bounded sessions. In all three cases, we saw how simple it was to support</p>
<p>those use cases via custom windowing and how much more difficult (or</p>
<p>expensive) it would be to support those use cases without it. Though custom</p>
<p>windowing doesn’t see broad support across the industry as yet, it’s a feature</p>
<p>that provides much needed flexibility for balancing trade-offs when building</p>
<p>data processing pipelines that need to handle complex, real-world use cases</p>
<p>over massive amounts of data as efficiently as possible.</p>
</details>



<details><summary>点击 原文</summary><h1><span id="summary"><strong>Summary</strong></span><a href="#summary" class="header-anchor">#</a></h1><p>Advanced windowing is a complex and varied topic. In this chapter, we</p>
<p>covered three advanced concepts:</p>
<ul>
<li>Processing-time windows</li>
</ul>
<p>We saw how this relates to event-time windowing, calling out the places</p>
<p>where it’s inherently useful and, most important, identifying those where</p>
<p>it’s not by specifically highlighting the stability of results that event-time</p>
<p>windowing affords us.</p>
<ul>
<li>Session windows</li>
</ul>
<p>We had our first introduction to the dynamic class of merging window</p>
<p>strategies and seeing just how much heavy lifting the system does for us</p>
<p>in providing such a powerful construct that you can simply drop into</p>
<p>place.</p>
<ul>
<li>Custom windows</li>
</ul>
<p>Here, we looked at three real-world examples of custom windows that are</p>
<p>difficult or impossible to achieve in systems that provide only a static set</p>
<p>of stock windowing strategies but relatively trivial to implement in a</p>
<p>system with custom-windowing support:</p>
<ul>
<li><em>Unaligned fixed windows</em>, which provide a more even distribution of</li>
</ul>
<p>outputs over time when using a watermark trigger in conjunction</p>
<p>with fixed windows.</p>
<ul>
<li><em>Per-element fixed windows</em>, which provide the flexibility to</li>
</ul>
<p>dynamically choose the size of fixed windows per element (e.g., to</p>
<p>provide customizable per-user or per-ad-campaign window sizes),</p>
<p>for greater customization of the pipeline semantics to the use case at</p>
<p>hand.</p>
<ul>
<li><em>Bounded-session windows</em>, which limit how large a given session</li>
</ul>
<p>may grow; for example, to counteract spam attempts or to place a</p>
<p>bound on the latency for completed sessions being materialized by</p>
<p>the pipeline.</p>
<p>After deep diving through watermarks in Chapter 3 with Slava and taking a</p>
<p>broad survey of advanced windowing here, we’ve now gone well beyond the</p>
<p>basics of robust stream processing in multiple dimensions. With that, we</p>
<p>conclude our focus on the Beam Model and thus Part I of the book.</p>
<p>Up next is Reuven’s Chapter 5 on consistency guarantees, exactly-once</p>
<p>processing, and side effects, after which we begin our journey into Part II,</p>
<p><em>Streams and Tables</em> with Chapter 6.</p>
<ol>
<li>As far as I know, Apache Flink is the only other system to support custom</li>
</ol>
<p>windowing to the extent that Beam does. And to be fair, its support extends</p>
<p>even beyond that of Beam’s, thanks to the ability to provide a custom window</p>
<p>evictor. Head asplode.</p>
<ol>
<li>And I’m not actually aware of any such systems at this time.</li>
<li>This naturally implies the use of keyed data, but because windowing is</li>
</ol>
<p>intrinsically tied to grouping by key anyway, that restriction isn’t particularly</p>
<p>burdensome.</p>
<ol>
<li>And it’s not critical that the element itself know the window size; you could</li>
</ol>
<p>just as easily look up and cache the appropriate window size for whatever the</p>
<p>desired dimension is; for example, per-user.</p>
</details>]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
  <entry>
    <title>《Streaming System》-第四章：高级窗口 [完整]</title>
    <url>/www6vHomeHexo/2000/03/14/streamingSystemChapter4/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BD%95%E6%97%B6%E4%BD%95%E5%9C%B0%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3">何时&#x2F;何地：处理时间窗口</a><ul>
<li><a href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3">事件时间窗口</a></li>
<li><a href="#%E8%A7%A6%E5%8F%91%E5%99%A8%E7%9A%84%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3">触发器的处理时间窗口</a></li>
<li><a href="#%E9%80%9A%E8%BF%87%E8%BF%9B%E5%85%A5%E6%97%B6%E9%97%B4%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E5%8C%96">通过进入时间进行处理时间窗口化 |</a></li>
</ul>
</li>
<li><a href="#%E4%BD%95%E5%9C%B0%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3">何地：会话窗口</a></li>
<li><a href="#%E4%BD%95%E5%9C%B0-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%AA%97%E5%8F%A3%E5%8C%96"><strong>何地</strong>: 自定义窗口化</a><ul>
<li><a href="#%E5%9B%BA%E5%AE%9A%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8F%98%E5%8C%96">固定窗口的变化</a></li>
<li><a href="#%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8F%98%E5%8C%96">会话窗口的变化</a></li>
<li><a href="#%E9%9D%9E%E4%B8%80%E5%88%80%E5%88%87">非一刀切</a></li>
</ul>
</li>
<li><a href="#%E6%91%98%E8%A6%81">摘要</a><ul>
<li><a href="#draft-here">Draft Here</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<p>大家好！希望大家和我一样喜欢第三章节。水位线是一个很有趣的话题，Slava比地球上任何人都了解它们。现在我们已经对水位线有了更深入的了解，我想深入探讨一些与<strong>“什么”</strong>，<strong>“何地”</strong>，<strong>“何时”</strong>和<strong>“如何”</strong>相关的高级主题。</p>
<p>我们首先看一下<em><strong>处理时间窗口</strong></em>，这是一个有趣的组合，既涉及“在哪里”，又涉及“何时”，以更好地理解它与事件时间窗口的关系，并了解在哪些情况下采用这种方法是正确的。然后，我们深入了解一些更高级的事件时间窗口概念，详细了解<em><strong>会话窗口</strong></em>，最后通过探索三种不同类型的自定义窗口：<em><strong>非对齐</strong></em>固定窗口，<em><strong>按键</strong></em>固定窗口和<em><strong>有界</strong></em>会话窗口，为什么<em><strong>广义自定义窗口</strong></em>是一个有用的（且令人惊讶地简单）概念。</p>
<h1><span id="何时x2f何地处理时间窗口">何时&#x2F;何地：处理时间窗口</span><a href="#何时x2f何地处理时间窗口" class="header-anchor">#</a></h1><p>处理时间窗口对于两个原因都很重要：</p>
<ul>
<li><p>对于某些用例，例如使用监控（例如，Web服务流量QPS），您希望在观察到它的同时分析传入的数据流，因此处理时间窗口是绝对正确的方法。</p>
</li>
<li><p>对于时间事件很重要的用例（例如，分析用户行为趋势、计费、评分等），处理时间窗口绝对不是正确的方法，识别这些情况非常关键。</p>
</li>
</ul>
<p>因此，值得深入了解处理时间窗口和事件时间窗口之间的区别，特别是考虑到许多流处理系统中处理时间窗口的普遍性。</p>
<p>在像本书中所呈现的基于事件时间的窗口作为一级概念的模型中工作时，有两种方法可以使用以实现处理时间窗口：</p>
<ul>
<li>触发器 Triggers<br> 忽略事件时间（即使用跨越所有事件时间的全局窗口）并使用触发器在处理时间轴上提供该窗口的快照。</li>
<li>进入时间 Ingress time<br> 将进入时间分配为数据的事件时间，随后使用常规事件时间窗口。这基本上就是Spark Streaming 1.x之类的东西所做的。</li>
</ul>
<p>请注意，这两种方法或多或少是等效的，尽管它们在多阶段流水线的情况下略有不同：在触发器版本中，多阶段流水线将独立地在每个阶段切片处理时间“窗口”，因此例如，一个阶段的窗口<em>N</em>中的数据可能会在下一个阶段中代替窗口<em>N</em>-1或<em>N</em>+1;在进入时间版本中，在将数据合并到窗口<em>N</em>后，由于通过水位线（在Cloud Dataflow情况下）、微批边界（在Spark Streaming情况下）或其他协调因素在引擎级别涉及的进展同步，它将在整个流水线的持续时间内保留在窗口<em>N</em>中。</p>
<p>正如我一再指出的那样，处理时间窗口的一个重大缺点是当输入的观察顺序发生变化时，窗口的内容也会发生变化。为了更具体地说明这一点，我们将查看以下三种用例：<em>事件时间</em>窗口、通过触发器的<em>处理时间</em>窗口和通过进入时间的<em>处理时间</em>窗口。</p>
<p>每个将应用于两个不同的输入集（因此总共有六个变体）。两个输入集将是完全相同的事件（即相同的值，在相同的事件时间发生），但是观察顺序不同。第一个集将是我们一直看到的观察顺序，标为白色；第二个则将所有值在处理时间轴上向右移动，如图4-1所示，标为紫色。您可以简单地想象，如果风从东边吹而不是从西边吹（即底层的复杂分布式系统以稍微不同的顺序进行了处理），那么紫色示例就是另一种现实可能发生的方式。</p>
<div id="dplayer16" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer16"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0401.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><em>图4-1。在处理时间中移动输入观察顺序，保持值和事件时间不变</em></p>
<h3><span id="事件时间窗口">事件时间窗口</span><a href="#事件时间窗口" class="header-anchor">#</a></h3><p>为了建立一个基准，让我们先比较事件时间中固定窗口和基于启发式水位线的两种观察顺序。我们将重用示例2-7&#x2F;图2-10中的早期&#x2F;晚期代码，以获得图4-2中所示的结果。左边实质上是我们之前看到的；右边是第二个观察顺序的结果。这里需要注意的重要事情是，即使输出的总体形状不同（由于处理时间中的不同观察顺序），<em>四个窗口的最终结果仍然相同</em>：14、18、3和12。</p>
<div id="dplayer17" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer17"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0402.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><em>图4-2. 对相同输入的两个不同处理时间顺序的事件时间窗口</em></p>
<h3><span id="触发器的处理时间窗口">触发器的处理时间窗口</span><a href="#触发器的处理时间窗口" class="header-anchor">#</a></h3><p>现在，我们将此与刚刚描述的两种处理时间方法进行比较。首先，我们将尝试触发器方法。 实现处理时间“窗口化”的方式有三个方面：|</p>
<ul>
<li><p><em>窗口 Windowing</em><br> 我们使用全局事件时间窗口，因为我们基本上是在使用事件时间窗格模拟处理时间窗口。</p>
</li>
<li><p><em>触发 Triggering</em><br> 我们在处理时间域中定期触发，基于所需处理时间窗口的大小。</p>
</li>
<li><p><em>累积 Accumulation</em><br> 我们使用丢弃模式使窗格彼此独立，从而使每个窗格像一个独立的处理时间“窗口”。</p>
</li>
</ul>
<p>相应的代码看起来像示例4-1；请注意，全局窗口是Beam的默认设置，因此没有特定的窗口策略覆盖。</p>
<p><em>示例4-1。通过重复，丢弃全局事件时间窗格的处理时间窗口</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">.apply(Window.triggering(Repeatedly(AlignedDelay(ONE_MINUTE)))</span><br><span class="line">             .discardingFiredPanes())</span><br><span class="line">.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>在流处理运行器上针对输入数据的两种不同排序执行时，结果如图4-3所示。这里有一些有趣的注释：</p>
<ul>
<li>因为我们是通过事件时间窗格模拟处理时间窗口，所以“窗口”在处理时间轴上被划分，这意味着它们的有效宽度在y轴上测量而不是x轴。</li>
<li>因为处理时间窗口对输入数据的遇到顺序很敏感，所以每个“窗口”的结果对于两个观察顺序中的每一个都不同，尽管事件本身在每个版本中都在技术上相同时发生。</li>
</ul>
<p>在左侧，我们得到12、18、18，而在右侧，我们得到7、36、5。</p>
<div id="dplayer18" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer18"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0403.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 

<p><em>图4-3. 通过触发器的处理时间“窗口”，针对相同输入数据的两种不同处理时间排序</em></p>
<h3><span id="通过进入时间进行处理时间窗口化">通过进入时间进行处理时间窗口化 |</span><a href="#通过进入时间进行处理时间窗口化" class="header-anchor">#</a></h3><p>最后，让我们看一下通过将输入数据的事件时间映射为它们的进入时间来实现的处理时间窗口。在代码方面，这里有四个值得一提的方面：</p>
<ul>
<li><p>时间转移  Time-shifting<br>当元素到达时，它们的事件时间需要被覆盖为进入时间。我们可以通过提供一个新的DoFn来在Beam中实现这一点，该函数使用outputWithTimestamp方法将元素的时间戳设置为当前时间。</p>
</li>
<li><p>窗口化 Windowing<br>返回使用标准事件时间固定窗口。</p>
</li>
<li><p>触发器  Triggering<br>因为进入时间提供了计算完美水位线的能力，所以我们可以使用默认的触发器，在这种情况下，当水位线通过窗口的结束时隐式地触发一次。</p>
</li>
<li><p>累积模式 Accumulation mode<br>因为我们每个窗口只有一个输出，所以累积模式是无关紧要的。</p>
</li>
</ul>
<p>实际代码可能看起来像Example 4-2中的代码。<br><em>Example 4-2. 通过重复、丢弃全局事件时间窗口的窗格来实现处理时间窗口</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;String&gt; raw = IO.read().apply(ParDo.of(</span><br><span class="line">    new DoFn&lt;String， String&gt;() &#123;</span><br><span class="line">        public void processElement(ProcessContext c) &#123;</span><br><span class="line">        c.outputWithTimestmap(new Instant());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">PCollection&lt;KV&lt;Team， Integer&gt;&gt; input =</span><br><span class="line">	raw.apply(ParDo.of(new ParseFn());</span><br><span class="line">PCollection&lt;KV&lt;Team， Integer&gt;&gt; totals = input</span><br><span class="line">	.apply(Window.info(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">	.apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>在流引擎上执行的效果如图4-4所示。当数据到达时，它们的事件时间会更新以匹配它们的进入时间（即到达时的处理时间），从而向右水平偏移到理想的水位线线上。这里有一些关于这个图的有趣注释：</p>
<ul>
<li>与其他处理时间窗口示例一样，当输入顺序改变时，我们会得到不同的结果，尽管输入的值和事件时间保持不变。</li>
<li>与其他示例不同，窗口再次在事件时间域中（因此沿着x轴）划分。尽管如此，它们并不是真正的事件时间窗口；我们只是将处理时间映射到事件时间域中，抹掉了每个输入的原始出现记录，并用一个新记录替换它，该记录代表了数据被管道首次观察到的时间。</li>
<li>尽管如此，由于水位线的存在，触发器的触发仍然会在与先前处理时间示例完全相同的时间发生。</li>
</ul>
<p>此外，产生的输出值与该示例中的输出值相同，如预期的：左侧为12、18、18，右侧为7、36、5。</p>
<ul>
<li>由于在使用进入时间时可以获得完美的水位线，因此实际水位线与理想水位线匹配，向右上方以1的斜率上升。</li>
</ul>
<div id="dplayer19" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer19"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0404.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图4-4。通过使用进入时间进行处理时间窗口，在相同输入的两个不同处理时间顺序上</em></p>
<p>虽然看到了不同实现处理时间窗口的方法是有趣的，但其中的重要教训自第一章以来一直是我一直在强调的：事件时间窗口是无序的，至少在极限情况下（在输入完成之前，实际窗格可能会有所不同）；处理时间窗口则不是。<strong>如果您关心事件实际发生的时间，您必须使用事件时间窗口，否则您的结果将毫无意义。</strong>现在我将结束我的演讲。</p>
<h1><span id="何地会话窗口">何地：会话窗口</span><a href="#何地会话窗口" class="header-anchor">#</a></h1><p>处理时间窗口已经够了。现在我们回到了经过验证的事件时间窗口，但现在我们将看看我最喜欢的动态数据驱动窗口之一：<em>会话</em>。</p>
<p>会话是一种特殊类型的窗口，可以捕获由不活动间隙终止的数据中的活动期间。它们在数据分析中特别有用，因为它们可以提供特定用户在特定时间段内参与某些活动的活动视图。这允许在会话中的活动之间进行关联，根据会话的长度推断参与水平等等。</p>
<p>从窗口的角度来看，会话有两个特别有趣的方面：</p>
<ul>
<li>它们是<em>数据驱动的窗口</em>的例子：窗口的位置和大小是输入数据本身的直接结果，而不是基于一些预定义的时间模式，如固定窗口和滑动窗口。</li>
<li>它们也是<em>不对齐的窗口</em>的例子。也就是说，窗口并不适用于数据的所有部分，而只适用于特定的数据子集（例如，每个用户）。这与固定窗口和滑动窗口等对齐窗口形成对比，后者通常适用于数据的所有部分。</li>
</ul>
<p>对于某些用例，可以提前使用共同标识符标记单个会话中的数据（例如，发出带有服务质量信息的心跳脉冲的视频播放器;对于任何给定的观看，所有脉冲都可以提前标记为单个会话ID）。在这种情况下，会话构建起来要容易得多，因为它基本上只是按键分组的一种形式。</p>
<p>然而，在更一般的情况下（即实际会话本身不是提前知道的情况下），会话必须仅从时间内的数据位置构建。处理无序数据时，这变得非常棘手。</p>
<p>图4-5展示了一个示例，其中五个独立记录被分组到具有60分钟间隙超时的会话窗口中。每个记录都开始于其自己的60分钟窗口中（原型会话）。合并重叠的原型会话会产生包含三个和两个记录的两个较大会话窗口。</p>
<img src="/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0405.png" class>
<p><em>图4-5。未合并的原型会话窗口及其结果合并的会话</em></p>
<p>提供一般会话支持的关键见解是，完整的会话窗口是一组较小、重叠的窗口的组合，每个窗口包含单个记录，序列中的每个记录与下一个记录之间的不活动间隙不大于预定义的超时时间。因此，即使我们以无序的方式观察会话中的数据，只要到达任何重叠的数据，我们就可以将其合并为最终的会话。</p>
<p>换个角度看，考虑到目前为止我们一直在使用的示例。如果我们指定了一个1分钟的会话超时时间，那么我们期望在图4-6中用虚线黑线划分出两个会话。每个会话都捕获用户的一次活动，会话中的每个事件都至少与会话中的另一个事件相隔不到一分钟。</p>
<img src="/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0406.png" class>
<p><em>图4-6。我们要计算的会话</em></p>
<p>为了看到窗口合并如何在遇到事件时随着时间建立这些会话，让我们看看它是如何运作的。我们将使用启用了缩回的早期&#x2F;准时&#x2F;后期引发的示例2-10，并将窗口更新为使用一分钟的间隙持续时间超时生成会话。示例4-3说明了这看起来像什么。</p>
<p><em>例4-3。使用会话窗口和撤回的早期&#x2F;准时&#x2F;延迟触发</em>   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team，Integer&gt;&gt;  totals=input</span><br><span class="line">  .apply（Window.into（Sessions.withGapDuration（ONE_MINUTE）)</span><br><span class="line">                .triggering（</span><br><span class="line">                 AfterWatermark（）</span><br><span class="line">                   .withEarlyFirings（AlignedDelay（ONE_MINUTE））</span><br><span class="line">                   .withLateFirings（AfterCount（1））））</span><br><span class="line">  .apply（Sum.integersPerKey（））;</span><br></pre></td></tr></table></figure>

<p>在流引擎上执行，您将获得类似于图4-7所示的内容（请注意，我已经留下了用虚线黑线注释的预期最终会话以供参考）。</p>
<div id="dplayer20" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer20"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0407.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script>
<p><em>图4-7。使用会话窗口和缩回的早期和后期引发的流引擎</em></p>
<p>这里有很多事情要做，所以我会向您介绍一些：</p>
<ul>
<li>当遇到值为5的第一个记录时，它被放置在一个单个的原型会话窗口中，该窗口从该记录的事件时间开始，并跨越会话间隙持续时间的宽度；例如，在该数据发生的时间点之后一分钟。我们将来遇到的任何重叠窗口都应该是同一个会话的一部分，并将被合并为这样的窗口。</li>
<li>到达的第二个记录是7，由于它与5的窗口不重叠，因此同样被放置在自己的原型会话窗口中。</li>
<li>同时，水位线已经过去了第一个窗口的末尾，因此值为5的结果在12:06之前实现为准时结果。不久之后，第二个窗口也作为具有值7的推测结果实现，当处理时间达到12:06时。</li>
<li>我们接下来观察到一对记录3和4，其原型会话重叠。因此，它们被合并在一起，当12:07的早期触发器触发时，将发出一个值为7的单个窗口。</li>
<li>紧随其后的8，它与值为7的两个窗口重叠。因此，它们全部合并在一起，形成一个新的组合会话，值为22。当水位线通过该会话的末尾时，它会将值为22的新会话以及以前发出但后来合并到其中的值为7的两个窗口的缩回作为材料化。</li>
<li>当9到达时，它与值为5的原型会话和值为22的会话一起形成单个较大的会话，值为36。当水位线过去时，36和5和22窗口的缩回都会立即发出。</li>
</ul>
<p>这是一些非常强大的东西。真正令人惊讶的是，在一个将流处理的维度分解为不同的、可组合的组件的模型中描述这样的东西是多么容易。最终，您可以更专注于有趣的业务逻辑，而不是将数据塑造成可用形式的细枝末节。</p>
<p>如果您不相信我，请查看这篇博客文章，了解如何在Spark Streaming 1.x上手动构建会话（请注意，这不是为了指责他们；Spark的人们已经在其他方面做得足够好，以至于有人确实费心记录构建特定类型的会话支持所需的内容在Spark 1.x之上；您无法对大多数其他系统做出同样的说法）。这非常复杂，他们甚至没有做正确的事件时间会话，也没有提供推测或后期引发或撤销。 [todo 后期引发 是否改成  后期触发 ]</p>
<hr>
<h1><span id="何地-自定义窗口化"><strong>何地</strong>: 自定义窗口化</span><a href="#何地-自定义窗口化" class="header-anchor">#</a></h1><p>到目前为止，我们主要讨论了预定义的窗口化策略：固定、滑动和会话。标准类型的窗口可以获得很多收益，但是有很多真实世界的用例需要能够定义自定义窗口化策略，这真的可以挽救一天（我们将看到其中三个）。</p>
<p>今天大多数系统不支持自定义窗口化，就像Beam这样，因此我们专注于Beam方法。在Beam中，自定义窗口化策略由两个部分组成：</p>
<ul>
<li><p>窗口分配<br>这将每个元素放置在初始窗口中。在极限情况下，这允许将每个元素放置在唯一的窗口中，这是非常强大的。</p>
</li>
<li><p>（可选）窗口合并<br>这允许窗口在分组时间合并，这使窗口随时间演变成为可能，我们早先在会话窗口中看到了这种情况。</p>
</li>
</ul>
<p>为了让您了解窗口化策略真正的简单性以及自定义窗口支持的实用性，我们将详细查看Beam中固定窗口和会话的股票实现，然后考虑一些需要自定义变化的实际用例。在这个过程中，我们将看到创建自定义窗口化策略的简单程度，以及当您的用例不太适合标准方法时，缺乏自定义窗口化支持的限制性有多大。</p>
<h3><span id="固定窗口的变化">固定窗口的变化</span><a href="#固定窗口的变化" class="header-anchor">#</a></h3><p>首先，让我们看看相对简单的固定窗口策略。固定窗口的股票实现与您想象的一样简单，包括以下逻辑：</p>
<ul>
<li><p>分配<br>根据其时间戳和窗口的大小和偏移参数，将元素放入相应的固定窗口中。</p>
</li>
<li><p>合并<br>没有。</p>
</li>
</ul>
<p>代码的缩写版本如示例4-4所示。</p>
<p><em>示例4-4. 简写的FixedWindows实现</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class FixedWindows extends WindowFn&lt;Object, IntervalWindow&gt; &#123;</span><br><span class="line">  private final Duration size;</span><br><span class="line">  private final Duration offset;</span><br><span class="line">  public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) &#123;</span><br><span class="line">    long start = c.timestamp().getMillis() - c.timestamp()</span><br><span class="line">                  .plus(size)</span><br><span class="line">                  .minus(offset)</span><br><span class="line">                  .getMillis() % size.getMillis();</span><br><span class="line">    return Arrays.asList(IntervalWindow(new Instant(start), size));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请记住，这里展示代码的重点并不是教您如何编写窗口化策略（尽管了解它们并指出它们有多简单很好）。实际上，它有助于对比支持某些相对基本用例的相对容易和困难的相对性，分别使用和不使用自定义窗口化。现在让我们考虑两种固定窗口主题的变化。</p>
<hr>
<p><strong>未对齐的固定窗口</strong></p>
<p>默认的固定窗口实现的一个特征是窗口在所有数据中都是对齐的。在我们的运行例子中，给定任何一个团队的从中午到下午1点的窗口与所有其他团队的相应窗口对齐，这些窗口也从中午到下午1点延伸。在您想要在其他维度（例如在团队之间）比较相似窗口的用例中，这种对齐非常有用。然而，它带来了一个相当微妙的成本。所有从中午到下午1点的活动窗口在大约相同的时间内变成完整的，这意味着每小时系统都会面临大量窗口的实现负载。</p>
<p>为了看到我的意思，让我们看一个具体的例子（示例4-5）。我们将从大多数示例中使用的分数总和管道开始，使用固定的两分钟窗口和单个水位线触发器。</p>
<p><em>示例4-5. 水位线完整性触发器（与示例2-6相同）</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(FixedWindows.of(TWO_MINUTES))</span><br><span class="line">                .triggering(AfterWatermark()))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>但在这种情况下，我们将并行查看来自相同数据集的两个不同键（见图4-8）。我们将看到这两个键的输出都对齐，因为窗口在所有键上都是对齐的。因此，每次水位线通过窗口的末尾时，我们就会获得<em>N</em>个窗格的实现，其中<em>N</em>是在该窗口中有更新的键的数量。在这个例子中，当<em>N</em>开始以千计、百万或更多来排序时，这种同步的突发性可能会变得有问题。</p>
<div id="dplayer21" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer21"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0408.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图4-8. 对齐的固定窗口</em></p>
<p>在不需要跨窗口比较的情况下，通常更希望在时间上均匀地分布窗口完成负载。这使系统负载更可预测，可以减少处理峰值负载的资源预配要求。然而，在大多数系统中，如果系统不支持它们，非对齐的固定窗口是不可用的。但是，通过自定义窗口支持，将默认固定窗口实现进行相对简单的修改即可提供非对齐的固定窗口支持。我们想要做的是继续保证所有被分组在一起的元素的窗口（即具有相同键的元素）具有相同的对齐方式，同时放宽跨不同键的对齐限制。默认的固定窗口策略的代码更改如示例4-6所示。</p>
<p><em>示例4-6. 简略的UnalignedFixedWindows实现</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class UnalignedFixedWindows</span><br><span class="line">    extends WindowFn&lt;KV&lt;K, V&gt;, IntervalWindow&gt; &#123;</span><br><span class="line">  private final Duration size;</span><br><span class="line">  private final Duration offset;</span><br><span class="line">  public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) &#123;</span><br><span class="line">    long perKeyShift = hash(c.element().key()) % size;</span><br><span class="line">    long start = perKeyShift + c.timestamp().getMillis()</span><br><span class="line">                   - c.timestamp()</span><br><span class="line">                      .plus(size)</span><br><span class="line">                      .minus(offset)</span><br><span class="line">    return Arrays.asList(IntervalWindow(new Instant(start), size));</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过这个改变，所有元素的窗口<em>具有相同的键</em>是对齐的，但是元素<em>具有不同键</em>的窗口（通常）是不对齐的，因此在成本上分散了窗口完成负载，但这也使得跨键的比较变得不那么有意义。我们可以将我们的管道切换到使用新的窗口策略，如示例4-7所示。</p>
<p><em>示例4-7. 使用单个水位线触发器的非对齐固定窗口</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(UnalignedFixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(AfterWatermark()))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>

<p>然后，您可以通过比较与之前相同数据集的不同固定窗口对齐方式来查看图4-9的结果（在这种情况下，我选择了两个对齐之间的最大相位移，以最清楚地说明好处，因为在大量键的随机选择相位将产生类似的效果）。</p>
<div id="dplayer22" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer22"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0409.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图4-9. 非对齐的固定窗口</em></p>
<p>请注意，我们没有在同时发出多个键的多个窗格的情况。相反，窗格以更均匀的节奏单独到达。这是另一个例子，在用例允许的情况下，在一个维度上进行权衡（跨键比较的能力）以换取另一个维度上的好处（减少峰值资源预配要求）。当您试图尽可能高效地处理大量数据时，这种灵活性至关重要。</p>
<p>现在让我们看一下固定窗口的第二个变体，它与正在处理的数据更紧密地联系在一起。</p>
<hr>
<p><strong>每个元素&#x2F;键固定窗口</strong></p>
<p>我们的第二个例子来自于Cloud Dataflow的早期采用者之一。这家公司为其客户生成分析数据，但每个客户都可以配置其要聚合指标的窗口大小。换句话说，每个客户都可以定义其固定窗口的特定大小。</p>
<p>只要可用窗口大小的数量本身固定，支持这样的用例并不太困难。例如，您可以想象提供选择30分钟、60分钟和90分钟固定窗口的选项，然后为每个选项运行一个单独的管道（或管道分支）。虽然不是理想的解决方案，但也不是太糟糕。但是，随着选项数量的增加，这很快就变得难以处理，在提供支持真正任意的窗口大小的极限情况下（这是该客户的用例所需的），完全是不切实际的。</p>
<p>幸运的是，因为每个客户处理的记录已经用于描述聚合所需窗口大小的元数据进行了注释，因此支持任意的每个用户固定窗口大小就像从股票固定窗口实现中更改了几行代码一样简单，如示例4-8所示。</p>
<p><em>示例4-8。修改（和缩写）支持每个元素窗口大小的FixedWindows实现</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class PerElementFixedWindows&lt;T extends HasWindowSize%gt;</span><br><span class="line">    extends WindowFn&lt;T, IntervalWindow&gt; &#123;</span><br><span class="line">  private final Duration offset;</span><br><span class="line">  public Collection&lt;IntervalWindow&gt; assignWindow(AssignContext c) &#123;</span><br><span class="line">    long perElementSize = c.element().getWindowSize();</span><br><span class="line">    long start = perKeyShift + c.timestamp().getMillis()</span><br><span class="line">                   - c.timestamp()</span><br><span class="line">                      .plus(size)</span><br><span class="line">                      .minus(offset)</span><br><span class="line">                      .getMillis() % size.getMillis();</span><br><span class="line">    return Arrays.asList(IntervalWindow(</span><br><span class="line">        new Instant(start), perElementSize));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这个改变，每个元素都被分配到一个固定大小的窗口中，该大小由元素本身携带的元数据所规定。将管道代码更改为使用此新策略再次很简单，如示例4-9所示。</p>
<p><em>示例4-9。单个水位线触发器的每个元素固定窗口大小</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(PerElementFixedWindows.of(TWO_MINUTES))</span><br><span class="line">               .triggering(AfterWatermark()))</span><br><span class="line">  .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>然后查看此管道的实际执行情况（图4-10），很容易看出键A的元素都具有两分钟的窗口大小，而键B的元素具有一分钟的窗口大小。</p>
<div id="dplayer23" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer23"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0410.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图4-10。每个键的自定义大小固定窗口</em></p>
<p>这确实不是您可以合理期望系统为您提供的东西；窗口大小偏好存储的性质对于尝试构建到标准API中来说过于特定于用例，因此不合理。尽管如此，正如该客户的需求所展示的那样，这样的用例确实存在。这就是自定义分窗提供的灵活性如此强大的原因。</p>
<hr>
<h3><span id="会话窗口的变化">会话窗口的变化</span><a href="#会话窗口的变化" class="header-anchor">#</a></h3><p>为了真正体现自定义窗口的实用性，让我们看一个最后的例子，这是会话的变化。会话窗口比固定窗口更加复杂。它的实现包括以下内容：</p>
<ul>
<li><p>分配<br>每个元素最初被放置到一个原型会话窗口中，该窗口从元素的时间戳开始，并延伸到间隙持续时间。</p>
</li>
<li><p>合并<br>在分组时间，所有符合条件的窗口都被排序，之后任何重叠的窗口都被合并在一起。</p>
</li>
</ul>
<p>会话代码的简略版（手动从多个辅助类合并而来）看起来像示例4-10所示。</p>
<p><em>示例4-10。简略的会话实现</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Sessions extends WindowFn&lt;Object, IntervalWindow&gt; &#123;</span><br><span class="line">  private final Duration gapDuration;</span><br><span class="line">  public Collection&lt;IntervalWindow&gt; assignWindows(AssignContext c) &#123;</span><br><span class="line">    return Arrays.asList(</span><br><span class="line">    new IntervalWindow(c.timestamp(), gapDuration));</span><br><span class="line">  &#125;</span><br><span class="line">  public void mergeWindows(MergeContext c) throws Exception &#123;</span><br><span class="line">    List&lt;IntervalWindow&gt; sortedWindows = new ArrayList&lt;&gt;();</span><br><span class="line">    for (IntervalWindow window : c.windows()) &#123;</span><br><span class="line">      sortedWindows.add(window);</span><br><span class="line">    &#125;</span><br><span class="line">    Collections.sort(sortedWindows);</span><br><span class="line">    List&lt;MergeCandidate&gt; merges = new ArrayList&lt;&gt;();</span><br><span class="line">    MergeCandidate current = new MergeCandidate();</span><br><span class="line">    for (IntervalWindow window : sortedWindows) &#123;</span><br><span class="line">      if (current.intersects(window)) &#123;</span><br><span class="line">         current.add(window);</span><br><span class="line">       &#125; else &#123;</span><br><span class="line">         merges.add(current);</span><br><span class="line">         current = new MergeCandidate(window);</span><br><span class="line">      &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     merges.add(current);</span><br><span class="line">     for (MergeCandidate merge : merges) &#123;</span><br><span class="line">       merge.apply(c);</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>与以前一样，看代码的重点不在于教你如何实现自定义窗口函数，甚至不在于sessions的实现看起来像什么；它真正的目的是展示您可以通过自定义窗口轻松支持新用途。</p>
<hr>
<p><strong>有界会话</strong></p>
<p>我曾多次遇到的一个自定义用例是有界会话：这些会话不允许超过某个特定大小，无论是在时间、元素计数或其他维度上。这可能是由于语义原因，也可能只是一种垃圾邮件保护练习。然而，考虑到限制类型的变化（一些用例关心事件时间内的总会话大小，一些关心元素总数，一些关心元素密度等），为有界会话提供干净简洁的API很困难。让用户实现其自己的自定义窗口逻辑更加实用，以适应其特定的用例。一个这样的用例示例，其中会话窗口受时间限制，可能类似于示例4-11（省略我们将在此处使用的部分构建器样板）。</p>
<p><em>示例4-11。缩写会话实现</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class BoundedSessions extends WindowFn&lt;Object, IntervalWindow&gt; &#123;</span><br><span class="line">  private final Duration gapDuration;</span><br><span class="line">  private final Duration maxSize;</span><br><span class="line">  public Collection&lt;IntervalWindow&gt; assignWindows(AssignContext c) &#123;</span><br><span class="line">    return Arrays.asList(</span><br><span class="line">      new IntervalWindow(c.timestamp(), gapDuration));</span><br><span class="line">  &#125;</span><br><span class="line">  private Duration windowSize(IntervalWindow window) &#123;</span><br><span class="line">    return window == null</span><br><span class="line">      ? new Duration(0)</span><br><span class="line">      : new Duration(window.start(), window.end());</span><br><span class="line">  &#125;</span><br><span class="line">  public static void mergeWindows(</span><br><span class="line">    WindowFn&lt;?, IntervalWindow&gt;.MergeContext c) throws Exception &#123;</span><br><span class="line">    List&lt;IntervalWindow&gt; sortedWindows = new ArrayList&lt;&gt;();</span><br><span class="line">    for (IntervalWindow window : c.windows()) &#123;</span><br><span class="line">      sortedWindows.add(window);</span><br><span class="line">    &#125;</span><br><span class="line">    Collections.sort(sortedWindows);</span><br><span class="line">    List&lt;MergeCandidate&gt; merges = new ArrayList&lt;&gt;();</span><br><span class="line">    MergeCandidate current = new MergeCandidate();</span><br><span class="line">    for (IntervalWindow window : sortedWindows) &#123;</span><br><span class="line">      MergeCandidate next = new MergeCandidate(window);</span><br><span class="line">      if (current.intersects(window)) &#123;</span><br><span class="line">        current.add(window);</span><br><span class="line">        if (windowSize(current.union) &lt;= (maxSize - gapDuration))</span><br><span class="line">           continue;</span><br><span class="line">        // Current window exceeds bounds, so flush and move to next</span><br><span class="line">        next = new MergeCandidate();</span><br><span class="line">       &#125;</span><br><span class="line">       merges.add(current);</span><br><span class="line">       current = next;</span><br><span class="line">     &#125;</span><br><span class="line">     merges.add(current);</span><br><span class="line">     for (MergeCandidate merge : merges) &#123;</span><br><span class="line">       merge.apply(c);</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>像以前一样，将我们的管道（在本例中是早期&#x2F;准时&#x2F;延迟版本的Example2-7）更新为使用此自定义窗口策略是微不足道的，如示例4-12所示。</p>
<p><em>示例4-12。通过早期&#x2F;准时&#x2F;延迟API进行早期、准时和延迟触发</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PCollection&lt;KV&lt;Team, Integer&gt;&gt; totals = input</span><br><span class="line">  .apply(Window.into(BoundedSessions</span><br><span class="line">                       .withGapDuration(ONE_MINUTE)</span><br><span class="line">                       .withMaxSize(THREE_MINUTES))</span><br><span class="line">               .triggering(</span><br><span class="line">                 AfterWatermark()</span><br><span class="line">                   .withEarlyFirings(AlignedDelay(ONE_MINUTE))</span><br><span class="line">                   .withLateFirings(AfterCount(1))))</span><br><span class="line">   .apply(Sum.integersPerKey());</span><br></pre></td></tr></table></figure>
<p>并在我们的运行示例上执行，它可能看起来像图4-11。</p>
<div id="dplayer24" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer24"),"loop":"yes","screenshot":"yes","video":{"url":"/www6vHomeHexo/2000/03/14/streamingSystemChapter4/stsy_0411.mp4"},"danmaku":{"api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> 
<p><em>图4-11。每个键的自定义大小的固定窗口</em></p>
<p>请注意，值为36且跨越[12:00.26,12:05.20)的大会话，在未经界限的会话实现中从图2-7中现在被分成两个较短的长度为2分钟和2分53秒的会话。</p>
<p>鉴于目前很少有系统提供自定义窗口支持，值得指出的是，如果使用仅支持无界会话实现的系统来实现这样的东西，将需要更多的工作量。您唯一的真正手段就是编写会话分组逻辑之后的代码，该代码查看生成的会话并在超过长度限制时将其切分。这将需要在事后分解会话的能力，这将取消增量聚合的好处（我们在第7章中更详细地查看），增加成本。它还会消除任何希望通过限制会话长度获得的垃圾邮件保护好处，因为会话首先需要增长到其完整大小，然后才能被切割或截断。</p>
<h3><span id="非一刀切">非一刀切</span><a href="#非一刀切" class="header-anchor">#</a></h3><p>我们现在已经看过三个现实世界的用例，每个用例都是数据处理系统通常提供的窗口类型的微妙变化：不对齐的固定窗口、每个元素的固定窗口和有界会话。在所有三种情况下，我们都看到了通过自定义窗口支持这些用例的简单性，以及在没有它的情况下支持这些用例需要多么困难（或昂贵）。虽然目前自定义窗口在整个行业中尚未得到广泛支持，但它是一项在构建需要尽可能高效地处理大量数据的复杂实际用例的数据处理管道时提供所需灵活性的功能。</p>
<h1><span id="摘要">摘要</span><a href="#摘要" class="header-anchor">#</a></h1><p>高级窗口是一个复杂而多样化的主题。 在本章中，我们涵盖了三个高级概念：</p>
<ul>
<li><p>处理时间窗口<br>我们看到这与事件时间窗口有关，强调了它在本质上有用的地方，并且最重要的是，通过特别强调事件时间窗口提供的结果稳定性来确定那些不行的地方。</p>
</li>
<li><p>会话窗口<br>我们首次介绍了合并窗口策略的动态类，并看到系统为我们提供了一个如此强大的结构，您可以简单地将其放置在适当的位置。</p>
</li>
<li><p>自定义窗口<br>在这里，我们看了三个现实世界的自定义窗口示例，这些示例在仅提供静态存储窗口策略的系统中难以实现或不可能实现，但在支持自定义窗口的系统中实现相对容易：</p>
<ul>
<li><em>不对齐的固定窗口</em>，当与固定窗口一起使用水位线触发器时，可以提供更均匀的输出时间分布。</li>
<li><em>每个元素固定窗口</em>，可以提供灵活性，动态选择每个元素的固定窗口大小（例如，提供可定制的每个用户或每个广告活动窗口大小），以更好地定制管道语义以适应所涉及的用例。</li>
<li><em>有界会话窗口</em>，限制给定会话的大小增长；例如，为了抵消垃圾邮件尝试或将延迟置于由管道实现的已完成会话上。</li>
</ul>
</li>
</ul>
<p>在与Slava深入水位线的第3章并在此进行高级窗口的广泛调查后，我们已经超越了多个维度中强大的流处理的基础知识。 随着这一点，我们结束了对Beam模型的专注，因此结束了本书的第一部分。</p>
<p>接下来是Reuven的第5章，介绍一致性保证，精确一次处理和副作用，之后我们开始进入第二部分“流和表格”，第6章。</p>
<hr>
<ol>
<li>据我所知，Apache Flink是唯一支持自定义窗口的系统，其程度达到了Beam的程度。 公平地说，由于可以提供自定义窗口驱逐器的能力，因此其支持甚至超过了Beam的支持。 头爆炸。</li>
<li>我实际上并不知道此类系统目前是否存在。</li>
<li>这自然意味着使用键入数据，但是因为窗口与按键分组密切相关，所以该限制并不特别繁重。</li>
<li>事实上，元素本身并不一定知道窗口大小；您可以轻松查找并缓存适当的窗口大小，以获取所需的维度；例如，按用户或按用户缓存适当的窗口大小。</li>
</ol>
<h2><span id="draft-here">Draft Here</span><a href="#draft-here" class="header-anchor">#</a></h2>]]></content>
      <categories>
        <category>Streaming System</category>
      </categories>
      <tags>
        <tag>Streaming System</tag>
      </tags>
  </entry>
</search>
