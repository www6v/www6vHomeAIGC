<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AIGC 汇总</title>
    <url>/www6vHomeAIGC/2023/11/16/gptSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#basic">Basic</a></li>
<li><a href="#deeplearning">DeepLearning</a></li>
<li><a href="#nlp">NLP</a></li>
<li><a href="#model">Model *</a></li>
<li><a href="#training">Training *</a></li>
<li><a href="#inference">Inference *</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#finetuning">FineTuning *</a></li>
<li><a href="#%E5%9E%82%E7%B1%BB%E6%A8%A1%E5%9E%8B">垂类模型</a></li>
<li><a href="#llops">LLOps</a></li>
<li><a href="#mlsys">MLSys</a></li>
<li><a href="#prompt">Prompt</a></li>
<li><a href="#langchain">Langchain</a></li>
<li><a href="#study">Study</a></li>
<li><a href="#research">Research</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2021/08/11/ai/" title="AI 应用场景">AI 应用场景</a> </li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiOverview/" title="人工智能 知识点">人工智能 知识点</a></li>
<li><a href="/www6vHomeAIGC/2022/06/07/aiMachineLearning/" title="Machine Learning">Machine Learning</a></li>
</ul>
<h2><span id="deeplearning">DeepLearning</span><a href="#deeplearning" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2022/06/11/aiDeepLearning/" title="Deep Learning">Deep Learning</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/03/28/gptPytorch/" title="(实战)PyTorch">(实战)PyTorch</a></strong></li>
</ul>
<h2><span id="nlp">NLP</span><a href="#nlp" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/05/gptNLPTask/" title="NLP &amp; LLM">NLP &amp; LLM</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/02/18/gptDocSimilarity/" title="短文本相似度">短文本相似度</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/28/gptDialogue/" title="多轮对话">多轮对话</a></li>
</ul>
<h2><span id="model">Model *</span><a href="#model" class="header-anchor">#</a></h2><ul>
<li>Backbone <ul>
<li><strong><a href="/www6vHomeAIGC/2022/11/30/gptTransformer/" title="(原理)Transformer">(原理)Transformer</a></strong> </li>
<li><a href="/www6vHomeAIGC/2023/02/16/gptTransformerCode/" title="(实战)Transformer">(实战)Transformer</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/18/gptEmbedding/" title="(原理)Embedding">(原理)Embedding</a></li>
</ul>
</li>
<li>Foundation Models<ul>
<li><a href="/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/" title="(综述)大模型">(综述)大模型</a></li>
<li><a href="/www6vHomeAIGC/2023/02/17/gptLargeModel/" title="大模型">大模型</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/11/gptFamily/" title="GPT 系列">GPT 系列</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/06/gptChatGLM/" title="ChatGLM">ChatGLM</a>    </li>
<li>LLaMA<ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/01/gptLlama/" title="LLaMA">LLaMA</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/" title="LLaMA 家族">LLaMA 家族</a>   </li>
<li><strong><a href="/www6vHomeAIGC/2024/09/04/gptLlama3-1/" title="Llama3.1">Llama3.1</a></strong></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/01/04/gptLeaderBoard/" title="大模型 排行榜">大模型 排行榜</a></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/" title="(原理)不可能三角">(原理)不可能三角</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptEmergent/" title="(原理)涌现现象">(原理)涌现现象</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/06/gptHallucination/" title="(原理)幻觉问题">(原理)幻觉问题</a>    </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptEval/" title="测评">测评</a></li>
</ul>
<h2><span id="training">Training  *</span><a href="#training" class="header-anchor">#</a></h2><ul>
<li>训练<ul>
<li><a href="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/" title="(原理)Training">(原理)Training</a></li>
<li><strong><a href="/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/" title="(实战)Pre-Training">(实战)Pre-Training</a></strong> </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptContinualPretraining/" title="(原理|实战)继续Pre-Training">(原理|实战)继续Pre-Training</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/21/gptChineseLlama/" title="(实战)Chinese-LLaMA PT+SFT">(实战)Chinese-LLaMA PT+SFT</a>   </li>
<li>分布式 <ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/" title="(原理)分布式训练">(原理)分布式训练</a></strong>     </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainPipelineParallelism/" title="(原理|实战)流水线并行(PP)">(原理|实战)流水线并行(PP)</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainTensorParallelism/" title="(原理)张量并行(TP)">(原理)张量并行(TP)</a></strong></li>
<li>DP<ul>
<li><strong><a href="/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/" title="(原理) Deepspeed Zero">(原理) Deepspeed Zero</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/03/25/gptTrainDistributedPractice/" title="(实战)DeepSpeed Training">(实战)DeepSpeed Training</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/09/19/gptTrainDDP/" title="(原理|实战)DDP">(原理|实战)DDP</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/19/gptTrainFSDP/" title="(原理|实战)FSDP">(原理|实战)FSDP</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>低精度<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/16/gptLowPrecision/" title="低精度训练">低精度训练</a></strong>    </li>
<li><strong><a href="/www6vHomeAIGC/2024/02/01/gptPrecision/" title="(原理|实战)混合精度">(原理|实战)混合精度</a></strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="inference">Inference *</span><a href="#inference" class="header-anchor">#</a></h2><ul>
<li>框架<ul>
<li><a href="/www6vHomeAIGC/2023/03/21/gptInferFramework/" title="(原理)推理-框架">(原理)推理-框架</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/" title="(实战)推理-lmdeploy">(实战)推理-lmdeploy</a> </li>
<li>vLLM<ul>
<li><strong><a href="/www6vHomeAIGC/2023/05/31/gptInfervLLM/" title="(原理) vLLM">(原理) vLLM</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/" title="(实战) vLLM">(实战) vLLM</a></strong></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/06/02/gptInferTensorRT/" title="(原理|实战)推理 TensorRT-LLM">(原理|实战)推理 TensorRT-LLM</a> </li>
<li>Ray<ul>
<li><a href="/www6vHomeAIGC/2023/06/11/gptInferRay/" title="(原理)推理 Ray">(原理)推理 Ray</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/16/gptInferRayPractice/" title="(实战)推理 Ray">(实战)推理 Ray</a></li>
</ul>
</li>
</ul>
</li>
<li>优化<ul>
<li><a href="/www6vHomeAIGC/2023/01/01/gptInference/" title="(总结)推理优化">(总结)推理优化</a></li>
<li><strong><a href="/www6vHomeAIGC/2023/08/14/gptInferenceSurvey/" title="(综述)推理优化">(综述)推理优化</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2024/09/11/gptInferenceSurvey1/" title="(综述)推理优化">(综述)推理优化</a></strong> </li>
<li>系统层优化<ul>
<li>KVCache<ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/01/gptInferKVCache/" title="(原理) KV Cache">(原理) KV Cache</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheOptimize/" title="(原理)KV Cache 优化">(原理)KV Cache 优化</a></strong></li>
<li>Compress  <ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheStreamingLLM/" title="Streaming LLM">Streaming LLM</a></strong>   Window </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheQuantization/" title="KV Cache 量化">KV Cache 量化</a></strong> Quantization</li>
</ul>
</li>
</ul>
</li>
<li>FlashAttention    <ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/13/gptFlashAttention/" title="(原理)Flash Attention">(原理)Flash Attention</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/06/gptInferFlashDecoding/" title="(原理)Flash Decoding">(原理)Flash Decoding</a></strong></li>
</ul>
</li>
<li>调度<ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/18/gptInferContinuousBatching/" title="Continuous Batching">Continuous Batching</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/18/gptInferChunkedPrefill/" title="Chunked Prefill">Chunked Prefill</a></strong></li>
</ul>
</li>
<li>PD 分离<ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/05/gptInferDistServe/" title="DistServe">DistServe</a></strong></li>
</ul>
</li>
<li><strong><a href="/www6vHomeAIGC/2023/10/06/gptInferSpeculativeDecoding/" title="Speculative Decoding">Speculative Decoding</a></strong></li>
</ul>
</li>
<li>模型层优化 <ul>
<li>模型压缩<ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/19/gptQuantization/" title="(原理)量化">(原理)量化</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/" title="(原理)PTQ-Weight Only">(原理)PTQ-Weight Only</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/" title="(实战)量化">(实战)量化</a></strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>其他<ul>
<li><strong><a href="/www6vHomeAIGC/2023/03/30/gptTemperature/" title="推理常见参数">推理常见参数</a></strong></li>
</ul>
</li>
</ul>
<h2><span id="data">Data</span><a href="#data" class="header-anchor">#</a></h2><ul>
<li>List<ul>
<li><a href="/www6vHomeAIGC/2023/01/08/gptDataSet/" title="(list)数据集">(list)数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/" title="(List) Pretrain 数据集">(List) Pretrain 数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/" title="(List)SFT数据集">(List)SFT数据集</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/" title="(survey)多模态  数据集">(survey)多模态  数据集</a></li>
</ul>
</li>
<li>DataProcess<ul>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/" title="(Survey)Dataset">(Survey)Dataset</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/05/gptDataProcess/" title="(Survey)数据处理">(Survey)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/" title="(实战)数据处理">(实战)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/" title="(原理|实战)Data  Annotation">(原理|实战)Data  Annotation</a></li>
</ul>
</li>
<li>Data Management<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataManagement/" title="(Survey)Data Management">(Survey)Data Management</a>  </li>
<li>Pretrain  <ul>
<li><a href="/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/" title="(质量过滤)RefinedWeb, Textbooks">(质量过滤)RefinedWeb, Textbooks</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/" title="Tokenizer">Tokenizer</a></li>
</ul>
</li>
<li>SFT <ul>
<li>Data Quality<ul>
<li>Instruction Quality<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/" title="(原理)LIMA, LESS">(原理)LIMA, LESS</a></li>
</ul>
</li>
<li>Instruction Diversity<ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/21/gptSelfInstruct/" title="(原理)SELF-INSTRUCT">(原理)SELF-INSTRUCT</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/27/gptDataSelfQA/" title="(原理|实战)Self-QA">(原理|实战)Self-QA</a></strong></li>
</ul>
</li>
<li>Instruction Complexity  <ul>
<li><a href="/www6vHomeAIGC/2023/03/18/gptDataWizard/" title="(原理)Wizard">(原理)Wizard</a></li>
</ul>
</li>
</ul>
</li>
<li>Task composition<ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/06/gptDatasetSFT/" title="(原理)SFT 数据组合">(原理)SFT 数据组合</a></strong></li>
</ul>
</li>
</ul>
<ul>
<li><strong><a href="/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/" title="(原理)SFT Scaling">(原理)SFT Scaling</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/05/05/gptDataSelection/" title="(原理)Data Selection">(原理)Data Selection</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="finetuning">FineTuning *</span><a href="#finetuning" class="header-anchor">#</a></h2><ul>
<li>PEFT<ul>
<li><strong><a href="/www6vHomeAIGC/2022/11/18/gptFineTuning/" title="(原理)PEFT">(原理)PEFT</a></strong> </li>
<li><a href="/www6vHomeAIGC/2022/12/28/gptFineTuningWhen/" title="(原理)Fine-Tuning 时机">(原理)Fine-Tuning 时机</a>  </li>
<li><a href="/www6vHomeAIGC/2022/12/20/gptFineTuningPEFT/" title="(实战)PEFT 概述">(实战)PEFT 概述</a></li>
</ul>
</li>
<li>Soft Prompt<ul>
<li><a href="/www6vHomeAIGC/2023/01/06/gptPromptTuning/" title="(原理)Prompt Tuning">(原理)Prompt Tuning</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/" title="P-Tuning">P-Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2024/01/28/gptPEFTPtuningPractice/" title="(实战)PEFT P-Tuning">(实战)PEFT P-Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/25/gptPromptTuningPractice/" title="(实战)PromptTuning">(实战)PromptTuning</a></li>
</ul>
</li>
<li>Lora<ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/05/gptPEFTLora/" title="(实战) Lora">(实战) Lora</a></strong> </li>
<li><a href="/www6vHomeAIGC/2024/01/12/gptPEFTQLora/" title="(实战)PEFT QLoRA">(实战)PEFT QLoRA</a></li>
</ul>
</li>
<li>Instruct Tuning *<ul>
<li><a href="/www6vHomeAIGC/2023/01/06/gptInstructTuning/" title="(原理)Instruct Tuning">(原理)Instruct Tuning</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/12/gptInstructTuningSurvey/" title="(Survey)Instruct Tuning">(Survey)Instruct Tuning</a></li>
</ul>
</li>
<li>BERT<ul>
<li><a href="/www6vHomeAIGC/2024/01/26/gptFineTuningBert/" title="Fine Tuning-Bert">Fine Tuning-Bert</a></li>
</ul>
</li>
</ul>
<h2><span id="垂类模型">垂类模型</span><a href="#垂类模型" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/04/gptDomain/" title="垂类大模型">垂类大模型</a></strong> </li>
<li><a href="/www6vHomeAIGC/2022/11/24/gptDomainFinance/" title="金融大模型">金融大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptDomainMed/" title="医疗大模型">医疗大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2024/02/07/gptDomainLaw/" title="法律大模型">法律大模型</a></li>
</ul>
<h2><span id="llops">LLOps</span><a href="#llops" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/" title="LLama-Factory">LLama-Factory</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/07/01/gptGPUComputing/" title="显存估算">显存估算</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/26/gptLLMOpsPaaS/" title="LLM PaaS">LLM PaaS</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/05/23/gptGPU/" title="GPU 算力">GPU 算力</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/28/gptLLMOps/" title="LLMOps">LLMOps</a></li>
</ul>
<h2><span id="mlsys">MLSys</span><a href="#mlsys" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainCommunication/" title="(原理)通信原语">(原理)通信原语</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/01/gptParameterServer/" title="(原理|实战)Parameter Server">(原理|实战)Parameter Server</a></strong></li>
</ul>
<h2><span id="prompt">Prompt</span><a href="#prompt" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/11/10/gptPromptEngineering/" title="(原理)Prompt Engineering">(原理)Prompt Engineering</a></li>
<li><a href="/www6vHomeAIGC/2023/02/08/gptCOT/" title="COT">COT</a> </li>
<li><a href="/www6vHomeAIGC/2021/05/28/gptPromptCode/" title="Prompt-Code">Prompt-Code</a></li>
<li><a href="/www6vHomeAIGC/2021/05/26/gptPrompt/" title="Prompt-How to use">Prompt-How to use</a></li>
</ul>
<h2><span id="langchain">Langchain</span><a href="#langchain" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/11/02/gptLangchain/" title="Langchain">Langchain</a></li>
<li><a href="/www6vHomeAIGC/2022/12/31/gptRetrievers/" title="Retrievers">Retrievers</a> </li>
<li><a href="/www6vHomeAIGC/2023/01/11/gptLangchainAgent/" title="Langchain  Agent">Langchain  Agent</a></li>
</ul>
<h2><span id="study">Study</span><a href="#study" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/08/01/gptStudy/" title="GPT  学习资源">GPT  学习资源</a></li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiStudyResouce/" title="人工智能-学习资源">人工智能-学习资源</a></li>
</ul>
<h2><span id="research">Research</span><a href="#research" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2024/02/11/gptPaperTools/" title="科研-工具">科研-工具</a></strong> </li>
<li><a href="/www6vHomeAIGC/2023/01/20/gptStudyPaper/" title="GPT 论文">GPT 论文</a></li>
<li><a href="/www6vHomeAIGC/2023/02/25/gptSurveyList/" title="Survey List">Survey List</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/04/gptAgentPaper/" title="Paper-Agent">Paper-Agent</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision 汇总</title>
    <url>/www6vHomeAIGC/2023/07/23/gptVisionSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#multimodal">Multimodal *</a><ul>
<li><a href="#survey">Survey</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3">视觉理解</a></li>
<li><a href="#%E7%94%9F%E6%88%90">生成</a></li>
<li><a href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83llmlmm">端到端训练LLM(LMM)</a></li>
<li><a href="#multimodal-agent">Multimodal Agent*</a></li>
</ul>
</li>
<li><a href="#vision">Vision</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="multimodal">Multimodal *</span><a href="#multimodal" class="header-anchor">#</a></h1><h3><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a></li>
<li><a href="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/" title="多模态 系列">多模态 系列</a></li>
</ul>
<h3><span id="视觉理解">视觉理解</span><a href="#视觉理解" class="header-anchor">#</a></h3><ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/02/gptMultimodalEncoder/" title="Vision Encoder">Vision Encoder</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/05/gptMultimodalConnector/" title="(原理)Connector">(原理)Connector</a></strong>  	</li>
<li>Segmentation<br>+ <a href="/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/" title="(原理|实战) SAM">(原理|实战) SAM</a></li>
</ul>
<h3><span id="生成">生成</span><a href="#生成" class="header-anchor">#</a></h3><ul>
<li>Diffusion<ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/" title="(原理)Diffusion">(原理)Diffusion</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/" title="(实战)Diffusion">(实战)Diffusion</a>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/01/gptDiffusionXL/" title="(原理)SD XL">(原理)SD XL</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/27/gptDiffusionunCLIP/" title="(原理)unCLIP">(原理)unCLIP</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/01/gptDiffusionGuidance/" title="(原理)Guidance">(原理)Guidance</a></strong></li>
</ul>
</li>
<li>Controllable  <ul>
<li><strong><a href="/www6vHomeAIGC/2023/07/29/gptDiffusionControllable/" title="(综述)Controllable">(综述)Controllable</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/07/17/gptDiffusionControllableWork/" title="(Work|实战)Controllable">(Work|实战)Controllable</a></strong></li>
<li>Spatial Control<ul>
<li>Dense control<ul>
<li>single<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionControlNet/" title="(原理|实战)ControlNet">(原理|实战)ControlNet</a></strong></li>
</ul>
</li>
<li>multi <ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionT2IAdapter/" title="(原理|实战)T2I-Adapter">(原理|实战)T2I-Adapter</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>layout&#x2F;box<ul>
<li>GLIGEN</li>
<li>Reco</li>
</ul>
</li>
</ul>
</li>
<li>Style control<ul>
<li>subject-driven<ul>
<li>concept customization [fine-tuning]<ul>
<li><strong><a href="/www6vHomeAIGC/2023/07/06/gptDiffusionFineTuning/" title="(work|实战) fine-tuning">(work|实战) fine-tuning</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2024/07/17/gptDiffusionDreamBooth/" title="(原理|实战)DreamBooth">(原理|实战)DreamBooth</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>image-driven<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionIPAdapter/" title="(原理|实战)IP-Adapter">(原理|实战)IP-Adapter</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>Sementic Control</li>
<li>其他<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionReferenceNet/" title="(原理|实战)ReferenceNet">(原理|实战)ReferenceNet</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>editing<ul>
<li><a href="/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/" title="(综述)Image Editing">(综述)Image Editing</a>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/07/27/gptDiffusionImageEditWork/" title="(Work|实战)Image Editing">(Work|实战)Image Editing</a></strong></li>
</ul>
</li>
<li>人像生图<ul>
<li><strong><a href="/www6vHomeAIGC/2024/08/03/gptMultimodalIDCreate/" title="人像生图">人像生图</a></strong></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/07/21/gptDiffusionDiT/" title="(原理|实战)DiT">(原理|实战)DiT</a></li>
</ul>
<h3><span id="端到端训练llmlmm">端到端训练LLM(LMM)</span><a href="#端到端训练llmlmm" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/" title="(图生文)BLIP-2, Flamingo">(图生文)BLIP-2, Flamingo</a> </li>
<li><strong><a href="/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/" title="(原理|实战) LLaVa 演化">(原理|实战) LLaVa 演化</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/" title="(原理|实战)MiniGPT4">(原理|实战)MiniGPT4</a>    </li>
<li>Train  *<ul>
<li><a href="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/" title="(原理)多模态预训练 概述">(原理)多模态预训练 概述</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/" title="(综述)多模态InstructTuning">(综述)多模态InstructTuning</a></li>
</ul>
</li>
</ul>
<h3><span id="multimodal-agent">Multimodal Agent*</span><a href="#multimodal-agent" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/" title="(原理)Agent 多模态">(原理)Agent 多模态</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/05/gptAgentWeb/" title="(原理)Web Agent">(原理)Web Agent</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/" title="Agent - UI-assistants">Agent - UI-assistants</a></li>
</ul>
<h1><span id="vision">Vision</span><a href="#vision" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeAIGC/2023/07/25/gptVisionTask/" title="CV 任务">CV 任务</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC应用 汇总</title>
    <url>/www6vHomeAIGC/2022/07/23/gptAppSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#rag">RAG *</a></li>
<li><a href="#agent">Agent *</a></li>
<li><a href="#application">Application</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="rag">RAG *</span><a href="#rag" class="header-anchor">#</a></h2><ul>
<li>Overview<ul>
<li><strong><a href="/www6vHomeAIGC/2022/11/02/gptRAG/" title="(综述)RAG">(综述)RAG</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/04/21/gptRAGModularRAG/" title="(原理)Modular RAG">(原理)Modular RAG</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2022/12/07/gptRAGPerformance/" title="(原理)Advanced RAG">(原理)Advanced RAG</a></strong></li>
<li><a href="/www6vHomeAIGC/2023/06/07/gptRAGEval/" title="RAG 评估">RAG 评估</a> </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGKG/" title="RAG KG">RAG KG</a></li>
</ul>
</li>
<li>实战<ul>
<li><a href="/www6vHomeAIGC/2022/12/31/gptRAGPractice/" title="(实战)RAG">(实战)RAG</a></li>
<li><a href="/www6vHomeAIGC/2023/05/09/gptRAGOptimize/" title="RAG 优化">RAG 优化</a></li>
<li>framework<ul>
<li><a href="/www6vHomeAIGC/2023/05/09/gptRAGFramework/" title="RAG Framework">RAG Framework</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/31/gptRAGchatchat/" title="(实战)RAG Langchain-Chatchat">(实战)RAG Langchain-Chatchat</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGQanything/" title="RAG Qanything">RAG Qanything</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/19/gptRAGRAGflow/" title="RAG RAGflow">RAG RAGflow</a></li>
</ul>
</li>
</ul>
</li>
<li>案例 <ul>
<li><a href="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/" title="(原理)RAG OpenAI案例">(原理)RAG OpenAI案例</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/18/gptRAGBaichuan/" title="(原理)RAG Baichuan案例">(原理)RAG Baichuan案例</a></li>
</ul>
</li>
<li>phase <ul>
<li><a href="/www6vHomeAIGC/2023/04/20/gptQueryTransformation/" title="(原理|实战)Query Transformation">(原理|实战)Query Transformation</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRouting/" title="(原理|实战)Query Routing">(原理|实战)Query Routing</a> </li>
<li><strong><a href="/www6vHomeAIGC/2023/05/21/gptRAGIndex/" title="(原理|实战)RAG Index">(原理|实战)RAG Index</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/05/14/gptRAGRerank/" title="(原理|实战)RAG Rerank">(原理|实战)RAG Rerank</a> </li>
<li>Agentic RAG<ul>
<li><a href="/www6vHomeAIGC/2023/06/25/gptAgenticRAG/" title="Agentic RAG">Agentic RAG</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/02/gptRAGSelfReflective/" title="(原理|实战)Self-Reflective RAG">(原理|实战)Self-Reflective RAG</a></li>
</ul>
</li>
</ul>
</li>
<li>Multimodal RAG  *<ul>
<li><a href="/www6vHomeAIGC/2023/03/14/gptRAGMultimodal/" title="(原理)多模态 RAG">(原理)多模态 RAG</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/14/gptRAGMultimodalPractice/" title="(实战)多模态 RAG">(实战)多模态 RAG</a>   </li>
<li><a href="/www6vHomeAIGC/2023/04/19/gptDocumentAI/" title="文档智能">文档智能</a></li>
</ul>
</li>
</ul>
<h2><span id="agent">Agent *</span><a href="#agent" class="header-anchor">#</a></h2><ul>
<li>Overview<ul>
<li><strong><a href="/www6vHomeAIGC/2022/11/02/gptAgent/" title="(原理)Agent">(原理)Agent</a></strong></li>
<li><a href="/www6vHomeAIGC/2023/04/06/gptAgentCategory/" title="Agent 分类[有趣|有用]">Agent 分类[有趣|有用]</a></li>
<li><a href="/www6vHomeAIGC/2023/01/01/gptAgentPractice/" title="(实战)Agent">(实战)Agent</a> </li>
<li><a href="/www6vHomeAIGC/2023/05/13/gptAgentChallenge/" title="(原理)Agent Challenge">(原理)Agent Challenge</a></li>
</ul>
</li>
<li>Project&amp;Product<ul>
<li><a href="/www6vHomeAIGC/2023/03/05/gptAgentList/" title="(List)Agent 开源 产品 平台">(List)Agent 开源 产品 平台</a></li>
</ul>
</li>
<li>Reflection<ul>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentReflection/" title="Reflection Agent">Reflection Agent</a></li>
</ul>
</li>
<li>Planning<ul>
<li><a href="/www6vHomeAIGC/2023/05/13/gptAgentPlanning/" title="Agent Planning">Agent Planning</a>     </li>
<li><a href="/www6vHomeAIGC/2023/03/02/gptAgentPlanAndExecute/" title="(原理|实战)Plan&amp;Execute,ReWOO">(原理|实战)Plan&amp;Execute,ReWOO</a></li>
</ul>
</li>
<li>Memory<ul>
<li><a href="/www6vHomeAIGC/2023/06/05/gptAgentMemory/" title="Agent  Memory">Agent  Memory</a></li>
</ul>
</li>
<li>Multi-agent collaboration<ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/21/gptMultiAgents/" title="(原理)Multi-Agents">(原理)Multi-Agents</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/05/07/gptMultiAgentsPractice/" title="(实战)LangGraph">(实战)LangGraph</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/05/gptAgentAutogen/" title="AutoGen">AutoGen</a></li>
</ul>
</li>
<li>Tool use  *<ul>
<li><a href="/www6vHomeAIGC/2022/11/16/gptFunctionCall/" title="(原理|实战) OpenAI Function Call">(原理|实战) OpenAI Function Call</a> </li>
<li><a href="/www6vHomeAIGC/2023/01/27/gptAgentTool/" title="(原理)Agent-Tools">(原理)Agent-Tools</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/" title="(原理)Gorilla">(原理)Gorilla</a>   </li>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentTuning/" title="Agent Tuning">Agent Tuning</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptAgentToolformer/" title="(原理)Toolformer">(原理)Toolformer</a></li>
</ul>
</li>
</ul>
<h2><span id="application">Application</span><a href="#application" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/05/09/gpt/" title="GPT-工具和应用">GPT-工具和应用</a></li>
<li><a href="/www6vHomeAIGC/2022/11/27/gptVectorStore/" title="向量数据库">向量数据库</a></li>
<li><a href="/www6vHomeAIGC/2023/01/03/gptNL2SQL/" title="NL2SQL">NL2SQL</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)推理优化</title>
    <url>/www6vHomeAIGC/2024/09/11/gptInferenceSurvey1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Towards-Efficient-Generative-Large-Language-Model-Serving-A-Survey-from-Algorithms-to-Systems-c1914500c33f4446ac7fbe8848354d91?pvs=4">Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Llama3.1</title>
    <url>/www6vHomeAIGC/2024/09/04/gptLlama3-1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="llama31">Llama3.1</span><a href="#llama31" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLaMA3-1-c2124d23077241afa9d92eb9a54a043a?pvs=4">Llama3.1</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>人像生图</title>
    <url>/www6vHomeAIGC/2024/08/03/gptMultimodalIDCreate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="instantid">InstantID</span><a href="#instantid" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><strong>InstantID</strong> 小红书</p>
</li>
<li><p>开源地址<br><a href="https://github.com/InstantID/InstantID">InstantID</a></p>
</li>
<li><p>Project page<br><a href="https://instantid.github.io/">Project page</a></p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/InstantID-0b96b4a8d12340a1900995ca7f33ef05?pvs=4">InstantID</a></p>
</li>
</ul>
<hr>
<h1><span id="photomaker">PhotoMaker</span><a href="#photomaker" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br>PhotoMaker  腾讯</p>
</li>
<li><p>开源地址<br><a href="https://github.com/TencentARC/PhotoMaker">Repo git</a> git</p>
</li>
<li><p>Project page<br><a href="https://photo-maker.github.io/">Project page</a>   </p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/PhotoMaker-f4b3e96a9ed046838b7255e026bd1abf?pvs=4">解析</a></p>
</li>
</ul>
<hr>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>【InstantID : ipAdaptor +controlnet,  image Contoll的思路】<br>【photomaker: image  Edit 的思路】</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DreamBooth</title>
    <url>/www6vHomeAIGC/2024/07/17/gptDiffusionDreamBooth/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dreambooth">DreamBooth</span><a href="#dreambooth" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DreamBooth-d591acda472a49c7a189009a53addd24?pvs=4">(原理|实战)DreamBooth</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态 系列</title>
    <url>/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#stage1-%E6%A8%A1%E5%9D%97%E7%8B%AC%E7%AB%8B2">Stage1: 模块独立[2]</a><ul>
<li><a href="#model">model</a></li>
</ul>
</li>
<li><a href="#stage2-%E6%A8%A1%E5%9D%97%E5%85%B1%E4%BA%AB2">Stage2: 模块共享[2]</a><ul>
<li><a href="#model-1">model</a></li>
</ul>
</li>
<li><a href="#stage3-%E8%8C%83%E5%BC%8F%E7%BB%9F%E4%B8%802">Stage3: 范式统一[2]</a><ul>
<li><a href="#model-2">model</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93-1">总结 [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#overview">Overview</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="stage1-模块独立2">Stage1: 模块独立[2]</span><a href="#stage1-模块独立2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/stage1.webp" class>

<h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>CLIP</li>
<li>ViLT</li>
<li>ALBEF</li>
</ul>
<h1><span id="stage2-模块共享2">Stage2: 模块共享[2]</span><a href="#stage2-模块共享2" class="header-anchor">#</a></h1><h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>VLMO</li>
<li>BLIP</li>
<li>BLIP2</li>
<li>BEiTv3</li>
</ul>
<h1><span id="stage3-范式统一2">Stage3: 范式统一[2]</span><a href="#stage3-范式统一2" class="header-anchor">#</a></h1><h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>Unified-IO</li>
<li>Uni-Perceiver</li>
<li>PaLi</li>
</ul>
<h1><span id="总结-1">总结 [1]</span><a href="#总结-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/multimodal.webp" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/653902791">多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/643969218">[Transformer 101系列] 多模态的大一统之路</a>  ***</p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/qq_52038588/article/details/133893013">多模态论文串讲</a> ***<br>   <a href="https://blog.csdn.net/qq_40168949/article/details/130374733">多模态论文串讲：ALBEF &amp; VLMo &amp; BLIP &amp; CoCa &amp; Beit V3</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409338&idx=1&sn=5445ff1e9bedc561393b6da63fdf71f9">图生文多模态大模型开源项目回顾：兼看20240307大模型进展早报</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662889725">图文多模态大模型综述</a></p>
<p>1xx. <a href="https://huyenchip.com/2023/10/10/multimodal.html">Multimodality and Large Multimodal Models (LMMs)</a><br>   <a href="https://baoyu.io/translations/lmm/multimodality-and-large-multimodal-models">多模态和多模态大模型 (LMM)[译]</a>  CLIP Flamingo</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/667942680">写在多模态征服一切之前（未来数据和模型应该是什么样的？）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)量化</title>
    <url>/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="量化实战">量化实战</span><a href="#量化实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/1213172a99a949ceba7e8e2164710a77?pvs=4">(实战)量化-推理</a></p>
<p><a href="https://candied-skunk-1ca.notion.site/PTQ-0a18129c05a8447fac9155a56a26ecab?pvs=4">(实战)量化-PTQ</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(质量过滤)RefinedWeb, Textbooks</title>
    <url>/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="动机1">动机[1]</span><a href="#动机1" class="header-anchor">#</a></h1><ul>
<li>作者执着证明网页数据好于专有数据<ul>
<li>网页数据的量级比公开数据大的多，仅用专有数据模型模型训练不到最佳效果</li>
<li>专有数据处理起来很麻烦</li>
<li>大部分专有数据其实在网页数据中也能找到</li>
</ul>
</li>
</ul>
<p>作者认为要想模型训练的大、耗费的人力少就不得不重新<strong>将网页数据精细化</strong>利用起来。</p>
<h1><span id="结论1">结论[1]</span><a href="#结论1" class="header-anchor">#</a></h1><ul>
<li>作者证明了仅用<strong>web数据</strong>如果经过恰当的<strong>清洗和过滤</strong>，可以获得超过使用了专有数据模型的效果。</li>
</ul>
<h1><span id="文本处理pipeline1">文本处理Pipeline[1]</span><a href="#文本处理pipeline1" class="header-anchor">#</a></h1><h3><span id="目标语言识别">目标语言识别</span><a href="#目标语言识别" class="header-anchor">#</a></h3><h3><span id="规则过滤">规则过滤</span><a href="#规则过滤" class="header-anchor">#</a></h3><h3><span id="通过机器学习方法过滤出高质量语料库">通过机器学习方法过滤出高质量语料库</span><a href="#通过机器学习方法过滤出高质量语料库" class="header-anchor">#</a></h3><h3><span id="去重deduplication">去重（Deduplication）</span><a href="#去重deduplication" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="refinedweb">RefinedWeb</span><a href="#refinedweb" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/641013454">数据为王：大模型预训练中的数据处理及思考—The RefinedWeb Dataset for Falcon LLM论文解读</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401484&idx=1&sn=c49b5ca5fc962ca757d3a082b74f037a">“超越LLama 65B”的Falcon40B语言模型为什么好：再看精细化的数据清洗的重要性 </a><br>   RefinedWeb Dataset for Falcon,   Falcon采用bloom架构</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402104&idx=1&sn=7d4924b2a5a840e4ff3de43299248b1d">再谈大模型的预训数据清洗与微调数据生成：RedPajama数据处理框架与entity-centric指令生成方法解读 </a><br>    llama数据的复现项目SlimPajama</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/637996787">【Falcon Paper】我们是靠洗数据洗败 LLaMA 的！</a> 未</p>
<h3><span id="textbooks-数量-gtscaling-law">Textbooks   数量-&gt;scaling law</span><a href="#textbooks-数量-gtscaling-law" class="header-anchor">#</a></h3><p>1xx. <a href="https://finisky.github.io/textbooks-are-all-you-need-summary/">数据为王: Textbooks Are All You Need </a>   以小博大  打破传统语言模型缩放定律<br>1xx. <a href="https://zhuanlan.zhihu.com/p/673021932">Textbooks Are All You Need II: phi-1.5 technical report 精读与翻译</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/672066480">小模型的惊人能力: Phi-2</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403821&idx=1&sn=7b96e0db09f05888078019cd20bc8390">再看多语种大模型预训数据如何清洗：兼论文档结构信息对大模型问答的重要性及实现思路 </a><br>二、再看训练数据集如何清洗：多语种开源训练数据集CulturaX</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-工具</title>
    <url>/www6vHomeAIGC/2024/02/11/gptPaperTools/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="科研-工具">科研-工具</span><a href="#科研-工具" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/115bfe21108480ce88fdcb7b9f9dbc4e?pvs=4">科研-工具</a></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>法律大模型</title>
    <url>/www6vHomeAIGC/2024/02/07/gptDomainLaw/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="法律大模型">法律大模型</span><a href="#法律大模型" class="header-anchor">#</a></h3><ul>
<li>ChatLaw </li>
<li>LawGPT_zh</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402872&idx=1&sn=0649e8f7490e057680cff1be16157209">再看法律领域微调模型及外挂知识库问答优化方案：从引入关键词、领域嵌入到知识库细化、意图识别及知识增强项目案例 </a></p>
<p>1xx. <a href="https://finisky.github.io/lawyer-llama-summary/">训练中文垂类大模型：Lawyer LLaMA </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)混合精度</title>
    <url>/www6vHomeAIGC/2024/02/01/gptPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="混合精度">混合精度</span><a href="#混合精度" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/d27bdf000e7c42eabd288f9d036ea5e7?pvs=4">(原理|实战)混合精度</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Precision</category>
      </categories>
      <tags>
        <tag>Precision</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT P-Tuning</title>
    <url>/www6vHomeAIGC/2024/01/28/gptPEFTPtuningPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="最佳实践1">最佳实践[1]</span><a href="#最佳实践1" class="header-anchor">#</a></h3><ul>
<li>要看losss, 也要看<strong>业务的loss</strong></li>
<li>生成模型常用的评价方法<ul>
<li><strong>BLEU 能评估</strong>流畅度**</li>
<li>结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）</li>
</ul>
</li>
<li>垂直模型<ul>
<li><strong>stf之后失去通用能力</strong></li>
<li>要有<strong>通用能力</strong>, 需要<strong>pre-train和STF中都融入通用的语料</strong></li>
</ul>
</li>
<li><strong>每个模型的学习率lr不一样</strong><ul>
<li>chatglm的学习率<br>LR&#x3D;2e-2</li>
</ul>
</li>
</ul>
<h3><span id="学习率">学习率</span><a href="#学习率" class="header-anchor">#</a></h3><ul>
<li>改的<strong>特别大</strong><br>模型训练的时候会<strong>震荡</strong></li>
<li>改的<strong>特别小</strong><br> 模型训练的时候会<strong>收敛非常慢</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《13-基于 ChatGLM2的 Fine-tuning 实战》 AI 大模型全栈工程师培养计划  2期<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm的p-tuning<br><a href="https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_pt2.sh">train_pt2.sh</a> git   基于法律文本的chatglm-2的P-tuning v2<br><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb">课件</a><br>bili有相关的总结的视频</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Fine Tuning-Bert</title>
    <url>/www6vHomeAIGC/2024/01/26/gptFineTuningBert/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="基于bert的二分类">基于bert的二分类</span><a href="#基于bert的二分类" class="header-anchor">#</a></h1><ul>
<li>代码 - 全参FT,非PEFT<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">SEED=<span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ALBERT是一种压缩过的BERT</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;albert-base-v2&quot;</span></span><br><span class="line">DATASET_NAME = <span class="string">&quot;glue&quot;</span> <span class="comment"># 一组NLP评测任务</span></span><br><span class="line">DATASET_TASK = <span class="string">&quot;mrpc&quot;</span> <span class="comment"># MRPC 是其中一个子任务 -- Microsoft Research Paraphrase Corpus</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Bert的基础上加了一个线性分类器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClassifier</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.bert_encoder = backbone</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="keyword">return</span> loss_fct(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask,labels=<span class="literal">None</span></span>):</span><br><span class="line">        output = self.bert_encoder(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        output = output.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = self.compute_loss(output, labels)</span><br><span class="line">            <span class="keyword">return</span> loss, output</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集对应的评估方法</span></span><br><span class="line">glue_metric = datasets.load_metric(DATASET_NAME, DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> glue_metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">raw_datasets = load_dataset(DATASET_NAME,DATASET_TASK)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="comment"># 验证集</span></span><br><span class="line">raw_valid_dataset = raw_datasets[<span class="string">&quot;validation&quot;</span>]</span><br><span class="line"></span><br><span class="line">columns = raw_train_dataset.column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">transformers.set_seed(SEED)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_fn</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = tokenizer(examples[<span class="string">&quot;sentence1&quot;</span>], examples[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;attention_mask&quot;</span>] = inputs[<span class="string">&quot;attention_mask&quot;</span>]</span><br><span class="line">    examples[<span class="string">&quot;labels&quot;</span>] = examples[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_train_dataset = raw_train_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenized_valid_dataset = raw_valid_dataset.<span class="built_in">map</span>(</span><br><span class="line">    process_fn,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=columns</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据校准器（自动生成batch）</span></span><br><span class="line">collater = DataCollatorWithPadding(</span><br><span class="line">    tokenizer=tokenizer, return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型 -- 其实Transformer可以直接用AutoModelForSequenceClassification</span></span><br><span class="line"><span class="comment">#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我手工写了分类器层，为了方便大家理解什么叫在Transformer上面做分类任务</span></span><br><span class="line">backbone = AutoModel.from_pretrained(MODEL_NAME)</span><br><span class="line">model = MyClassifier(backbone)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./output&quot;</span>,        <span class="comment"># checkpoint保存路径</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;steps&quot;</span>,    <span class="comment"># 每N步做一次eval</span></span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,             <span class="comment"># 训练epoch数</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">8</span>,  <span class="comment"># 每张卡的batch大小</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,   <span class="comment"># 累加几个step做一次参数更新</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">8</span>,  <span class="comment"># evaluation batch size</span></span><br><span class="line">    logging_steps=<span class="number">20</span>,             <span class="comment"># 每20步eval一次</span></span><br><span class="line">    save_steps=<span class="number">20</span>,                <span class="comment"># 每20步保存一个checkpoint</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,             <span class="comment"># 学习率</span></span><br><span class="line">    warmup_ratio=<span class="number">0.1</span>,               <span class="comment"># 预热（可选）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练器</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, <span class="comment"># 待训练模型</span></span><br><span class="line">    args=training_args, <span class="comment"># 训练参数</span></span><br><span class="line">    data_collator=collater, <span class="comment"># 数据校准器</span></span><br><span class="line">    train_dataset=tokenized_train_dataset, <span class="comment"># 训练集</span></span><br><span class="line">    eval_dataset=tokenized_valid_dataset, <span class="comment"># 验证集</span></span><br><span class="line">    compute_metrics=compute_metrics, <span class="comment"># 评价指标</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用wandb（与huggingface.co同步的机制）</span></span><br><span class="line">os.environ[<span class="string">&quot;WANDB_DISABLED&quot;</span>] = <span class="string">&quot;true&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb">Bert fine-tuning 二分类</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>Fine-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT QLoRA</title>
    <url>/www6vHomeAIGC/2024/01/12/gptPEFTQLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86-1">技术原理 [1]</a></li>
<li><a href="#%E5%AE%9E%E6%88%981-2">实战1 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="技术原理-1">技术原理 [1]</span><a href="#技术原理-1" class="header-anchor">#</a></h1><p>使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。<br>QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。</p>
<ul>
<li><p>4bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。</p>
</li>
<li><p>双量化：对第一次量化后的那些常量再进行一次量化，减少存储空间。</p>
</li>
<li><p>分页优化器:  使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</p>
</li>
</ul>
<img src="/www6vHomeAIGC/2024/01/12/gptPEFTQLora/qlora.png" class>

<p>实验证明，无论是使用16bit、8bit还是4bit的适配器方法，都能够复制16bit全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。</p>
<h1><span id="实战1-2">实战1 [2]</span><a href="#实战1-2" class="header-anchor">#</a></h1><h1><span id="参数">参数</span><a href="#参数" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/636215898">大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/636644164">高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香</a>  先是训练llama-7b, 再是训练llama-65b<br><a href="https://github.com/www6v/llm-action/tree/main/train/qlora">qlora</a> git</p>
</li>
<li><p><a href="https://github.com/zyds/transformers-code/tree/master/04-Kbit%20Training/27-4bits_training">4bits_training</a><br><a href="https://www.bilibili.com/video/BV1DQ4y1t7e8/">【手把手带你实战HuggingFace Transformers-低精度训练篇】4bit量化与QLoRA模型训练</a> V</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671089942">[大模型微调技术] LoRA、QLoRA、QA-LoRA 原理笔记</a><br>1xx. <a href="https://cloud.tencent.com/developer/article/2375230">大模型实操 | LoRA、QLoRA微调大模型实战技巧分享，含常见QA解答！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Flash Decoding</title>
    <url>/www6vHomeAIGC/2023/10/06/gptInferFlashDecoding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="flash-decoding">Flash Decoding</span><a href="#flash-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Flash-Decoding-110bfe2110848085aa6dde60217c486a?pvs=4">(原理)Flash Decoding</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Infer</category>
      </categories>
      <tags>
        <tag>Infer</tag>
      </tags>
  </entry>
  <entry>
    <title>Speculative Decoding</title>
    <url>/www6vHomeAIGC/2023/10/06/gptInferSpeculativeDecoding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="speculative-decoding">Speculative Decoding</span><a href="#speculative-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Speculative-Decoding-117bfe2110848060a00efd475a0abbac?pvs=4">Speculative Decoding</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>DistServe</title>
    <url>/www6vHomeAIGC/2023/10/05/gptInferDistServe/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="distserve">DistServe</span><a href="#distserve" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DistServe-dd4ae7040b78496f9a60c0291941922b?pvs=4">DistServe</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Parameter Server</title>
    <url>/www6vHomeAIGC/2023/10/01/gptParameterServer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="parameter-server">Parameter Server</span><a href="#parameter-server" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Parameter-Server-b7ba4d2c8de44339997cff6697f51df5?pvs=4">(原理|实战)Parameter Server</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MLSys</category>
      </categories>
      <tags>
        <tag>MLSys</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Self-QA</title>
    <url>/www6vHomeAIGC/2023/09/27/gptDataSelfQA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="self-qa">Self-QA</span><a href="#self-qa" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Self-QA-10ebfe21108480999eadec23ffadb6fe?pvs=4">(原理|实战)Self-QA</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Data</category>
      </categories>
      <tags>
        <tag>Data</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM PaaS</title>
    <url>/www6vHomeAIGC/2023/09/26/gptLLMOpsPaaS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llm-paas">LLM PaaS</span><a href="#llm-paas" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/9ddf2032d70b4722ad34a48cb305d80b?pvs=4">LLM PaaS</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)FSDP</title>
    <url>/www6vHomeAIGC/2023/09/19/gptTrainFSDP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="fsdp">FSDP</span><a href="#fsdp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/FSDP-2fc101f3b7ac4796b74d5e9287ff8210?pvs=4">FSDP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DDP</title>
    <url>/www6vHomeAIGC/2023/09/19/gptTrainDDP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ddp">DDP</span><a href="#ddp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DDP-3576fea6254a42c2ab520f5e6c4ccb86?pvs=4">DDP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>Chunked Prefill</title>
    <url>/www6vHomeAIGC/2023/09/18/gptInferChunkedPrefill/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="chunked-prefill">Chunked Prefill</span><a href="#chunked-prefill" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/chunked-prefill-102bfe21108480a7af99fee1f56fd5af?pvs=4">Chunked Prefill</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Continuous Batching</title>
    <url>/www6vHomeAIGC/2023/09/18/gptInferContinuousBatching/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="continuous-batching">Continuous Batching</span><a href="#continuous-batching" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Continuous-batching-3ce74a6d992944fba6314e21b3c3ec22">Continuous Batching</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)通信原语</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainCommunication/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="通信原语">通信原语</span><a href="#通信原语" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/1b545b71538a4c5cb14e5771da8b4835?pvs=4">(原理)通信原语</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)张量并行(TP)</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainTensorParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="张量并行tp">张量并行(TP)</span><a href="#张量并行tp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/TP-35acabf325004c16b9ce93f82cb175c2?pvs=4">(原理)张量并行(TP)</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)流水线并行(PP)</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainPipelineParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="流水线并行pp">流水线并行(PP)</span><a href="#流水线并行pp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PP-f528f9a456184d1db006808039c0d2ee?pvs=4">(原理|实战)流水线并行(PP)</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>KV Cache 量化</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheQuantization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache-量化">KV Cache 量化</span><a href="#kv-cache-量化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Quantization-c6fa20cf425a4211af150b4987711f47?pvs=4">KV Cache 量化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Streaming LLM</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheStreamingLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="streaming-llm">Streaming LLM</span><a href="#streaming-llm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/StreamingLLM-5141d463ddf84b4783c369459c71eec8?pvs=4">Streaming LLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)KV Cache 优化</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache-优化">KV Cache 优化</span><a href="#kv-cache-优化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/KV-cache-bd0a35015c9845bd8e17d5c902dba152?pvs=4">(原理)KV cache优化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)unCLIP</title>
    <url>/www6vHomeAIGC/2023/08/27/gptDiffusionunCLIP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="unclip">unCLIP</span><a href="#unclip" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/unCLIP-7603f64564a54cb4af08a1cf38c890e1?pvs=4">(原理)unCLIP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ReferenceNet</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionReferenceNet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="referencenet">ReferenceNet</span><a href="#referencenet" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/ReferenceNet-22a14d3a669e4d7f8c99409e34252349?pvs=4">(原理|实战)ReferenceNet</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)IP-Adapter</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionIPAdapter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ip-adapter">IP-Adapter</span><a href="#ip-adapter" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/IP-Adapter-40dfd1f30d38456b8776a90871716c73?pvs=4">(原理|实战)IP-Adapter</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)T2I-Adapter</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionT2IAdapter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="t2i-adapter">T2I-Adapter</span><a href="#t2i-adapter" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/T2I-Adapter-26eef5080f084dacb6e89d643d31e53d?pvs=4">(原理|实战)T2I-Adapter</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ControlNet</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionControlNet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="controlnet">ControlNet</span><a href="#controlnet" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/ControlNet-76c3d54a18424e20a25a3e4a8a71b8e3?pvs=4">(原理|实战)ControlNet</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>低精度训练</title>
    <url>/www6vHomeAIGC/2023/08/16/gptLowPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="低精度训练">低精度训练</span><a href="#低精度训练" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/cb067e2d0bc545d898cd43dd1091c8b3?pvs=4">低精度训练</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>低精度</category>
      </categories>
      <tags>
        <tag>低精度</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)推理优化</title>
    <url>/www6vHomeAIGC/2023/08/14/gptInferenceSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/A-Survey-on-Efficient-Inference-for-Large-Language-Models-22145473188e437881bf566241492bea?pvs=4">A Survey on Efficient Inference for Large Language Models</a></p>
<h1><span id="inference-papers">Inference Papers</span><a href="#inference-papers" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Inference-Papers-bd22ef1d8c274d6f9951c394a95ff427?pvs=4">Inference Papers</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Connector</title>
    <url>/www6vHomeAIGC/2023/08/05/gptMultimodalConnector/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="connector">Connector</span><a href="#connector" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Connector-6db25052e8e542c29e43f503fd572475?pvs=4">Connector</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Encoder</title>
    <url>/www6vHomeAIGC/2023/08/02/gptMultimodalEncoder/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="vision-encoder">Vision Encoder</span><a href="#vision-encoder" class="header-anchor">#</a></h1><h3><span id="multimodal-learning"><strong>multimodal learning</strong></span><a href="#multimodal-learning" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/" title="(原理)CLIP">(原理)CLIP</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/" title="(实战)CLIP">(实战)CLIP</a></li>
</ul>
<h3><span id="supervised-learning"><strong>Supervised Learning</strong></span><a href="#supervised-learning" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalVit/" title="(原理|实战)ViT, ViLT">(原理|实战)ViT, ViLT</a></li>
</ul>
<h3><span id="self-distillation"><strong>self-distillation</strong></span><a href="#self-distillation" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/DINO-ac5b90014a01494ea1311f4d24af38dd?pvs=4">DINO</a> </li>
<li><a href="https://candied-skunk-1ca.notion.site/DINOv2-b3d258bbb20f42bbac0ef6ca6f093f9d?pvs=4">DINOv2</a></li>
</ul>
<h3><span id="auto-encoding"><strong>Auto-encoding</strong></span><a href="#auto-encoding" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/VQ-VAE-e56af23289844662b653be10667bf239?pvs=4">VQVAE</a></li>
</ul>
<h3><span id="masked-modeling">Masked Modeling</span><a href="#masked-modeling" class="header-anchor">#</a></h3><ul>
<li>MAE</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Guidance</title>
    <url>/www6vHomeAIGC/2023/08/01/gptDiffusionGuidance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="guidance">Guidance</span><a href="#guidance" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Guidance-bcb0b5a85b5f454fa84875eaeb518983?pvs=4">Guidance</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SD XL</title>
    <url>/www6vHomeAIGC/2023/08/01/gptDiffusionXL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="sdxl">SDXL</span><a href="#sdxl" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SDXL-0446aba46c8e400d8583fde17d8df264?pvs=4">SDXL</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)Controllable</title>
    <url>/www6vHomeAIGC/2023/07/29/gptDiffusionControllable/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> <a href="http://doi.org/10.1007/s11390-024-3814-0">A Survey of Multimodal Controllable Diffusion Models</a></li>
</ul>
<h1><span id="论文解析">论文解析</span><a href="#论文解析" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/A-Survey-of-Multimodal-Controllable-Diffusion-Models-b736974658bd4ad79f4128690b0cfb3a?pvs=4">论文解析</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(Work|实战)Image Editing</title>
    <url>/www6vHomeAIGC/2023/07/27/gptDiffusionImageEditWork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#prompt-to-prompt">Prompt-to-Prompt</a></li>
<li><a href="#instructpix2pix">InstructPix2Pix</a></li>
<li><a href="#mgie">MGIE</a></li>
<li><a href="#pix2pix-zero">pix2pix-zero</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="prompt-to-prompt">Prompt-to-Prompt</span><a href="#prompt-to-prompt" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/Prompt-to-Prompt-3ae01e342c6b4b41adc58c6ec5233020">解析</a></li>
</ul>
<h1><span id="instructpix2pix">InstructPix2Pix</span><a href="#instructpix2pix" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/InstructPix2Pix-aedf2e9b6acd48fab6928f717065288c?pvs=4">解析</a></li>
</ul>
<h1><span id="mgie">MGIE</span><a href="#mgie" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/MGIE-d6bfedcc92ae42a48a64ac199ce2aa14?pvs=4">解析</a></li>
</ul>
<h1><span id="pix2pix-zero">pix2pix-zero</span><a href="#pix2pix-zero" class="header-anchor">#</a></h1><ul>
<li>论文解析<br><a href="https://candied-skunk-1ca.notion.site/pix2pix-zero-f79b1c40cde44f40aeeb8eb34482aa28?pvs=4">论文解析</a></li>
</ul>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><ul>
<li><p>Prompt-to-Prompt<br>train-free，察觉到了attention map的妙用</p>
</li>
<li><p>pix2pix-zero<br>察觉到了attention map的妙用  </p>
</li>
<li><p>InstructPix2Pix<br>trainable，training数据基于Prompt-to-Prompt </p>
</li>
<li><p>MGIE<br>基于LMM</p>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>CV 任务</title>
    <url>/www6vHomeAIGC/2023/07/25/gptVisionTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="分类-1">分类 [1]</span><a href="#分类-1" class="header-anchor">#</a></h1><h3><span id="image-level">image-level</span><a href="#image-level" class="header-anchor">#</a></h3><ul>
<li><p>image recognition</p>
</li>
<li><p>(Retrieval)image-text retrieval</p>
</li>
<li><p>Caption(image captioning) </p>
</li>
<li><p>VQA(visual question answering)</p>
</li>
</ul>
<h3><span id="region-level">region-level</span><a href="#region-level" class="header-anchor">#</a></h3><ul>
<li><p>Object Detection object detection</p>
<ul>
<li>DETR -&gt; DINO -&gt; Grounding DINO</li>
</ul>
</li>
<li><p>dense caption</p>
</li>
<li><p>phrase grounding</p>
</li>
</ul>
<h3><span id="pixel-level">pixel-level</span><a href="#pixel-level" class="header-anchor">#</a></h3><ul>
<li>Segmentation<ul>
<li>generic segmetation</li>
<li>referring segmetation</li>
</ul>
</li>
</ul>
<h1><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h1><ul>
<li><p>对比</p>
<ul>
<li>[CNN  更深的网络]</li>
<li>[transformer 没有局限]</li>
</ul>
</li>
<li><p>CV任务</p>
<ul>
<li>分类（Classification）</li>
<li>检测（Detection）</li>
<li>分割（Segmentation）</li>
<li>跟踪（Tracking）</li>
<li>行为识别（Action Recognition）</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>[<a href="https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036">CVPR Tutorial Talk] Towards General Vision Understanding Interface</a><br><a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf">pdf</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Vision</category>
      </categories>
      <tags>
        <tag>Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DiT</title>
    <url>/www6vHomeAIGC/2023/07/21/gptDiffusionDiT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a> </p>
</li>
<li><p>开源地址<br> <a href="https://www.wpeebles.com/DiT">Scalable Diffusion Models with Transformers</a> git<br> <a href="https://github.com/facebookresearch/DiT">DiT Repo</a>  git</p>
</li>
</ul>
<h1><span id="arch">Arch</span><a href="#arch" class="header-anchor">#</a></h1><p><img src="https://www.wpeebles.com/images/DiT/block.png" alt="DiT"></p>
<h1><span id="code10">code[10]</span><a href="#code10" class="header-anchor">#</a></h1><ul>
<li>DiT<br><a href="https://github.com/www6v/mnist-dits/blob/main/dit.py">dit</a> git</li>
<li>DiTBlock<br><a href="https://github.com/www6v/mnist-dits/blob/main/dit_block.py">dit_block</a> git</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://www.bilibili.com/video/BV12E421T7Zi/">AI大讲堂：文生视频谁能敌？专业拆解【DiT模型】</a> V<br>   DiT原文: <a href="https://arxiv.org/abs/2212.09748">https://arxiv.org/abs/2212.09748</a><br>   Code: <a href="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a><br>   Huggingface: <a href="https://huggingface.co/spaces/wpeebles/DiT">https://huggingface.co/spaces/wpeebles/DiT</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV12J4m1379T/">14步手搓sora!Diffusion Transformer, DiT工作原理</a>  V </p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.bilibili.com/video/BV13K421h79z/">【Sora重要技术】复现DiT（Diffusion Transformer）模型</a> V ***<br>   <a href="https://github.com/owenliang/mnist-dits">dits Repo</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(Work|实战)Controllable</title>
    <url>/www6vHomeAIGC/2023/07/17/gptDiffusionControllableWork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>




<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/controllable-763edf3a43b94e03a1ff0faee9ac41c2?pvs=4">ControlNet + t2i_adapter + Custom Diffusion </a>  diffusers</li>
</ul>
<h1><span id="总结metaso">总结[metaso]</span><a href="#总结metaso" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th><strong>功能定位</strong></th>
<th><strong>性能与效率</strong></th>
<th><strong>应用场景</strong></th>
<th>总结</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ControlNet</strong></td>
<td>主要用于对图像生成过程中的<strong>特定部分</strong>进行控制</td>
<td><strong>较大</strong>且可能需要较多计算资源</td>
<td>适用于需要对图像特定区域进行<strong>精细控制</strong>的场景</td>
<td>【ControlNet  <strong>结构控制</strong>， image2image】</td>
</tr>
<tr>
<td><strong>T2I-Adapter</strong></td>
<td>专注于将<strong>文本提示转换为图像</strong></td>
<td><strong>较小</strong>且更高效，适合资源受限的环境</td>
<td>适用于需要<strong>从文本描述生成图像</strong>的场景</td>
<td>【T2I-Adapter  <strong>多种控制</strong>, text2image】</td>
</tr>
<tr>
<td><strong>IP-Adapter</strong></td>
<td>用于<strong>分析图像提示并提取特征</strong>，再将其用于图像生成</td>
<td>在图像质量和对齐方面表现优异</td>
<td>适用于需要结合图像和文本提示进行<strong>复杂图像生成</strong>的场景</td>
<td>【IP-Adapter <strong>风格特征控制</strong>,  text2image|image2image 】</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Diffusion</title>
    <url>/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#api-based">API-based</a><ul>
<li><a href="#diffusion-models-class-%E5%AE%98%E6%96%B9%E8%AF%BE%E7%A8%8B">diffusion-models-class [官方课程]</a></li>
<li><a href="#diffusers-%E9%87%8D%E7%82%B9pipeline-10">diffusers 重点pipeline [10]</a></li>
</ul>
</li>
<li><a href="#ui-based">UI-based</a><ul>
<li><a href="#stable-diffusion-webui">stable-diffusion-webui</a></li>
<li><a href="#comfyui">comfyui</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#api-based-1">API-based</a></li>
<li><a href="#ui-based-1">UI-based</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="api-based">API-based</span><a href="#api-based" class="header-anchor">#</a></h1><h3><span id="diffusion-models-class-官方课程">diffusion-models-class [官方课程]</span><a href="#diffusion-models-class-官方课程" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-1-An-Introduction-to-Diffusion-Models-f0ee4c8bc4914ef8961b48241064b2b7?pvs=4">Unit 1: An Introduction to Diffusion Models</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-2-Fine-Tuning-Guidance-and-Conditioning-27180b80a58e4bd2860019c4237a8532?pvs=4">Unit 2: Fine-Tuning, Guidance and Conditioning</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-3-Stable-Diffusion-a8770ac5b0214c2f9cfce878812b5bf8?pvs=4">Unit 3: Stable Diffusion</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-4-Going-Further-with-Diffusion-Models-e997fe47d4e64069bba59ac81b7a4718?pvs=4">Unit 4: Going Further with Diffusion Models</a></li>
</ul>
<h3><span id="diffusers-重点pipeline-10">diffusers 重点pipeline [10]</span><a href="#diffusers-重点pipeline-10" class="header-anchor">#</a></h3><ul>
<li>controlnet 【controllable】</li>
<li>dreambooth 【fine tuning】</li>
<li>instruct_pix2pix 【image edit】</li>
</ul>
<h1><span id="ui-based">UI-based</span><a href="#ui-based" class="header-anchor">#</a></h1><h3><span id="stable-diffusion-webui">stable-diffusion-webui</span><a href="#stable-diffusion-webui" class="header-anchor">#</a></h3><ul>
<li><p>stable-diffusion-webui-colab[11]<br>没试过，colab要充值</p>
</li>
<li><p>stable-diffusion-webui   on   阿里serverless [12]  </p>
<img src="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/webui.jpg" class></li>
</ul>
<h3><span id="comfyui">comfyui</span><a href="#comfyui" class="header-anchor">#</a></h3><ul>
<li>ComfyUI  on  阿里serverless[13]<img src="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/comfyUI.jpg" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="api-based">API-based</span><a href="#api-based" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://github.com/huggingface/diffusers/tree/main/examples">Repo diffusers</a> git</li>
</ol>
<h3><span id="ui-based">UI-based</span><a href="#ui-based" class="header-anchor">#</a></h3><ol start="11">
<li><p><a href="https://www.bilibili.com/video/BV1QS421A7zF/">可白嫖且很香—轻轻松松在colab上部署Stable Diffusion大模型！</a> V<br> <a href="https://github.com/camenduru/stable-diffusion-webui-colab">stable-diffusion-webui-colab Repo</a> git<br> <a href="https://github.com/camenduru/stable-diffusion-webui-colab/tree/drive">Install the WebUI Colab to Google Drive </a> git 运行这3个脚本</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV19u411x7Bk/">超详细云端部署Stable Diffusion教程！</a> V<br>【用FC的应用模版部署】【3个月免费的serverless+NAS】</p>
</li>
<li><p><a href="https://alidocs.dingtalk.com/i/p/x9JOGOjr65om4QLAy0mVPNbMnOEE8z89">函数计算 ComfyUI 使用文档</a><br>用 ComfyUI 自制“粘土滤镜<br>【用FC的应用模版部署】【3个月免费的serverless+NAS】</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(work|实战) fine-tuning</title>
    <url>/www6vHomeAIGC/2023/07/06/gptDiffusionFineTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="fine-tuning">Fine-tuning</span><a href="#fine-tuning" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/work-fine-tuning-71037b41f5c04bd8adf2cfc1c5881bfe?pvs=4">Fine-tuning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)Image Editing</title>
    <url>/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%911">图像编辑[1]</a><ul>
<li><a href="#%E5%A4%A7%E7%B1%BB">大类</a></li>
<li><a href="#approaches">APPROACHES</a></li>
</ul>
</li>
<li><a href="#%E8%AE%BA%E6%96%872">论文[2]</a></li>
<li><a href="#survey2">Survey[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2402.17525">《Diffusion Model-Based Image Editing: A Survey》</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">Repo</a> git</p>
</li>
</ul>
<h1><span id="图像编辑1">图像编辑[1]</span><a href="#图像编辑1" class="header-anchor">#</a></h1><h3><span id="大类">大类</span><a href="#大类" class="header-anchor">#</a></h3><ul>
<li>从图片编辑的任务方面可以被分为3个大类<ul>
<li>语义编辑semantic editing </li>
<li>风格编辑stylistic editing</li>
<li>结构编辑structural editing</li>
</ul>
</li>
</ul>
<h3><span id="approaches">APPROACHES</span><a href="#approaches" class="header-anchor">#</a></h3><ul>
<li><p>TRAINING-BASED APPROACHES</p>
<ul>
<li>InstructPix2Pix</li>
</ul>
</li>
<li><p>TESTING-TIME FINETUNING APPROACHES</p>
</li>
<li><p>TRAINING AND FINETUNING FREE APPROACHES</p>
</li>
</ul>
<hr>
<h1><span id="论文2">论文[2]</span><a href="#论文2" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工</p>
</li>
<li><p>开源地址<br> <a href="https://github.com/xinchengshuai/Awesome-Image-Editing">A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models</a></p>
</li>
</ul>
<h1><span id="survey2">Survey[2]</span><a href="#survey2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/survey.png" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol>
<li><p>《Diffusion Model-Based Image Editing: A Survey》<br><a href="https://blog.csdn.net/huzimu_/article/details/136547375">论文阅读：Diffusion Model-Based Image Editing: A Survey</a><br><a href="https://mp.weixin.qq.com/s/MFbCt0XfOf9fV0YbdkmR6g">基于扩散模型的图像编辑：首篇综述</a></p>
</li>
<li><p>《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》<br><a href="https://www.jiqizhixin.com/articles/2024-06-28-14">300多篇相关研究，复旦、南洋理工最新多模态图像编辑综述论文</a></p>
</li>
</ol>
<p>1xx. 《LLMs Meet Multimodal Generation and Editing: A Survey》 *<br>Image Generation， Image Editing<br><a href="https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation">Repo</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>显存估算</title>
    <url>/www6vHomeAIGC/2023/07/01/gptGPUComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="显存估算">显存估算</span><a href="#显存估算" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/e4e6bd5f7c43430fa2e805c5a2777308?pvs=4">显存估算</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent - UI-assistants</title>
    <url>/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="app-agent">App Agent</span><a href="#app-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/677071947">AppAgent源码分析&amp;思考</a><br><a href="https://github.com/mnotgod96/AppAgent">https://github.com/mnotgod96/AppAgent</a><br><a href="https://icoz69.github.io/">https://icoz69.github.io/</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/681424409">【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent</a><br>   <a href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a></p>
<p>1xx. <a href="https://github.com/OpenAdaptAI/OpenAdapt">https://github.com/OpenAdaptAI/OpenAdapt</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Diffusion</title>
    <url>/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="diffusion-原理">Diffusion 原理</span><a href="#diffusion-原理" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Diffusion-8cdcc8079d7d42c2a193ae6baf06246e?pvs=4">(原理)Diffusion</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)CLIP</title>
    <url>/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#clip-training-9">CLIP Training [9]</a></li>
<li><a href="#simple-demo10">Simple Demo[10]</a></li>
<li><a href="#open_clip11">open_clip[11]</a></li>
<li><a href="#chinese-clip">Chinese-CLIP</a><ul>
<li><a href="#%E6%96%B9%E6%B3%9520">方法[20]</a></li>
<li><a href="#demo21">demo[21]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
<li><a href="#chinese-clip-1">Chinese-CLIP</a></li>
<li><a href="#xxx">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="clip-training-9">CLIP Training [9]</span><a href="#clip-training-9" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - ResNet or Vision Transformer</span><br><span class="line"># text_encoder - CBOW or Text Transformer</span><br><span class="line"># I[n, h, w, c] - minibatch of aligned images</span><br><span class="line"># T[n, l] - minibatch of aligned texts</span><br><span class="line"># W_i[d_i, d_e] - learned proj of image to embed</span><br><span class="line"># W_t[d_t, d_e] - learned proj of text to embed</span><br><span class="line"># t - learned temperature parameter</span><br><span class="line"># extract feature representations of each modality</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 1、图像/文字数据过image/text encoder，提取单模态特征</span><br><span class="line"># 每张图片对应一个基本特征I_i</span><br><span class="line"># 每张文字对应一个基本特征T_i</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_f = image_encoder(I) #[n, d_i]</span><br><span class="line">T_f = text_encoder(T) #[n, d_t]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 2. 图像/文字的基本特征过多模态Embedding，提取多模态特征</span><br><span class="line"># 同时对这两个多模态特征做Layer Norm</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1) # [n, d_i] * [d_i, d_e] = [n, d_e]</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # [n, d_t] * [d_t, d_e] = [n, d_e]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 3、计算图片-文字向量的余弦相似度</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t) # [n, n]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 4、计算Loss</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t)/2</span><br></pre></td></tr></table></figure>



<ul>
<li>CLIP分为<strong>按行计算Loss</strong>和<strong>按列计算Loss</strong></li>
<li><strong>按行计算Loss</strong>，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字。</li>
<li><strong>按列计算Loss</strong>，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片。</li>
<li><strong>最后将这两个Loss相加取平均</strong>，代表我们在模型优化过程中<strong>考虑了“图片-&gt;文字”和“文字-&gt;图片”的双向关系</strong>。</li>
</ul>
<h1><span id="simple-demo10">Simple Demo[10]</span><a href="#simple-demo10" class="header-anchor">#</a></h1><p>【基于clip on  resnet,   数据集为mnist中的&lt;数字文本，数字图片&gt;对】</p>
<h1><span id="open_clip11">open_clip[11]</span><a href="#open_clip11" class="header-anchor">#</a></h1><ul>
<li>Training CLIP</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m training.main \</span><br><span class="line">    --save-frequency <span class="number">1</span> \</span><br><span class="line">    --zeroshot-frequency <span class="number">1</span> \</span><br><span class="line">    --report-to tensorboard \</span><br><span class="line">    --train-data=<span class="string">&quot;/path/to/train_data.csv&quot;</span>  \      <span class="comment"># 训练数据 </span></span><br><span class="line">    --val-data=<span class="string">&quot;/path/to/validation_data.csv&quot;</span>  \   <span class="comment"># 验证数据</span></span><br><span class="line">    --csv-img-key filepath \</span><br><span class="line">    --csv-caption-key title \</span><br><span class="line">    --imagenet-val=/path/to/imagenet/root/val/ \</span><br><span class="line">    --warmup <span class="number">10000</span> \      <span class="comment">#</span></span><br><span class="line">    --batch-size=<span class="number">128</span> \    <span class="comment">#</span></span><br><span class="line">    --lr=<span class="number">1e-3</span> \           <span class="comment">#</span></span><br><span class="line">    --wd=<span class="number">0.1</span> \       </span><br><span class="line">    --epochs=<span class="number">30</span> \         <span class="comment">#</span></span><br><span class="line">    --workers=<span class="number">8</span> \</span><br><span class="line">    --model RN50          <span class="comment"># 模型</span></span><br></pre></td></tr></table></figure>

<h1><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h1><h3><span id="方法20">方法[20]</span><a href="#方法20" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/chinese-clip.webp" class>

<p>我们的核心方法在于把训练分为<strong>两阶段</strong>（如上图所示），<strong>第一阶段</strong>和LiT是一致的，<strong>冻结图像塔</strong>，<strong>让文本塔表示接近图像塔表示</strong>。当训练继续但下游精度不能再产生显著提升，即下游零样本检索的精度，我们就把训练切换到<strong>第二阶段</strong>，即<strong>解除图像塔的参数冻结，继续用contrastive tuning预训练</strong>，同样直到下游精度没有显著提升。<strong>后者的意义在于让图像塔能拟合中文世界的图像数据的分布，学习中文世界的知识</strong>。更多实验参数欢迎查看论文的附录部分。</p>
<h3><span id="demo21">demo[21]</span><a href="#demo21" class="header-anchor">#</a></h3><p>代码都看过</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 图片库特征抽取代码</span></span><br><span class="line">python3 extract_embeddings.py </span><br><span class="line"><span class="comment"># 图片特征在faiss向量数据库建立索引   </span></span><br><span class="line">python3 build_index.py</span><br><span class="line"><span class="comment"># 可视化应用界面 </span></span><br><span class="line">streamlit run app.py</span><br></pre></td></tr></table></figure>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="9">
<li><p><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13K421v7Ar/">【多模态】复现OpenAI的CLIP模型</a> V<br><a href="https://github.com/owenliang/mnist-clip">mnist-clip Repo</a> git</p>
</li>
<li><p><a href="https://github.com/mlfoundations/open_clip">open_clip Repo</a> git<br><a href="https://colab.research.google.com/drive/1TEUe2j2oXi-sKiteGYUhsCtdvXocI24w#scrollTo=YPHN7PJgKOzb">Interacting with open_clip</a></p>
</li>
</ol>
<h3><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h3><ol start="20">
<li><p><a href="https://zhuanlan.zhihu.com/p/580546929">中文CLIP模型卷土重来，这次加量不加价！</a> 论文</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/680405647">AIGC之图片生成——基于clip内容检索</a><br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu/tree/main/APP_example/clip_retrieval">clip_retrieval</a> git<br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu">demos Repo</a>  readme有解释</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/539374033">【已重新开源】CLIP的中文副本？说不定有惊喜呢</a></p>
<p>1xx. <a href="https://github.com/www6v/Chinese-CLIP">Chinese-CLIP Repo</a> git</p>
<p>1xx. <a href="https://modelscope.cn/studios/iic/chinese_clip_applications/summary">中文CLIP文到图搜索应用</a> demo</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. langchain 中有CLIP的实现</p>
<p>1xx. <a href="https://github.com/jina-ai/clip-as-service">GitHub - jina-ai&#x2F;clip-as-service: Scalable embedding, reasoning, ranking for images and sentences with CLIP</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RAG</title>
    <url>/www6vHomeAIGC/2023/06/25/gptAgenticRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="agentic-rag-1">Agentic RAG [1]</span><a href="#agentic-rag-1" class="header-anchor">#</a></h1><p>Agentic RAG 和简单 RAG 的最大区别在于 <strong>Agentic RAG 引入了 Agent 的动态编排机制，因此可以根据用户提问的不同意图，引入反馈和查询改写机制，并进行“多跳”式的知识推理，从而实现对复杂提问的回答</strong>。</p>
<ul>
<li><p>Self-RAG 是相对初级的 Agentic RAG，RAGFlow 中也已提供了相关实现。实践证明，Self-RAG 对于较复杂的多跳问答和多步推理可以明显提升性能。</p>
</li>
<li><p>Adaptive RAG</p>
<ul>
<li>开放域问答</li>
<li>多跳问答</li>
<li>自适应检索</li>
</ul>
</li>
<li><p>Adaptive RAG [2]<br>Adaptive-RAG的核心在于它能够通过一个<strong>分类器</strong>来评估问题的复杂性，然后根据这个评估结果选择最合适的处理策略。<br><strong>分类器是</strong>一个<strong>较小的语言模型</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s/A8kfbH70sdU5Gd20K9Y0Lw">Agentic RAG 与图任务编排</a><br>Self-RAG     Adaptive RAG</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/688547968">Adaptive-RAG：性能提升50%以上的高效RAG策略</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/wuyMN7CLAT9HGYlmjLWUtA">LlamaIndex团队技术报告：“RAG的尽头是Agent”</a></p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/">Building Agentic RAG with LlamaIndex - DeepLearning.AI</a><br>    <a href="https://github.com/www6v/deeplearningAI/tree/master/Building%20Agentic%20RAG%20with%20Llamaindex">Building Agentic RAG with LlamaIndex-Repo</a> git<br>    <a href="https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent/">Building a Custom Agent - LlamaIndex</a><br>    <a href="https://llamahub.ai/?tab=agent">Llama Hub</a></p>
<p>​</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG KG</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGKG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="vectorkg-rag1516">Vector+KG RAG[15][16]</span><a href="#vectorkg-rag1516" class="header-anchor">#</a></h3><h3><span id="rag-多跳问题">RAG 多跳问题</span><a href="#rag-多跳问题" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="rag-多跳问题">RAG 多跳问题</span><a href="#rag-多跳问题" class="header-anchor">#</a></h3><p>1xx. <a href="https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/">Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</a></p>
<p><a href="https://cloud.tencent.com.cn/developer/article/2409038">知识图谱和 LLM：多跳问答-腾讯云开发者社区-腾讯云</a></p>
<p><a href="https://blog.csdn.net/qq_41185868/article/details/138514051">LLMs之KG-RAG：KG-RAG&#x2F;GraphRAG(基于知识图谱的RAG系统)的简介(可以解决多跳问题&#x2F;同时支持结构化和非结构化数据查询)、经验技巧、案例应用之详细攻略-CSDN博客</a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_36931982/article/details/139118215">MultiHop-RAG：多跳查询的基准检索增强生成_rag多跳查询-CSDN博客</a></p>
<h3><span id="llmkg-知识图谱">LLM+KG  知识图谱</span><a href="#llmkg-知识图谱" class="header-anchor">#</a></h3><ol start="15">
<li><p><a href="https://neo4j.com/developer-blog/unstructured-knowledge-graph-neo4j-langchain/">Enhanced QA Integrating Unstructured Knowledge Graph Using Neo4j and LangChain</a>  </p>
</li>
<li><p><a href="https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/">Using a Knowledge Graph to implement a DevOps RAG application</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/buV1j4DtDiVavtGCJIsedQ">大模型辅助图谱构建的4个策略对比：兼看大模型与知识图谱结合的3个综述 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG RAGflow</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGRAGflow/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ragflow12elmo">RAGflow[1,2][ELmo]</span><a href="#ragflow12elmo" class="header-anchor">#</a></h1><p>RAGFlow 是一个端到端的 RAG 引擎，它解决数据的问题，因为如果不对用户数据加以区分和清晰，识别其中的语义，就容易导致 Garbage In Garbage Out。RAGFlow 包含了如下的完整 RAG 流程，确保数据从 Garbage In Garbage Out 变为 Quality In Quality Out。</p>
<p>RAGFlow 的最大特色，就是多样化的文档智能处理，因此它没有采用现成的 RAG 中间件，而是完全重新研发了一套智能文档理解系统，并以此为依托构建 RAG 任务编排体系。</p>
<p>这个系统的特点包含：</p>
<ol>
<li>它是一套基于 AI 模型的<strong>智能文档处理系统</strong>；</li>
<li>它是一套包含<strong>各种不同模板</strong>的智能文档处理系统；</li>
<li>智能文档处理的<strong>可视化和可解释性</strong>；</li>
<li>RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="ragflow">RAGFlow</span><a href="#ragflow" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV12T42117VT/">RAGFlow：采用OCR和深度文档理解结合的新一代 RAG 引擎</a> V</li>
<li><a href="https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs">检索增强生成引擎 RAGFlow 正式开源！</a><br>1xx. <a href="https://mp.weixin.qq.com/s/8qms4nxVsX43WSWolXgx7w">7.8K Star RAG 引擎：基于深度文档理解，最大程度降低幻觉、无限上下文快速完成 “大海捞针” 测试！</a></li>
</ol>
<p>1xx.  <a href="http://demo.ragflow.io/">RAGFlow Demo</a><br>    <a href="https://github.com/infiniflow/ragflow">ragflow Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG Qanything</title>
    <url>/www6vHomeAIGC/2023/06/19/gptRAGQanything/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="qanything">QAnything</span><a href="#qanything" class="header-anchor">#</a></h1><h3><span id="arch1">Arch[1]</span><a href="#arch1" class="header-anchor">#</a></h3><p><img src="https://github.com/netease-youdao/QAnything/raw/master/docs/images/qanything_arch.png" alt="Arch"></p>
<ul>
<li><p>索引（indexing）<br>通过Embedding为每一个文本块生成一个向量表示，用于计算<strong>文本向量</strong>和<strong>问题向量</strong>之间的<strong>相似度</strong>。创建索引将原始文本块和Embedding向量以键值对的形式存储，以便将来进行快速和频繁的搜索。</p>
</li>
<li><p>检索（Retrieval）<br>使用Embedding模型将用户输入问题转换为向量，计算问题的Embedding向量和语料库中文本块Embedding向量之间的相似度，选择<strong>相似度最高的前K个文档块</strong>作为当前问题的增强上下文信息。</p>
</li>
<li><p>生成（Generation）<br>将检索得到的前K个文本块和用户问题一起送进大模型，让大模型基于给定的文本块来回答用户的问题。</p>
</li>
</ul>
<h3><span id="1st-retrievalembedding1">1st Retrieval（embedding）[1]</span><a href="#1st-retrievalembedding1" class="header-anchor">#</a></h3><ul>
<li><p>Bcembedding模型 [3]</p>
<ul>
<li>中英双语和跨语种能力</li>
<li>多领域覆盖</li>
</ul>
</li>
<li><p>Embedding 可以给出一个得分，但是这个得分描述的更多的是<strong>相似性</strong>。Embedding本质上是一个<strong>双编码器</strong>，两个文本在模型内部没有任何信息交互。只在最后计算两个向量的余弦相似度时才进行唯一一次交互。所以Embedding检索只能把<strong>最相似的</strong>文本片段给你，<strong>没有</strong>能力来判断候选文本和query之间的<strong>相关性</strong>。但是<strong>相似又不等于相关</strong>。</p>
</li>
</ul>
<p>【embedding -&gt; 相似性】</p>
<img src="/www6vHomeAIGC/2023/06/19/gptRAGQanything/embedding.png" class>

<h3><span id="2nd-retrievalrerank1">2nd Retrieval（rerank）[1]</span><a href="#2nd-retrievalrerank1" class="header-anchor">#</a></h3><ul>
<li><p>Rerank [3]</p>
</li>
<li><p>Rerank本质是一个<strong>Cross-Encoder</strong>的模型。Cross-Encoder能让两个文本片段一开始就在BERT模型各层中通过self-attention进行交互。</p>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/06/19/gptRAGQanything/reranker.png" class>

<p>【rerank -&gt; 相关性】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="qanything">QAnything</span><a href="#qanything" class="header-anchor">#</a></h3><ol>
<li><a href="https://github.com/netease-youdao/QAnything">QAnything Repo</a> git</li>
<li>xxx</li>
<li><a href="https://www.bilibili.com/video/BV1HF4m1w7rY/">有道QAnything背后的故事：关于RAG的一点经验分享</a> V<br> <a href="https://mp.weixin.qq.com/s/FUex1Q984-IhQ-FoLZTf5Q">有道QAnything背后的故事—关于RAG的一点经验分享</a>   文字版<br>[公众号有其他文章]</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489671&idx=1&sn=564a232c3c7919c70a7a1cf5efa77628">前沿重器[45] RAG开源项目Qanything源码阅读1-概述+服务</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/16/gptInferRayPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a><ul>
<li><a href="#%E7%8E%AF%E5%A2%83">环境</a></li>
<li><a href="#%E5%AE%9E%E6%88%981">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%98320">实战3[20]</a></li>
<li><a href="#%E5%AE%9E%E6%88%984">实战4</a></li>
</ul>
</li>
<li><a href="#monitor40">monitor[40]</a><ul>
<li><a href="#ray-dashboard41">Ray Dashboard[41]</a></li>
<li><a href="#ray-logging">Ray logging</a></li>
<li><a href="#built-in-ray-serve-metrics">Built-in Ray Serve metrics</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%981-1">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982-1">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%983">实战3</a></li>
<li><a href="#%E5%AE%9E%E6%88%984-1">实战4</a></li>
<li><a href="#monitor">monitor</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><h3><span id="环境">环境</span><a href="#环境" class="header-anchor">#</a></h3><p>modelscope  GPU</p>
<h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ul>
<li><p>脚本[1]</p>
</li>
<li><p>遇到的异常[2]</p>
</li>
</ul>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ul>
<li><p>脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## 变更模型名字</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## import &#x27;modelscope&#x27; package</span></span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>异常[11]</p>
</li>
</ul>
<h3><span id="实战320">实战3[20]</span><a href="#实战320" class="header-anchor">#</a></h3><ul>
<li>脚本<br>vllm   0.2.3 -&gt; 报异常<br>vllm  0.3.3 -&gt; 报另一个异常</li>
</ul>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ul>
<li><p>脚本 [30]</p>
</li>
<li><p>异常 [31]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 运行这个命令报异常</span><br><span class="line">python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="monitor40">monitor[40]</span><a href="#monitor40" class="header-anchor">#</a></h1><h3><span id="ray-dashboard41">Ray Dashboard[41]</span><a href="#ray-dashboard41" class="header-anchor">#</a></h3><h3><span id="ray-logging">Ray logging</span><a href="#ray-logging" class="header-anchor">#</a></h3><p>Loki  grafana</p>
<h3><span id="built-in-ray-serve-metrics">Built-in Ray Serve metrics</span><a href="#built-in-ray-serve-metrics" class="header-anchor">#</a></h3><p>Prometheus </p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://docs.ray.io/en/master/serve/tutorials/vllm-example.html">Serve a Large Language Model with vLLM</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/60750288/invalid-device-id-when-using-pytorch-dataparallel">Invalid device id when using pytorch dataparallel！</a>  运行时碰到的异常</p>
</li>
</ol>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://github.com/vllm-project/vllm/blob/main/examples/offline_inference_distributed.py">examples&#x2F;offline_inference_distributed.py</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device</a></p>
</li>
</ol>
<h3><span id="实战3">实战3</span><a href="#实战3" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://github.com/asprenger/ray_vllm_inference">Ray vLLM Interence</a></li>
</ol>
<p>1xx. <a href="https://github.com/ray-project/langchain-ray/tree/main">GitHub - ray-project&#x2F;langchain-ray: Examples on how to use LangChain and Ray</a> git</p>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ol start="30">
<li><p><a href="https://blog.csdn.net/engchina/article/details/135455197">在甲骨文云上用 Ray +Vllm 部署 Mixtral 8*7B 模型_mixtral 8x7b 部署-CSDN博客</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device-CSDN博客</a></p>
</li>
</ol>
<h3><span id="monitor">monitor</span><a href="#monitor" class="header-anchor">#</a></h3><ol start="40">
<li><p><a href="https://docs.ray.io/en/master/serve/monitoring.html">Monitor Your Application</a></p>
</li>
<li><p><a href="https://docs.ray.io/en/master/ray-observability/getting-started.html">Ray Dashboard </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Flash Attention</title>
    <url>/www6vHomeAIGC/2023/06/13/gptFlashAttention/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="flash-attention">Flash Attention</span><a href="#flash-attention" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Flash-Attention-2e424082ae3a46b8b1ddd24ead847dd9?pvs=4">(原理)Flash Attention</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>FlashAttention</category>
      </categories>
      <tags>
        <tag>FlashAttention</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战) vLLM</title>
    <url>/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="vllm-实战">vLLM 实战</span><a href="#vllm-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vllm-a35a50c4cd2c4875a8de173575275217?pvs=4">(实战) vLLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/11/gptInferRay/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="architecture-overview">Architecture Overview</span><a href="#architecture-overview" class="header-anchor">#</a></h1><h3><span id="application-concepts-1">Application concepts [1]</span><a href="#application-concepts-1" class="header-anchor">#</a></h3><ul>
<li>Task - A remote function invocation. </li>
<li>Object - An application value.</li>
<li>Actor - a stateful worker process (an instance of a <code>@ray.remote</code> class).</li>
<li>Driver - The program root, or the “main” program.</li>
<li>Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment.</li>
</ul>
<h3><span id="design-1">Design [1]</span><a href="#design-1" class="header-anchor">#</a></h3><ul>
<li>Components<ul>
<li>One or more worker processes</li>
<li>A raylet. <ul>
<li>scheduler</li>
<li>object store</li>
</ul>
</li>
<li>head node<ul>
<li>Global Control Service (GCS)</li>
<li>driver process(es)</li>
<li>cluster-level services</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="spark-vs-ray10">Spark vs. Ray[10]</span><a href="#spark-vs-ray10" class="header-anchor">#</a></h1><ul>
<li><p>总的来说，Ray和Spark的主要差别在于他们的<strong>抽象层次</strong>。<strong>Spark</strong>对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。<strong>Ray</strong>的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，<strong>Ray揭示和暴露了并行，而Spark抽象和隐藏了并行</strong>。</p>
</li>
<li><p>就架构而言，<strong>Spark</strong>采用<strong>BSP模型</strong>，是无副作用的，而<strong>Ray</strong>本质上是一个<strong>RPC 框架+Actor框架+对象存储</strong>。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://developer.volcengine.com/articles/7241442880106004536">基于 Ray 的大规模离线推理</a> 字节<br>   <a href="https://mp.weixin.qq.com/s/mU2RymHIHj8mJiDWBUAdWA">字节跳动基于 Ray 的大规模离线推理</a></p>
<p>1xx. <a href="https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.eg7m6lz2y48u">Ray Design Patterns</a> 查看-&gt;模式</p>
<p>1xx. <a href="https://blog.csdn.net/2401_83124266/article/details/136428395">大模型训练部署利器–开源分布式计算框架Ray原理介绍</a></p>
<h3><span id="spark-vs-ray">Spark vs. Ray</span><a href="#spark-vs-ray" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.zhihu.com/question/432813259/answer/2335473370">加州大学伯克利分校为何能连续孵化出 Mesos,Spark,Alluxio,Ray 等重量级开源项目?</a> 孙挺Sunt</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/junerli/article/details/138476201">分布式领域计算模型及Spark&amp;Ray实现对比</a></p>
<h3><span id="internal">Internal</span><a href="#internal" class="header-anchor">#</a></h3><ol>
<li><a href="https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.iyrm5j2gcdoq">Ray v2 Architecture</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/111340572">Ray 分布式计算框架介绍</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/344736949">Ray 1.0 架构解读</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 评估</title>
    <url>/www6vHomeAIGC/2023/06/07/gptRAGEval/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>1xx. <a href="https://www.bilibili.com/video/BV1Jz421Q7Lw/">如何利用RAGAs评估RAG系统的好坏</a></p>
<p><a href="https://github.com/blackinkkkxi/RAG_langchain/blob/main/learn/evaluation/RAGAS-langchian.ipynb">使用LangChain和RAGAS对RAG系统进行自动有效评估</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1aZ421W7DB/">一次搞懂RAG评估，三个角度LangChain，LlamaIndex，RAGAS</a><br>   <a href="https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0053">RAG评估资料大全 </a><br>   RAG评估指标：两种视角理解评估指标<br>   <a href="https://docs.smith.langchain.com/old/cookbook/testing-examples/rag_eval">RAG Evaluation</a><br>   <a href="https://docs.smith.langchain.com/old/cookbook/testing-examples/ragas">RAG evaluation with RAGAS</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404511&idx=2&sn=fefb78c1d920cb5b437f2e3da9935637">再看大模型RAG检索增强如何评估：RAGAS开源自动化评估框架</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404476&idx=2&sn=d07b27dc9162ab0aaec3108004e4cfbe">大模型RAG检索增强问答如何评估：噪声、拒答、反事实、信息整合四大能力评测任务探索 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>AutoGen</title>
    <url>/www6vHomeAIGC/2023/06/05/gptAgentAutogen/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="autogen">AutoGen</span><a href="#autogen" class="header-anchor">#</a></h1><h3><span id="demo-0123">Demo [0,1,2,3]</span><a href="#demo-0123" class="header-anchor">#</a></h3><h3><span id="autogen-studio10">AutoGen Studio[10]</span><a href="#autogen-studio10" class="header-anchor">#</a></h3><h3><span id="pattern20">Pattern[20]</span><a href="#pattern20" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="demos">Demos</span><a href="#demos" class="header-anchor">#</a></h3><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/671782824">AutoGen实战应用(一)：代码生成、执行和调试</a><br> 基于官方例子</p>
</li>
<li><p><a href="https://microsoft.github.io/autogen/docs/Examples">Examples</a><br><a href="https://github.com/www6v/AIGC/tree/master/agent/autogen">Repo</a> git</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/664937747">Autogen 新手指南：基础概念和应用</a><br> Autogen的高级应用 官方例子</p>
</li>
<li><p><a href="https://developer.aliyun.com/article/1394332">AutoGen多代理对话项目示例和工作流程分析</a><br> 自定义方法fetch_prices,  多agent</p>
</li>
</ol>
<p>1xx. <a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat/">Multi-agent Conversation Framework</a>   </p>
<p>1xx. <a href="https://microsoft.github.io/autogen/blog">Blog</a></p>
<h3><span id="autogen-studio">AutoGen Studio</span><a href="#autogen-studio" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/680797754">LLM之Agent（十）| 本地安装Microsoft AutoGen Studio 2.0教程</a></li>
</ol>
<h3><span id="pattern">Pattern</span><a href="#pattern" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://github.com/www6v/deeplearningAI/tree/master/AI%20Agentic%20Design%20Patterns%20with%20AutoGen">AI Agentic Design Patterns with AutoGen</a></li>
</ol>
<p>1xx. <a href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/">Agentic Design Patterns Part 1</a></p>
<h3><span id="源码">源码</span><a href="#源码" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/670586507">Autogen的基本框架,人工智能的管理系统——Autogen系列02</a><br>    源码解析<br>1xx. <a href="https://zhuanlan.zhihu.com/p/699819907">AUTOGEN | 上手与源码分析</a>    </p>
<h3><span id="paper">paper</span><a href="#paper" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/660027092">AutoGen：通过多agent对话支持下一代 LLM 应用程序</a> paper 中文</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent  Memory</title>
    <url>/www6vHomeAIGC/2023/06/05/gptAgentMemory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="memory-sources1">Memory Sources[1]</span><a href="#memory-sources1" class="header-anchor">#</a></h1><ul>
<li><p>Inside-trial Information: 智能体在进行本次任务时与环境交互的历史信息。</p>
</li>
<li><p>Cross-trial Information: 智能体在此前完成该类任务的历史经验信息。</p>
</li>
<li><p>External Knowledge: 智能体在当前交互环境之外所获得的信息。</p>
</li>
</ul>
<p>与充当<strong>短期记忆</strong>的<strong>inside-trial information</strong>相反，<strong>cross-trial information</strong>可以被视为<strong>长期记忆</strong>；</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/696745373">论文分享：A Survey on the Memory Mechanism of Large Language Model based Agents</a><br>1xx. <a href="https://blog.csdn.net/DLparkour/article/details/138506437">论文阅读：A Survey on the Memory Mechanism of Large Language Model based Agents</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/696105075">【Agent技术洞察】01-增强大语言模型 Agents 的工作记忆能力</a></p>
<h3><span id="项目">项目</span><a href="#项目" class="header-anchor">#</a></h3><p><a href="https://github.com/kingjulio8238/memary">memary Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)推理 TensorRT-LLM</title>
    <url>/www6vHomeAIGC/2023/06/02/gptInferTensorRT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#tensorrt-llm">TensorRT-LLM</a><ul>
<li><a href="#key-features-2">key features [2]</a></li>
</ul>
</li>
<li><a href="#tensorrt-llm-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2">TensorRT-LLM 推理部署</a></li>
<li><a href="#triton-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2">Triton 推理部署</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="tensorrt-llm">TensorRT-LLM</span><a href="#tensorrt-llm" class="header-anchor">#</a></h1><h3><span id="key-features-2">key features [2]</span><a href="#key-features-2" class="header-anchor">#</a></h3><ul>
<li>Flash Attention</li>
<li>MHA&#x2F;MQA&#x2F;GQA</li>
<li><strong>Quantization</strong><ul>
<li>Weight-Only</li>
<li>SmoothQuant</li>
<li><strong>GPTQ</strong></li>
<li><strong>AWQ</strong></li>
<li>FP8</li>
</ul>
</li>
<li>Paged <strong>KV Cache</strong> for the Attention</li>
<li>Multi-GPU Multi-Node</li>
<li><strong>TP(Tensor Parallelism)&#x2F;PP(Pipeline Parallelism)</strong></li>
<li>In-flight <strong>Batching</strong></li>
</ul>
<h1><span id="tensorrt-llm-推理部署">TensorRT-LLM 推理部署</span><a href="#tensorrt-llm-推理部署" class="header-anchor">#</a></h1><p>[基于docker的部署]</p>
<h1><span id="triton-推理部署">Triton 推理部署</span><a href="#triton-推理部署" class="header-anchor">#</a></h1><p>[基于k8s的部署]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol start="2">
<li><a href="https://github.com/NVIDIA/TensorRT-LLM/">TensorRT-LLM</a> git</li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/666849728">TensorRT-LLM保姆级教程（一）-快速入门</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/667572720">TensorRT-LLM保姆级教程（二）-离线环境搭建、模型量化及推理</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/669576221">TensorRT-LLM（持续更新）</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/629336492">模型推理服务化框架Triton保姆式教程（一）：快速入门</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/634143650">模型推理服务化框架Triton保姆式教程（二）：架构解析</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/634444666">模型推理服务化框架Triton保姆式教程（三）：开发实践</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) KV Cache</title>
    <url>/www6vHomeAIGC/2023/06/01/gptInferKVCache/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/KV-Cache-52168038d1874bce9d5cf68c5930f5c1?pvs=4">(原理) KV Cache</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) vLLM</title>
    <url>/www6vHomeAIGC/2023/05/31/gptInfervLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="vllm">vLLM</span><a href="#vllm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vLLM-ccb00d32fef14f92b0f7ab4c1c1db390?pvs=4">(原理) vLLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)RAG Langchain-Chatchat</title>
    <url>/www6vHomeAIGC/2023/05/31/gptRAGchatchat/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="langchain-chatchat-架构">Langchain-Chatchat 架构</span><a href="#langchain-chatchat-架构" class="header-anchor">#</a></h1>

<ul>
<li>组件<ul>
<li>本地知识库</li>
<li>Embedding 模型</li>
<li>向量数据库</li>
<li>Prompt Template</li>
</ul>
</li>
</ul>
<h1><span id="langchain-chatchat">Langchain-Chatchat</span><a href="#langchain-chatchat" class="header-anchor">#</a></h1><ul>
<li>部署 <ul>
<li>windows 10 [5]<br>部署本地， 没显存，卡</li>
<li>Linux [2]<br>部署   32C125G ，没显存， 推理很慢 </li>
<li>Docker</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/chatchat-space/Langchain-Chatchat">Langchain-Chatchat </a> master<br>Langchain 与 ChatGLM 等语言模型的本地知识库问答<br><a href="https://github.com/chatchat-space/Langchain-Chatchat/tree/v0.2.4">Langchain-Chatchat</a>  v0.2.4<br><a href="https://gitee.com/deepeye/langchain-ChatGLM">langchain-ChatGLM</a>  gitee </p>
</li>
<li><p><a href="https://github.com/www6v/Langchain-Chatchat-Colab">Colab for Langchain-Chatchat</a>   linux 可以部署  v0.2.6</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/649055955">langChain-ChatGLM 尝试，踩坑记录</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651189680">Langchain-Chatchat + 阿里通义千问Qwen 保姆级教程 | 次世代知识管理解决方案</a>    Langchain-Chatchat + 通义千问</p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_43094965/article/details/133044128">win10 安装 Langchain-Chatchat 避坑指南（2023年9月18日v0.2.4版本，包含全部下载内容！）</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>多轮对话</title>
    <url>/www6vHomeAIGC/2023/05/28/gptDialogue/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D">传统的多轮对话</a><ul>
<li><a href="#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D12">多轮对话[1][2]</a></li>
<li><a href="#%E9%9A%BE%E7%82%B93">难点[3]</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B-2">基础能力 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Ellm%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D4">基于LLM的多轮对话[4]</a><ul>
<li><a href="#types">Types</a></li>
<li><a href="#evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="传统的多轮对话">传统的多轮对话</span><a href="#传统的多轮对话" class="header-anchor">#</a></h1><h3><span id="多轮对话12">多轮对话[1][2]</span><a href="#多轮对话12" class="header-anchor">#</a></h3><ul>
<li>NLU<ul>
<li>意图(intent)分类 [3]   </li>
<li>槽位抽取</li>
</ul>
</li>
<li>DM<br>DST + DP(Policy)</li>
<li>NLG</li>
</ul>
<blockquote>
<p>多轮-上下文</p>
</blockquote>
<h3><span id="难点3">难点[3]</span><a href="#难点3" class="header-anchor">#</a></h3><ul>
<li>上下文信息丢失</li>
<li>指代词识别</li>
</ul>
<h3><span id="基础能力-2">基础能力 [2]</span><a href="#基础能力-2" class="header-anchor">#</a></h3><ul>
<li>意图识别</li>
<li>情绪识别</li>
</ul>
<h1><span id="基于llm的多轮对话4">基于LLM的多轮对话[4]</span><a href="#基于llm的多轮对话4" class="header-anchor">#</a></h1><h3><span id="types">Types</span><a href="#types" class="header-anchor">#</a></h3><ul>
<li><p>Task-oriented dialogue system<br> NLU -&gt; DST -&gt; DPL-&gt; NLG</p>
</li>
<li><p>open-domain dialogue system</p>
</li>
</ul>
<h3><span id="evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</span><a href="#evolution-of-lm-based-dialogue-system" class="header-anchor">#</a></h3><ul>
<li><p>Fusion within Task-oriented dialogue system(task)</p>
<ul>
<li>task<ul>
<li>NLU  DST  DPL【可有可无】</li>
<li>NLG【保留】</li>
</ul>
</li>
<li><strong>end2end Task-oriented DS的出现</strong></li>
</ul>
</li>
<li><p>fusion between TOD and ODD(data)</p>
<ul>
<li>TOD -&gt; ODD<br>Q-TOD</li>
<li>ODD -&gt; TOD<br>UniDS</li>
</ul>
</li>
<li><p>fusion between dialogue modal and language model(model)<br><strong>LLM as DM</strong><br>【LLM本身有对话的能力，需要被激发出来】【instruction tuning】<br>【chat模型是做价值观对齐】</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1dt4y1S7kn/">自然语言处理：多轮对话在工业中的应用-贪心学院</a> *** V</li>
<li><a href="https://www.bilibili.com/video/BV1vZ4y147Qv/">1-人-人对话数据驱动的多轮对话技术探索与实践-孙超博</a> V 美团</li>
<li><a href="https://www.bilibili.com/video/BV1Yt4y1S75w/">人工智能如何在多轮对话中进行意图理解——祝凯华</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1Mb4y137yB/">基于大模型对话系统的前世今生</a>  V<br>《An Survey of the Evolution of Language Model-Based Dialogue Systems》</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489281&idx=1&sn=0273bf49530a93df16ecf5cb5fbc8f65">前沿重器[37] | 大模型对任务型对话的作用研究</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>对话</category>
      </categories>
      <tags>
        <tag>对话</tag>
      </tags>
  </entry>
  <entry>
    <title>LLama-Factory</title>
    <url>/www6vHomeAIGC/2023/05/24/gptLLamaFactory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llama-factory">LLama-Factory</span><a href="#llama-factory" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLama-Factory-b6e286a1e9054c5399eebc5ffaeac82e?pvs=4">LLama-Factory</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLamaFactory</category>
      </categories>
      <tags>
        <tag>LLamaFactory</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 算力</title>
    <url>/www6vHomeAIGC/2023/05/23/gptGPU/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="gpu算力">GPU算力</span><a href="#gpu算力" class="header-anchor">#</a></h1><h3><span id="免费1">免费[1]</span><a href="#免费1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/free.JPG" class>

<ul>
<li>modelscope 100小时 GPU</li>
</ul>
<h3><span id="专业收费2">专业收费[2]</span><a href="#专业收费2" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/cost.JPG" class>


<h1><span id="显卡">显卡</span><a href="#显卡" class="header-anchor">#</a></h1><ul>
<li><p>显卡天梯榜<br> <a href="https://topic.expreview.com/GPU">显卡天梯榜</a></p>
</li>
<li><p>显卡<br>显卡 &#x3D; GPU +  显存</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1fC4y1N7qV/">5种在线GPU算力资源白嫖指南</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1q5411z7HM/">5种专业在线GPU算力资源白嫖指南</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV1Pv4y1f7VV/">【PyTorch深度学习】01 GPU购买与白嫖指南</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)RAG Index</title>
    <url>/www6vHomeAIGC/2023/05/21/gptRAGIndex/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="rag-index">RAG Index</span><a href="#rag-index" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/RAG-Index-108bfe2110848011b8d3e9ac4fd9138a?pvs=4">(原理|实战)RAG Index</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)RAG Rerank</title>
    <url>/www6vHomeAIGC/2023/05/14/gptRAGRerank/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#reranker-22">Reranker [22]</a></li>
<li><a href="#%E4%BA%A7%E5%93%81">产品</a><ul>
<li><a href="#bge-ranker-20">BGE Ranker [20]</a></li>
<li><a href="#bce24">BCE[24]</a></li>
<li><a href="#%E4%BC%98%E7%A7%80%E7%9A%84%E7%BB%84%E5%90%88-21">优秀的组合 [21]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="reranker-22">Reranker [22]</span><a href="#reranker-22" class="header-anchor">#</a></h1><p>A reranking model — also known as a <strong>cross-encoder</strong> — is a type of model that,<strong>given a query and document pair, will output a similarity score.</strong> </p>
<p><img src="https://www.pinecone.io/_next/image/?url=https://cdn.sanity.io/images/vr8gru94/production/9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&w=3840&q=65" alt="Rerankers"></p>
<h1><span id="产品">产品</span><a href="#产品" class="header-anchor">#</a></h1><h3><span id="bge-ranker-20">BGE Ranker [20]</span><a href="#bge-ranker-20" class="header-anchor">#</a></h3><p><strong>交叉编码器</strong>将对查询和答案实时计算相关性分数，这比**向量模型(即双编码器)**更准确，但比向量模型更耗时。 因此，它可以用来对嵌入模型返回的前k个文档重新排序。 我们在多语言数据上训练了交叉编码器，数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。 </p>
<h3><span id="bce24">BCE[24]</span><a href="#bce24" class="header-anchor">#</a></h3><p>中文效果比BGE好[老刘说nlp]</p>
<h3><span id="优秀的组合-21">优秀的组合 [21]</span><a href="#优秀的组合-21" class="header-anchor">#</a></h3><p>OpenAI + CohereRerank<br>Voyage + big-reranker-large</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="20">
<li><p><a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md">BGE Reranker</a><br>  <a href="https://www.bilibili.com/video/BV1sQ4y137Ft/">transformers二次开发——bge-reranker模型微调流程</a> V<br><a href="https://mp.weixin.qq.com/s/XnkQFCdbvjox1Y06IbIlYw">RAG 再添新利器！智源开源最强检索排序模型 BGE Re-Ranker v2.0 </a></p>
</li>
<li><p><a href="https://luxiangdong.com/2023/11/06/rerank-ev/#">提升RAG——选择最佳Embedding和重新排名模型 </a><br>  <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">Boosting RAG: Picking the Best Embedding &amp; Reranker models</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/series/rag/rerankers/">Rerankers and Two-Stage Retrieval</a>     ***          文中的第二阶段就是指Reranker</p>
</li>
<li><p><a href="https://github.com/netease-youdao/BCEmbedding">youdao RerankerModal</a> BCE</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a><br>   <a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Query Routing</title>
    <url>/www6vHomeAIGC/2023/05/14/gptRAGRouting/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="类型1">类型[1]</span><a href="#类型1" class="header-anchor">#</a></h1><ul>
<li><strong>LLM Routers</strong><ul>
<li>LLM Completion Routers</li>
<li>LLM Function Calling Routers</li>
</ul>
</li>
<li><strong>Semantic Routers</strong> [2]</li>
<li>Zero Shot Classification Routers</li>
<li>Language Classification Routers</li>
</ul>
<p><img src="https://miro.medium.com/v2/format:webp/1*fJnUoOwsykBTU1MyLgHQFg.png" alt="Routers"></p>
<h1><span id="logical-and-semantic-routing3">Logical and Semantic routing[3]</span><a href="#logical-and-semantic-routing3" class="header-anchor">#</a></h1><h3><span id="logical-routing">Logical routing</span><a href="#logical-routing" class="header-anchor">#</a></h3><h3><span id="semantic-routing">Semantic routing</span><a href="#semantic-routing" class="header-anchor">#</a></h3><p>【基于embedding的相似度匹配】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220">Routing in RAG-Driven Applications</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1H64y1E75Y/">Sematic router 让LLM更加快速做出决策</a> V<br><a href="https://github.com/aurelio-labs/semantic-router/">semantic-router Repo</a> git</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a><br><a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git<br><a href="https://www.bilibili.com/video/BV1eJ4m1p7kj/">RAG(检索增强） 从入门到精通 路由（routing)</a> V</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent Challenge</title>
    <url>/www6vHomeAIGC/2023/05/13/gptAgentChallenge/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>





<h1><span id="问题和局限性-4">问题和局限性 [4]</span><a href="#问题和局限性-4" class="header-anchor">#</a></h1><ul>
<li><p>记忆召回问题<br>只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好</p>
</li>
<li><p>错误累积问题</p>
</li>
<li><p>探索效率问题<br>中途引入人工的判断干预和反馈输入</p>
</li>
<li><p>任务终止与结果验证<br>模型 agent 的工作如何终止也是一个挑战</p>
</li>
</ul>
<h1><span id="挑战-8">挑战 [8]</span><a href="#挑战-8" class="header-anchor">#</a></h1><h3><span id="如何让-agent-选择合适的工具">如何让 agent 选择合适的工具</span><a href="#如何让-agent-选择合适的工具" class="header-anchor">#</a></h3><ul>
<li>Toolformer - fine tune</li>
<li>Gorilla - retrieval，fine tune</li>
</ul>
<h3><span id="不必要的工具使用">不必要的工具使用</span><a href="#不必要的工具使用" class="header-anchor">#</a></h3><p>“Human Input”也写成一种工具，让模型来主动发起对人类的提问<br><a href="https://python.langchain.com/docs/integrations/tools/human_tools">Human as a tool</a></p>
<h3><span id="agent-返回的格式不稳定">Agent 返回的格式不稳定</span><a href="#agent-返回的格式不稳定" class="header-anchor">#</a></h3><p>这里常见的做法是让 LLM <strong>按照 json 这类常见的 schema 来返回</strong>，一般稳定性会高一些（相比“Action:”这种）。<br>此外自动修复重试也很实用，可以利用 LangChain 里的 <strong>output parsers</strong> 来帮助完成。</p>
<h3><span id="记住之前的操作避免重复">记住之前的操作，避免重复</span><a href="#记住之前的操作避免重复" class="header-anchor">#</a></h3><p>AutoGPT - retrieval 结合近期操作记录</p>
<h3><span id="处理超长的-observation">处理超长的 observation</span><a href="#处理超长的-observation" class="header-anchor">#</a></h3><p>需要用一些工具从中<strong>提取有用信息</strong>，或者<strong>放到外部存储中再借助 retrieval 来使用</strong>。</p>
<h3><span id="专注于目标">专注于目标</span><a href="#专注于目标" class="header-anchor">#</a></h3><p>简单的做法是<strong>在 prompt 结尾处再把目标加上</strong>，引起 agent 的注意。<br>另外像 BabyAGI，HuggingGPT 这种把 <strong>planning 和 execution 分开</strong>的做法也是很有用。<strong>拆分的比较细</strong>的任务往往步骤比较短，也不容易丢失目标。</p>
<h3><span id="结果评估">结果评估</span><a href="#结果评估" class="header-anchor">#</a></h3><ul>
<li><strong>评估最终结果</strong>是否正确</li>
<li><strong>过程的细化评估</strong><ul>
<li>选择的中间步骤是否正确。</li>
<li>生成 action 的 input 是否正确。</li>
<li>生成的步骤序列是否合理高效。</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="4">
<li><a href="https://zhuanlan.zhihu.com/p/622947810">AutoGPT与LLM Agent解析</a> *** </li>
<li><a href="https://zhuanlan.zhihu.com/p/633033220">LLM 全栈开发指南补遗</a>  Agents  ***<br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/">Harrison Chase: Agents</a>  ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/679032270">LLM Agent 现状和一些思考 （202401）</a><br>   当前 Agent 的缺陷和挑战</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/kCXZN7Wli-RCvZXRb6mF7g">Agent开发者坦白：窘境中前行</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent Planning</title>
    <url>/www6vHomeAIGC/2023/05/13/gptAgentPlanning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="types1">Types[1]</span><a href="#types1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/05/13/gptAgentPlanning/plans.webp" class>

<ul>
<li><p><strong>任务分解</strong> </p>
</li>
<li><p>多计划选择</p>
</li>
<li><p>外部规划器辅助规划</p>
</li>
<li><p><strong>反思和提炼</strong>[20] </p>
</li>
<li><p>记忆增强规划</p>
</li>
</ul>
<h1><span id="任务分解">任务分解</span><a href="#任务分解" class="header-anchor">#</a></h1><ul>
<li><p>ReACT 范式 [2]<br>把<strong>融合了Reasoning和Acting</strong>的一种范式，推理过程是浅显易懂，仅仅<strong>包含thought-action-observation步骤</strong>，很容易判断推理的过程的正确性，使用ReAct做决策甚至超过了强化学习.  </p>
<ul>
<li>chain-of-thought推理-问题<br> 事实幻想（fact hallucination）和错误传递（error propagation）</li>
</ul>
</li>
<li><p>Plan-and-execute agents [2]<br>本质上是先计划再执行，即先把用户的问题分解成一个个的子任务，然后再执行各个子任务，最后合并输出得到结果</p>
</li>
</ul>
<h1><span id="patterns">Patterns</span><a href="#patterns" class="header-anchor">#</a></h1><ul>
<li>Self-ask [2]<br>Self-ask是一种follow-up的使用范式，仅仅包含follow-up, immediate answer步骤，至于follow-up多少个step，完全由它自己决定，估计这就是Self-ask的名字的由来。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《Understanding the planning of LLM agents: A survey》<br><a href="https://mp.weixin.qq.com/s/1POXDVJDv3ob1HqpKjb3Mg">大语言模型智能体规划能力综述: 分类、任务分解、选择、反思、记忆增强 </a> 翻译<br>  <a href="https://zhuanlan.zhihu.com/p/693264551">Agent四大范式 | 综述：全面理解Agent工作原理</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/642357544">2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用</a> ***  论文+代码</p>
</li>
<li><a href="/www6vHomeAIGC/2023/04/07/gptAgentReflection/" title="Reflection Agent">Reflection Agent</a> self</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/NhpJMmIcnF57qEuUkxD4kQ">AI Agent规划能力全面拆解</a></p>
<p>1xx. <a href="https://baoyu.io/translations/ai-paper/2311.11797-igniting-language-intelligence-the-hitchhikers-guide-from-chain-of-thought-reasoning-to-language-agents">引领语言智能：从思维链推理到语言智能体的探索指南 [译]</a> paper</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488040&idx=1&sn=f404a5fc2b0380eac00564046abc77d5">2023年大语言模型智能体规划技术(LLM Agent Planning)研究进展汇总</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG 优化</title>
    <url>/www6vHomeAIGC/2023/05/09/gptRAGOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%B4%E7%B4%A0rag-embedding">朴素RAG Embedding</a><ul>
<li><a href="#embedding-%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%A1%88%E5%8F%8A%E5%B1%80%E9%99%90%E6%80%A71">Embedding 召回方案及局限性[1]</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">解决方案</a></li>
</ul>
</li>
<li><a href="#%E8%A1%8C%E4%B8%9A%E9%97%AE%E7%AD%943">行业问答[3]</a><ul>
<li><a href="#%E6%8C%91%E6%88%98">挑战</a></li>
<li><a href="#%E4%BC%98%E5%8C%96">优化</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="朴素rag-embedding">朴素RAG Embedding</span><a href="#朴素rag-embedding" class="header-anchor">#</a></h1><h3><span id="embedding-召回方案及局限性1">Embedding 召回方案及局限性[1]</span><a href="#embedding-召回方案及局限性1" class="header-anchor">#</a></h3><ul>
<li>召回<strong>精度低</strong></li>
<li><strong>粒度过粗</strong></li>
<li>不支持条件查询&#x2F;统计</li>
<li>不能替代信息提取</li>
</ul>
<h3><span id="解决方案">解决方案</span><a href="#解决方案" class="header-anchor">#</a></h3><ul>
<li><p>问题理解——准确识别<strong>用户意图</strong>(传统NLP)  [2]</p>
</li>
<li><p>基于<strong>关键词Embedding</strong>的入库和搜索 [2]</p>
<ul>
<li><strong>关键词提取</strong><ul>
<li>实现信息抽取（Information Extraction，IE）<ul>
<li>实体关系三元组抽取(RE, Relation Extraction )</li>
<li>命名实体识别(NER, Name-Entity Recognition)</li>
<li>事件抽取(EE, Event Extraction)</li>
</ul>
</li>
</ul>
</li>
<li>基于 LLM 提取 [不推荐]<ul>
<li>结果不准确、开销也大</li>
</ul>
</li>
<li><strong>传统 NLP 方法提取</strong>[推荐]<ul>
<li>名词短语提取与整合</li>
<li>依存分析</li>
<li>成分句法分析</li>
</ul>
</li>
<li>总结<br>从<strong>完整语句的 Embedding</strong>，切换为<strong>关键词 Embedding</strong>：</li>
<li>优势<ul>
<li>相比传统 Embedding，大幅提升<strong>召回精准度</strong>。</li>
<li>使用传统 NLP 在专项问题处理上，相比 LLM 提供更好的精度和性能。</li>
</ul>
</li>
</ul>
</li>
<li><p>知识库存储选型</p>
<ul>
<li>Vector Store<ul>
<li>分片:  区分<strong>层级结构</strong></li>
</ul>
</li>
<li>Relational Database</li>
<li>Graph Database   <ul>
<li><strong>图数据检索</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="行业问答3">行业问答[3]</span><a href="#行业问答3" class="header-anchor">#</a></h1><h3><span id="挑战">挑战</span><a href="#挑战" class="header-anchor">#</a></h3><ul>
<li>版面复杂多样</li>
<li>文本分块<br><strong>存在知识点被分割、不完整的情况</strong>。</li>
<li>多因素影响内容召回效果<ul>
<li>例如：文档内容相似度高(专业文档细分领域、版本迭代等)；</li>
<li>通用的<strong>向量相似度算法</strong>效果不好(问题与问题匹配 VS问题与答案匹配)；</li>
<li>召回率受文档库增大而降低</li>
</ul>
</li>
</ul>
<h3><span id="优化">优化</span><a href="#优化" class="header-anchor">#</a></h3><ul>
<li><p>向量化上的优化</p>
<ul>
<li>训练目标优化为提升<strong>Query与段落的相关性</strong>，使得<strong>问题和相关段落的语义向量表示更接近</strong>，训练模型有<strong>sbert</strong>，<strong>cosent</strong>等</li>
</ul>
</li>
<li><p>关键信息上的优化</p>
<ul>
<li>在文档内容的信息压缩上，进行文本<strong>关键词和摘要的提取</strong><ul>
<li>从完整语句的Embedding，切换为<strong>关键词Embedding</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/664921095">RAG探索之路的血泪史及曙光</a>  腾讯<br> Embedding, Retrieval</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/641132245">LLM+Embedding构建问答系统的局限性及优化方案</a><br>基于关键词Embedding的入库和搜索的流程图,  结合传统nlp任务<br>1xx. <a href="https://zhuanlan.zhihu.com/p/627655485">基于大语言模型构建知识问答系统</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404338&idx=1&sn=3c8f8c44ac7a1d925216b40833525b25">再看业界大模型行业问答的困难及若干业界实践：兼看智能客服常用路线及多场景prompt </a><br>问题 优化</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407281&idx=2&sn=f39b46cad1787123b485d76dff33bc93">大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407056&idx=1&sn=0a0ce93a9199a2eae36493a515e42181">温故而知新:大模型RAG问答研发的7个失分点及MOE专家组合模型的若干浅析 </a><br>   《Seven Failure Points When Engineering a Retrieval Augmented Generation System》<br>   <a href="https://baoyu.io/translations/ai-paper/2401.05856v1-seven-failure-points-when-engineering-a-retrieval-augmented-generation-system">在构建检索增强型生成系统时的七大挑战 [译]</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403693&idx=1&sn=e47f34cd58f103d37998dbbfd01c41ee">大模型行业落地实践的一些总结和观点：大模型行业问答落地中的现实挑战以及潜在的缓解策略</a><br>   《DataFunCon2023深圳站-20231125-刘焕勇-大模型行业问答的现实挑战及潜在的缓解策略》 pdf</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404651&idx=2&sn=335db95e104a5b09e33ac2245bae4fd2">再看RAG在真实金融文档问答场景的实践方案：SMP2023 金融大模型挑战赛的两种代表实现思路</a></p>
<p>1xx. <a href="https://baoyu.io/translations/rag/mastering-rag-how-to-architect-an-enterprise-rag-system">构建企业级 RAG 系统的高级指南 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG Framework</title>
    <url>/www6vHomeAIGC/2023/05/09/gptRAGFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%86%E6%9E%B6-0">框架 [0]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="框架-0">框架 [0]</span><a href="#框架-0" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/infiniflow/ragflow/tree/main"><strong>ragflow</strong></a> </p>
</li>
<li><p><a href="https://github.com/netease-youdao/QAnything/tree/master"><strong>QAnything</strong></a> </p>
</li>
<li><p><a href="https://github.com/chatchat-space/Langchain-Chatchat/releases/tag/v0.2.8"><strong>langchainchat</strong></a></p>
</li>
<li><p><a href="https://github.com/labring/FastGPT"><strong>FastGPT</strong></a>  </p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/"><strong>LangChain</strong></a> </p>
</li>
<li><p><a href="https://github.com/run-llama/llama_index/"><strong>LlamaIndex</strong></a></p>
</li>
<li><p><a href="https://github.com/langchain4j/langchain4j">langchain4j</a> </p>
</li>
<li><p><a href="https://github.com/Azure/GPT-RAG">GPT-RAG</a> </p>
</li>
<li><p><a href="https://github.com/Unstructured-IO/unstructured"><strong>Unstructured</strong></a></p>
</li>
<li><p><a href="https://github.com/StanGirard/quivr">Quivr</a> </p>
</li>
<li><p><a href="https://github.com/langgenius/dify"><strong>Dify</strong></a> </p>
</li>
<li><p><a href="https://github.com/weaviate/Verba">Verba</a> </p>
</li>
<li><p><a href="https://github.com/danswer-ai/danswer">danswer</a></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407281&idx=2&sn=f39b46cad1787123b485d76dff33bc93">大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 </a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/ZoI4Dscm9f9m5-q4Dq4bag">大模型RAG问答开源框架的两个风向:兼看大模型安全的学术评测</a><br>   RAGFlow - 引入文档理解及溯源机制<br>   QAnything - 优化embeddding+召回侧方向的</p>
<p>1xx. <a href="https://llamahub.ai/">LlamaHub</a><br>      Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/XRfSqYwvuGB6sDJzRm0QVA">FlashRAG：可能是最全的、最快搭建RAG的开源框架 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)LangGraph</title>
    <url>/www6vHomeAIGC/2023/05/07/gptMultiAgentsPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#langgraph-1">LangGraph [1]</a><ul>
<li><a href="#agent-supervisor">Agent Supervisor</a></li>
<li><a href="#multi-agent-collaboration">Multi Agent Collaboration</a></li>
<li><a href="#hierarchical-agent-teams">Hierarchical Agent Teams</a></li>
</ul>
</li>
<li><a href="#framework">Framework</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#langgraph">LangGraph</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="langgraph-1">LangGraph [1]</span><a href="#langgraph-1" class="header-anchor">#</a></h1><h3><span id="agent-supervisor">Agent Supervisor</span><a href="#agent-supervisor" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb">Agent Supervisor Repo</a> git</p>
<h3><span id="multi-agent-collaboration">Multi Agent Collaboration</span><a href="#multi-agent-collaboration" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb">Basic Multi-agent Collaboration</a> git</p>
<h3><span id="hierarchical-agent-teams">Hierarchical Agent Teams</span><a href="#hierarchical-agent-teams" class="header-anchor">#</a></h3><p><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb">Hierarchical Agent Teams</a> git</p>
<h1><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h1><ul>
<li>LangGraph</li>
<li>AutoGen</li>
<li>MetaGPT</li>
<li>CrewAI - OpenAI</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="langgraph">LangGraph</span><a href="#langgraph" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.langchain.dev/langgraph-multi-agent-workflows/">LangGraph: Multi-Agent Workflows</a></li>
<li><a href="https://www.bilibili.com/video/BV1F541117kW/">LangGraph：Multi-Agent 实战</a> V</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agents</category>
      </categories>
      <tags>
        <tag>Agents</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Data Selection</title>
    <url>/www6vHomeAIGC/2023/05/05/gptDataSelection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#ifd1">IFD[1]</a></li>
<li><a href="#mods2">MoDS[2]</a></li>
<li><a href="#deita-3">DEITA [3]</a><ul>
<li><a href="#%E5%A4%8D%E6%9D%82%E6%80%A7%E8%AF%84%E5%88%86">复杂性评分</a></li>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%AF%84%E5%88%86">质量评分</a></li>
<li><a href="#%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%AD%9B%E9%80%89">多样性筛选</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="ifd1">IFD[1]</span><a href="#ifd1" class="header-anchor">#</a></h1><ul>
<li>三个步骤<ul>
<li>Learning from Brief Experience<br>利用少量进行进行<strong>模型初学</strong> </li>
<li>Evaluating Based on Experience<br>利用初学模型计算原始数据中所有<strong>IFD指标</strong><ul>
<li>算法<ul>
<li>条件回答分数（ Conditioned Answer Score，CAS）</li>
<li>直接答案分数（Direct Answer Score，DAS）</li>
<li>指令跟随难度（Instruction-Following Difficulty，IFD）分数</li>
</ul>
</li>
</ul>
</li>
<li>Retraining from Self-Guided Experience<br>利用<strong>樱桃数据</strong>进行模型<strong>重训练</strong></li>
</ul>
</li>
</ul>
<h1><span id="mods2">MoDS[2]</span><a href="#mods2" class="header-anchor">#</a></h1><ul>
<li><p>质量筛选<br>采用OpenAssistant的<strong>reward-model</strong>-debertav3-large-v2模型（一个基于<strong>DeBERTa架构</strong>设计的奖励模型）对数据进行<strong>质量打分</strong>。</p>
</li>
<li><p>多样性筛选<br>为了避免所选质量数据高度相似，通过<strong>K-Center-Greedy算法</strong>进行数据筛选，在最大化多样性的情况下，使指令数据集最小。<br>在该步骤中，采用<strong>BERT模型</strong>为指令数据生成句向量来计算不同数据之间的距离。</p>
</li>
<li><p>必要性筛选</p>
</li>
</ul>
<h1><span id="deita-3">DEITA [3]</span><a href="#deita-3" class="header-anchor">#</a></h1><h3><span id="复杂性评分">复杂性评分</span><a href="#复杂性评分" class="header-anchor">#</a></h3><ul>
<li>复杂性评估的方法  <ul>
<li>Random Selection：随机选择样本。</li>
<li>Instruction Length：按照指令的长度计算复杂性。</li>
<li><strong>Perplexity</strong>：通过预训练模型计算回复的困惑度作为复杂性指标，困惑值越大意味着数据样本越难。</li>
<li><strong>Direct Scoring</strong>：利用ChaGPT给指令的复杂性打分。</li>
<li>Instruction Node：利用ChatGPT将指令转换成语义树，通过树的节点数作为复杂性指标。</li>
<li><strong>Instag Complexity</strong>：利用ChatGPT对部分数据进行打标签，再训练一个Llama模型，再利用训练后的Llama模型对全量数据预测，标签越多说明数据约复杂。</li>
<li><strong>IFD</strong>：指令跟随难度作为复杂性指标。</li>
</ul>
</li>
</ul>
<p>DEITA评估复杂性的方法，主要先对一个小规模种子数据集（2k）进行数据复杂性<strong>扩展</strong>，再利<strong>用ChatGPT对扩展数据进行打分</strong>，并<strong>训练一个Llama1-7B的模型</strong>，最后利用训练后的模型对数据的打分作为复杂性评估指标。</p>
<h3><span id="质量评分">质量评分</span><a href="#质量评分" class="header-anchor">#</a></h3><ul>
<li>质量评估的方法有<ul>
<li>Random Selection：随机选择样本。</li>
<li>Response Length：采用输出长度作为质量评估指标。</li>
<li>Direct Scoring：利用ChatGPT直接评估对特定指令输出结果的准确性。</li>
</ul>
</li>
</ul>
<p>DEITA评估质量的方法，<strong>与评估复杂性方法一致</strong>。先对一个小规模种子数据集（2k，与复杂性数据一致）进行数据质量扩展，再利用ChatGPT对扩展数据进行打分并训练一个Llama1-7B的模型，最后利用训练后的模型对数据的打分作为质量评估指标。</p>
<p><strong>数据质量扩展</strong>，通过特殊的提示词利用ChatGPT对数据的回复部分进行改写，主要是增强回复的有用性、相关性、丰富深度、创造力和提供额外的细节描述。</p>
<h3><span id="多样性筛选">多样性筛选</span><a href="#多样性筛选" class="header-anchor">#</a></h3><p>多样性筛选方法，首先将数据池中的数据按照复杂性和质量的综合得分（复杂性分数*质量分数）进行降序<strong>排序</strong>；<br>然后按顺序逐个取出样本数据x ，<strong>计算x 与筛选池中相邻最近的样本之间距离值</strong>，其中，数据利用Llama1-13B模型进行向量表征，距离计算采用<strong>余弦相似度</strong>。<br>如果<strong>距离值小于 r时</strong>，认为该样本与筛选池中数据相似程度不高，可以<strong>纳入筛选池</strong>；否则<strong>不纳入筛选池</strong>。当筛选池中样本数达到规定样本个数，完成多样性筛选。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/658128530">如何从数据集中自动识别高质量的指令数据-IFD指标的使用</a><br>《From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning》<br>ChatLaw就这么训的</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/671183709">大模型微调技巧 | 高质量指令数据筛选方法-MoDS</a><br>《MoDS: Model-oriented Data Selection for Instruction Tuning》<br> 质量筛选， 多样性筛选，必要性筛选   </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/675928711">DEITA-大模型指令微调的数据高效筛选方法</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/687339776">DEITA：融合复杂度、质量、多样性的高效数据筛选</a><br>   复杂度、质量、多样性</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报 </a><br>《A Survey on Data Selection for Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Data Selection</category>
      </categories>
      <tags>
        <tag>Data Selection</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)LIMA, LESS</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lima-1kimi">LIMA [1][kimi]</a></li>
<li><a href="#less-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-10">LESS 核心思想 [10]</a></li>
<li><a href="#less10kimi">LESS[10][kimi]</a><ul>
<li><a href="#%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95">实验方法：</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论：</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#lima">LIMA</a></li>
<li><a href="#less">LESS</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="lima-1kimi">LIMA [1][kimi]</span><a href="#lima-1kimi" class="header-anchor">#</a></h1><p>LIMA（Less Is More for Alignment）的实验通过一系列设计精良的步骤来探究数据质量、多样性以及数量对模型性能的影响，从而得出了<strong>提高数据质量和增加提示多样性比单纯增加数据量更能提升模型性能的结论</strong>。以下是<strong>实验方法</strong>的关键步骤：</p>
<ol>
<li><p><strong>精心策划的微调数据</strong>：LIMA模型在<strong>1000个</strong>精心策划的提示和回复上进行了<strong>微调</strong>，这些数据被设计为模拟真实用户与AI助手的交互。</p>
</li>
<li><p><strong>消融实验</strong>：通过消融实验，研究者们观察了在增加数据量的同时不增加提示多样性时，模型性能的提升是否有限；而在优化数据质量时，性能是否有显著提升。</p>
</li>
<li><p><strong>数据构造</strong>：研究者从Stack Exchange、wikiHow和Pushshift Reddit数据集收集数据，并进行了<strong>质量和多样性</strong>的控制。这些数据集被用来构造训练样本，以确保输入的多样性和输出的一致性。</p>
</li>
<li><p><strong>质量与多样性的对比</strong>：研究者比较了经过质量过滤的Stack Exchange数据和同质化的wikiHow数据对模型性能的影响。结果显示，更<strong>多样化的Stack Exchange数据在性能上优于同质化的wikiHow数据</strong>。 【多样化】</p>
</li>
<li><p><strong>数量的对比</strong>：研究者对从Stack Exchange抽取的指数级增加的训练集进行了测试，发现<strong>训练集的翻倍并没有改善响应质量</strong>，从而说明单纯增加数据量并不一定能提升性能。【数量】</p>
</li>
<li><p><strong>质量控制的实验</strong>：研究者还比较了未经过任何质量或风格过滤的Stack Exchange数据集与经过过滤的数据集上训练的模型性能，发现<strong>过滤后</strong>的数据集上训练的模型性能<strong>更优</strong>。【质量】</p>
</li>
<li><p><strong>人类评估</strong>：为了评估LIMA模型的性能，研究者进行了人类偏好研究，将LIMA的输出与其他几个基线模型的输出进行比较，并让人群工作者选择他们更喜欢的输出。</p>
</li>
</ol>
<p>通过这些实验步骤，LIMA的研究得出了<strong>数据质量和提示多样性对于提升模型性能的重要性远超过单纯增加数据量的结论</strong>。这些发现支持了“浅层对齐假说”，即模型在预训练阶段已经学习到了几乎所有知识和能力，而微调过程主要是学习与人类交互的风格和格式。</p>
<ul>
<li><p>总结 [1]</p>
<p>消融实验显示，<strong>当扩大数据量而不同时扩大提示多样性时，收益会大大减少，而在优化数据质量时，收益会大大增加</strong><br>【<strong>数量</strong> &lt;–&gt; <strong>多样性</strong>  <strong>质量</strong>】</p>
</li>
</ul>
<h1><span id="less-核心思想-10">LESS 核心思想 [10]</span><a href="#less-核心思想-10" class="header-anchor">#</a></h1><p>通过仅给出<strong>少数体现特定能力的示例</strong>，从大量指令数据集中<strong>有效地选择5%有影响力的数据</strong>用于目标指令微调，结果优于全量数据集进行微调，并且所选子集在不同模型参数规模和不同模型系列中仍然普遍有效。</p>
<h1><span id="less10kimi">LESS[10][kimi]</span><a href="#less10kimi" class="header-anchor">#</a></h1><p>LESS（Selecting Influential Data for Targeted Instruction Tuning）的实验方法和相应的结论如下：</p>
<h3><span id="实验方法">实验方法：</span><a href="#实验方法" class="header-anchor">#</a></h3><ol>
<li><p><strong>热身训练（Warmup Training）</strong>：使用LoRA（Low-Rank Adaptation）技术对预训练模型进行热身训练，以适应特定的数据分布。</p>
</li>
<li><p><strong>梯度数据存储（Gradient Data Store）</strong>：构建了一个具有投影低维梯度特征的梯度数据存储，该存储可以重复用于不同的目标任务。</p>
</li>
<li><p><strong>数据选择算法</strong>：利用数据存储和算法选择与体现特定能力的少数示例最相似的训练数据点。?</p>
</li>
<li><p><strong>模型训练</strong>：使用选择的数据子集来训练目标模型。</p>
</li>
<li><p><strong>评估</strong>：在不同的下游任务上评估LESS选择的数据子集的性能，包括MMLU、TYDIQA和BBH数据集。</p>
</li>
</ol>
<h3><span id="结论">结论：</span><a href="#结论" class="header-anchor">#</a></h3><ol>
<li><p><strong>LESS的有效性</strong>：LESS<strong>在不同的模型中都是有效的</strong>，能够在多个评估数据集上提高性能。</p>
</li>
<li><p><strong>数据子集的性能</strong>：<strong>使用LESS选择的5%的数据通常优于使用完整数据集进行训练的结果</strong>。这表明完整数据集可能包含与特定目标任务无关或有害的数据点。</p>
</li>
<li><p><strong>数据的可转移性</strong>：使用较小模型选择的数据可以提高较大模型和不同模型系列的性能，证明了LESS选择的数据具有高度的可转移性。</p>
</li>
<li><p><strong>与其他方法的比较</strong>：LESS是唯一一致有效的方法，相较于其他基线方法（如随机选择、BM25、DSIR、RDS）表现出更好的性能。</p>
</li>
<li><p><strong>计算成本</strong>：LESS的计算成本较高，但由于其有效性，这一成本是合理的。</p>
</li>
<li><p><strong>定性分析</strong>：LESS选择的数据能够体现预期下游应用所需的推理技能，而不是仅仅基于表面形式线索。</p>
</li>
<li><p><strong>局限性</strong>：LESS需要热身训练阶段，这增加了计算负载。此外，使用补全Token的平均梯度可能导致性能问题。还有，最小化验证损失并不总能提高任务性能，且数据选择中的线性度假设是LESS的一个限制。</p>
</li>
</ol>
<p>总体而言，LESS通过选择与目标任务高度相关的数据点，能够在指令微调中实现高效的性能提升，尽管存在一些局限性和计算成本。</p>
<p>【总结:  少量有<strong>质量</strong>的数据  优于  全量数据 】 【数据选择算法】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="lima">LIMA</span><a href="#lima" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/c50HrOfKOqgqGPVRHf6EpA">大模型微调究竟需要多少数据：从三个现有代表工作看几组结论及一点思考 </a><br>  <strong>指令格式的多样性</strong><br>  《LIMA: Less Is More for Alignment》<br>  《MAYBE ONLY 0.5% DATA IS NEEDED》</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/jinniulema/article/details/133915276">【论文笔记】LIMA: Less Is More for Alignment</a></p>
<h3><span id="less">LESS</span><a href="#less" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/686007325">LESS：仅选择5%有影响力的数据优于全量数据集进行目标指令微调</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/686687923">LESS 实践：用少量的数据进行目标指令微调</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Data Management</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataManagement/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%863">数据管理[3]</a><ul>
<li><a href="#pretraining-of-llm">Pretraining of LLM</a></li>
<li><a href="#sft">SFT</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据管理3">数据管理[3]</span><a href="#数据管理3" class="header-anchor">#</a></h1><h3><span id="pretraining-of-llm">Pretraining of LLM</span><a href="#pretraining-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
<ul>
<li><strong>Scaling Laws</strong></li>
<li>Data Repetition</li>
</ul>
</li>
<li><p>Data Quality</p>
<ul>
<li>Deduplication<ul>
<li>N-gram和Hash技术<br><strong>MinHash算法</strong></li>
<li>神经网络方法</li>
<li>语义去重<br>SemDeDup</li>
</ul>
</li>
<li>Quality Filtering   <ul>
<li><strong>分类器</strong></li>
<li><strong>启发式规则</strong></li>
<li>阈值过滤<br>例如基于困惑度（Perplexity）</li>
</ul>
</li>
<li>Diversity &amp; Age<ul>
<li><strong>数据多样性（Diversity）</strong></li>
<li>数据时效性（Age）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="sft">SFT</span><a href="#sft" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
</li>
<li><p>Data Quality</p>
<ul>
<li>Instruction Quality<br>Instruction Mining,  LIMA</li>
<li><strong>Instruction Diversity</strong><br><strong>Self-Instruct</strong>,  <strong>#InsTag</strong>， Alpaca</li>
<li><strong>Instruction Complexity</strong><br><strong>WizardLM</strong>,  <strong>#InsTag</strong>, <strong>Evol-Instruct</strong></li>
<li>Prompt Design</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li>《Data Management For Large Language Models: A Survey》huawei<br> <a href="https://blog.csdn.net/weixin_60760661/article/details/136058893">大模型的数据管理——论文精读</a><br> <a href="https://github.com/www6v/data_management_LLM">Data Management for LLM</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DataManagement</category>
      </categories>
      <tags>
        <tag>DataManagement</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT Scaling</title>
    <url>/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="sft-scaling">SFT Scaling</span><a href="#sft-scaling" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SFT-Scaling-36916e81271a4c1d963d9f357b919508?pvs=4">(原理)SFT Scaling</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Data  Annotation</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h3><p>1xx.  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408650&idx=2&sn=ef8424969be749489188ebd810800f08">如何利用大模型进行数据标注与知识蒸馏：兼看ActiveRAG上下文去噪的大模型RAG问答范式</a><br>   大模型用于数据标注<br>   《Large Language Models for Data Annotation: A Survey》<br>1xx. <a href="https://mp.weixin.qq.com/s/U3kWk_jPaeBzloOhUJ5DXQ">LLM数据标注技术调研：定义、框架、提示、反馈、评价、挑战、机遇 </a> 翻译<br>   《Large Language Models for Data Annotation: A Survey》</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399919&idx=1&sn=66fc1dfdba57744a80c6869b8cf941af">ChatGPT用于数据标注是否可行：基于推特分类、生成内容排序任务的代表性实验报告介绍 </a></p>
<h3><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h3><p>1xx.  AutoLabel - 自助数据标注 </p>
<p>1xx. <a href="https://opendatalab.github.io/labelU/">LabelU 介绍</a><br>   <a href="https://github.com/opendatalab/labelU">LabelU Repo</a> git 上海人工智能实验室</p>
<p>1xx. <a href="https://developer.aliyun.com/article/1311807">InsTag：大语言模型监督微调数据标签标注工具</a>  有相关的paper<br>   <a href="https://www.modelscope.cn/studios/lukeminglkm/instagger_demo/summary">InsTag指令打标工具</a> demo</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(List) Pretrain 数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="pretrain数据集">Pretrain数据集</span><a href="#pretrain数据集" class="header-anchor">#</a></h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/641187337">LLM大模型数据集之谜</a> Pretrain数据集</li>
<li><a href="https://github.com/brightmart/nlp_chinese_corpus">大规模中文自然语言处理语料</a></li>
<li><a href="https://github.com/Glanvery/LLM-Travel/blob/main/LLM_Pretrain_Datasets.md">开源的可用于LLM Pretrain数据集</a> Pretrain数据集</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399359&idx=1&sn=502c65376e14b20a7dc1ceb35c62141d">大规模语言模型训练必备数据集-The Pile：涵盖22类、800GB的多样性文本数据集概述 </a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>3、RLFH强化与预训练数据集 </li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403405&idx=1&sn=cb53c35efda2b771b4c1f289ae97c1d3">大模型研发必备：两大开源可用且清洗过的中文文本语料库及大模型FLOPS、参数量快速估计工具推荐 </a>           书生·万卷1.0 ,    wudao数据集</li>
<li><a href="https://mp.weixin.qq.com/s/JDkKlD9IKvagCYucPey6UQ">大规模中文开源文本训练数据集的几点启发：兼看两个知识图谱与大模型的练手竞赛 </a><br> 万卷数据集   wudao数据集  MVBNC数据集  OpenNewsArchive</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(List)SFT数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="sft数据集12">SFT数据集[1][2]</span><a href="#sft数据集12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>1、通用指令微调数据</p>
</li>
<li><p><a href="https://github.com/chaoswork/sft_datasets">开源SFT数据集整理</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Modular RAG</title>
    <url>/www6vHomeAIGC/2023/04/21/gptRAGModularRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="modular-rag">Modular RAG</span><a href="#modular-rag" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Modular-RAG-108bfe21108480468c35c5b45d991778?pvs=4">(原理)Modular RAG</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Query Transformation</title>
    <url>/www6vHomeAIGC/2023/04/20/gptQueryTransformation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#transformation-%E5%A4%9A%E6%A0%B7%E6%80%A7">Transformation-多样性</a><ul>
<li><a href="#multi-query%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%AD%96%E7%95%A53">Multi Query多查询策略[3]</a></li>
<li><a href="#rag-fusion%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A53">RAG-Fusion多查询结果融合策略[3]</a></li>
<li><a href="#decomposition%E9%97%AE%E9%A2%98%E5%88%86%E8%A7%A3%E7%AD%96%E7%95%A53">Decomposition问题分解策略[3]</a></li>
<li><a href="#query-rewrite-12">query rewrite [1][2]</a></li>
</ul>
</li>
<li><a href="#transformation-%E6%8A%BD%E8%B1%A1%E5%8C%96">Transformation-抽象化</a><ul>
<li><a href="#step-back%E9%97%AE%E7%AD%94%E5%9B%9E%E9%80%80%E7%AD%96%E7%95%A5-3">Step Back问答回退策略 [3]</a></li>
<li><a href="#step-back-prompting-12">Step-back Prompting [1][2]</a></li>
</ul>
</li>
<li><a href="#transformation-%E5%85%B7%E4%BD%93%E5%8C%96">Transformation-具体化</a><ul>
<li><a href="#hyde%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A53">HyDE混合策略[3]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="transformation-多样性">Transformation-多样性</span><a href="#transformation-多样性" class="header-anchor">#</a></h1><h3><span id="multi-query多查询策略3">Multi Query多查询策略[3]</span><a href="#multi-query多查询策略3" class="header-anchor">#</a></h3><p>该方法<strong>从多个角度重写用户问题</strong>，为每个重写的问题检索文档，返回所有查询的唯一文档。</p>
<h3><span id="rag-fusion多查询结果融合策略3">RAG-Fusion多查询结果融合策略[3]</span><a href="#rag-fusion多查询结果融合策略3" class="header-anchor">#</a></h3><p>将多个召回查询的结果进行<strong>合并</strong></p>
<h3><span id="decomposition问题分解策略3">Decomposition问题分解策略[3]</span><a href="#decomposition问题分解策略3" class="header-anchor">#</a></h3><ul>
<li><p>Answer recursively迭代式回答<br>在问题分解的基础上，逐步迭代出答案，<strong>将上一步问题的答案，与下一步骤的答案进行拼接</strong>，送入大模型进行问答</p>
</li>
<li><p>Answer individually<br>也可以<strong>让每个subquery分别进行处理</strong>，然后得到答案，然后再拼接成一个QA pairspprompt最终形成答案。</p>
</li>
</ul>
<h3><span id="query-rewrite-12">query rewrite [1][2]</span><a href="#query-rewrite-12" class="header-anchor">#</a></h3><ul>
<li><a href="https://arxiv.org/pdf/2305.14283.pdf">论文</a><strong>使用LLM重写用户查询</strong>，而不是直接使用原始用户查询进行检索。<br>因为对于LLM 而言，<strong>原始查询不可能总是最佳检索结果</strong>，可以让LLM重写查询。</li>
<li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb">Repo</a> git<br>【问题的多样化】</li>
</ul>
<h1><span id="transformation-抽象化">Transformation-抽象化</span><a href="#transformation-抽象化" class="header-anchor">#</a></h1><h3><span id="step-back问答回退策略-3">Step Back问答回退策略 [3]</span><a href="#step-back问答回退策略-3" class="header-anchor">#</a></h3><p>Step Back问答回退，首先提示LLM提出一个<strong>关于高级概念或原则的通用后退问题</strong>，并检索有关它们的相关事实，使用此基础来帮助回答用户问题。</p>
<h3><span id="step-back-prompting-12">Step-back Prompting [1][2]</span><a href="#step-back-prompting-12" class="header-anchor">#</a></h3><ul>
<li><a href="https://arxiv.org/pdf/2310.06117.pdf">论文</a>使用退一步提示，<strong>使用LLM生成”后退”(Step back prompting)问题</strong>。<br>使用检索时，”后退”问题和原始问题都会被用来进行检索，然后这两个结果都会被用来作为语言模型回复的基础。</li>
<li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb">Repo</a> git<br>【问题的抽象化】</li>
</ul>
<h1><span id="transformation-具体化">Transformation-具体化</span><a href="#transformation-具体化" class="header-anchor">#</a></h1><h3><span id="hyde混合策略3">HyDE混合策略[3]</span><a href="#hyde混合策略3" class="header-anchor">#</a></h3><p>LLM将<strong>问题</strong>转换为回答问题的<strong>假设文档</strong>。<strong>使用嵌入的假设文档检索真实文档</strong>，前提是doc-doc相似性搜索可以产生更多相关匹配。</p>
<ul>
<li>HyDE<br>At a high level, HyDE is an embedding technique that takes queries, <strong>generates a hypothetical answer</strong>, and then embeds that generated document and uses that as the final example.</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406156&idx=1&sn=d91a4df105c4fc4c9523f7141bc1c24d">知识图谱用于细粒度大模型幻觉评估：兼论Langchain-RAG问答中的问题改写范式 </a><br> RAG:  rewrite , Step back, fusion </p>
</li>
<li><p><a href="https://blog.langchain.dev/query-transformations/">Query Transformations</a>  </p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg">一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化</a> ***   原理paper，代码示例<br>Multi Query多查询策略， Decomposition问题，RAG-Fusion， Step Back， HyDE混合<br><a href="https://github.com/langchain-ai/rag-from-scratch">rag-from-scratch Repo</a> git<br><a href="https://www.bilibili.com/video/BV1Vx421U7a4/">RAG(检索增强） 从入门到精通 虚拟文档嵌入（Hyde)</a> V</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/393914267">业界总结｜搜索中的Query理解</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/149429784">智能扩充机器人的“标准问”库之Query生成</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489295&idx=1&sn=fcb269e47dc27fcaf31201aa1c75dafb">前沿重器[38] | 微软新文query2doc：用大模型做query检索拓展</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>文档智能</title>
    <url>/www6vHomeAIGC/2023/04/19/gptDocumentAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%96%87%E6%A1%A3%E7%90%86%E8%A7%A3-10">文档理解 [10]</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84ocr-free%E5%BE%AE%E8%B0%83%E6%96%B9%E6%A1%88">基于大模型的OCR-FREE微调方案</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E7%89%88%E5%BC%8F%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE%E9%9B%86">文档版式分析数据集</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a><ul>
<li><a href="#mplug-docowl15-20">mPLUG-DocOwl1.5 [20]</a></li>
<li><a href="#textmonkey-20">TextMonkey [20]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%85%B6%E4%BB%96-1">其他</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="文档理解-10">文档理解 [10]</span><a href="#文档理解-10" class="header-anchor">#</a></h1><h3><span id="基于大模型的ocr-free微调方案">基于大模型的OCR-FREE微调方案</span><a href="#基于大模型的ocr-free微调方案" class="header-anchor">#</a></h3><ul>
<li>LLaVAR [12]</li>
<li>TextMonkey [11]</li>
</ul>
<h3><span id="文档版式分析数据集">文档版式分析数据集</span><a href="#文档版式分析数据集" class="header-anchor">#</a></h3><h1><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h1><h3><span id="mplug-docowl15-20">mPLUG-DocOwl1.5  [20]</span><a href="#mplug-docowl15-20" class="header-anchor">#</a></h3><p>DocOwl1.5由mPLUG-Owl2初始化，使用<strong>ViT&#x2F;L-14作为视觉编码器</strong>，并使用带有模态自适应模块的7B大模型作为<strong>解码器</strong>。<br>每个子图像由ViT&#x2F;L-14编码为1,024个特征，然后由<strong>H-Reducer缩减为256个特征</strong>。</p>
<h3><span id="textmonkey-20">TextMonkey [20]</span><a href="#textmonkey-20" class="header-anchor">#</a></h3><p>为了减少图像特征的冗余，继承了<strong>Qwen-VL</strong>中的图像<strong>重采样器</strong>，在每个窗口中都会使用。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="10">
<li><a href="https://mp.weixin.qq.com/s/FsjoUUFssMv2UkbxM-IJ3A">值得一看的文档理解前沿方案及版式分析开源数据：三种模式、九大数据集 </a></li>
<li><a href="https://github.com/Yuliang-Liu/Monkey">Monkey</a><br><a href="http://vlrlab-monkey.xyz:7684/">Monkey Demo</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/670175648">LLaVAR：增强的视觉指令微调</a><br><a href="https://llavar.github.io/">LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding</a></li>
</ol>
<p>1xx. <a href="https://huggingface.co/blog/zh/document-ai">加速 Document AI (文档智能) 发展</a><br>    <a href="https://baijiahao.baidu.com/s?id=1755096032832674219&wfr=spider&for=pc">加速 Document AI (文档智能) 发展</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/d2Nns1qashMbcXPMG-4McQ">阿里面向企业数字化的文档智能技术与应用</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://mp.weixin.qq.com/s/1MSOZfbKcPW1BTT4f9XvQg">也看跨模态大模型遇见文档理解：mPLUG-DocOwl1.5及TextMonkey方案中的数据工程 </a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DocumentAI</category>
      </categories>
      <tags>
        <tag>DocumentAI</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)RAG Baichuan案例</title>
    <url>/www6vHomeAIGC/2023/04/18/gptRAGBaichuan/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="baichuan-rag1">Baichuan RAG[1]</span><a href="#baichuan-rag1" class="header-anchor">#</a></h1><ul>
<li>借鉴了Meta的CoVe技术</li>
<li>自研的TSF（Think-Step Further)技术<br>猜测其本质应该是对Step-back prompting方法的改良</li>
<li>自研了Baichuan-Text-Embedding向量模型 </li>
<li>混合检索<br>向量检索与稀疏检索并行的</li>
<li>self-Critique</li>
</ul>
<h1><span id="总结2">总结[2]</span><a href="#总结2" class="header-anchor">#</a></h1><ol>
<li><strong>多轮问答等场景的召回和传统搜索引擎的召回分布还不太一样。</strong>百川借助子问题检索效果更高的特点，对原始复杂问题进行拆解、拓展来解决复杂问题检索质量偏差的问题。</li>
<li><strong>对于没见过的语料直接用向量检索的结果可能不太理想。</strong>百川在大量语料上利用无监督方法训练embedding模型来优化效果。而行业大模型更倾向于私有的数据，要提升私有数据的训练效果还得继续在私有化数据上训练效果会更佳。</li>
<li><strong>Query拓展 + 多路召回 + Rerank + self-Critique可能是现阶段比较好的一种RAG方式，但是其也会带来更多成本。</strong>总体思路有点像ReAct[3]系列的进阶版本，其在搜索侧和答案修正侧都做了更多的一些工作来优化实际效果。其缺点是需要多次调用大模型，会带来额外的成本，真实线上是否采用这种策略还有待验证。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407638&idx=1&sn=5c167b4a11bc483f5790ef1e0340d670">大模型RAG问答行业最佳案例及微调、推理双阶段实现模式：基于模块化(Modular)RAG自定义RAG Flow</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/675770700">百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/658469464">LLM&#x2F;百川Baichuan2-53B搜索增强-开放API</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650901201&idx=1&sn=3a9bd61403fb4b024ec5d8c128990495">大模型+搜索构建完整技术栈，百川智能用搜索增强给企业定制化下了一剂「猛药」</a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_27590277/article/details/135421245">百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Embedding</title>
    <url>/www6vHomeAIGC/2023/04/18/gptEmbedding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#example-5">example [5]</a></li>
<li><a href="#embedding-%E4%BB%B7%E5%80%BC-6">Embedding 价值 [6]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8-6">应用 [6]</a></li>
<li><a href="#%E5%A4%A9%E6%A2%AF%E6%A6%9C">天梯榜</a></li>
<li><a href="#example7">example[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

 

<h1><span id="example-5">example [5]</span><a href="#example-5" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong>:   t-SNE  </li>
<li>K-Means 聚类</li>
<li>文本搜索  相似度搜索</li>
</ul>
<h1><span id="embedding-价值-6">Embedding 价值 [6]</span><a href="#embedding-价值-6" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong><br>将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</li>
<li>捕捉语义信息<br>Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。</li>
<li>泛化能力<br>由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示</li>
</ul>
<h1><span id="应用-6">应用 [6]</span><a href="#应用-6" class="header-anchor">#</a></h1><ul>
<li>语义表示和语义相似度</li>
<li>词语关系和类比推理</li>
<li>上下文理解</li>
<li>文本分类和情感分析</li>
<li>机器翻译和生成模型</li>
</ul>
<h1><span id="天梯榜">天梯榜</span><a href="#天梯榜" class="header-anchor">#</a></h1><p>  <a href="https://huggingface.co/spaces/mteb/leaderboard">mteb&#x2F;leaderboard</a></p>
<h1><span id="example7">example[7]</span><a href="#example7" class="header-anchor">#</a></h1><ul>
<li>m3e模型</li>
<li>bge模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="5">
<li><p><a href="https://github.com/www6v/openai-quickstart/blob/main/openai_api/embedding.ipynb">embedding</a> git</p>
</li>
<li><p>《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding</p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/135311471">一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge</a></p>
</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Hk4y1X7aG/">如何选取RAG中的embedding模型</a><br>   <a href="https://huggingface.co/spaces/mteb/leaderboard">huggingface embedding模型排行榜</a><br>   <a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence Bert</a><br>   <a href="https://github.com/blackinkkkxi/RAG_langchain">Demo Repo</a>  git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/qIh07eU8_lYL2gBVzTFzKA">引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案</a><br>   <a href="https://github.com/xlang-ai/instructor-embedding">Repo</a> git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406715&idx=1&sn=a680597afdb7d5439a11302c7911795f">也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 </a>        《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/676589001">如何提高LLMs的文本表征(Text Embedding)能力?</a><br>    《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1ex4y1S7u5/?p=2">文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速）</a>   V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Embedding</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Gorilla</title>
    <url>/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E6%96%B9%E6%B3%95%E8%AE%BA1">方法论[1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%B6%E9%9B%86">数据集收集</a></li>
<li><a href="#gorilla">Gorilla</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://ar5iv.labs.arxiv.org/html/2305.15334">Gorilla: Large Language Model Connected with Massive APIs</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/ShishirPatil/gorilla">gorilla</a> git</p>
</li>
</ul>
<h1><span id="方法论1">方法论[1]</span><a href="#方法论1" class="header-anchor">#</a></h1><h3><span id="数据集收集">数据集收集</span><a href="#数据集收集" class="header-anchor">#</a></h3><p><strong>API文档</strong></p>
<ul>
<li>HuggingFace平台托管和提供了约203,681个模型。然而，其中许多模型的文档质量较差，缺乏依赖项，模型卡中没有信息等问题。</li>
<li>为了筛选出质量较好的模型，从每个领域选择了前20个模型。考虑了多模态数据领域的7个领域，CV领域的8个领域，NLP领域的12个领域，音频领域的5个领域，表格数据领域的2个领域和强化学习领域的2个领域。</li>
<li>经过筛选，从HuggingFace获得了总共925个模型。从TensorFlow Hub获得了801个模型，并从Torch Hub获得了95个模型。</li>
<li>这些模型的信息被转换为<strong>JSON对象</strong>，其中包含了领域（domain）、框架（framework）、功能（functionality）、API名称（api_name）、API调用（api_call）、API参数（api_arguments）、环境要求（environment_requirements）、示例代码（example_code）、性能（performance）和描述（description）等字段。</li>
<li>选择这些字段是为了将其泛化到机器学习领域之外的其他领域，包括RESTful API调用。<br><strong>总而言之</strong>，通过筛选和整理，从HuggingFace、TensorFlow Hub和Torch Hub等平台获取了<strong>总共1,645个模型</strong>的信息，并将其以<strong>JSON对象</strong>的形式进行了记录和描述。这些信息包括模型的领域、框架、功能、API调用示例、性能等，以便在机器学习和其他领域中使用和参考。</li>
</ul>
<p><strong>指令生成 （Instruction Generation ）</strong></p>
<ul>
<li>在<strong>self-instruct</strong>范例[42]的指导下，使用GPT-4生成了合成指令数据。</li>
<li>提供了三个上下文示例和一个参考API文档，要求模型生成调用API的真实世界用例。</li>
<li>明确指示模型在创建指令时不要使用任何API名称或提示。</li>
<li>为每个三个模型中心构建了六个示例（<strong>指令-API对</strong>），共计18个点，这些数据是手动生成或修改的。</li>
<li>对于<strong>1,645个</strong>API数据点中的每一个，从相应的六个指令示例中随机选择3个，生成<strong>总共10个指令-API对</strong>。</li>
<li>强调只需要使用GPT-4生成指令，可以与开源替代方案（如LLaMA、Alpaca等）进行交换。<br><strong>总而言之</strong>，通过使用GPT-4生成指令，并结合上下文示例和参考API文档，在每个模型中心构建了六个示例，共计18个点。这些示例被用于<strong>生成1,645个API数据点中的每一个的指令-API对，生成总共10个对应关系</strong>。与开源替代方案相比，GPT-4的指令生成功能被应用在这个过程中。</li>
</ul>
<h3><span id="gorilla">Gorilla</span><a href="#gorilla" class="header-anchor">#</a></h3><p><strong>带有约束的API调用（API Call with Constraints）</strong></p>
<ul>
<li>API调用通常具有固有的<strong>约束</strong>，这些约束要求LLM不仅理解API调用的功能，还要<strong>根据不同的约束参数对调用进行分类</strong>。</li>
<li>机器学习API调用中常见的约束集是参数大小和准确性的下限。这些约束要求LLM能够根据提示理解和回答问题，例如根据提示选择参数少于10M的图像分类模型，并且至少保持70%的ImageNet准确率。</li>
<li><strong>对LLM来说，理解和推理出请求中嵌入的各种约束是一个巨大的挑战</strong>。LLM需要细致地理解用户的功能描述，并能够正确地处理伴随这些调用的复杂约束。</li>
<li>这个挑战凸显了在实际API调用中对LLM的复杂要求。仅仅理解API调用的基本功能是不够的，<strong>模型还必须能够应对伴随这些调用的约束，如参数大小和准确性要求</strong>。<br>总而言之，在机器学习API调用中，LLM面临着理解和处理约束的挑战。除了理解API调用的基本功能外，LLM还需要能够识别和满足伴随调用的约束要求，如参数大小和准确性的下限。这需要模型具备更细致的理解和推理能力，以满足实际API调用的复杂需求。</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/640697382">Gorilla：与大规模API相连的大型语言模型</a> ***</li>
</ol>
<p>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b">Gorilla：链接海量API的大型语言模型</a> V<br>1xx. <a href="https://zhuanlan.zhihu.com/p/632583909">大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？</a> ***</p>
<p>1xx. <a href="https://gorilla.cs.berkeley.edu/">Gorilla: Large Language Model Connected with Massive APIs</a><br>1xx. <a href="https://gorilla.cs.berkeley.edu/blog.html">Gorilla blog</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent Tuning</title>
    <url>/www6vHomeAIGC/2023/04/07/gptAgentTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="tuning">Tuning</span><a href="#tuning" class="header-anchor">#</a></h1><h3><span id="agenttuning">AgentTuning</span><a href="#agenttuning" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404626&idx=1&sn=da5ac106548dd30f14a57a5ce4d90f08">基于llama7B的文本嵌入模型ANGLE：兼看Agent微调数据的生成方案</a>  AgentTuning<br>1xx. <a href="https://zhuanlan.zhihu.com/p/671295938">LLM之Agent（五）| AgentTuning：清华大学与智谱AI提出AgentTuning提高大语言模型Agent能力</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663362992?utm_id=0">AgentTuning解读</a></p>
<h3><span id="agenttuning-实战">AgentTuning 实战</span><a href="#agenttuning-实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/690012170">2024年大模型Agent tuning关键技术Fireact, Agent-FLAN, AgentOhana, Agent LUMOS, STE, ETO,MoE, DebateGPT等</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/UCO_m38QcWdCoT_DIFc96A">Agent-FLAN 技术报告——社区翻译版 </a></p>
<p>1xx. <a href="https://cloud.tencent.com/developer/article/2421687">LLM 大模型学习必知必会系列(九)：Agent微调最佳实践，用消费级显卡训练属于自己的Agent！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Reflection Agent</title>
    <url>/www6vHomeAIGC/2023/04/07/gptAgentReflection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="react">ReAct</span><a href="#react" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://react-lm.github.io/">ReAct: Synergizing Reasoning and Acting in Language Models</a> paper</li>
</ol>
<h3><span id="reflexion">Reflexion</span><a href="#reflexion" class="header-anchor">#</a></h3><ol start="21">
<li><a href="https://zhuanlan.zhihu.com/p/639254455">【论文阅读】Reflexion: 大模型如何从错误经验中学习？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/671508578">Reflexion: 带言语强化学习的语言智体</a><br>  1xx. <a href="https://blog.langchain.dev/reflection-agents/">Reflection Agents</a><br>  <a href="https://www.bilibili.com/video/BV1KJ4m1a7rZ/">LangGraph：Reflection Agents 实战</a> V</li>
</ol>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p><a href="https://zhuanlan.zhihu.com/p/691370751">Agent四大范式 | CRITIC：吴恩达力推Agent设计范式</a></p>
<h3><span id="practice">Practice</span><a href="#practice" class="header-anchor">#</a></h3><p><a href="https://github.com/andrewyng/translation-agent">Translation Agent: Agentic translation using reflection workflow</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent 分类[有趣|有用]</title>
    <url>/www6vHomeAIGC/2023/04/06/gptAgentCategory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%9C%89%E8%B6%A3%E7%9A%84ai%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%9A%84ai">有趣的AI：更像人的AI</a><ul>
<li><a href="#%E5%A5%BD%E7%9C%8B%E7%9A%84%E7%9A%AE%E5%9B%8A-%E5%A4%9A%E6%A8%A1%E6%80%81">好看的皮囊 多模态</a></li>
<li><a href="#%E6%9C%89%E8%B6%A3%E7%9A%84%E7%81%B5%E9%AD%82">有趣的灵魂</a></li>
</ul>
</li>
<li><a href="#%E6%9C%89%E7%94%A8%E7%9A%84ai%E6%9B%B4%E5%83%8F%E5%B7%A5%E5%85%B7%E7%9A%84ai">有用的AI：更像工具的AI</a><ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B">大模型基础能力</a></li>
<li><a href="#1p-3p-%E4%BA%A7%E5%93%81%E6%B3%95%E5%88%99">1P-3P 产品法则</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E5%92%8C%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7">解决复杂任务和使用工具</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="有趣的ai更像人的ai">有趣的AI：更像人的AI</span><a href="#有趣的ai更像人的ai" class="header-anchor">#</a></h1><h3><span id="好看的皮囊-多模态">好看的皮囊  多模态</span><a href="#好看的皮囊-多模态" class="header-anchor">#</a></h3><ul>
<li><p>多模态<strong>理解能力</strong></p>
<ul>
<li>多模态数据端到端预训练的模型<br>Gemini </li>
<li>工程化<br>  projection layer</li>
<li>直接用文本去粘接 encoder、decoder 和文本大模型</li>
<li>eg【自己动手做出Gemini演示视频的效果】</li>
</ul>
</li>
<li><p>多模态<strong>生成能力</strong></p>
<ul>
<li>视频生成<ul>
<li>Live2D，3D 模型</li>
<li>DeepFake<br>录制一个真人视频， 把视频中的人脸换成指定的人脸照片</li>
<li>Image Animation<br>给定一张照片，随后根据这张照片生成一系列的对应视频 </li>
<li><strong>Video Diffusion</strong><br>对物理世界的建模<br>成本最高</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="有趣的灵魂">有趣的灵魂</span><a href="#有趣的灵魂" class="header-anchor">#</a></h3><ul>
<li><p>个性</p>
<ul>
<li>基于prompt<br>完整地刻画出一个人物的历史、个性、记忆和性格<br>长文本</li>
<li>基于微调的 agent<ul>
<li>更关键的还是数据<ul>
<li><strong>对话性语料</strong> &amp; <strong>事实性语料</strong></li>
<li>第一步，我们先用对话性语料去微调他的个性和说话风格</li>
<li>第二步，再去把事实性语料进行数据清洗后，基于各种角度提问，生成这个人物第一人称口吻的回答，这叫做<strong>数据增强</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>慢思考</strong>与记忆</p>
<ul>
<li>组件<br><strong>记忆、情感</strong>、任务规划、工具</li>
<li>长期记忆<ul>
<li>事实性的记忆<ul>
<li>总结<br>文本总结  MemGPT</li>
<li><strong>RAG 和信息压缩</strong></li>
<li>长上下文  <strong>长上下文</strong><br>结合持久化 KV Cache<br>成本还是太高<br>【eg.  文本总结 + RAG】</li>
</ul>
</li>
<li>程序性的记忆<ul>
<li>few-shot</li>
<li>微调<br>短期来看仍然是效果最好的路线</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="有用的ai更像工具的ai">有用的AI：更像工具的AI</span><a href="#有用的ai更像工具的ai" class="header-anchor">#</a></h1><h3><span id="大模型基础能力">大模型基础能力</span><a href="#大模型基础能力" class="header-anchor">#</a></h3><ul>
<li><strong>复杂任务的规划和分解</strong></li>
<li>遵循复杂指令</li>
<li><strong>自主使用工具</strong></li>
<li>减少幻觉</li>
</ul>
<h3><span id="1p-3p-产品法则">1P-3P 产品法则</span><a href="#1p-3p-产品法则" class="header-anchor">#</a></h3><ul>
<li><p>分类</p>
<ul>
<li>个人助理类</li>
<li>商业智能类</li>
</ul>
</li>
<li><p>OpenAI 的 1P-3P 产品法则</p>
<ul>
<li>只要一两个人（1P）开发的产品就自己（first Party）做<ul>
<li>1P 产品例子<ul>
<li>导游</li>
<li>企业 ERP 助手</li>
<li>大模型采集数据</li>
<li><strong>手机语音助手</strong><br>RPA（机器人流程自动化）<ul>
<li>腾讯的<strong>AppAgent</strong><br>视觉方案</li>
</ul>
</li>
<li>会议和生活记录器</li>
</ul>
</li>
</ul>
</li>
<li>需要三个人（3P）以上开发的产品就让第三方（third Party）做</li>
</ul>
</li>
</ul>
<h3><span id="解决复杂任务和使用工具">解决复杂任务和使用工具</span><a href="#解决复杂任务和使用工具" class="header-anchor">#</a></h3><ul>
<li>慢思考<ul>
<li><strong>思维链</strong><br> 思维链是非常自然的一种慢思考的模式</li>
<li>复杂任务的规划和分解   <ul>
<li>用<strong>多步的网络搜索</strong>去回答难题</li>
</ul>
</li>
<li>AI 需要能够按照流程<strong>调用工具</strong><ul>
<li>工具使用属于过程记忆，使用场景和条件不是语言可以明确描述的<br>使用 <strong>fine-tuning 方法</strong>告诉模型一些工具使用的样例，甚至在预训练时就加入</li>
<li>工具使用可以用代码形式表达，因此属于<strong>代码生成能力</strong><br>使用<strong>RAG方法</strong>获取到工具使用的代码</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/689816790">AI Agent 应该更有趣还是更有用？</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(survey)多模态  数据集</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#survey0">Survey[0]</a></li>
<li><a href="#pre-training%E6%95%B0%E6%8D%AE%E9%9B%86">Pre-training数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86">SFT数据集</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
<li><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86">预训练数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86-1">SFT数据集</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="survey0">Survey[0]</span><a href="#survey0" class="header-anchor">#</a></h1><ul>
<li>Pre-training</li>
<li>Adaptation</li>
</ul>
<h1><span id="pre-training数据集">Pre-training数据集</span><a href="#pre-training数据集" class="header-anchor">#</a></h1><ul>
<li><p>LAION[1]<br><a href="https://laion.ai/projects/">LAION</a></p>
</li>
<li><p>wukong[1]<br><a href="https://zhuanlan.zhihu.com/p/473794131">[论文]中文多模态数据集WuKong &amp; FILIP &amp; LiT-tuning</a><br><a href="https://zhuanlan.zhihu.com/p/551622338">Wukong：一亿规模的中文跨模态预训练基准</a></p>
</li>
<li><p>MMDialog<br><a href="https://zhuanlan.zhihu.com/p/584894471">百万量级的多模态对话数据集来了，153万张图片4000多主题</a> </p>
</li>
<li><p>OBELISC[2]</p>
</li>
<li><p>ShareGPT4V[3]<br>opensource</p>
</li>
</ul>
<h1><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h1><ul>
<li>LAMM</li>
<li>MultiIntruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol start="0">
<li><a href="https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw">多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 </a><br>《A Survey of Multimodal Large Language Model from A Data-centric Perspective》</li>
</ol>
<h3><span id="预训练数据集">预训练数据集</span><a href="#预训练数据集" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/686757824">多模态数据集收集</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/670149958">[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents</a><br>从网页文档里得到的数据集</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/669485001">超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能</a><br><a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">ShareGPT4V</a> git</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/527182857">多模态预训练数据集</a></p>
<p>1xx. <a href="https://opendatalab.org.cn/">OpenDataLab</a></p>
<h3><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h3><p>1xx. <a href="https://datac.blog.csdn.net/article/details/135434897">【LMM 015】LAMM：多模态指令微调数据集，框架和基准</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678489834">[NeurIPS2023] LAMM：多模态指令微调数据集、框架、评测基准</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV12p4y1M7RV/">Talk | ACL’23 杰出论文，MultiIntruct：通过多模态指令集微调提升VLM的零样本学习</a><br>1xx. <a href="https://blog.csdn.net/qq_45978862/article/details/132008907">【ACL2023】MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Dataset</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《Datasets for Large Language Models: A Comprehensive Survey》</p>
</li>
<li><p>开源地址<br> <a href="https://github.com/lmmlzn/Awesome-LLMs-Datasets">Awesome-LLMs-Datasets</a> git</p>
</li>
</ul>
<h1><span id="微调数据">微调数据</span><a href="#微调数据" class="header-anchor">#</a></h1><h3><span id="微调数据的构造方式">微调数据的构造方式</span><a href="#微调数据的构造方式" class="header-anchor">#</a></h3><ul>
<li>人工生成的数据集(HG)</li>
<li>模型构建的数据集(MC)<ul>
<li>Alpaca</li>
<li>BELLE</li>
<li>Self-Instruct</li>
<li>ShareGPT</li>
<li>Wizard</li>
</ul>
</li>
<li>现有数据集的收集和改进(CI) </li>
<li>使用多种方法创建的数据集</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409066&idx=1&sn=54e68bbbd45b4cc5bef8fd446fa187f8">大模型训练数据集(从预训到强化)全面综述：兼看20240229大模型早报 </a>***</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>推理常见参数</title>
    <url>/www6vHomeAIGC/2023/03/30/gptTemperature/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="推理常见参数">推理常见参数</span><a href="#推理常见参数" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/4200b90adfd246ab93bfb1b330aa1bb2?pvs=4">推理常见参数</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Temperature</category>
      </categories>
      <tags>
        <tag>Temperature</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PyTorch</title>
    <url>/www6vHomeAIGC/2023/03/28/gptPytorch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="pytorch-实战">PyTorch 实战</span><a href="#pytorch-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PyTorch-bae29b5883fd45f7a20c97918382da12?pvs=4">(实战)PyTorch</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)PTQ-Weight Only</title>
    <url>/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="weight-only">Weight Only</span><a href="#weight-only" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Weight-Only-ab8e3953cef144a3aef44716d4068216?pvs=4">Weight Only</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)DeepSpeed Training</title>
    <url>/www6vHomeAIGC/2023/03/25/gptTrainDistributedPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="deepspeed-training">DeepSpeed Training</span><a href="#deepspeed-training" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/98541b7f8be2493eb1deda3629677d26?pvs=4">DeepSpeed Training</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Deepspeed</category>
      </categories>
      <tags>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>P-Tuning</title>
    <url>/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="p-tuning2">P-Tuning[2]</span><a href="#p-tuning2" class="header-anchor">#</a></h1><ul>
<li>P-Tuning 的创新之处在于将提示（Prompt）转化为<strong>可学习的嵌入层（Embedding Layer）</strong></li>
</ul>
<h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/ptuning.png" class>

<ul>
<li><p>一个关于“The capital of Britain is [MASK]” 示例：</p>
<ul>
<li>蓝色是上下文 “Britain” </li>
<li>红色是目标单词 “[MASK]”， </li>
<li>橙色区域是提示词。</li>
</ul>
</li>
<li><p>传统方式 与 P-Tuning 对比： </p>
<ul>
<li>在（a）中，提示生成器只接收离散奖励； </li>
<li>在（b）中，连续的<strong>提示嵌入（Prompt Embedding）</strong> 和<strong>提示编码器（Prompt Encoder）</strong>以可微的方式进行 优化。</li>
</ul>
</li>
</ul>
<h1><span id="p-tuning-v22">P-Tuning v2[2]</span><a href="#p-tuning-v22" class="header-anchor">#</a></h1><h3><span id="背景">背景</span><a href="#背景" class="header-anchor">#</a></h3><p>之前的方法在以下两方面有所<strong>限制</strong>：<br>• 模型规模差异：在大型预训练模型中，Prompt Tuning 和<br>P-Tuning 能取得与全面微调相似的效果，但在参数较少<br>的模型上则表现不佳。<br>• 任务类型差异：无论是 Prompt Tuning 还是 P-Tuning，<br>在序列标注任务上的表现都较差。</p>
<h3><span id="目的">目的</span><a href="#目的" class="header-anchor">#</a></h3><p>P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模的预训练模型上，针对各种下游任务都能达到类似全面微调（Fine-tuning）的效果。</p>
<h3><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/ptuning-v2.png" class>
<p>在每一层都加入了Prompts tokens 作为输入,  而不是仅仅加在输入层</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/24/gptPEFTPtuning/compare.png" class>
<ul>
<li>P-tuning  和 Prompt Tuning 仅仅更新<strong>第一个Transformer层</strong></li>
<li>Prefix tuning 和 P-Tuning v2 针对<strong>每一个Transformer 层</strong>进行更新</li>
<li>Prefix tuning 和 P-Tuning 需要<strong>重新参数化(PromptEncoder)</strong>, 而Prompt Tuning 和 P-Tuning v2则<strong>不需要</strong></li>
<li>简单将<strong>P-Tuning</strong>认为是针对 <strong>Prompt Tuning</strong>的<strong>改进</strong>,    <strong>P-Tuning v2</strong> 认为是针对 <strong>Prefix tuning</strong> 的<strong>改进</strong>.</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://aicarrier.feishu.cn/file/H1YvbRyacopEs6xzgZ8c9DDcnIh">大模型参数高效微调技术原理及实践</a> pdf<br><a href="https://www.bilibili.com/video/BV1qw411c7Hd/">如何高效微调大模型？技术原理与最佳实践揭秘！</a> V *** </p>
</li>
<li><p>《3-大模型微调技术揭秘-PEFT》 Ai大模型微调</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) Deepspeed Zero</title>
    <url>/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="deepspeed-zero">Deepspeed Zero</span><a href="#deepspeed-zero" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Zero-Deepspeed-85c9344a27624649a36b66f3e2e4c4d1?pvs=74">(原理) Deepspeed Zero</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Zero</category>
      </categories>
      <tags>
        <tag>Zero</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理-框架</title>
    <url>/www6vHomeAIGC/2023/03/21/gptInferFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E6%A1%86%E6%9E%B61">推理 框架[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-框架1">推理 框架[1]</span><a href="#推理-框架1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/21/gptInferFramework/inference.jpg" class>

<ul>
<li><p>inference execute engine(server)<br>vLLM，TensorRT， deepspeed</p>
</li>
<li><p>inference execute engine(pc&#x2F;edge 移动端)<br> llama.cpp<br> mlc-llm<br> ollama</p>
</li>
<li><p>inference Server<br>Triton Server,  Ray</p>
</li>
<li><p>Chat Server [2]<br>FastChat, XInference,  modelscope  SWIFT</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzA5MTIxNTY4MQ==&scene=1&album_id=2959126655292211206">探秘LLM应用开发</a>   8-19</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2422454">LLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference&#x2F;FastChat等框架]</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142079&idx=1&sn=07d9033203c0064408fe0af33d1f9414">一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142012&idx=1&sn=dafb0b676cdf6d41fd9bd54f9b6a82d3">一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/659792625">大模型推理框架概述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)数据处理</title>
    <url>/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a><ul>
<li><a href="#%E5%8D%83%E5%B8%86llama-2%E4%B8%AD%E6%96%87%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B2%BE%E7%AE%80">数据精简</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">数据配比</a></li>
</ul>
</li>
<li><a href="#%E5%BA%A6%E5%B0%8F%E6%BB%A1%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B31">度小满轩辕金融大模型[31]</a><ul>
<li><a href="#%E9%80%9A%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%B5%81%E6%B0%B4%E7%BA%BF">通用的数据清洗流水线</a></li>
<li><a href="#%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%9C%80%E4%BD%B3%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">增量预训练 最佳数据配比</a></li>
<li><a href="#%E6%9E%84%E9%80%A0%E9%80%9A%E7%94%A8%E5%92%8C%E9%87%91%E8%9E%8D%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE">构造通用和金融指令数据</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B-1">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h1><h2><span id="千帆llama-2中文增强技术介绍-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</span><a href="#千帆llama-2中文增强技术介绍-sft30" class="header-anchor">#</a></h2><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><ul>
<li>Self-instruct</li>
<li>wizard [20]</li>
</ul>
<h3><span id="数据精简">数据精简</span><a href="#数据精简" class="header-anchor">#</a></h3><ul>
<li>低质量过滤</li>
<li>相似数据过滤</li>
</ul>
<h3><span id="数据配比">数据配比</span><a href="#数据配比" class="header-anchor">#</a></h3><ul>
<li>领域数据</li>
<li>多语言数据</li>
</ul>
<h2><span id="度小满轩辕金融大模型31">度小满轩辕金融大模型[31]</span><a href="#度小满轩辕金融大模型31" class="header-anchor">#</a></h2><h3><span id="通用的数据清洗流水线">通用的数据清洗流水线</span><a href="#通用的数据清洗流水线" class="header-anchor">#</a></h3><ul>
<li>文本抽取<ul>
<li>多来源数据收集</li>
<li>正文提取</li>
</ul>
</li>
<li>数据清洗<ul>
<li>规则过滤</li>
<li>模型过滤</li>
</ul>
</li>
<li>去重与校验<ul>
<li>MinHashLSH</li>
<li>质量校验</li>
</ul>
</li>
</ul>
<h3><span id="增量预训练-最佳数据配比">增量预训练 最佳数据配比</span><a href="#增量预训练-最佳数据配比" class="header-anchor">#</a></h3><ul>
<li><p><strong>英文数据  vs 中文数据</strong><br><strong>1  :  3</strong></p>
</li>
<li><p>中文数据中的  <strong>通用数据 vs 金融数据</strong><br>从 9:1 变成  <strong>4:1</strong></p>
<ul>
<li>通用领域指令数据<br> 8大类 50小类</li>
<li>金融领域指令数据<br> 4大类 20小类</li>
</ul>
</li>
</ul>
<h3><span id="构造通用和金融指令数据">构造通用和金融指令数据</span><a href="#构造通用和金融指令数据" class="header-anchor">#</a></h3>
<ul>
<li>Self-Instruct</li>
<li>Evol-Instruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="30">
<li>《千帆增强版 Llama 2》 百度 有ppt</li>
<li>《金融行业实战：度小满轩辕金融大模型应用探索与开发实践》 百度  有ppt</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399885&idx=1&sn=4f49c5148715c38aa4eaee3080435f17">也看大模型训练语料如何清洗：Common Crawl概述、代表性清洗方案及代码实现          </a> 代码<br>   <a href="https://zhuanlan.zhihu.com/p/610659484?utm_id=0">GPT-3 训练语料 Common Crawl 处理流程</a></p>
<p>1xx. <a href="https://github.com/alibaba/data-juicer/blob/main/README_ZH.md">Data-Juicer: 为大语言模型提供更高质量、更丰富、更易“消化”的数据</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Wizard</title>
    <url>/www6vHomeAIGC/2023/03/18/gptDataWizard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#wizard-%E6%96%B9%E6%B3%95">Wizard 方法</a><ul>
<li><a href="#%E8%87%AA%E5%8A%A8%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96-1">自动指令数据进化 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F-%E5%A4%9A%E6%A0%B7%E6%80%A7-%E5%A4%8D%E6%9D%82%E5%BA%A6">质量-&gt; 多样性, 复杂度</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="wizard-方法">Wizard 方法</span><a href="#wizard-方法" class="header-anchor">#</a></h1><h3><span id="自动指令数据进化-1">自动指令数据进化 [1]</span><a href="#自动指令数据进化-1" class="header-anchor">#</a></h3><p>1）指令进化</p>
<ul>
<li>In-Depth Evolving 提示 [深度]<ul>
<li>五种类型的提示来增强指令<br>增加约束 + 深化 + 具体化 + 增加推理步骤 + 复杂化输入</li>
<li>核心部分<br><strong>In-Depth Evolving的提示的核心部分是 “你的目标是将一个给定的提示改写成更复杂的版本，使那些著名的人工智能系统（如ChatGPT和GPT4）更难处理。但改写后的提示必须是合理的，能被人理解，并能被人回应”</strong></li>
</ul>
</li>
<li>In-Breadth Evolving提示 [广度]<ul>
<li>目的<br>旨在提高<strong>主题覆盖率</strong>、<strong>技能覆盖率</strong>和整体数据集的<strong>多样性</strong></li>
</ul>
</li>
</ul>
<p>2）响应生成</p>
<p>3）消除进化<br>   即过滤未能进化的指令</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="质量-gt-多样性-复杂度">质量-&gt; 多样性, 复杂度</span><a href="#质量-gt-多样性-复杂度" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401462&idx=1&sn=764f0302918174cea29ae22ac5760033">如何构造复杂多样的微调指令数据：WizardLM复杂指令构造思想与实验分析工作总结 </a><br> <a href="https://github.com/nlpxucan/WizardLM">WizardLM</a> git</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)多模态</title>
    <url>/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#overview-0">overview [0]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3-1">视觉理解 [1]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90-1">视觉生成 [1]</a><ul>
<li><a href="#human-alignments-in-visual-generation-10">Human Alignments in Visual Generation  [10]</a><ul>
<li><a href="#spatial-controllable-t2i-generation">spatial controllable T2I generation</a></li>
<li><a href="#text-based-editing">text-based editing</a></li>
<li><a href="#text-promots-following">text promots following</a></li>
<li><a href="#concept-customization">concept customization</a></li>
</ul>
</li>
<li><a href="#text-to-image-generation-%E6%8A%80%E6%9C%AF%E6%B5%81%E6%B4%BE4%E7%B1%BB">Text-to-Image Generation  技术流派（4类）</a></li>
</ul>
</li>
<li><a href="#%E7%BB%9F%E4%B8%80%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B2">统一的视觉模型[2]</a></li>
<li><a href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%AD%E7%BB%83llm2">端到端的方式训练LLM[2]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent3">多模态 Agent[3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BF%BB%E8%AF%91">翻译</a></li>
<li><a href="#%E8%A7%A3%E8%AF%BB">解读</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023   - microsoft</p>
</li>
<li><p>开源地址<br><a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings">Computer Vision in the Wild (CVinW)</a></p>
</li>
</ul>
<h1><span id="overview-0">overview [0]</span><a href="#overview-0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/overview.jpeg" class>

<h1><span id="视觉理解-1">视觉理解 [1]</span><a href="#视觉理解-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding.png" class>

<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding-method.png" class>


<h1><span id="视觉生成-1">视觉生成 [1]</span><a href="#视觉生成-1" class="header-anchor">#</a></h1><h2><span id="human-alignments-in-visual-generation-10">Human Alignments in Visual Generation  [10]</span><a href="#human-alignments-in-visual-generation-10" class="header-anchor">#</a></h2><p>四种alignment的方式</p>
<h3><span id="spatial-controllable-t2i-generation">spatial controllable T2I generation</span><a href="#spatial-controllable-t2i-generation" class="header-anchor">#</a></h3><ul>
<li><p><strong>结合位置分布的文字描述</strong>（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于<strong>对位置要求比较高的创意设计（海报等）</strong></p>
<ul>
<li><p>直接讲原来clip那种image-level的text description升级为基于区域的text description</p>
<ul>
<li><strong>reco</strong></li>
<li>gligen</li>
</ul>
</li>
<li><p>将box描述变为spatial condition</p>
<ul>
<li><strong>controlnet</strong></li>
</ul>
</li>
<li><p>无需fintinue，直接变为inference-guide</p>
<ul>
<li>universal guidance for diffusion model</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="text-based-editing">text-based editing</span><a href="#text-based-editing" class="header-anchor">#</a></h3><ul>
<li><p><strong>给一张图和对应的修改文字，输出要求的图</strong>，常用于ps等产品</p>
<ul>
<li><p>diffusion process manipulations</p>
<ul>
<li><strong>promot2promot</strong></li>
</ul>
</li>
<li><p>text instruction editing</p>
<ul>
<li><strong>InstructPix2Pix</strong></li>
</ul>
</li>
<li><p>Editing with external pre-trained models</p>
</li>
</ul>
</li>
</ul>
<h3><span id="text-promots-following">text promots following</span><a href="#text-promots-following" class="header-anchor">#</a></h3><ul>
<li><strong>直接给文字描述，生成对应的图</strong>，这个是<strong>目前常见文生图产品的交互方式</strong>，常用于c端或者b端用户图像内容生成。但其对<strong>更细节的控制存在一定的难度</strong><ul>
<li>Inference-time manipulation</li>
<li>StructureDiffusion</li>
<li><strong>Attend-and-Excite</strong></li>
<li>Model tuning to follow text prompt</li>
<li>ddpo</li>
</ul>
</li>
</ul>
<h3><span id="concept-customization">concept customization</span><a href="#concept-customization" class="header-anchor">#</a></h3><ul>
<li><p>给一张图，提取图片中的关键内容，做<strong>各种风格（背景&#x2F;动作）变换</strong>，更用于<strong>不那么精细的广义产品</strong>，比2的运用范围更加广义</p>
<ul>
<li><p>Concept Customization</p>
<ul>
<li><strong>Textual Inversion</strong></li>
<li><strong>[DreamBooth]</strong></li>
</ul>
</li>
<li><p>Multi-concept customization</p>
<ul>
<li>Custom Diffusion</li>
</ul>
</li>
<li><p>Customization without test-time finetuning</p>
<ul>
<li>SuTI</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/align.jpg" class>
<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/generation.png" class>

<h2><span id="text-to-image-generation-技术流派4类">Text-to-Image Generation  技术流派（4类）</span><a href="#text-to-image-generation-技术流派4类" class="header-anchor">#</a></h2><ul>
<li>Generative adversarial networks (GAN)</li>
<li>Variational autoencoder (VAE)</li>
<li>Discrete image token prediction</li>
<li><strong>Diffusion model</strong></li>
</ul>
<h1><span id="统一的视觉模型2">统一的视觉模型[2]</span><a href="#统一的视觉模型2" class="header-anchor">#</a></h1><h1><span id="端到端的方式训练llm2">端到端的方式训练LLM[2]</span><a href="#端到端的方式训练llm2" class="header-anchor">#</a></h1><h1><span id="多模态-agent3">多模态 Agent[3]</span><a href="#多模态-agent3" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="翻译">翻译</span><a href="#翻译" class="header-anchor">#</a></h3><p>《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》  </p>
<ol start="0">
<li><p><a href="https://blog.csdn.net/qq_41185868/article/details/133594461">AGI之MFM：《Multimodal Foundation Models: From Specialists to General-Purpose Assistants多模态基础模型：从专家到通用助</a> 翻译</p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594554">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之视觉理解、视觉生成</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594624">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之统一的视觉模型、加持LLMs的大型多模态模型</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133606408">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之与LLM协同工作的多模态智能体、结论和研究趋势</a></p>
</li>
</ol>
<h3><span id="解读">解读</span><a href="#解读" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/669757416">大模型系列04 -文本图像生成</a></li>
</ol>
<p>1xx.   <a href="https://blog.csdn.net/qq_41200212/article/details/134663233">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>  </p>
<p>1xx.  对应第二章节<br>  <a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Zhengyuan_Tutorial_T2I2023.pdf">《Alignments in Text-to-Image Generation》</a><br>   <a href="https://www.bilibili.com/video/BV14P411v7Un/">[CVPR2023 Tutorial Talk] Alignments in Text-to-Image Generation</a> V</p>
<p>1xx. 对应第三章节<br><a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf">《From Specialist to Generalist:<br>Towards General Vision Understanding Interface》</a><br>  <a href="https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036">[CVPR Tutorial Talk] Towards General Vision Understanding Interface</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(图生文)BLIP-2, Flamingo</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#blip-2">BLIP-2</a><ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5-1">两阶段的训练策略 [1]</a></li>
<li><a href="#%E6%9E%B6%E6%9E%843">架构[3]</a></li>
<li><a href="#code-2">code [2]</a></li>
</ul>
</li>
<li><a href="#flamingo1">Flamingo[1]</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#blip2">blip2</a></li>
<li><a href="#flamingo">Flamingo</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="blip-2">BLIP-2</span><a href="#blip-2" class="header-anchor">#</a></h1><h3><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h3><p>用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，<strong>只训练Qformer</strong>，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练</p>
<h3><span id="两阶段的训练策略-1">两阶段的训练策略 [1]</span><a href="#两阶段的训练策略-1" class="header-anchor">#</a></h3><p>BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。</p>
<ul>
<li>第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(<strong>ITC</strong>)，Image-grounded Text Generation(<strong>ITG</strong>)，Image-Text Matching(<strong>ITM</strong>)让Qformer学会如何从<strong>视觉编码器中抽取文本相关的特征</strong>。</li>
<li>第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。</li>
</ul>
<h3><span id="架构3">架构[3]</span><a href="#架构3" class="header-anchor">#</a></h3><ul>
<li><strong>两个阶段训练</strong><ul>
<li>阶段一<br>获得高质量的 <strong>图文对齐向量表征</strong><br>通过<strong>ITC ITM  ITG 三个损失函数</strong>获得了很好的图片文本 <strong>对齐向量表征能力</strong>，仅训练<strong>Qformer</strong>中很少的参数<br>【ITM:  image-text 是否是匹配的 |    image 和text 都能相互看到】<br>【ITG: image生成text |    image 能全看到, text只能逐个的看】<br>【ITC: image和text的对比学习, 对比学习分类分错了的  送入ITM 负样本 |  image和 text  之间是不能看到的】</li>
<li>阶段二<br>通过向量表征进行<strong>文字生成</strong></li>
</ul>
</li>
</ul>
<h3><span id="code-2">code [2]</span><a href="#code-2" class="header-anchor">#</a></h3><h1><span id="flamingo1">Flamingo[1]</span><a href="#flamingo1" class="header-anchor">#</a></h1><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><p>它在Frozen模型的基础上做进一步的改进，不同点主要有两个：一是使用了更大的LLMs，二是<strong>冻结视觉编码器</strong>，引入<strong>perceiver resampler</strong>和<strong>XAttn-Dense</strong>两个适配单元作为可训练的模块。</p>
<ul>
<li>perceiver resampler：<br>  类似DETR，通过设计多个Perceiver Resampler来生成<strong>64个固定长度的tokens</strong>，主要作用在于可以<strong>从图像中提取固定长度的特征向量</strong>，能够解决图像甚至多帧视频的<strong>feature map不一致的问题</strong>。【图像和文本对齐】</li>
<li>XAttn-Dense：在每一层LLM上都会增加<strong>corss- attention</strong>以入到<strong>LLM中与视觉向量进行交互</strong>，<strong>融合多模态信息</strong>。【融合】</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="blip2">blip2</span><a href="#blip2" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130757157?spm=1001.2014.3001.5502">基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）</a></p>
</li>
<li><p><a href="https://github.com/www6v/LAVIS/tree/main/projects/blip2">blip2</a> git<br><a href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">blip2_instructed_generation</a> git 运行过</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Ek4y1G74J">强推！科大讯飞和中科院终于把多模态大模型讲明白了，CLIP、blip、blip2三种模型原理一口气学完</a> V ***</p>
</li>
</ol>
<p>1xx.  <a href="https://www.bilibili.com/video/BV18u4y137ZV/">AI论文精读之多模态大模型BLIP-2</a> V</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践</a> *</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/606364639">BLIP2：下一代多模态模型的雏形</a></p>
<h3><span id="flamingo">Flamingo</span><a href="#flamingo" class="header-anchor">#</a></h3><p>1xx. <a href="https://www.bilibili.com/video/BV1pu411G7ce">[论文速览]Flamingo: a Visual Language Model for Few-Shot Learning[2204.14198]</a> V<br>1xx.  <a href="https://zhuanlan.zhihu.com/p/511517344">DeepMind出手！多模态小样本打败精调</a><br>1xx. <a href="https://github.com/Luodian/Otter">Otter  on OpenFlamingo</a> git<br>1xx. <a href="https://github.com/mlfoundations/open_flamingo">open_flamingo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态InstructTuning</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</a><ul>
<li><a href="#single-turn">Single-turn</a></li>
<li><a href="#multi-turn">Multi-turn</a></li>
</ul>
</li>
<li><a href="#vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</a><ul>
<li><a href="#annotation-adaption">Annotation Adaption</a></li>
<li><a href="#self-instruct">Self-Instruct</a></li>
</ul>
</li>
<li><a href="#high-quality-vlit-data2">High-Quality VLIT Data[2]</a><ul>
<li><a href="#correctness">Correctness</a></li>
<li><a href="#diversity">Diversity</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#method-12">Method [1][2]</a><ul>
<li><a href="#annotation-adaption-si">Annotation Adaption-&gt; SI</a></li>
<li><a href="#self-instruct-aa">Self-Instruct -&gt; AA</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</span><a href="#datasets-for-visual-instruction-tuning1" class="header-anchor">#</a></h1><h3><span id="single-turn">Single-turn</span><a href="#single-turn" class="header-anchor">#</a></h3><ul>
<li><p>MiniGPT-4<br><strong>MiniGPT-4</strong> [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 <strong>randomly selects 5000 images from the Conceptual Caption dataset</strong> [38], [39] and prompts its <strong>pre-trained VLM model</strong> to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.</p>
</li>
<li><p>MultiInstruct<br>MultiInstruct [43] build a comprehensive instruction dataset that covers 62 diverse multimodal tasks from 10 broad categories, such VQA, Image-text matching, grounded generation, and so on. These tasks include 34 existing tasks derived from 21 public dataset and 28 new tasks extended from them. Each task is equipped with 5 instruction templates to prompt the model to perform the specific task.</p>
</li>
</ul>
<h3><span id="multi-turn">Multi-turn</span><a href="#multi-turn" class="header-anchor">#</a></h3><ul>
<li>LLaVA<br><strong>LLaVA-Instruct-158k</strong> [9] contains 158 image-text instruction data, including <strong>58k conversation data</strong> asking about the visual content of the image,<strong>23k description data</strong>, and <strong>77k complex reasoning data</strong> where the question may involve multi-step reasoning process.</li>
</ul>
<h1><span id="vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</span><a href="#vlit-data-construction-strategy2" class="header-anchor">#</a></h1><h3><span id="annotation-adaption">Annotation Adaption</span><a href="#annotation-adaption" class="header-anchor">#</a></h3><ul>
<li>MiniGPT-4</li>
</ul>
<h3><span id="self-instruct">Self-Instruct</span><a href="#self-instruct" class="header-anchor">#</a></h3><ul>
<li>LLaVA</li>
</ul>
<h1><span id="high-quality-vlit-data2">High-Quality VLIT Data[2]</span><a href="#high-quality-vlit-data2" class="header-anchor">#</a></h1><h3><span id="correctness">Correctness</span><a href="#correctness" class="header-anchor">#</a></h3><h3><span id="diversity">Diversity</span><a href="#diversity" class="header-anchor">#</a></h3><h3><span id="complexity">Complexity</span><a href="#complexity" class="header-anchor">#</a></h3><h1><span id="method-12">Method [1][2]</span><a href="#method-12" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Method</th>
<th>Training Paradigm[2]</th>
<th>Vision Encoder</th>
<th>Language Encoder</th>
<th>Inst[2]</th>
<th>Tuning Data</th>
</tr>
</thead>
<tbody><tr>
<td>MiniGPT-4</td>
<td>FA → VLIT</td>
<td>EvaCLIP ViT</td>
<td>Vicuna</td>
<td>AA</td>
<td>CC3M, CC12M, SBU, LAION 400M, MiniGPT-3.5K</td>
</tr>
<tr>
<td>MiniGPT-v2</td>
<td></td>
<td>EVA</td>
<td>LLaMA2-chat</td>
<td>AA+SI</td>
<td>LAION, CC3M, SBU, GRIT-20M, COCO caption, Text Captions, RefCOCO, RefCOCO+, RefCOCOg, GQA, VQA-v2, OCR-VQA, OKVQA, AOK-VQA, Flickr30k Dataset, Unnatural Instruction Dataset</td>
</tr>
<tr>
<td>LLaVa</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td>SI</td>
<td>CC3M Concept-balanced 595K, LLaVA-Instruct-158K</td>
</tr>
<tr>
<td>LLaVA-1.5</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td></td>
<td>LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG</td>
</tr>
<tr>
<td>MultiInstruct</td>
<td>VLIT</td>
<td>OFA</td>
<td>OFA</td>
<td>AA</td>
<td>VQAv2, Visual7w, GQA, OK-VQA, Visual Genome, MSCOCO, RefCOCO, COCO-Text, TDIUC, IQA, VAW, MOCHEG, WikiHow</td>
</tr>
<tr>
<td>Otter</td>
<td></td>
<td>CLIP ViT</td>
<td>MPT</td>
<td>SI</td>
<td>MIMIC-IT</td>
</tr>
<tr>
<td>LAMM</td>
<td>VLIT</td>
<td>CLIP ViT-L&#x2F;14</td>
<td>Vicuna</td>
<td>SI</td>
<td>Language-Assisted Multi-Modal Instruction-Tuning Dataset</td>
</tr>
<tr>
<td>Qwen-VL</td>
<td>FA → VLIT(Multi-Task Tuning)</td>
<td>ViT</td>
<td>Qwen-7B</td>
<td></td>
<td>LAION-en&amp;zh, DataComp, Coyo, CC12M&amp;3M, SBU, COCO, In-house Data, GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D, GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg, SynthDoG-en&amp;zh, Common Crawl pdf&amp;HTML</td>
</tr>
<tr>
<td>CogVLM</td>
<td>FA → VLIT</td>
<td>EVA2-CLIP-E</td>
<td>Vicuna-7Bv-1.5</td>
<td></td>
<td>VQAv2, TextVQA</td>
</tr>
<tr>
<td>StableLLaVA</td>
<td>FA → VLIT</td>
<td>CLIP-ViT-L&#x2F;14</td>
<td>LLaMA</td>
<td>AA</td>
<td>Synthesized Image-Dialogue Dataset</td>
</tr>
</tbody></table>
<h3><span id="annotation-adaption-gt-si">Annotation Adaption-&gt; SI</span><a href="#annotation-adaption-gt-si" class="header-anchor">#</a></h3><h3><span id="self-instruct-gt-aa">Self-Instruct -&gt; AA</span><a href="#self-instruct-gt-aa" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey》 ***  第4 5章  南洋大学 </p>
</li>
<li><p>《Vision-Language Instruction Tuning: A Review and Analysis》 ***  第2 3 4 5章   腾讯</p>
</li>
<li><p>《Instruction Tuning for Large Language Models: A Survey》 第5章</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)MiniGPT4</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#introduction1">INTRODUCTION[1]</a></li>
<li><a href="#method1">METHOD[1]</a><ul>
<li><a href="#first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</a></li>
<li><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</a></li>
<li><a href="#second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="introduction1">INTRODUCTION[1]</span><a href="#introduction1" class="header-anchor">#</a></h1><p>MiniGPT-4 增加了一个<strong>投影层</strong>，将<strong>编码的视觉特征与 Vicuna 语言模型对齐</strong>，并<strong>冻结了所有其他视觉和语言组件</strong></p>
<h1><span id="method1">METHOD[1]</span><a href="#method1" class="header-anchor">#</a></h1><ul>
<li><p>图 1</p>
</li>
<li><p>MiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，</p>
<ul>
<li>使用 <strong>Vicuna作为语言解码器</strong>，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。</li>
<li>视觉感知方：采用与 <strong>BLIP-2</strong> 相同的<strong>视觉编码器</strong>，<strong>ViT Backbone</strong>及其预先训练好的 <strong>Q-Former</strong>。<br>语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。</li>
</ul>
</li>
</ul>
<h3><span id="first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</span><a href="#first-pretraining-stage" class="header-anchor">#</a></h3><ul>
<li><p>第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。</p>
</li>
<li><p>Traditional alignment method [2]</p>
<ul>
<li>Input: Image</li>
<li>Output: Caption</li>
<li>Training Objective: Maximize the likelihood of GT captions</li>
<li>Training Dataset 组合数据集 [postprocessed by BLIP] <ul>
<li>Conceptual Caption</li>
<li>SBU </li>
<li>LAION</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</span><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain" class="header-anchor">#</a></h3><ul>
<li>Create a dataset with detailed, human-perfered descriptions[2][1]<ul>
<li>model  generates descriptions<br>在初始阶段，我们使用从第一个预训练阶段得到的模型来<strong>生成输入图像的描述</strong>。      </li>
<li>polishing and filtering by chatgpt<br>上述自动生成的图片说明包含<strong>噪音或不连贯的描述</strong>，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了<strong>ChatGPT</strong>，通过以下提示对描述进行<strong>修补</strong></li>
<li>further polishing and filtering by rules &amp; human<br>完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。</li>
</ul>
</li>
</ul>
<h3><span id="second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></span><a href="#second-stage-finetuning" class="header-anchor">#</a></h3><ul>
<li>第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。</li>
</ul>
<p>【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】  [2]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><a href="https://datac.blog.csdn.net/article/details/135399033">【LMM 009】MiniGPT-4：使用 Vicuna 增强视觉语言理解能力的多模态大模型</a> *** </li>
<li><a href="https://www.bilibili.com/video/BV1n24y1F7kv/">MiniGPT-4、表格推理、代码生成、生成式推理-来自斯坦福、北大、阿卜杜拉、达摩院的四位论文一作思辨大模型</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV12Q4y1b7nY/">miniGPT4：多模态图文理解训练</a> V<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 </a><br>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d282e4b007b201a34052">使用大型语言模型为MiniGPT-4构建视觉语言理解能力</a> V</li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/627671257">大杀器，多模态大模型MiniGPT-4入坑指南</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) LLaVa 演化</title>
    <url>/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llava-演化">LLaVa 演化</span><a href="#llava-演化" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/LLaVa-cef875377c394636a64cf57edbb0026e?pvs=4">(原理|实战) LLaVa 演化</a></li>
</ul>
<h1><span id="llava-实战">LLaVa 实战</span><a href="#llava-实战" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/LLaVa-0bf9f127dc7c41e796050bcb8f7fb1b3?pvs=4">(实战) LLaVa </a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)多模态 RAG</title>
    <url>/www6vHomeAIGC/2023/03/14/gptRAGMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag-12">多模态RAG [1][2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="多模态rag-12">多模态RAG [1][2]</span><a href="#多模态rag-12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> *** </p>
</li>
<li><p>《Retrieving Multimodal Information for Augmented Generation: A Survey》<br><a href="https://zhuanlan.zhihu.com/p/665078079">万字综述：2023年多模态检索增强生成技术(mRAG)最新进展与趋势-图片、代码、图谱、视频、声音、文本</a><br><a href="https://zhuanlan.zhihu.com/p/678812531">多模态RAG综述</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409004&idx=2&sn=7f36d3ff5e170442486a5d413373c563">朴素多模态RAG如何实现？兼看RAG上下文过滤方案FILCO及202402大模型早报 </a>    多模态RAG 两种实现方式</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/9xU7OOBqee4sM_TZuzQ-Ew">视觉的跨界 Wiki-LLaVA | lmage + Question 的奇妙反应，生成多模态大型语言模型（MLLMs）！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)多模态 RAG</title>
    <url>/www6vHomeAIGC/2023/03/14/gptRAGMultimodalPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81rag-%E5%A4%9A%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%99%A8-1011">多模态RAG-多向量检索器 [10][11]</a><ul>
<li><a href="#semi-structured-tables-text-rag-20">semi-structured (tables + text) RAG [20]</a></li>
<li><a href="#multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG  [13]</a></li>
<li><a href="#private-multi-modal-text-tables-images-rag-22">private multi-modal (text + tables + images)  RAG [22]</a></li>
<li><a href="#%E7%BB%84%E4%BB%B6">组件</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
<li><a href="#notebook">notebook</a></li>
<li><a href="#template">template</a></li>
<li><a href="#llamaindex">llamaindex</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="多模态rag-多向量检索器-1011">多模态RAG-多向量检索器 [10][11]</span><a href="#多模态rag-多向量检索器-1011" class="header-anchor">#</a></h1><h3><span id="semi-structured-tables-text-rag-20">semi-structured (tables + text) RAG [20]</span><a href="#semi-structured-tables-text-rag-20" class="header-anchor">#</a></h3><p> 分析pdf中表格 </p>
<h3><span id="multi-modal-text-tables-images-rag-13">multi-modal (text + tables + images) RAG  [13]</span><a href="#multi-modal-text-tables-images-rag-13" class="header-anchor">#</a></h3><p>分析PDF中图片</p>
<ul>
<li><p><strong>Option 1</strong>  [基于CLIP] [23][30][32][33]</p>
<ul>
<li>Use multimodal embeddings <strong>(such as <a href="https://openai.com/research/clip">CLIP</a>)</strong> to embed images and text</li>
<li>Retrieve both using similarity search</li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis<br> {选项1：对文本和表格生成summary，然后应用多模态embedding模型把文本&#x2F;表格summary、原始图片转化成embedding存入多向量检索器。对话时，根据query召回原始文本&#x2F;表格&#x2F;图像。然后将其喂给多模态LLM生成应答结果。}[10]</li>
</ul>
</li>
<li><p><strong>Option 2</strong>   [21] </p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce <strong>text summaries from images</strong></li>
<li>Embed and retrieve text </li>
<li>Pass text chunks to an LLM for answer synthesis<br>【将图片转成摘要，和其他文本信息整合在文本粒度进行检索】[12]<br> {选项2：首先应用多模态大模型（GPT4-V、LLaVA、FUYU-8b）生成图片summary。然后对文本&#x2F;表格&#x2F;图片summary进行向量化存入多向量检索器中。当生成应答的多模态大模型不具备时，可根据query召回原始文本&#x2F;表格+图片summary。}[10]</li>
</ul>
</li>
<li><p>Option 3 [24] [31][34]</p>
<ul>
<li>Use a multimodal LLM (such as <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, or <a href="https://www.adept.ai/blog/fuyu-8b">FUYU-8b</a>) to produce text summaries from images</li>
<li>Embed and retrieve image summaries with a reference to the raw image </li>
<li>Pass <strong>raw images and text chunks</strong> to a multimodal LLM for answer synthesis<br> 【实际模型输入使用的是图片】<br>   【图片概要依然是用于检索（GPT-4V，LLaVA，FUYU-8b）】[12]<br>  {选项3：前置阶段同选项2相同。对话时，根据query召回原始文本&#x2F;表格&#x2F;图片。构造完整Prompt，访问多模态大模型生成应答结果。}[10]</li>
</ul>
</li>
</ul>
<h3><span id="private-multi-modal-text-tables-images-rag-22">private multi-modal (text + tables + images)  RAG [22]</span><a href="#private-multi-modal-text-tables-images-rag-22" class="header-anchor">#</a></h3><h3><span id="组件">组件</span><a href="#组件" class="header-anchor">#</a></h3><ul>
<li>pdf解析<br>unstructured</li>
<li>store<br>MultiVectorRetriever - 元数据+数据</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://www.zhihu.com/question/628651389/answer/3321989558">检索增强生成（RAG）有什么好的优化方案？</a> </p>
</li>
<li><p><a href="https://blog.langchain.dev/semi-structured-multi-modal-rag/">Multi-Vector Retriever for RAG on tables, text, and images</a> ***<br><a href="https://blog.csdn.net/lichunericli/article/details/135724777">基于多向量检索器的多模态RAG实现：用于表格、文本和图像</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/665814914">langchain的multi model RAG-以多模态pdf文件为例子</a></p>
</li>
<li><p><a href="https://blog.langchain.dev/multi-modal-rag-template/">Multi-modal RAG on slide decks</a></p>
</li>
</ol>
<p>1xx. <a href="https://docs.google.com/presentation/d/19x0dvHGhbJOOUWqvPKrECPi1yI3makcoc-8tFLj9Sos/edit?ref=blog.langchain.dev&pli=1#slide=id.g2642e7050fc_0_370">Using Multi-Modal LLMs</a>  page21</p>
<h3><span id="notebook">notebook</span><a href="#notebook" class="header-anchor">#</a></h3><ol start="20">
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi_Structured_RAG</a>  notebook<br><a href="https://github.com/www6v/AIGC/blob/master/Advanced-RAG/01_semi_structured_data.ipynb">Advanced-RAG semi_structured_data</a>   notebook  {半结构化-解析pdf中的表格，  运行没问题，能问表格中的数据}</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb">Semi_structured_and_multi_modal_RAG</a> notebook </p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/blob/master/langchain-cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a>  notebook {多模态- 解析pdf中的图片  运行有问题}<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb">Private Semi-structured and Multi-modal RAG w&#x2F; LLaMA2 and LLaVA</a> notebook</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb">Chroma multi-modal RAG</a> notebook</p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb">Multi-modal RAG</a> notebook</p>
</li>
</ol>
<h3><span id="template">template</span><a href="#template" class="header-anchor">#</a></h3><ol start="30">
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-multi-modal-local">rag-multi-modal-local</a><br>OpenCLIP(image embedding)  + bakllava(answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-multi-modal-mv-local">rag-multi-modal-mv-local</a><br>bakllava(image summaries embedding) +  bakllava (answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-chroma-multi-modal">rag-chroma-multi-modal</a><br>OpenCLIP(image embedding) + GPT-4V (answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-gemini-multi-modal">rag-gemini-multi-modal</a><br>OpenCLIP(image embedding) + Gemini(answer synthesis)</li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-chroma-multi-modal-multi-vector">rag-chroma-multi-modal-multi-vector</a><br>GPT-4V(image summaries embedding) + GPT-4V (answer synthesis)</li>
</ol>
<h3><span id="llamaindex">llamaindex</span><a href="#llamaindex" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s/93CdvD8FLZjaA7E724bf7g">朴素多模态RAG如何实现？兼看RAG上下文过滤方案FILCO及202402大模型早报 </a><br>1xx. <a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval/">Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index&#x2F;Retriever</a><br>1xx. <a href="https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206">Multimodal RAG pipeline with LlamaIndex and Neo4j</a><br>1xx. <a href="https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb">neo4j_llama_multimodal.ipynb</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Instruct Tuning</title>
    <url>/www6vHomeAIGC/2023/03/12/gptInstructTuningSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://arxiv.org/abs/2308.10792">大语言模型指令微调综述</a><br>    <a href="https://zhuanlan.zhihu.com/p/654054370">一篇关于LLM指令微调的综述</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/657138921">[论文]大语言模型指令调优综述</a><br>1xx. <a href="https://blog.csdn.net/qq_41185868/article/details/132613338">Paper：《Instruction Tuning for Large Language Models: A Survey—大型语言模型的指令调优的综述》翻译与解读</a><br>1xx. <a href="https://github.com/xiaoya-li/Instruction-Tuning-Survey">Instruction Tuning for Large Language Models: A Survey</a> git</p>
<p>【前面大部分是Instruct-Tuning， 中间一部分是Multi-modality Instruction Tuning】</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Instruct-Tuning</category>
      </categories>
      <tags>
        <tag>Instruct-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(List)Agent 开源 产品 平台</title>
    <url>/www6vHomeAIGC/2023/03/05/gptAgentList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#agentgeneral">Agent（General）</a></li>
<li><a href="#agenttoolassistant">Agent(tool&#x2F;assistant)</a></li>
<li><a href="#agentsimulation">Agent(simulation)</a></li>
<li><a href="#agent">Agent</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#%E5%88%86%E7%B1%BB-101112">分类 [10][11][12]</a></li>
<li><a href="#hugginggpt">HuggingGPT</a></li>
<li><a href="#babyagi-aigc">BabyAGI [AIGC]</a></li>
<li><a href="#autogpt10">AutoGPT[10]</a></li>
</ul>
</li>
<li><a href="#platform20">Platform[20]</a><ul>
<li><a href="#%E5%AD%97%E8%8A%82-coze2122">字节 Coze[21,22]</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6-appbuilder">百度 AppBuilder</a></li>
<li><a href="#dify">Dify</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#platform">Platform</a></li>
<li><a href="#%E4%BA%A7%E5%93%81-%E8%B0%83%E7%A0%94">产品 &amp;调研</a></li>
<li><a href="#modelscope-agent-agentfabric">modelscope-agent AgentFabric</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="agentgeneral">Agent（General）</span><a href="#agentgeneral" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent（General）</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><strong>AutoGen</strong> <a href="https://www.bilibili.com/video/BV1DH4y1Z7Ep">video</a> paper ***</td>
<td>Multi Agent</td>
<td>customizable, <strong>conversable</strong>,  <strong>seamlessly allow human participation</strong> [微软]</td>
<td><a href="https://github.com/microsoft/autogen">code</a><img src="https://img.shields.io/github/stars/microsoft/autogen.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>MetaGPT</strong> paper***</td>
<td>Multi Agent-role base</td>
<td>覆盖软件公司全生命流程</td>
<td><a href="https://github.com/geekan/MetaGPT">code</a><img src="https://img.shields.io/github/stars/geekan/MetaGPT.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>CrewAI</strong><a href="https://www.bilibili.com/video/BV12C4y1Y7xm">video</a></td>
<td>Multi Agent</td>
<td>流程定义更灵活</td>
<td><a href="https://github.com/joaomdmoura/CrewAI">code</a><img src="https://img.shields.io/github/stars/joaomdmoura/CrewAI.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>BabyAGI</strong></td>
<td><strong>plan and execute</strong></td>
<td></td>
<td><a href="https://github.com/yoheinakajima/babyagi">code</a><img src="https://img.shields.io/github/stars/yoheinakajima/babyagi.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>AutoGPT</strong></td>
<td>General</td>
<td></td>
<td><a href="https://github.com/Torantulino/Auto-GPT">code</a><img src="https://img.shields.io/github/stars/Torantulino/Auto-GPT.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td><strong>LangGraph</strong> <a href="https://www.bilibili.com/video/BV1VN4y1n7bt/">video</a> ***</td>
<td>flow engineering</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>XAgent</strong> <a href="https://www.bilibili.com/video/BV1D34y1M74F">video</a></td>
<td><strong>双循环，人可参与</strong></td>
<td>autogpt，babyagi - 没法收敛，有时候会不可控<br>metagpt，chatdev sop优化- 有一定的局限性，通用性不够好[面壁智能]</td>
<td><a href="https://github.com/OpenBMB/XAgent">code</a><img src="https://img.shields.io/github/stars/OpenBMB/XAgent.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>Agents <a href="https://www.bilibili.com/video/BV1C8411k7UL">video</a> paper</td>
<td>single agent|multi agent</td>
<td>基于SOP</td>
<td><a href="https://github.com/aiwaves-cn/agents">code</a></td>
</tr>
<tr>
<td>phidata</td>
<td></td>
<td>Memory, knowledge and tools for LLMs</td>
<td><a href="https://github.com/phidatahq/phidata">code</a></td>
</tr>
<tr>
<td>MiniAGI <a href="https://www.bilibili.com/video/BV1Hh4y1k7Jz">video</a></td>
<td></td>
<td>simple general-purpose autonomous agent based on the OpenAI API</td>
<td><a href="https://github.com/muellerberndt/mini-agi">code</a></td>
</tr>
<tr>
<td>AL Legion</td>
<td></td>
<td>An LLM-powered autonomous agent platform</td>
<td><a href="https://github.com/eumemic/ai-legion">code</a></td>
</tr>
</tbody></table>
<h1><span id="agenttoolx2fassistant">Agent(tool&#x2F;assistant)</span><a href="#agenttoolx2fassistant" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent(tool&#x2F;assistant)</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ResearchGPT</strong> ***</td>
<td>plan and execute</td>
<td>融合论文拆解+网络爬虫</td>
<td><a href="https://github.com/assafelovic/gpt-researcher">code</a><img src="https://img.shields.io/github/stars/assafelovic/gpt-researcher.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>WorkGPT</td>
<td>tools</td>
<td>A GPT agent framework for invoking APIs</td>
<td><a href="https://github.com/team-openpm/workgpt">code</a></td>
</tr>
<tr>
<td><strong>AgentTuning</strong><a href="https://www.bilibili.com/video/BV1E84y197Cj/">video</a>  paper</td>
<td>指令微调Agent</td>
<td>[清华]</td>
<td><a href="https://github.com/THUDM/AgentTuning">code</a></td>
</tr>
<tr>
<td><strong>ModelScope-Agent</strong> <a href="https://www.bilibili.com/video/BV1u34y137if">video</a></td>
<td>tools, MSAgent-Bench 训练agent</td>
<td>[阿里魔塔]</td>
<td><a href="https://github.com/modelscope/modelscope-agent">code</a></td>
</tr>
<tr>
<td>open-interpreter</td>
<td>os agent</td>
<td>A natural language interface for computers</td>
<td><a href="https://github.com/KillianLucas/open-interpreter">code</a></td>
</tr>
<tr>
<td>Qwen-Agent <a href="https://www.bilibili.com/video/BV1c34y1P7Yg">video</a></td>
<td></td>
<td>Agent framework and applications built upon Qwen, featuring Function Calling, Code Interpreter, RAG, and Chrome extension</td>
<td><a href="https://github.com/QwenLM/Qwen-Agent">code</a></td>
</tr>
<tr>
<td>AutoGPT-Plugins</td>
<td></td>
<td>Auo-GPT插件</td>
<td><a href="https://github.com/Significant-Gravitas/Auto-GPT-Plugins">code</a></td>
</tr>
<tr>
<td>GPTEngineer</td>
<td>code  assistant</td>
<td><strong>自动</strong>工具构建和代码生成</td>
<td><a href="https://github.com/AntonOsika/gpt-engineer">code</a><img src="https://img.shields.io/github/stars/AntonOsika/gpt-engineer.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>Aider</td>
<td>code  assistant</td>
<td><strong>交互式</strong></td>
<td><a href="https://github.com/paul-gauthier/aider">code</a></td>
</tr>
</tbody></table>
<h1><span id="agentsimulation">Agent(simulation)</span><a href="#agentsimulation" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent(simulation)</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>Hyperwriteai <a href="https://www.bilibili.com/video/BV1BZ421B7ar/">video</a><a href="https://www.hyperwriteai.com/personal-assistant">website</a>***</td>
<td>web agent, os agent</td>
<td></td>
<td><a href="https://github.com/OthersideAI/self-operating-computer">code</a></td>
</tr>
<tr>
<td><strong>MultiON</strong> <a href="https://www.bilibili.com/video/BV1mt421W7sw/">video</a>***</td>
<td>web agent</td>
<td></td>
<td></td>
</tr>
<tr>
<td>webarena paper</td>
<td>web agent</td>
<td>WebArena: A Realistic Web Environment for Building Autonomous Agents  网络拟真环境，可用于自主智能体的测试，支持在线购物，论坛，代码仓库etc</td>
<td><a href="https://github.com/web-arena-x/webarena">code</a></td>
</tr>
<tr>
<td>MiniWoB++：</td>
<td>web agent</td>
<td>a web interaction benchmark for reinforcement learning</td>
<td><a href="https://github.com/Farama-Foundation/miniwob-plusplus">code</a></td>
</tr>
<tr>
<td><strong>OpenAgents</strong> <a href="https://www.bilibili.com/video/BV1wM41197N7/">video</a> paper***</td>
<td>web agent</td>
<td>[港大]</td>
<td><a href="https://github.com/xlang-ai/OpenAgents">code</a></td>
</tr>
<tr>
<td>Mobile-Agent <a href="https://www.bilibili.com/video/BV1Xv42117hh/">video</a></td>
<td>app agent</td>
<td>[阿里]</td>
<td></td>
</tr>
<tr>
<td><strong>AppAgent</strong> <a href="https://www.bilibili.com/video/BV1De411S7ka">video</a> [paper]</td>
<td>app agent</td>
<td>[腾讯]</td>
<td></td>
</tr>
<tr>
<td>CAMEL</td>
<td></td>
<td>CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society</td>
<td><a href="https://github.com/camel-ai/camel">code</a></td>
</tr>
<tr>
<td><strong>Generative Agents</strong></td>
<td></td>
<td>斯坦福AI小镇</td>
<td><a href="https://github.com/joonspk-research/generative_agents">code</a><img src="https://img.shields.io/github/stars/joonspk-research/generative_agents.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>AgentVerse</td>
<td></td>
<td>多模型交互环境</td>
<td><a href="https://github.com/OpenBMB/AgentVerse">code</a><img src="https://img.shields.io/github/stars/OpenBMB/AgentVerse.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>GPTeam</td>
<td>Multi Agent</td>
<td>多智能体交互</td>
<td><a href="https://github.com/101dotxyz/GPTeam">code</a></td>
</tr>
<tr>
<td><strong>ChatDev</strong><a href="https://www.bilibili.com/video/BV1334y1T7K5/">video</a></td>
<td>Multi Agent</td>
<td>虚拟软件公司[面壁智能]</td>
<td><a href="https://github.com/OpenBMB/ChatDev">code</a><img src="https://img.shields.io/github/stars/OpenBMB/ChatDev.svg?style=flat-square" alt="GitHub Badge"></td>
</tr>
<tr>
<td>GPT PILOT</td>
<td></td>
<td><strong>交互式</strong></td>
<td></td>
</tr>
<tr>
<td>DevOpsGPT</td>
<td></td>
<td>自动</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Agent</th>
<th>类型</th>
<th>描述</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>TaskingAI <a href="https://www.bilibili.com/video/BV1gp4y1m75S/">video</a></td>
<td>LLMOps</td>
<td></td>
<td><a href="https://github.com/TaskingAI/TaskingAI">code</a></td>
</tr>
<tr>
<td>DIFY <a href="https://www.bilibili.com/video/BV14V411Q7wP/">video</a></td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>AutoGen Studio</strong> <a href="https://www.bilibili.com/video/BV1fi4y1i7g7/">video</a> ***</td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>L3AGI <a href="https://www.bilibili.com/video/BV1s94y1K7fP">video</a></td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>agenta</td>
<td>LLMOps</td>
<td>The all-in-one LLM developer platform: prompt management, evaluation, human feedback, and deployment all in one place.</td>
<td><a href="https://github.com/Agenta-AI/agenta">code</a></td>
</tr>
<tr>
<td>SuperAGI</td>
<td>LLMOps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>N8N <a href="https://www.bilibili.com/video/BV1vT4y1h7UM/">video</a></td>
<td>work flow</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TaskWeaver <a href="https://www.bilibili.com/video/BV16C4y1c7rd">video</a></td>
<td></td>
<td>以代码为中心[微软]</td>
<td></td>
</tr>
<tr>
<td>ProAgent <a href="https://www.bilibili.com/video/BV1eu4y1b7DN">video</a></td>
<td>work flow</td>
<td>[清华]</td>
<td></td>
</tr>
<tr>
<td>Prompt flow <a href="https://www.bilibili.com/video/BV1aG411m7A4/">video</a></td>
<td></td>
<td>[微软]</td>
<td></td>
</tr>
<tr>
<td>AgentGPT <a href="https://www.bilibili.com/video/BV1V94y1s7uT">video</a></td>
<td>agent store, agent template</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bisheng *</td>
<td></td>
<td>dify+flowise的结合体</td>
<td></td>
</tr>
</tbody></table>
<h1><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h1><h3><span id="分类-101112">分类 [10][11][12]</span><a href="#分类-101112" class="header-anchor">#</a></h3><ul>
<li><p>Action agents  </p>
<ul>
<li>Function Call</li>
<li>ReACT</li>
</ul>
</li>
<li><p>Simulation agents<br>  生成式智能体， CAMEL，  Generative Agents</p>
</li>
<li><p>Automomous Agent<br>  <strong>AutoGPT</strong>， <strong>BabyAGI</strong>,  <strong>AutoGen</strong><br>  <strong>MetaGPT</strong>     ChatDev</p>
</li>
<li><p>跨模态Agents<br>  HuggingGPT</p>
</li>
</ul>
<h3><span id="hugginggpt">HuggingGPT</span><a href="#hugginggpt" class="header-anchor">#</a></h3><h3><span id="babyagi-aigc">BabyAGI  [AIGC]</span><a href="#babyagi-aigc" class="header-anchor">#</a></h3><p>Plan-and-execute agents<br>The <strong>planning</strong> is almost always done <strong>by an LLM</strong>.<br>The <strong>execution</strong> is usually done by a <strong>separate agent (equipped with tools)</strong>.</p>
<h3><span id="autogpt10">AutoGPT[10]</span><a href="#autogpt10" class="header-anchor">#</a></h3><p>AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下</p>
<ol>
<li>AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals </li>
<li>Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如<code>browser_website(url = &quot;www.baidu.com&quot;)</code> )</li>
<li>这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，</li>
<li>回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）</li>
</ol>
<h1><span id="platform20">Platform[20]</span><a href="#platform20" class="header-anchor">#</a></h1><h3><span id="字节-coze2122">字节 Coze[21,22]</span><a href="#字节-coze2122" class="header-anchor">#</a></h3><p>优势:  有RAG，结构化数据<br>劣势:  只能发布到飞书，微信</p>
<h3><span id="百度-appbuilder">百度 AppBuilder</span><a href="#百度-appbuilder" class="header-anchor">#</a></h3><h3><span id="dify">Dify</span><a href="#dify" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/awesome-ai-agents">awesome-ai-agents</a> git list</li>
<li><a href="https://github.com/www6v/DecryptPrompt">DecryptPrompt</a>  ***  git list</li>
<li><a href="https://space.bilibili.com/471000665/video?tid=0&pn=1&keyword=&order=pubdate">AIGCLINK</a> V</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/664281311">「Agent」通俗易懂地聊聊AI Agent（附66个开源+44个闭源Agent项目）</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/IubrsvB0ypn8KC-_SMPvLQ">主流Agent框架及金融Agent-FinRobot：兼看面向实体增强的细粒度实体描述知识库项目 </a><br>   Agent-FinRobot  基于autogen 实现</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/642357544">2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用</a> ***  论文+代码</li>
<li>公开课</li>
<li>公开课</li>
</ol>
<h3><span id="platform">Platform</span><a href="#platform" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1Bm411B79m/">AgentBuilder 中小企业如何选择：coze、dify、appbuilder、毕晟</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1DA4m1w72p/">COZE：中小企业均可0门槛创建业务agent</a> V</li>
<li><a href="https://mp.weixin.qq.com/s/WDkYZF9-JRhzM467Uf8lUA">利用Coze 实现吴恩达的4种 AI Agent 设计模式</a></li>
</ol>
<h3><span id="产品-amp调研">产品 &amp;调研</span><a href="#产品-amp调研" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s/RrymA1NwHzz9Q34wITZo6w">2024 年最完整的 AI Agents 清单来了，涉及 13 个领域，上百个 Agents！ </a>  开源 闭源 ***<br>1xx. <a href="https://mp.weixin.qq.com/s/v_BvcMqd-FpwRbOD09MR0A">全球AI Agent大盘点，大语言模型创业一定要参考的60个AI智能体</a><br>1xx. <a href="https://mp.weixin.qq.com/s/KbDOBkmYsTiwkjg2-YoRrQ">大模型时代的APP–2024年 AI Agent行业报告</a> ***</p>
<h3><span id="modelscope-agent-agentfabric">modelscope-agent  AgentFabric</span><a href="#modelscope-agent-agentfabric" class="header-anchor">#</a></h3><p>1xx. <a href="https://cloud.tencent.com/developer/article/2421616">LLM 大模型学习必知必会系列(十)：基于AgentFabric实现交互式智能体应用,Agent实战-腾讯云开发者社区-腾讯云</a></p>
<p>1xx. <a href="https://github.com/modelscope/modelscope-agent/tree/master/apps/agentfabric">modelscope-agent&#x2F;apps&#x2F;agentfabric at master · modelscope&#x2F;modelscope-agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/669397935">Modelscope Agent实操（一）：0代码创建、发布并分享一个专属Agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/672235050">从agentfabric开始体验魔搭Agent</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/679918404">社区供稿 | GLM-4适配ModelScope-Agent最佳实践</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Web Agent</title>
    <url>/www6vHomeAIGC/2023/03/05/gptAgentWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="web-scenarios-1">web scenarios [1]</span><a href="#web-scenarios-1" class="header-anchor">#</a></h1><p>在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。</p>
<p>通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。</p>
<p>为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。</p>
<p>Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs. </p>
<p>Agents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping [392] and search engine retrieval [90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management [391], agents often face challenges in performance. </p>
<p>In order to enable successful interactions between agents and more realistic web pages, some researchers [393; 394] have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web [389] combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [388] in real-world scenarios and extract valuable information. Furthermore, WebGum [390] empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive understanding of web pages.</p>
<h1><span id="papers-2">papers [2]</span><a href="#papers-2" class="header-anchor">#</a></h1><p><strong>In web scenarios</strong></p>
<ul>
<li>[2023&#x2F;10] <strong>OpenAgents: An Open Platform for Language Agents in the Wild.</strong> <em>XLang Lab (The University of Hong Kong) arXiv.</em> [<a href="https://arxiv.org/abs/2310.10634">paper</a>] [<a href="https://docs.xlang.ai/">project page</a>] [<a href="https://github.com/xlang-ai/OpenAgents">code</a>] [<a href="https://chat.xlang.ai/">demo</a>]  ***</li>
<li>[2023&#x2F;07] <strong>WebArena: A Realistic Web Environment for Building Autonomous Agents.</strong> <em>Shuyan Zhou (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.13854">paper</a>] [<a href="https://webarena.dev/">code</a>] *</li>
<li>[2023&#x2F;07] <strong>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.</strong> <em>Izzeddin Gur (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.12856">paper</a>]</li>
<li>[2023&#x2F;06] <strong>SYNAPSE: Leveraging Few-Shot Exemplars for<br>Human-Level Computer Control.</strong> <em>Longtao Zheng (Nanyang Technological University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.07863">paper</a>] [<a href="https://github.com/ltzheng/synapse">code</a>] *</li>
<li>[2023&#x2F;06] <strong>Mind2Web: Towards a Generalist Agent for the Web.</strong> <em>Xiang Deng (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.06070">paper</a>] [<a href="https://osu-nlp-group.github.io/Mind2Web/">code</a>] ***</li>
<li>[2023&#x2F;05] <strong>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</strong> <em>Hiroki Furuta (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11854">paper</a>]</li>
<li>[2023&#x2F;03] <strong>Language Models can Solve Computer Tasks.</strong> <em>Geunwoo Kim (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17491">paper</a>] [<a href="https://github.com/posgnu/rci-agent">code</a>]</li>
<li>[2022&#x2F;07] <strong>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.01206">paper</a>] [<a href="https://webshop-pnlp.github.io/">code</a>] *</li>
<li>[2021&#x2F;12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano (OpenAI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332">paper</a>]</li>
<li>[2023&#x2F;05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> <em>Wangchunshu Zhou (AIWaves) et al. arXiv.</em> [<a href="https://arxiv.org/pdf/2309.07870.pdf">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]  ***</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《The Rise and Potential of Large Language Model Based Agents: A Survey》、</li>
<li><a href="https://github.com/woooodyy/llm-agent-paper-list">The Rise and Potential of Large Language Model Based Agents: A Survey</a> git<br>1xx. <a href="https://sites.google.com/view/mm-webnav/">Multimodal Web Navigation with Instruction-Finetuned Foundation Models</a><br>1xx. <a href="https://hub.baai.ac.cn/view/28104">Google DeepMind｜具备规划长程上下文理解和程序合成能力的真实世界WebAgent</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/662146234">LLMs-Agent 论文: WebAgent, 2023, Izzeddin Gur et al., Google DeepMind.</a><br>1xx. <a href="https://github.com/www6v/OpenAgents">OpenAgents</a></li>
</ol>
<h3><span id="web-agent">web  agent</span><a href="#web-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper-Agent</title>
    <url>/www6vHomeAIGC/2023/03/04/gptAgentPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>1xx. <a href="https://github.com/zjunlp/LLMAgentPapers">LLM Agents Papers</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662506575">28 篇最新论文解读 LLMs-based Agents</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/661741663">ICLR’24 Agent论文合集：RL-based、LLM-based 前沿研究汇总</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/665282216">ICLR’24 大语言模型智能体最新研究进展丨智能体应用篇</a><br>1xx. <a href="https://mp.weixin.qq.com/s/GGRWQJ-eBvHerB9H9JPCjg">ICLR’24 大语言模型智能体最新研究进展丨智能体能力篇 </a><br>   <a href="https://mp.weixin.qq.com/s/eYnZY1GFWMKdU_Z57iSEJg">ICLR’24 大语言模型智能体最新研究进展 </a><br>1xx. <a href="https://github.com/www6v/LLM-Agents-Papers"> LLM-Agents-Papers</a></p>
<p>1xx. <a href="https://www.bilibili.com/read/cv27126779/">AGI新突破！52篇论文尽览大模型Agent最新研究进展！</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)多模态预训练 概述</title>
    <url>/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83">多模态预训练</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84transformer">架构Transformer</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">模型 - 自监督学习</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1">下游任务</a><ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83">多模态下游任务-模型微调</a></li>
</ul>
</li>
<li><a href="#%E6%9B%B4%E5%A4%A7%E6%9B%B4%E5%BC%BA%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">更大更强的多模态预训练模型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/overview.png" class>


<h1><span id="多模态预训练">多模态预训练</span><a href="#多模态预训练" class="header-anchor">#</a></h1><h2><span id="数据集">数据集</span><a href="#数据集" class="header-anchor">#</a></h2><ul>
<li>大规模无标注</li>
<li>内容杂  噪音多</li>
</ul>
<h2><span id="架构transformer">架构Transformer</span><a href="#架构transformer" class="header-anchor">#</a></h2><ul>
<li><p>基于transformer encoder-理解任务<br>单流 - vl-bert  UNITER<br>双流 - ViLBERT， CLIP（双流结构，对比学习）</p>
</li>
<li><p>基于transformer decoder-生成任务<br>DALL-E  （VQVAE+GPT,  Text-to-Image Generation）<br>现在都用 → SD 扩散模型</p>
</li>
<li><p>基于encoder+decoder-理解+生成<br>文本的decoder</p>
</li>
</ul>
<ol>
<li>encoder + decoder 串行,  交叉注意力</li>
<li>encoder + decoder 并行</li>
</ol>
<h2><span id="模型-自监督学习">模型 - 自监督学习</span><a href="#模型-自监督学习" class="header-anchor">#</a></h2><ul>
<li><p>模态内掩码学习<br>文本 语音 视觉自身token级别mask</p>
</li>
<li><p>模态间掩码学习<br>不同模态信息的相互预测<br>mask视觉， 输出对应文本</p>
</li>
<li><p>模态间匹配学习<br>匹配与否的分类问题 - 正负样本(二分类)<br>对比学习 - 模态间的图文匹配对</p>
</li>
</ul>
<h1><span id="下游任务">下游任务</span><a href="#下游任务" class="header-anchor">#</a></h1><h2><span id="多模态下游任务-模型微调">多模态下游任务-模型微调</span><a href="#多模态下游任务-模型微调" class="header-anchor">#</a></h2><ul>
<li><p>模型微调</p>
<ul>
<li>p+ finetune（全参数）</li>
<li>p+ prompt-tuning</li>
<li>p+ adaptor-tuning</li>
<li>p+ lora</li>
</ul>
</li>
<li><p>多模态下游任务</p>
<ul>
<li>理解： text&#x2F;audio&#x2F;visual 内容生成</li>
<li>生成： 跨模态 检索&#x2F;问答&#x2F;推理</li>
</ul>
</li>
</ul>
<h1><span id="更大更强的多模态预训练模型">更大更强的多模态预训练模型</span><a href="#更大更强的多模态预训练模型" class="header-anchor">#</a></h1><ul>
<li><strong>强大的语言模型</strong></li>
<li>更大的视觉模型</li>
<li>更大规模的预训练数据</li>
<li>更多模态形式的数据</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV13P411q7tH/">中科院刘静：多模态预训练的进展回顾与展望</a>  V</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Self-Reflective RAG</title>
    <url>/www6vHomeAIGC/2023/03/02/gptRAGSelfReflective/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#cognitive-architecture-2">Cognitive Architecture [2]</a></li>
<li><a href="#crag">CRAG</a></li>
<li><a href="#self-rag-3">Self-RAG [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#crag-1">CRAG</a></li>
<li><a href="#self-rag">SELF-RAG</a></li>
<li><a href="#self-rag">Self-RAG</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="cognitive-architecture-2">Cognitive Architecture [2]</span><a href="#cognitive-architecture-2" class="header-anchor">#</a></h1><ul>
<li>Cognitive architectures for RAG [1]</li>
</ul>
<h1><span id="crag">CRAG</span><a href="#crag" class="header-anchor">#</a></h1><h1><span id="self-rag-3">Self-RAG [3]</span><a href="#self-rag-3" class="header-anchor">#</a></h1><p>Self-RAG 则是更加主动和智能的实现方式，主要步骤概括如下：</p>
<ol>
<li>判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回</li>
<li>平行处理每个片段：生产prompt+一个片段的生成结果</li>
<li>使用**反思字段(Reflection tokens)**，检查输出是否相关，选择最符合需要的片段；</li>
<li>再重复检索</li>
<li>生成结果会引用相关片段，以及输出结果是否符合该片段，便于查证事实。</li>
</ol>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/agentic-rag-with-langgraph/">Self-Reflective RAG with LangGraph</a></p>
</li>
<li><p><a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/">OpenAI’s Bet on a Cognitive Architecture</a></p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/2301_78285120/article/details/136103211">写的太通透了！大模型自省式 RAG 与 LangGraph 的实践！</a></p>
<h3><span id="crag">CRAG</span><a href="#crag" class="header-anchor">#</a></h3><p>1xx. <a href="https://arxiv.org/pdf/2401.15884.pdf">Corrective Retrieval Augmented Generation</a> Figure 2<br>1xx. <a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb">Corrective RAG (CRAG)</a> git</p>
<p>1xx. 【社区第十三讲】 老刘说NLP线上交流</p>
<h3><span id="self-rag">SELF-RAG</span><a href="#self-rag" class="header-anchor">#</a></h3><ol start="3">
<li><a href="https://zhuanlan.zhihu.com/p/661465330?utm_id=0">NLP（廿一）：从 RAG 到 Self-RAG —— LLM 的知识增强</a> ***</li>
</ol>
<p>1xx. <a href="https://arxiv.org/pdf/2310.11511.pdf">SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND<br>CRITIQUE THROUGH SELF-REFLECTION</a> Figure 1<br>1xx. <a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb">Self-RAG</a> git</p>
<h3><span id="self-rag">Self-RAG</span><a href="#self-rag" class="header-anchor">#</a></h3><p>1xx. <a href="https://github.com/www6v/self-rag">original implementation of SELF-RAG</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Plan&amp;Execute,ReWOO</title>
    <url>/www6vHomeAIGC/2023/03/02/gptAgentPlanAndExecute/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="plan-and-execute-0">Plan-and-execute [0]</span><a href="#plan-and-execute-0" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Figure 2 - 基于prompt [1]</li>
</ul>
</li>
<li><p>代码</p>
<ul>
<li>plan [2]<ul>
<li>Planning Step</li>
<li>Re-Plan Step</li>
</ul>
</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li>冗余的提示和重复的执行 -&gt; ReWOO</li>
</ul>
</li>
</ul>
<h1><span id="rewoo-0">ReWOO [0]</span><a href="#rewoo-0" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Abstract [10]<br>增强语言模型（ALM）将大型语言模型（LLM）的推理能力与允许知识检索和执行操作的工具相结合。现有的ALM系统以交错的方式触发LLM的思考过程，同时从这些工具中获取观察结果。<strong>具体而言，LLM推理过程中会调用外部工具，然后在获取工具响应后停止，基于之前的响应令牌来决定下一步的操作。这种范式虽然直观且易于实现，但常常由于冗余的提示和重复的执行而导致计算复杂度极高</strong>。本研究首次解决了这些挑战，提出了一种模块化的范式ReWOO（无观察推理），<strong>将推理过程与外部观察结果分离，从而显著减少了令牌的消耗</strong>。</li>
<li>Figure 1 [10]<br>Planner里有格式化的#E</li>
<li>Figure 2  [10]</li>
</ul>
</li>
<li><p>代码 [11]</p>
<ul>
<li>Executor-tool_execution() -&gt; 状态机</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li>是否可以并行？-&gt; LLMCompiler</li>
</ul>
</li>
</ul>
<h1><span id="llmcompiler">LLMCompiler</span><a href="#llmcompiler" class="header-anchor">#</a></h1><ul>
<li><p>原理</p>
<ul>
<li>Abstract [20]<br>LLM的多函数调用能力催生了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多函数调用方法通常需要<strong>针对每个函数进行顺序推理和执行，这可能导致较高的延迟、成本以及不准确的行为</strong>。为了解决这个问题，我们引入了LLMCompiler，它可以<strong>并行执行函数，以高效地编排多函数调用</strong>。LLMCompiler<strong>借鉴了经典编译器的原理</strong>，在LLM中使用<strong>三个组件</strong>来简化并行函数调用：（i）LLM规划器，制定执行策略和依赖关系；（ii）任务获取单元，分派和更新函数调用任务；（iii）执行器，以并行方式执行这些任务。通过LLMCompiler，用户可以指定工具以及可选的上下文示例，LLMCompiler会自动计算函数调用的优化编排。重要的是，LLMCompiler可以与LLaMA-2等开源模型以及OpenAI的GPT模型一起使用。</li>
<li>Figure 2  [20]</li>
</ul>
</li>
<li><p>代码 [21]</p>
<ul>
<li>Planner</li>
<li>Task Fetching Unit </li>
<li>Joiner</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="plan-and-execute">Plan-and-execute</span><a href="#plan-and-execute" class="header-anchor">#</a></h3><ol start="0">
<li><a href="https://www.bilibili.com/video/BV1vJ4m1s7Zn/">LangGraph：Plan-Execute Agents 实战</a> V<br><a href="https://blog.langchain.dev/planning-agents/">Plan-and-Execute Agents</a> ***</li>
<li><a href="https://arxiv.org/pdf/2305.04091.pdf">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought<br>Reasoning by Large Language Models</a>  Figure 2</li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb">plan-and-execute</a>    git</li>
</ol>
<h3><span id="rewoo">ReWOO</span><a href="#rewoo" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://arxiv.org/pdf/2305.18323.pdf">ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</a></li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb">Reasoning without Observation</a> git<br><a href="https://www.bilibili.com/video/BV1Au4m1N7ix/">ReWoo Agent框架代码实现</a> V<br>1xx.  <a href="https://zhuanlan.zhihu.com/p/671491031">ReWOO: 高效增强语言模型中解偶观测和推理</a></li>
</ol>
<h3><span id="llmcompiler">LLMCompiler</span><a href="#llmcompiler" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://arxiv.org/pdf/2312.04511v1.pdf">An LLM Compiler for Parallel Function Calling</a></li>
<li><a href="https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb">LLMCompiler</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ViT, ViLT</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalVit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/pdf/2010.11929">AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a> </p>
</li>
<li><p>开源地址<br><a href="https://github.com/google-research/vision_transformer">vision_transformer</a> git</p>
</li>
</ul>
<h1><span id="model">model</span><a href="#model" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/01/gptMultimodalVit/model.png" class>


<h1><span id="代码1">代码[1]</span><a href="#代码1" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="vit">ViT</span><a href="#vit" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV1fH4y1H7mV/">【Sora重要技术】复现ViT（Vision Transformer）模型</a> V<br>  <a href="https://github.com/owenliang/mnist-vit">mnist-vit Repo</a> git</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Uu411o7oY">VIT (Vision Transformer) 模型论文+代码(源码)从零详细解读，看不懂来打我</a> V</p>
<p>1xx. <a href="https://blog.csdn.net/qq_43449643/article/details/135623953">详解VIT（Vision Transformer)模型原理, 代码级讲解</a><br>   <a href="https://github.com/yangyunfeng-cyber/Useful-DL-Projects-for-Exercise/blob/main/VIT/vit_model.py">VIT Repo</a> git  ***</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1xm4y1b7Pw/">ViT｜ Vision Transformer ｜理论 + 代码</a>  V<br>   <a href="https://65d8gk.axshare.com/">PPT</a></p>
<h3><span id="vilt">ViLT</span><a href="#vilt" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/369733979">ViLT：最简单的多模态Transformer</a><br>1xx. <a href="https://github.com/dandelin/vilt">ViLT</a> git<br>1xx. <a href="https://blog.csdn.net/qq_42030496/article/details/134641704">ViLT 论文精读【论文精读】</a><br>   <a href="https://www.bilibili.com/video/BV14r4y1j74y/">ViLT 论文精读【论文精读】</a> V<br>1xx. <a href="https://blog.csdn.net/m0_56722835/article/details/125071550">多模态ViLT模型下游任务微调原理及代码</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ViT</category>
      </categories>
      <tags>
        <tag>ViT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) SAM</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文1">论文[1]</span><a href="#论文1" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><a href="https://arxiv.org/abs/2304.02643">Segment Anything Model论文</a></p>
</li>
<li><p>开源地址<br> <a href="https://github.com/facebookresearch/segment-anything">Segment Anything Model模型源码</a> Git</p>
</li>
<li><p>官网<br> <a href="https://segment-anything.com/">Segment Anything Model官网</a><br><a href="https://segment-anything.com/demo">Segment Anything Model官网demo网页端</a></p>
</li>
</ul>
<h1><span id="模型2">模型[2]</span><a href="#模型2" class="header-anchor">#</a></h1><h3><span id="image-encoder">image encoder</span><a href="#image-encoder" class="header-anchor">#</a></h3><h3><span id="prompt-encoder">prompt encoder</span><a href="#prompt-encoder" class="header-anchor">#</a></h3><h3><span id="mask-decoder">mask decoder</span><a href="#mask-decoder" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/mask%20decoder.webp" class>

<p>在prompt embeddings中插入一个可学习的token，用于docoder的输出。<br>（1）prompt toekns+output tokens进行self attn,<br>（2）用得到的token和image embedding进行 cross attn（token作为Q）<br>（3）point-wise MLP 更新token<br>（4）用image embedding和（3）的token进行cross atten（image embedding作为Q）<br>重复上述步骤2次，再将attn再通过残差进行连接，最终输出masks和iou scores。</p>
<h1><span id="sam应用3">SAM应用[3]</span><a href="#sam应用3" class="header-anchor">#</a></h1><ul>
<li>图像分割</li>
<li>目标检测</li>
<li>图像修复( image inpainting)</li>
<li><strong>模型微调</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/weixin_44386956/article/details/130262260">【模型解读】【代码复现】Segment Anything Model(SAM)</a> *</li>
<li><a href="https://zhuanlan.zhihu.com/p/620355474">【论文解读】MetaAi SAM(Segment Anything) 分割一切</a> ***<br>最下面有应用</li>
<li><a href="https://zhuanlan.zhihu.com/p/630529550">Segment Anything(sam)项目整理汇总</a> ***<br>1xx. <a href="https://www.bilibili.com/video/BV1aV4y1d7gC/">【Segment Anything 模型深度解构】GPT时代，干翻计算机视觉第一步！</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)CLIP</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#clip-%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%81%9A%E4%BA%86%E5%93%AA%E4%BA%9B%E4%BA%8B%E6%83%85elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</a></li>
<li><a href="#clip-zero-shot%E6%8E%A8%E7%90%861">CLIP Zero-shot推理[1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E5%B1%80%E9%99%90">局限</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="clip-在训练过程中做了哪些事情elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</span><a href="#clip-在训练过程中做了哪些事情elmo1" class="header-anchor">#</a></h1><p>在训练过程中，CLIP（Contrastive Language-Image Pre-training）模型主要进行了以下几个步骤：</p>
<ol>
<li><strong>数据预处理</strong> : CLIP 使用了一个大规模的数据集，包含 4 亿个 “图像 - 文本” 对，这些数据需要进行清洗和预处理，以便于模型学习。</li>
<li><strong>特征提取</strong> : 通过 Text Encoder 和 Image Encoder 分别对文本和图像进行特征提取。Text Encoder 通常是基于 Transformer 的模型，而 Image Encoder 可以是基于 CNN（卷积神经网络）或者 VIT（Vision Transformer）的模型。</li>
<li><strong>对比学习</strong> : CLIP 采用对比学习的策略，通过对比正确的图像 - 文本对与错误的图像 - 文本对，使得模型能够学习到正确对的特征表示与其他对的区分。具体来说，CLIP 通过 InfoNCE 损失函数来最大化正确对的相似度，同时最小化错误对的相似度。</li>
<li><strong>特征空间对齐</strong> : 通过对比学习，CLIP 将图像和文本的特征映射到一个共享的多模态特征空间中，使得图像特征和文本特征可以直接进行相似度比较。</li>
<li><strong>参数优化</strong> : 通过反向传播和梯度下降等方法，不断调整模型参数，以最小化损失函数，从而优化模型性能。</li>
<li><strong>Zero-shot 推理能力的培养</strong> : 在训练过程中，CLIP 学习了如何通过文本提示（prompts）来进行 zero-shot 的图像分类，即在没有直接观测到的类别标签下，通过文本描述来识别图像内容。</li>
<li><strong>模型评估与调整</strong> : 通过在验证集上的评估，调整模型结构和超参数，以提高模型的泛化能力和性能。</li>
</ol>
<p>通过这些训练步骤，CLIP 能够学习到一个强大的多模态特征表示，使其能够在多种视觉任务上进行 zero-shot 或 few-shot 的推理。</p>
<h1><span id="clip-zero-shot推理1">CLIP Zero-shot推理[1]</span><a href="#clip-zero-shot推理1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>首先，我们创建一个<strong>标签全集</strong>，如图中（2）所示，并得到每一个<strong>标签的特征向量</strong></li>
<li>然后，我们取一张图片，如图中（3）所示，过Image Encoder后得到该<strong>图片的特征向量</strong></li>
<li>最后，计算图片向量和文字向量间的<strong>相似度</strong>，取相似度最高的那条label即可。</li>
</ul>
<h3><span id="局限">局限</span><a href="#局限" class="header-anchor">#</a></h3><p>  当你喂给CLIP一张图时，不管这张图片它是否有见过，CLIP都不会生成一个全新的标签，而是去全集标签中找一个最相似的给你。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a> 代码</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/493489688">神器CLIP：连接文本和图像，打造可迁移的视觉模型</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/486857682">【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/646790176">CLIP 模型解读</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/521151393">详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50&#x2F;101</a></p>
<p>1xx. <a href="https://blog.csdn.net/lsb2002/article/details/132275132">openai多模态大模型：clip详解及实战</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/555314976">CLIP：多模态领域革命者</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Tokenizer</title>
    <url>/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="tokenizer-分词">tokenizer 分词</span><a href="#tokenizer-分词" class="header-anchor">#</a></h3><ul>
<li>单词分词法</li>
<li>单字分词法</li>
<li>子词分词法<br>BPE [GPT系列], WordPiece</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/630696264">大模型词表扩充必备工具SentencePiece</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/458452872">NLP（二）：浅谈分词</a><br>1xx. <a href="https://www.bilibili.com/video/BV1vN411p7t2/">https://www.bilibili.com/video/BV1vN411p7t2/</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400849&idx=1&sn=58006756cccde4d06d273df59e2c8dd8">开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tokenizer</category>
      </categories>
      <tags>
        <tag>Tokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey List</title>
    <url>/www6vHomeAIGC/2023/02/25/gptSurveyList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="awesome-llm-survey">Awesome-LLM-Survey</span><a href="#awesome-llm-survey" class="header-anchor">#</a></h1><ul>
<li><a href="#awesome-llm-survey">Awesome-LLM-Survey</a><ul>
<li><p><a href="#general-survey">General Survey</a> *** </p>
</li>
<li><p><a href="#training-of-llm">Training of LLM</a></p>
<ul>
<li><a href="#instruction-tuning">Instruction Tuning</a> *** </li>
<li><a href="#human-alignment-for-llm">Human Alignment for LLM</a> *</li>
</ul>
</li>
<li><p><a href="#prompt-of-llm">Prompt of LLM</a></p>
<ul>
<li><a href="#chain-of-thought-for-llm">Chain of Thought for LLM</a> *** </li>
<li><a href="#prompt-engineering-for-llm">Prompt Engineering for LLM</a> * </li>
<li><a href="#retrieval-augmented-llm">Retrieval-Augmented LLM</a> ***</li>
</ul>
</li>
<li><p><a href="#challenge-of-llm">Challenge of LLM</a></p>
<ul>
<li><p><a href="#hallucination-in-llm">Hallucination in LLM</a> ***</p>
</li>
<li><p><a href="#compression-for-llm">Compression for LLM</a> ***</p>
</li>
<li><p><a href="#evaluation-of-llm">Evaluation of LLM</a></p>
</li>
<li><p><a href="#reasoning-with-llm">Reasoning with LLM</a></p>
</li>
<li><p><a href="#long-context-for-llm">Long-Context for LLM</a></p>
</li>
<li><p><a href="#factuality-in-llm">Factuality in LLM</a></p>
</li>
<li><p><a href="#knowledge-for-llm">Knowledge for LLM</a></p>
</li>
<li><p><a href="#self-correction-for-llm">Self-Correction for LLM</a></p>
</li>
<li><p><a href="#tool-using-of-llm">Tool Using of LLM</a> ***</p>
</li>
<li><p><a href="#agent-of-llm">Agent of LLM</a> ***</p>
</li>
<li><p><a href="#efficiency-of-llm">Efficiency of LLM</a> *** </p>
</li>
<li><p><a href="#data-of-llm">Data of LLM</a> ***</p>
</li>
<li><p><a href="#continual-learning-of-llm">Continual Learning of LLM</a></p>
</li>
</ul>
</li>
<li><p><a href="#mulitmodal-of-llm">Mulitmodal of LLM</a></p>
<ul>
<li><a href="#visual-llm">Visual LLM</a></li>
</ul>
</li>
<li><p><a href="#llm-for-domain-application">LLM for Domain Application</a></p>
<ul>
<li><a href="#llm-for-health">LLM for Health</a></li>
<li><a href="#llm-for-finance">LLM for Finance</a> ***</li>
<li><a href="#llm-for-education">LLM for Education</a></li>
<li><a href="#llm-for-law">LLM for Law</a></li>
<li><a href="#llm-for-mental-health">LLM for Mental Health</a></li>
</ul>
</li>
<li><p><a href="#llm-for-downstream-tasks">LLM for Downstream Tasks</a></p>
<ul>
<li><p><a href="#llm-for-recommendation">LLM for Recommendation</a></p>
</li>
<li><p><a href="#llm-for-information-retrieval">LLM for Information Retrieval</a></p>
</li>
<li><p><a href="#llm-for-software-engineering">LLM for Software Engineering</a></p>
</li>
<li><p><a href="#llm-for-time-series">LLM for Time Series</a></p>
</li>
<li><p><a href="#detection-of-llms-generated-content">Detection of LLMs-Generated Content</a></p>
</li>
<li><p><a href="#llm-for-information-extraction">LLM for Information Extraction</a></p>
</li>
</ul>
</li>
<li><p><a href="#star-history">Star History</a></p>
</li>
</ul>
</li>
</ul>
<hr>
<h2><span id="general-survey">General Survey</span><a href="#general-survey" class="header-anchor">#</a></h2><ul>
<li><p>Challenges and Applications of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.10169">[paper]</a> *** </p>
</li>
<li><p>A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT, 2023.02 <a href="https://arxiv.org/abs/2302.09419">[paper]</a> ***</p>
</li>
<li><p>A Survey of Large Language Models, 2023.11 <a href="https://arxiv.org/abs/2303.18223">[paper]</a><a href="https://github.com/RUCAIBox/LLMSurvey">[project]</a>  ***</p>
</li>
<li><p>A Comprehensive Overview of Large Language Models *</p>
</li>
<li><p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</p>
</li>
<li><p>Pre-Trained Models: Past, Present and Future</p>
</li>
<li><p>A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4, 2023.10 <a href="https://arxiv.org/pdf/2310.12321.pdf">[paper]</a></p>
</li>
<li><p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, 2023.04  <a href="https://arxiv.org/abs/2304.13712">[paper]</a><a href="https://github.com/Mooler0410/LLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects, 2023.12 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.23589741.v4">[paper]</a> <a href="https://github.com/anas-zafar/LLM-Survey">[project]</a></p>
</li>
<li><p>The future of gpt: A taxonomy of existing chatgpt research, current challenges, and possible future directions, 2023.04 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4413921">[paper]</a></p>
</li>
<li><p>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges, 2023.10 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.24171183.v1">[paper]</a></p>
</li>
<li><p>Understanding LLMs: A Comprehensive Overview from Training to Inference, 2024.01 <a href="https://arxiv.org/pdf/2401.02038.pdf">[paper]</a></p>
</li>
</ul>
<hr>
<h2><span id="training-of-llm">Training of LLM</span><a href="#training-of-llm" class="header-anchor">#</a></h2><h3><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h3><ul>
<li>Are Prompts All the Story? No. A Comprehensive and Broader View of Instruction Learning, 2023.03 <a href="https://arxiv.org/pdf/2303.10475.pdf">[paper]</a> <a href="https://github.com/RenzeLou/awesome-instruction-learning">[project]</a></li>
<li>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a></li>
<li>Instruction Tuning for Large Language Models: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.10792">[paper]</a>  ***</li>
</ul>
<h3><span id="human-alignment-for-llm">Human Alignment for LLM</span><a href="#human-alignment-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>AI Alignment: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19852">[paper]</a><a href="https://www.alignmentsurvey.com/">[project]</a></p>
</li>
<li><p>Large Language Model Alignment: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.15025">[paper]</a></p>
</li>
<li><p>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Model, 2023.08 <a href="https://arxiv.org/abs/2308.12014">[paper]</a><a href="https://github.com/ValueCompass/Alignment-Goal-Survey">[project]</a></p>
</li>
<li><p>Aligning Large Language Models with Human: A Survey, 2023.07 <a href="https://arxiv.org/abs/2307.12966">[paper]</a><a href="https://github.com/GaryYufei/AlignLLMHumanSurvey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="prompt-of-llm">Prompt of LLM</span><a href="#prompt-of-llm" class="header-anchor">#</a></h2><h3><span id="chain-of-thought-for-llm">Chain of Thought for LLM</span><a href="#chain-of-thought-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
<li><p>A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future, 2023.09 <a href="https://arxiv.org/abs/2309.06256">[paper]</a><a href="https://github.com/zchuz/CoT-Reasoning-Survey">[project]</a></p>
</li>
<li><p>Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents, 2023.11 <a href="https://arxiv.org/pdf/2311.11797.pdf">[paper]</a> <a href="https://github.com/Zoeyyao27/CoT-Igniting-Agent">[project]</a></p>
</li>
</ul>
<h3><span id="prompt-engineering-for-llm">Prompt Engineering for LLM</span><a href="#prompt-engineering-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Prompting Frameworks for Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12785.pdf">[paper]</a><a href="https://github.com/lxx0628/Prompting-Framework-Survey">[project]</a></p>
</li>
<li><p>Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review, 2023.10 <a href="https://arxiv.org/pdf/2310.14735.pdf">[paper]</a></p>
</li>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="retrieval-augmented-llm">Retrieval-Augmented LLM</span><a href="#retrieval-augmented-llm" class="header-anchor">#</a></h3><ul>
<li><p>Retrieving Multimodal Information for Augmented Generation: A Survey  *** </p>
</li>
<li><p>Retrieval-Augmented Generation for AI-Generated Content: A Survey *** </p>
</li>
<li><p>A Survey on Retrieval-Augmented Text Generation, 2022.02 <a href="https://arxiv.org/abs/2202.01110">[paper]</a></p>
</li>
<li><p>Retrieval-Augmented Generation for Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.10997.pdf">[paper]</a> <a href="https://github.com/Tongji-KGLLM/RAG-Survey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="challenge-of-llm">Challenge of LLM</span><a href="#challenge-of-llm" class="header-anchor">#</a></h2><h3><span id="hallucination-in-llm">Hallucination in LLM</span><a href="#hallucination-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.07914">[paper]</a></p>
</li>
<li><p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, 2023.11 <a href="https://arxiv.org/pdf/2311.05232">[paper]</a><a href="https://github.com/LuckyyySTA/Awesome-LLM-hallucination">[project]</a> ***</p>
</li>
<li><p>A Survey of Hallucination in “Large” Foundation Models, 2023.09  <a href="https://arxiv.org/paper/2309.05922">[paper]</a><a href="https://github.com/vr25/hallucination-foundation-model-survey">[project]</a></p>
</li>
<li><p>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models, 2023.09 <a href="https://arxiv.org/abs/2309.01219">[paper]</a><a href="https://arxiv.org/abs/2309.01219">[project]</a></p>
</li>
<li><p>Cognitive Mirage: A Review of Hallucinations in Large Language Models, 2023.09 <a href="https://arxiv.org/paper/2309.06794.paper">[paper]</a><a href="https://github.com/hongbinye/Cognitive-Mirage-Hallucinations-in-LLMs">[project]</a></p>
</li>
<li><p>Augmenting LLMs with Knowledge: A survey on hallucination prevention, 2023.09 <a href="https://arxiv.org/pdf/2309.16459.pdf">[paper]</a></p>
</li>
<li><p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models, 2024.01 <a href="https://arxiv.org/pdf/2401.01313.pdf">[paper]</a></p>
</li>
<li><p>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment, 2023.08 <a href="https://arxiv.org/abs/2308.05374">[paper]</a></p>
</li>
</ul>
<h3><span id="compression-for-llm">Compression for LLM</span><a href="#compression-for-llm" class="header-anchor">#</a></h3><ul>
<li>A Survey on Model Compression for Large Language Models, 2023.08 <a href="https://arxiv.org/abs/2308.07633">[paper]</a>  ***</li>
<li>A Comprehensive Survey of Compression Algorithms for Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.15347.pdf">paper</a>]</li>
</ul>
<h3><span id="evaluation-of-llm">Evaluation of LLM</span><a href="#evaluation-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Evaluating Large Language Models: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19736.pdf">[paper]</a><a href="https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers">[project]</a> ***</p>
</li>
<li><p>A Survey on Evaluation of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.03109">[paper]</a><a href="https://llm-eval.github.io/">[project]</a> ***</p>
</li>
</ul>
<h3><span id="reasoning-with-llm">Reasoning with LLM</span><a href="#reasoning-with-llm" class="header-anchor">#</a></h3><ul>
<li><p>Reasoning with Language Model Prompting: A Survey, 2022.12 <a href="https://arxiv.org/abs/2212.09597">[paper]</a><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">[project]</a></p>
</li>
<li><p>A Survey of Reasoning with Foundation Models, 2023.12 [[papaer]] (<a href="https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)">https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)</a> ***</p>
</li>
</ul>
<h3><span id="long-context-for-llm">Long-Context for LLM</span><a href="#long-context-for-llm" class="header-anchor">#</a></h3><ul>
<li>Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12351">[paper]</a></li>
<li>Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding, 2023.12 <a href="https://arxiv.org/abs/2312.17044">[paper]</a></li>
</ul>
<h3><span id="factuality-in-llm">Factuality in LLM</span><a href="#factuality-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>A Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity, 2023.10 <a href="https://arxiv.org/abs/2310.07521">[paper]</a><a href="https://github.com/wangcunxiang/LLM-Factuality-Survey">[project]</a></p>
</li>
<li><p>Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models, 2023.10 <a href="https://arxiv.org/pdf/2310.16570.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="knowledge-for-llm">Knowledge for LLM</span><a href="#knowledge-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges, 2023.11 <a href="https://arxiv.org/pdf/2311.15766">[paper]</a></p>
</li>
<li><p>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications, 2023.11 <a href="https://arxiv.org/pdf/2311.05876.pdf">[paper]</a></p>
</li>
<li><p>Knowledge Editing for Large Language Models: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.16218.pdf">[paper]</a></p>
</li>
<li><p>Editing Large Language Models: Problems, Methods, and Opportunities, 2023.05 <a href="https://arxiv.org/abs/2305.13172">[paper]</a><a href="https://github.com/zjunlp/EasyEdit">[project]</a></p>
</li>
<li><p>Building trust in conversational ai: A comprehensive review and solution architecture for explainable, privacy-aware systems using llms and knowledge graph, 2023.08 <a href="https://arxiv.org/pdf/2308.13534.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="self-correction-for-llm">Self-Correction for LLM</span><a href="#self-correction-for-llm" class="header-anchor">#</a></h3><ul>
<li>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies, 2023.08 <a href="https://arxiv.org/abs/2308.03188">[paper]</a><a href="https://github.com/teacherpeterpan/self-correction-llm-papers">[project]</a></li>
</ul>
<h3><span id="tool-using-of-llm">Tool Using of LLM</span><a href="#tool-using-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Foundation Models for Decision Making: Problems, Methods, and Opportunities, 2023.03 <a href="https://arxiv.org/abs/2303.04129">[paper]</a></p>
</li>
<li><p>Augmented Language Models: a Survey, 2023.02 <a href="https://arxiv.org/abs/2302.07842">[paper]</a> ***</p>
</li>
<li><p>Tool Learning with Foundation Models</p>
</li>
</ul>
<h3><span id="agent-of-llm">Agent of LLM</span><a href="#agent-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Understanding the planning of LLM agents: A survey, 2024 </p>
</li>
<li><p>A Survey on Large Language Model based Autonomous Agents, 2023.08 <a href="https://arxiv.org/abs/2308.11432">[paper]</a><a href="https://github.com/Paitesanshi/LLM-Agent-Survey">[project]</a> ***</p>
</li>
<li><p>The Rise and Potential of Large Language Model Based Agents: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.07864">[paper]</a><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">[project]</a> ***</p>
</li>
<li><p>Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives, 2023.12 <a href="https://arxiv.org/pdf/2312.11970.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="efficiency-of-llm">Efficiency of LLM</span><a href="#efficiency-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning ***</p>
</li>
<li><p>The Power of Scale for Parameter-Efficient Prompt Tuning</p>
</li>
<li><p>Efficient Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03863">[paper]</a><a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">[project]</a> ***</p>
</li>
<li><p>The Efficiency Spectrum of Large Language Models: An Algorithmic Survey, 2023.12 <a href="https://arxiv.org/pdf/2310.10844.pdf">[paper]</a><a href="https://github.com/tding1/Efficient-LLM-Survey">[project]</a></p>
</li>
<li><p>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment, 2023.12 <a href="https://arxiv.org/pdf/2312.12148.pdf">[paper]</a></p>
</li>
<li><p>A Survey on Hardware Accelerators for Large Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.09890.pdf">paper</a>]</p>
</li>
</ul>
<h3><span id="data-of-llm">Data of LLM</span><a href="#data-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Management For Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.01700">[paper]</a><a href="https://github.com/ZigeW/data_management_LLM">[project]</a></p>
</li>
<li><p>Data-centric Artificial Intelligence: A Survey</p>
</li>
</ul>
<h3><span id="continual-learning-of-llm">Continual Learning of LLM</span><a href="#continual-learning-of-llm" class="header-anchor">#</a></h3><ul>
<li>Continual Learning with Pre-Trained Models: A Survey, 2024.01 <a href="https://arxiv.org/pdf/2401.16386">[paper]</a> <a href="https://github.com/sun-hailong/LAMDA-PILOT">[project]</a></li>
</ul>
<hr>
<h2><span id="mulitmodal-of-llm">Mulitmodal of LLM</span><a href="#mulitmodal-of-llm" class="header-anchor">#</a></h2><h3><span id="visual-llm">Visual LLM</span><a href="#visual-llm" class="header-anchor">#</a></h3><ul>
<li><p>An Empirical Study of Training End-to-End Vision-and-Language Transformers, 2022.03 *** microsoft +</p>
</li>
<li><p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants, 2023.09 *** microsoft +</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 ***  大学 +</p>
</li>
<li><p>MM-LLMs: Recent Advances in MultiModal Large Language Models, 2024.02 *** 腾讯  +</p>
</li>
<li><p>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a> *** + </p>
</li>
<li><p>How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model, 2023.11 <a href="https://arxiv.org/pdf/2311.07594.pdf">[paper]</a> *</p>
</li>
<li><p>A Survey on Multimodal Large Language Models, 2023.06  <a href="https://arxiv.org/abs/2306.13549">[paper]</a> <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">[project]</a> ***</p>
</li>
<li><p>Multimodal Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.13165.pdf">[paper]</a> **</p>
</li>
<li><p>Large Language Models Meet Computer Vision: A Brief Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.16673.pdf">[paper]</a> *</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 <a href="https://arxiv.org/pdf/2307.13721.pdf">[paper]</a><a href="https://github.com/awaisrauf/Awesome-CV-Foundational-Models">[project]</a> ***  + </p>
</li>
<li><p>Video Understanding with Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17432.pdf">[paper]</a> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">[project]</a></p>
</li>
</ul>
<hr>
<h2><span id="llm-for-domain-application">LLM for Domain Application</span><a href="#llm-for-domain-application" class="header-anchor">#</a></h2><h3><span id="domain">domain</span><a href="#domain" class="header-anchor">#</a></h3><ul>
<li>Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey</li>
</ul>
<h3><span id="llm-for-health">LLM for Health</span><a href="#llm-for-health" class="header-anchor">#</a></h3><ul>
<li><p>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge, 2023.11 <a href="https://arxiv.org/pdf/2311.05112">[paper]</a><a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large Language Models Illuminate a Progressive Pathway to Artificial  Healthcare Assistant: A Review, 2023.10 <a href="https://arxiv.org/pdf/2311.01918">[paper]</a><a href="https://github.com/mingze-yuan/Awesome-LLM-Healthcare">[project]</a></p>
</li>
<li><p>Large AI Models in Health Informatics: Applications, Challenges, and the Future, 2023.03 <a href="https://arxiv.org/abs/2303.11568">[paper]</a><a href="https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models">[project]</a></p>
</li>
<li><p>A SWOT (Strengths, Weaknesses, Opportunities, and Threats) Analysis of ChatGPT in the Medical Literature: Concise Review, 2023.11 <a href="https://www.jmir.org/2023/1/e49368/PDF">[paper]</a></p>
</li>
<li><p>ChatGPT in Healthcare: A Taxonomy and Systematic Review, 2023.03 <a href="https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1">[paper]</a></p>
</li>
</ul>
<h3><span id="llm-for-finance">LLM for Finance</span><a href="#llm-for-finance" class="header-anchor">#</a></h3><ul>
<li><p>Large Language Models in Finance: A Survey, 2023.09 <a href="https://arxiv.org/abs/2311.10723">[paper]</a></p>
</li>
<li><p>A Survey of Large Language Models in Finance (FinLLMs) ***</p>
</li>
</ul>
<h3><span id="llm-for-education">LLM for Education</span><a href="#llm-for-education" class="header-anchor">#</a></h3><ul>
<li>ChatGPT and Beyond: The Generative AI Revolution in Education, 2023.11 <a href="https://arxiv.org/pdf/2311.15198">[paper]</a></li>
</ul>
<h3><span id="llm-for-law">LLM for Law</span><a href="#llm-for-law" class="header-anchor">#</a></h3><ul>
<li>Large Language Models in Law: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03718">[paper]</a></li>
</ul>
<h3><span id="llm-for-mental-health">LLM for Mental Health</span><a href="#llm-for-mental-health" class="header-anchor">#</a></h3><ul>
<li>A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement, 2023.10 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10601652/">[paper]</a></li>
<li>Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects, 2023.12 <a href="https://arxiv.org/pdf/2312.04578.pdf">[paper]</a></li>
<li>Large Language Models in Mental Health Care: a Scoping Review, 2024.01 <a href="https://arxiv.org/pdf/2401.02984.pdf">[paper]</a></li>
</ul>
<hr>
<h2><span id="llm-for-downstream-tasks">LLM for Downstream Tasks</span><a href="#llm-for-downstream-tasks" class="header-anchor">#</a></h2><h3><span id="llm-for-recommendation">LLM for Recommendation</span><a href="#llm-for-recommendation" class="header-anchor">#</a></h3><ul>
<li>User Modeling in the Era of Large Language Models: Current Research and Future Directions, 2023.12 <a href="https://doi.org/10.48550/arXiv.2312.11518">[paper]</a><a href="https://github.com/TamSiuhin/LLM-UM-Reading">[project]</a></li>
<li>A Survey on Large Language Models for Personalized and Explainable  Recommendations, 2023.11 <a href="https://arxiv.org/pdf/2311.12338">[paper]</a></li>
<li>Large Language Models for Generative Recommendation: A Survey and Visionary Discussions, 2023.09 <a href="https://arxiv.org/abs/2309.01157">[paper]</a></li>
<li>A Survey on Large Language Models for Recommendation, 2023.08 <a href="https://arxiv.org/abs/2305.19860">[paper]</a><a href="https://github.com/WLiK/LLM4Rec-Awesome-Papers">[project]</a></li>
<li>How Can Recommender Systems Benefit from Large Language Models: A Survey, 2023.06 <a href="https://arxiv.org/abs/2306.05817">[paper]</a><a href="https://github.com/CHIANGEL/Awesome-LLM-for-RecSys">[project]</a></li>
</ul>
<h3><span id="llm-for-information-retrieval">LLM for Information Retrieval</span><a href="#llm-for-information-retrieval" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Information Retrieval: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.07107">[paper]</a><a href="https://github.com/RUC-NLPIR/LLM4IR-Survey">[project]</a></li>
</ul>
<h3><span id="llm-for-software-engineering">LLM for Software Engineering</span><a href="#llm-for-software-engineering" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Software Engineering: Survey and Open Problems, 2023.10 <a href="https://arxiv.org/abs/2310.03533">[paper]</a></li>
<li>Large Language Models for Software Engineering: A Systematic Literature Review, 2023.08 <a href="https://arxiv.org/abs/2308.10620">[paper]</a></li>
</ul>
<h3><span id="llm-for-time-series">LLM for Time Series</span><a href="#llm-for-time-series" class="header-anchor">#</a></h3><ul>
<li>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook, 2023.10 <a href="https://arxiv.org/abs/2310.10196">[paper]</a><a href="https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM">[project]</a></li>
</ul>
<h3><span id="detection-of-llms-generated-content">Detection of LLMs-Generated Content</span><a href="#detection-of-llms-generated-content" class="header-anchor">#</a></h3><ul>
<li>A Survey on Detection of LLMs-Generated Content, 2023.10 <a href="https://arxiv.org/abs/2310.15654">[paper]</a><a href="https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection">[project]</a></li>
<li>A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions, 2023.10 <a href="https://arxiv.org/pdf/2310.14724.pdf">[paper]</a><br><a href="https://github.com/NLP2CT/LLM-generated-Text-Detection">[project]</a></li>
<li>Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text, 2023.09 <a href="https://arxiv.org/pdf/2309.07689.pdf">[paper]</a></li>
<li></li>
</ul>
<h3><span id="llm-for-information-extraction">LLM for Information Extraction</span><a href="#llm-for-information-extraction" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Generative Information Extraction: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17617.pdf">[paper]</a> <a href="https://github.com/quqxui/Awesome-LLM4IE-Papers">[project]</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/Awesome-LLM-Survey">Awesome-LLM-Survey</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA 家族</title>
    <url>/www6vHomeAIGC/2023/02/24/gptLlamaFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llama-家族1">LLaMA 家族[1]</span><a href="#llama-家族1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>项目</th>
<th>描述</th>
<th>数据集</th>
</tr>
</thead>
<tbody><tr>
<td>LLaMa</td>
<td>基座模型</td>
<td>公开可用的数据集(1T token)</td>
</tr>
<tr>
<td>Stanford Alpaca</td>
<td>结合英文语料通过Self Instruct方式微调LLaMA 7B</td>
<td>Self Instruct from davinci-003 API(52K)</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune)</td>
<td>用户共享对话(70K sample)</td>
</tr>
<tr>
<td>BELLE</td>
<td>结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA</td>
<td></td>
</tr>
<tr>
<td>Chinese-LLaMA&#x2F;Chinese-Alpaca</td>
<td>通过中文数据预训练&#x2F;指令微调LLaMA</td>
<td></td>
</tr>
<tr>
<td>姜子牙系列模型Ziya-LLaMA-13B-v1</td>
<td>基于LLaMA-13B的中英文模型</td>
<td></td>
</tr>
<tr>
<td>ChatLLaMA(英文版)</td>
<td>LLaMA的RLHF版</td>
<td></td>
</tr>
<tr>
<td>ColossalChat</td>
<td>通过self-instruct技术指令微调LLaMA且加上RLHF</td>
<td></td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/llama2-famaly.jpg" class>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="家族">家族</span><a href="#家族" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/129709105">LLaMA的解读与其微调：Alpaca-LoRA&#x2F;Vicuna&#x2F;BELLE&#x2F;中文LLaMA&#x2F;姜子牙&#x2F;LLaMA 2</a> ***</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485019&idx=1&sn=e3417472c0c1f98aede498fbe905e1a0&">我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/618695885">NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究</a></p>
<p>1xx. &lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;  v</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402185&idx=2&sn=55901b89381e27aedee56c69041f6af8">近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 </a>    llama-2-7b-32k -  LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。</p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx.  <a href="https://zhuanlan.zhihu.com/p/618321077">从0到1复现斯坦福羊驼（Stanford Alpaca 7B）</a><br>    GPUs: 8 卡 A800 80GB GPUs</p>
<h3><span id="汉化">汉化</span><a href="#汉化" class="header-anchor">#</a></h3><p>1xx.  <a href="https://www.bilibili.com/video/BV1Np4y1j783/">掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享</a> v</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Chinese-LLaMA PT+SFT</title>
    <url>/www6vHomeAIGC/2023/02/21/gptChineseLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-%E6%A8%A1%E5%9E%8B-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87">代码、模型、数据集准备</a><ul>
<li><a href="#%E4%BB%A3%E7%A0%81%E5%87%86%E5%A4%87-5">代码准备 [5]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D-%E5%8F%8A-tokenizer-%E5%87%86%E5%A4%87-4">模型权重 及 Tokenizer 准备 [4]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87-3">数据集准备 [3]</a></li>
</ul>
</li>
<li><a href="#%E8%AF%8D%E8%A1%A8%E6%89%A9%E5%85%85">词表扩充</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82">模型训练细节</a><ul>
<li><a href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E9%A2%84%E8%AE%AD%E7%BB%83">第二阶段预训练</a></li>
<li><a href="#%E5%B0%86-lora-%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将 LoRA 权重与基础模型合并</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E7%B2%BE%E8%B0%83">指令精调</a></li>
<li><a href="#%E5%B0%86%E5%A4%9A%E4%B8%AAlora%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将多个LoRA权重与基础模型合并</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86">模型推理</a></li>
<li><a href="#%E7%BB%93%E8%AF%AD">结语</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="环境搭建">环境搭建</span><a href="#环境搭建" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install transformers==4.28.1 sentencepiece==0.1.97 google protobuf deepspeed -i https://pypi.tuna.tsinghua.ed</span></span><br><span class="line">u.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/huggingface/peft.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git checkout 13e53fc</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install torch==1.13.1</span></span><br></pre></td></tr></table></figure>
<h1><span id="代码-模型-数据集准备">代码、模型、数据集准备</span><a href="#代码-模型-数据集准备" class="header-anchor">#</a></h1><h3><span id="代码准备-5">代码准备 [5]</span><a href="#代码准备-5" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3e2f2529</span></span><br><span class="line">git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意: 一定要用 commitid &#x3D;3e2f2529的代码， 用最新代码会有很多异常</p>
</blockquote>
<h3><span id="模型权重-及-tokenizer-准备-4">模型权重 及 Tokenizer 准备 [4]</span><a href="#模型权重-及-tokenizer-准备-4" class="header-anchor">#</a></h3><h3><span id="数据集准备-3">数据集准备 [3]</span><a href="#数据集准备-3" class="header-anchor">#</a></h3><h1><span id="词表扩充">词表扩充</span><a href="#词表扩充" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python3 merge_tokenizers.py \</span></span><br><span class="line"><span class="language-bash">  --llama_tokenizer_dir /root/internLM/model/skyline2006/llama-7b \</span></span><br><span class="line"><span class="language-bash">  --chinese_sp_model_file /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/chinese_sp.model</span></span><br></pre></td></tr></table></figure>

<h1><span id="模型训练细节">模型训练细节</span><a href="#模型训练细节" class="header-anchor">#</a></h1><h3><span id="第二阶段预训练">第二阶段预训练</span><a href="#第二阶段预训练" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改运行脚本run_pt.sh</span><br><span class="line"></span><br><span class="line">lr=2e-4</span><br><span class="line">lora_rank=8</span><br><span class="line">lora_alpha=32</span><br><span class="line">lora_trainable=&quot;q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj&quot;</span><br><span class="line">modules_to_save=&quot;embed_tokens,lm_head&quot;</span><br><span class="line">lora_dropout=0.05</span><br><span class="line"></span><br><span class="line">pretrained_model=/root/internLM/model/skyline2006/llama-7b #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  #</span><br><span class="line">dataset_dir=/root/internLM/shu-master/books  #</span><br><span class="line">data_cache=/root/cache/books #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/output_dir #</span><br><span class="line">RANDOM=100 #</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>具体执行过程如下所示：<br>sh run_pt.sh </p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/3.png" class>

<p>模型输出文件：</p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/result.png" class>


<h3><span id="将-lora-权重与基础模型合并">将 LoRA 权重与基础模型合并</span><a href="#将-lora-权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python merge_llama_with_chinese_lora.py \</span><br><span class="line">    --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">    --tokenizer_path /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  \</span><br><span class="line">    --lora_model /root/internLM/llamazh/output_dir/lora/ \</span><br><span class="line">    --output_type huggingface \</span><br><span class="line">    --output_dir /root/internLM/llamazh/pt_merged/book-merge-hf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并LLaMA和LoRA后的权重</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ll -h /root/internLM/llamazh/pt_merged/book-merge-hf</span></span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 10:48 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 10:47 ../</span><br><span class="line">-rw-r--r-- 1 root root  598 Feb 23 10:47 config.json</span><br><span class="line">-rw-r--r-- 1 root root  132 Feb 23 10:47 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9.3G Feb 23 10:48 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3.6G Feb 23 10:48 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root  27K Feb 23 10:48 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root  411 Feb 23 10:47 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root 741K Feb 23 10:47 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root  727 Feb 23 10:47 tokenizer_config.json</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">原始llama权重</span></span><br><span class="line">ll -h /root/internLM/model/skyline2006/llama-7b</span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 21 20:04 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 21 19:38 ../</span><br><span class="line">-rw-r--r-- 1 root root   43 Feb 21 19:38 .mdl</span><br><span class="line">-rw------- 1 root root 3.6K Feb 21 20:04 .msc</span><br><span class="line">-rw-r--r-- 1 root root   36 Feb 21 20:04 .mv</span><br><span class="line">-rw------- 1 root root  11K Feb 21 19:39 LICENSE</span><br><span class="line">-rw------- 1 root root 9.0K Feb 21 20:04 README.md</span><br><span class="line">-rw------- 1 root root  22M Feb 21 19:38 alpaca_data.json</span><br><span class="line">-rw------- 1 root root  427 Feb 21 19:38 config.json</span><br><span class="line">-rw------- 1 root root  302 Feb 21 19:39 configuration.json</span><br><span class="line">-rw------- 1 root root 1.2K Feb 21 19:39 default_offload_opt_param.json</span><br><span class="line">-rw------- 1 root root  124 Feb 21 19:39 generation_config.json</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:40 pytorch_model-00001-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:42 pytorch_model-00002-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:44 pytorch_model-00003-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:47 pytorch_model-00004-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:50 pytorch_model-00005-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:51 pytorch_model-00006-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:53 pytorch_model-00007-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00008-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00009-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00010-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00011-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00012-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00013-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00014-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:58 pytorch_model-00015-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00016-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00017-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00018-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00019-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00020-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00021-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00022-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00023-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00024-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00025-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00026-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00027-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00028-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00029-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00030-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00031-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00032-of-00033.bin</span><br><span class="line">-rw------- 1 root root 501M Feb 21 20:04 pytorch_model-00033-of-00033.bin</span><br><span class="line">-rw------- 1 root root  25K Feb 21 20:04 pytorch_model.bin.index.json</span><br><span class="line">-rw------- 1 root root    2 Feb 21 20:04 special_tokens_map.json</span><br><span class="line">-rw------- 1 root root 489K Feb 21 20:04 tokenizer.model</span><br><span class="line">-rw------- 1 root root  141 Feb 21 20:04 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h3><span id="指令精调">指令精调</span><a href="#指令精调" class="header-anchor">#</a></h3><p>修改模型精调脚本run_sft.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pretrained_model=/root/internLM/llamazh/pt_merged/book-merge-hf  #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf #</span><br><span class="line">dataset_dir=/root/internLM/Chinese-LLaMA-Alpaca-main/data #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/sft_output  #</span><br><span class="line">#peft_model=path/to/peft/model/dir</span><br><span class="line">validation_file=/root/internLM/llm-action-main/train/chinese-llama-alpaca/alpaca_eval.json  #</span><br><span class="line">RANDOM=1000</span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; sh run_sft.sh </span><br></pre></td></tr></table></figure>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result2.png" class>


<p>模型输出文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -al -h /root/internLM/llamazh/sft_output/lora</span></span><br><span class="line">total 819M</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 15:29 .</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 15:29 ..</span><br><span class="line">-rw-r--r-- 1 root root  501 Feb 23 15:29 adapter_config.json</span><br><span class="line">-rw-r--r-- 1 root root 819M Feb 23 15:29 adapter_model.bin</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="将多个lora权重与基础模型合并">将多个LoRA权重与基础模型合并</span><a href="#将多个lora权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python merge_llama_with_chinese_lora.py \</span><br><span class="line">     --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">     --tokenizer_path /root/internLM/llamazh/output_dir,/root/internLM/llamazh/sft_output \</span><br><span class="line">     --lora_model /root/internLM/llamazh/output_dir/lora/,/root/internLM/llamazh/sft_output/lora \</span><br><span class="line">     --output_type huggingface \</span><br><span class="line">     --output_dir /root/internLM/llamazh/all_output</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ll  /root/internLM/llamazh/all_output</span><br><span class="line">total 13449132</span><br><span class="line">drwxr-xr-x 2 root root       4096 Feb 23 16:38 ./</span><br><span class="line">drwxr-xr-x 7 root root       4096 Feb 23 16:38 ../</span><br><span class="line">-rw-r--r-- 1 root root         21 Feb 23 16:38 added_tokens.json</span><br><span class="line">-rw-r--r-- 1 root root        598 Feb 23 16:38 config.json</span><br><span class="line">-rw-r--r-- 1 root root        132 Feb 23 16:38 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9943340890 Feb 23 16:38 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3827767515 Feb 23 16:38 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root      26788 Feb 23 16:38 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root        435 Feb 23 16:38 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root     757958 Feb 23 16:38 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root        747 Feb 23 16:38 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h1><span id="模型推理">模型推理</span><a href="#模型推理" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py \</span><br><span class="line">     --base_model /root/internLM/llamazh/all_output \</span><br><span class="line">     --with_prompt \</span><br><span class="line">     --interactive</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py      --base_model /root/internLM/llamazh/all_output      --with_prompt      --<span class="keyword">in</span></span><br><span class="line">teractive</span><br><span class="line">Loading checkpoint shards: <span class="number">100</span>%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| <span class="number">2</span>/<span class="number">2</span> [<span class="number">00</span>:<span class="number">26</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">13.09</span>s/it]</span><br><span class="line">Vocab of the base model: <span class="number">49954</span></span><br><span class="line">Vocab of the tokenizer: <span class="number">49954</span></span><br><span class="line">Start inference <span class="keyword">with</span> instruction mode.</span><br><span class="line">=====================================================================================</span><br><span class="line">+ 该模式下仅支持单轮问答，无多轮对话能力。</span><br><span class="line">+ 如要进行多轮对话，请使用llama.cpp或llamachat工具。</span><br><span class="line">-------------------------------------------------------------------------------------</span><br><span class="line">+ This mode only supports single-turn QA.</span><br><span class="line">+ If you want to experience multi-turn dialogue, please use llama.cpp <span class="keyword">or</span> llamachat.</span><br><span class="line">=====================================================================================</span><br><span class="line">Input:who are you？</span><br><span class="line">Response:  I am <span class="number">10</span> years old, my name <span class="keyword">is</span> Lilly.</span><br></pre></td></tr></table></figure>

<h1><span id="结语">结语</span><a href="#结语" class="header-anchor">#</a></h1><p>整个训练流程:<br>词表扩充+预训练(继续预训练)  -&gt;  输出lora模型<br>指令精调sft   -&gt;  输出lora模型<br>合并2个lora模型，在进行推理</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/631360711">中文LLaMA&amp;Alpaca大语言模型词表扩充+预训练+指令精调</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/">Chinese-LLaMA-Alpaca</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki">中文文档</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC">预训练脚本</a></li>
<li><a href="https://github.com/shjwudp/shu">继续预训练 DataSet</a></li>
<li><a href="https://www.modelscope.cn/models/skyline2006/llama-7b/summary">llama-7b</a> 基础模型</li>
<li><a href="https://github.com/www6v/AIGC/tree/master/chinese-llama-alpaca">chinese-llama-alpaca</a>  git 代码以这个为主<br><a href="https://github.com/www6v/llm-action/tree/main/train/chinese-llama-alpaca">chinese-llama-alpaca</a> 参考这个代码，有很多遗漏的文件，都补齐了，已提交到AIGC</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SELF-INSTRUCT</title>
    <url>/www6vHomeAIGC/2023/02/21/gptSelfInstruct/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="self-instruct">SELF-INSTRUCT</span><a href="#self-instruct" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SELF-INSTRUCT-10dbfe21108480adb3c9c6b4d13b57d0?pvs=4">(原理)SELF-INSTRUCT</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>STF</category>
      </categories>
      <tags>
        <tag>STF</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent 多模态</title>
    <url>/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#survey">Survey</a><ul>
<li><a href="#%E7%B1%BB%E5%9E%8B-i%E6%97%A0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E9%97%AD%E6%BA%90-llms-%E4%BD%9C%E4%B8%BA%E8%A7%84%E5%88%92%E5%99%A8">类型 I：无长期记忆的闭源 LLMs 作为规划器。</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B-ii%E6%97%A0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E5%BE%AE%E8%B0%83-llms-%E4%BD%9C%E4%B8%BA%E8%A7%84%E5%88%92%E5%99%A8">类型 II：无长期记忆的微调 LLMs 作为规划器。</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B-iv%E5%85%B7%E6%9C%89%E6%9C%AC%E5%9C%B0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E8%A7%84%E5%88%92%E5%99%A8">类型 IV：具有本地长期记忆的规划器。</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent1">多模态 Agent[1]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent10">多模态 Agent[10]</a><ul>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a></li>
<li><a href="#works">works</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
<li><a href="#xxx">xxx</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81agent">多模态Agent</a></li>
<li><a href="#xxx-1">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2402.15116">《Large Multimodal Agents: A Survey》</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/jun0wanan/awesome-large-multimodal-agents">Repo</a> git</p>
</li>
</ul>
<h1><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h1><h3><span id="类型-i无长期记忆的闭源-llms-作为规划器">类型 I：无长期记忆的闭源 LLMs 作为规划器。</span><a href="#类型-i无长期记忆的闭源-llms-作为规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2303.04671.pdf"><strong>Visual ChatGPT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2303.11381.pdf"><strong>MM-REACT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2303.08128.pdf"><strong>ViperGPT</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2303.17580.pdf"><strong>HuggingGPT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2304.09842.pdf"><strong>Chameleon</strong></a> ***</p>
<p><a href="https://arxiv.org/pdf/2311.00571.pdf"><strong>LLaVA-Interactive</strong></a> ***</p>
<p><a href="https://arxiv.org/pdf/2401.01614"><strong>SeeAct</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2310.01415.pdf"><strong>GPT-Driver</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2401.16158.pdf"><strong>Mobile-Agent</strong></a> </p>
<h3><span id="类型-ii无长期记忆的微调-llms-作为规划器">类型 II：无长期记忆的微调 LLMs 作为规划器。</span><a href="#类型-ii无长期记忆的微调-llms-作为规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2306.08640.pdf"><strong>LLaVA-Plus</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2305.18752.pdf"><strong>GPT4Tools</strong></a> </p>
<h3><span id="类型-iv具有本地长期记忆的规划器">类型 IV：具有本地长期记忆的规划器。</span><a href="#类型-iv具有本地长期记忆的规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2311.05997.pdf"><strong>JARV IS-1</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2312.13771.pdf"><strong>AppAgent</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2307.07162.pdf"><strong>DLAH</strong></a> </p>
<h1><span id="多模态-agent1">多模态 Agent[1]</span><a href="#多模态-agent1" class="header-anchor">#</a></h1><ul>
<li><p>核心组件</p>
<ul>
<li><strong>感知</strong>组件关注处理多模态信息</li>
<li><strong>规划器</strong>负责推理和制定计划</li>
<li><strong>行动</strong>组件执行计划</li>
<li><strong>记忆</strong>组件则涉及长期和短期记忆</li>
</ul>
</li>
<li><p>四种类型</p>
<ul>
<li>无长期记忆的闭源 LLMs 作为规划器</li>
<li>无长期记忆的微调 LLMs 作为规划器</li>
<li>具有间接长期记忆的规划器 </li>
<li>具有本地长期记忆的规划器</li>
</ul>
</li>
<li><p>多智能体协作</p>
<ul>
<li>讨论了 LMAs 如何通过协作框架共同实现共同目标。</li>
</ul>
</li>
</ul>
<h1><span id="多模态-agent10">多模态 Agent[10]</span><a href="#多模态-agent10" class="header-anchor">#</a></h1><h3><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/tasks.JPG" class>

<ul>
<li><p>MM-ReAct </p>
</li>
<li><p>HuggingGPT[21, 22] </p>
</li>
<li><p>Chameleon</p>
</li>
<li><p>Visual ChatGPT [20]</p>
</li>
</ul>
<h3><span id="works">works</span><a href="#works" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/works.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488499&idx=1&sn=ac8c5092ddc8fd724965d12aff3f9392">2024年大型多模态智能体(Large Multimodal Agents)综述：组件, 分类，协作，评估，应用，展望</a> ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/678203245">智体AI在多模态交互领域的综述（上）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678222381">智体AI在多模态交互领域的综述（下）</a></p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.bilibili.com/video/BV1mM411X7Zn/">多模态 Agents：用大模型语言模型串联多模态专家</a> V</li>
</ol>
<h3><span id="多模态agent">多模态Agent</span><a href="#多模态agent" class="header-anchor">#</a></h3><p>1xx. <a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> self<br>1xx. <a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a> self</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="20">
<li><p>《Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models》<br><a href="https://github.com/chenfei-wu/TaskMatrix">Visual ChatGPT</a> git</p>
</li>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130856470">LLMs的自动化工具系统（HuggingGPT、AutoGPT、WebGPT、WebCPM）</a>  </p>
</li>
<li><p><a href="https://github.com/microsoft/JARVIS">HuggingGPT</a> git<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb">hugginggpt in langchain</a> git<br><a href="https://github.com/camille-vanhoffelen/langchain-huggingGPT">langchain-huggingGPT</a> git</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/VXe6CHI_29Rw8xaOjfbqOQ">Visual Programming——实现通用人工智能的另一种方式 </a> 2022  best paper</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)量化</title>
    <url>/www6vHomeAIGC/2023/02/19/gptQuantization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="量化">量化</span><a href="#量化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/9e2982aada064c5895ae2f862f1d33c3?pvs=4">量化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>短文本相似度</title>
    <url>/www6vHomeAIGC/2023/02/18/gptDocSimilarity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/111414376">短文本相似度算法研究</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/113133510">Sentence-Bert论文笔记</a><br>1xx. <a href="https://www.bilibili.com/video/BV13h4y1a7z6/">SentenceBert模型：文本语义去重</a> V</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/113017752">传统方法TF-IDF解决短文本相似度问题</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/113224707">传统方法BM25解决短文本相似度问题</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>短文本相似度</category>
      </categories>
      <tags>
        <tag>短文本相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型</title>
    <url>/www6vHomeAIGC/2023/02/17/gptLargeModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="http://arthurchiao.art/blog/llm-practical-guide-zh/">[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）</a>   实战<br>1xx. <a href="https://zhuanlan.zhihu.com/p/597586623">通向AGI之路：大型语言模型（LLM）技术精要</a> *** </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403847&idx=1&sn=9af731e9f8418a2d869f5464530c8bd6">必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 </a> 12个综述</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Transformer</title>
    <url>/www6vHomeAIGC/2023/02/16/gptTransformerCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://github.com/www6v/AIGC/blob/master/transformer/transformer.ipynb">transformer.ipynb</a> git<br>   <a href="https://www.bilibili.com/video/BV1nc411y7m4/">Transformer代码实现</a></p>
<p>1xx. <a href="https://paperswithcode.com/method/transformer">Transformer</a><br>   <a href="https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201">transformer.py</a> git</p>
<p>1xx. <a href="http://arthurchiao.art/blog/transformers-from-scratch-zh/">[译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）</a> V, github<br>    Transformers from scratch</p>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/130090649">从零实现Transformer的简易版与强大版：从300多行到3000多行</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/398039366">Transformer源码详解（Pytorch版本）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>COT</title>
    <url>/www6vHomeAIGC/2023/02/08/gptCOT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="cot4">CoT[4]</span><a href="#cot4" class="header-anchor">#</a></h1><ul>
<li><p>CoT(Chain of Thought)</p>
<ul>
<li>CoT-SC(Self Consistency)</li>
</ul>
</li>
<li><p>ToT(Tree of Thoughts)<br>分为了Thought Decomposition，Thought Generator，State Evaluator，Search algorithms</p>
</li>
<li><p>GoT(Graph of Thoughts)</p>
</li>
<li><p>AoT(Algorithm of Thoughts)</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="4">
<li><a href="https://zhuanlan.zhihu.com/p/654034193">2023年能够解决复杂问题的思维链技术：Cot，ToT，GoT，AoT</a></li>
</ol>
<p>1xx. <a href="https://github.com/zchuz/CoT-Reasoning-Survey">CoT-Reasoning-Survey </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404176&idx=1&sn=2eafdf5426bfe1347869b9af268d4238">大模型COT思维链推理的几个关键问题：从评测基准、结构变体到增强方案的系统综述 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/X2lcVLFFlFgQCzacret4Vg">大模型思维链推理的综述：进展、前沿和未来</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>COT</category>
      </categories>
      <tags>
        <tag>COT</tag>
      </tags>
  </entry>
  <entry>
    <title>医疗大模型</title>
    <url>/www6vHomeAIGC/2023/02/07/gptDomainMed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="医疗大模型">医疗大模型</span><a href="#医疗大模型" class="header-anchor">#</a></h3><ul>
<li>LLaMA<ul>
<li>ChatDoctor  </li>
<li>华驼&#x2F;本草  哈工大</li>
<li>PMC-LLaMA 上海交大</li>
</ul>
</li>
<li>ChatGLM-6B<ul>
<li>ChatGLM-Med  哈工大</li>
<li>DoctorGLM</li>
<li>明医 (MING)  MedicalGPT-zh  上海交通大学</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402886&idx=1&sn=0552d60744645a84d13bb0cef57f321c">再看23个医疗领域微调大模型集合：兼看CareLlama医疗模型的一些实践经验与开放医疗数据 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402589&idx=1&sn=3ba9d50fad433adeb8dd6c623b06c42d">大模型遇上心理健康咨询：MeChat、QiaoBan、SoulChat、MindChat四大心理健康领域微调模型总结 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402638&idx=1&sn=b9329498806e2b93b2d6817a17941bff">大模型常见错误、反馈的来源及自我修正方法：兼论两个有趣的同名中医微调垂域模型 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>测评</title>
    <url>/www6vHomeAIGC/2023/02/07/gptEval/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402223&idx=1&sn=f2ec30cd04600129bb90bc9c81413d95">一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 </a><br>1xx. <a href="https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese">https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402549&idx=1&sn=07a8af1db44df6125939c5c9e90f6267">如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403046&idx=1&sn=0a9b612e9790c0bf49d5cede8fda365c">大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 </a> 二、CEVAL榜单评测中能够得到一些启示<br><a href="https://github.com/hkust-nlp/ceval/blob/main/resources/tutorial.md">1. C-Eval 数据集评测简明教程</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403295&idx=1&sn=126c949d0a00eb85b4a3a6b0106f55a6&poc_token=HApExGWjou7N5NVcTKmJpWt9LZ8ul6wynjV5VHnQ">大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 </a>   CEVAl</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>eval</category>
      </categories>
      <tags>
        <tag>eval</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT 数据组合</title>
    <url>/www6vHomeAIGC/2023/02/06/gptDatasetSFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="sft-数据组合">SFT 数据组合</span><a href="#sft-数据组合" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SFT-0f98cbb2b48d46a182a19ed0ee9fc719?pvs=4">(原理)SFT 数据组合</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)幻觉问题</title>
    <url>/www6vHomeAIGC/2023/02/06/gptHallucination/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="幻觉-vs-事实性1">幻觉 vs 事实性[1]</span><a href="#幻觉-vs-事实性1" class="header-anchor">#</a></h1><p><strong>幻觉</strong>主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于”生成与某些来源相关的无意义或不真实的内容”。这与<strong>事实性问题</strong>不同，后者强调模型学习、获取和利用事实性知识的能力。</p>
<p>举例说明两者的<strong>区别</strong>：</p>
<p>如果一个LLM在被要求创作”一个关于兔子和狼交朋友的童话故事”时，创作出了一个关于”兔子和狗交朋友”的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。<br>如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是<strong>幻觉</strong>，而<strong>不是事实性问题</strong>。<br>例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是<strong>幻觉</strong>。</p>
<p>相反，如果LLM避免给出直接答案，而是说”我不知道”，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是<strong>事实性问题</strong>，而<strong>不是幻觉</strong>。</p>
<p>此外，值得注意的是，<strong>幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的</strong>。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404394&idx=1&sn=d7cfcf2cd9aa6756d3cbff938f5f4cf2">再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403998&idx=1&sn=400cc902434bc04df508a55e192d2455">再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405983&idx=2&sn=95dc9c7a12bed99b63c775d4b90519d8">也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 </a></p>
<p>1xx. <a href="https://arxiv.org/abs/2309.01219">大模型幻觉综述</a><br>   <a href="https://arxiv.org/abs/2309.05922">大模型幻觉综述</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405791&idx=2&sn=d7dada69e6d5ab5fba1333d234b947ef">网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 </a> 大模型幻觉综述</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403602&idx=1&sn=f2365b05630094f8d0de7ff784abe233">大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介</a> 大模型微调遗忘</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403341&idx=1&sn=86cdaaf2c3a73439d2591a2f3dd0b9e0">值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642648601">大模型的幻觉问题调研: LLM Hallucination Survey</a><br>   <a href="https://mp.weixin.qq.com/s?__biz=MzU5NDg2MjgxMg==&mid=2247485189&idx=1&sn=95d6eb333dde007f262a2955b90bc7ec">人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） </a><br>   <a href="https://mp.weixin.qq.com/s/eGMwNz0F1dQsNDnsLNYr8Q">大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二）</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/N7NOsLHr8HYCMp5XGCBDjg">LLM之幻觉（一）：大语言模型幻觉解决方案综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Hallucination</category>
      </categories>
      <tags>
        <tag>Hallucination</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)数据处理</title>
    <url>/www6vHomeAIGC/2023/02/05/gptDataProcess/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-pipeline">数据处理 pipeline</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%80%9A%E7%94%A8-1">数据处理[通用] [1]</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%BF%87%E6%BB%A4">质量过滤</a></li>
<li><a href="#%E5%86%97%E4%BD%99%E5%8E%BB%E9%99%A4">冗余去除</a></li>
<li><a href="#%E8%AF%8D%E5%85%83%E5%88%87%E5%88%86">词元切分</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%862">数据处理[2]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%87%E8%AE%B0">数据标记</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">数据准备</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-1">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F">数据质量</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据处理-pipeline">数据处理 pipeline</span><a href="#数据处理-pipeline" class="header-anchor">#</a></h1><h2><span id="数据处理通用-1">数据处理[通用] [1]</span><a href="#数据处理通用-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/data_process.png" class>

<h3><span id="质量过滤">质量过滤</span><a href="#质量过滤" class="header-anchor">#</a></h3><ul>
<li>基于<strong>分类器</strong>的方法</li>
<li>基于<strong>启发 式</strong>的方法</li>
</ul>
<h3><span id="冗余去除">冗余去除</span><a href="#冗余去除" class="header-anchor">#</a></h3><p>可以在<strong>句子级</strong>、<strong>文档级</strong>和<strong>数据集级</strong>等不同粒度上去重<br>在实践中应该 共同使用这三个级别的去重</p>
<h3><span id="词元切分">词元切分</span><a href="#词元切分" class="header-anchor">#</a></h3><ul>
<li>BPE</li>
<li>WordPiece</li>
<li>Unigram 词元分析</li>
</ul>
<h2><span id="数据处理2">数据处理[2]</span><a href="#数据处理2" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/pipeline.webp" class>
<h3><span id="数据标记">数据标记</span><a href="#数据标记" class="header-anchor">#</a></h3><ul>
<li>包标签</li>
<li>半监督标签</li>
<li>主动学习</li>
<li>数据编程</li>
<li>远程监督</li>
</ul>
<h3><span id="数据准备">数据准备</span><a href="#数据准备" class="header-anchor">#</a></h3><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《大规模语言模型》 </li>
<li>《Data-centric Artificial Intelligence: A Survey》 大学<br><a href="https://zhuanlan.zhihu.com/p/620890799">Data-centric Artificial Intelligence: A Survey</a><br> <a href="https://cloud.tencent.com/developer/article/2359824">机器学习数据工程的概述</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/639207933">大模型时代下数据的重要性</a> 综述</p>
<p>1xx. <a href="https://hub.baai.ac.cn/view/28740">大模型研发核心：数据工程、自动化评估及与知识图谱的结合</a><br>   <a href="https://mp.weixin.qq.com/s/SvDnQD886E3DBtw8k9asgg">大模型研发核心：数据工程、自动化评估及与知识图谱的结合 </a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_16949707/article/details/133875958">符尧：别卷大模型训练了，来卷数据吧！【干货十足】</a> 看最后的5个结论 </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488088&idx=1&sn=f401a5a12e7b3727a15abbcff1a0ec51">合成数据(Synthetic data)微调大语言模型实战指南：背景、方案、案例、代码、评估 </a></p>
<ol start="50">
<li><a href="/www6vHomeAIGC/2023/01/06/gptInstructTuning/" title="(原理)Instruct Tuning">(原理)Instruct Tuning</a> self</li>
</ol>
<h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/420295576">哈工大｜15种NLP数据增强方法总结与对比</a></p>
<h3><span id="数据质量">数据质量</span><a href="#数据质量" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403976&idx=1&sn=694db5e2b3085b1610e8d19daa93a474">再看大模型预训数据质量如何评估：困惑度、错误L2范数和记忆化三种度量方法的效果对比分析研究</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP &amp; LLM</title>
    <url>/www6vHomeAIGC/2023/02/05/gptNLPTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="nlp-amp-llm">NLP &amp; LLM</span><a href="#nlp-amp-llm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/NLP-LLM-10dbfe21108480ae8b5cc825540816b0?pvs=4">NLP &amp; LLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Toolformer</title>
    <url>/www6vHomeAIGC/2023/02/03/gptAgentToolformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a>  </p>
</li>
<li><p>开源地址<br><a href="https://github.com/lucidrains/toolformer-pytorch">Implementation of Toolformer</a>  git</p>
</li>
</ul>
<h1><span id="toolformer1">Toolformer[1]</span><a href="#toolformer1" class="header-anchor">#</a></h1><ul>
<li>🔑关键词和摘要<ul>
<li>Keywords: Large-scale PLMs,  Tool Learning</li>
<li>xxx<ul>
<li>驱动语言模型去使用简单的模型来调用外部的工具</li>
<li>Toolformer通过语言模型的方法去决定去调用哪些API，传入哪些参数</li>
<li>Tooformer是在自监督层面执行的，只需要对每个API的语言描述</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>方法   <ul>
<li>Toolformer调用示例：xxx</li>
<li>关键要素：<ul>
<li>模型对工具的使用应该是自监督的，这样可以省去很大的标注开销</li>
<li>模型应该自行地去决定在何时间，用何方法来调用工具</li>
</ul>
</li>
<li><strong>方法概要：</strong><ul>
<li>受到in-context learning的启发，给定少量的人写的关于API的描述，让模型去自行生成潜在API调用的语言建模数据</li>
<li>构建一个自监督的Loss函数，让模型来决定哪些API的调用有助于它的语言建模的预测</li>
</ul>
</li>
<li><strong>方法细节：</strong><ul>
<li>xxx<ul>
<li>给定一个纯文本数据集，构建出一个带有API调用的数据集，然后在此数据集上做微调</li>
<li>第一步：使用in-context learning来生成大量的潜在可能的API调用</li>
<li>第二步：执行这些API，返回得到结果</li>
<li>第三步：检查返回的结果是否有助于语言模型的预测，过滤掉其他的API</li>
</ul>
</li>
<li>API调用采样<ul>
<li>给每一个API来撰写提示来鼓励模型使用这些API，例如QA的提示是 xxx</li>
<li>对于文本的每一个位置，如果这个位置是<api>（即API调用的开始）的概率大于一个阈值，则将此位置保留到一个集合I中</api></li>
<li>对于集合I中的每一个位置，通过模型生成最多m个API调用，并且以结尾（如果生成的调用没有以结尾，直接舍去）</li>
</ul>
</li>
<li>API执行<ul>
<li>去执行所有的API调用，返回文本序列</li>
</ul>
</li>
<li>API过滤<ul>
<li>构建自监督的语言模型的loss函数</li>
<li>第一个的含义：进行API的调用，并且使用API结果的Loss</li>
<li>第二个的含义：空字符串的Loss和调用API但不返回结果Loss的最小值</li>
<li>这时我们希望模型使用API并且返回结果对语言建模有帮助，且帮助很明显-&gt;前者的loss显著比后者小</li>
</ul>
</li>
<li>微调和推理<ul>
<li>在经过如上操作后，就可以得到带有API调用的数据集，然后将模型在上面进行微调</li>
<li>当模型在解码阶段输出”-&gt;”符号时，意味着需要调用API了，调用得到返回结果然后拼接上去</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实验<ul>
<li>模型：GPT-J （67亿参数）</li>
<li>原始数据：CCNet</li>
<li>知识探测任务LAMA<ul>
<li>Toolformer可以大幅超过之前的方法，甚至是GPT-3等大模型</li>
</ul>
</li>
<li>数学数据集</li>
<li>问答</li>
<li>这里即使是Toolformer也无法超越GPT-3，可见预训练规模可以囊括更多知识</li>
<li>模型规模的影响</li>
<li>模型的参数量到一定规模后才拥有使用工具的能力</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>将语言模型使用外部工具的进行很自然的结合</li>
<li><strong>不需要标注大量数据，使用自监督的方法进行学习</strong></li>
</ul>
</li>
<li>缺点<ul>
<li><strong>工具无法交互，也无法链式使用（每个API调用都是独立的）</strong></li>
<li>定义的工具尚且有限，扩展工具则需要用模型标注新的数据</li>
<li>随着基础模型zero-shot能力的增强，这种需要构建数据并且fine-tune的做法可能会比较麻烦</li>
</ul>
</li>
</ul>
</li>
<li>OpenBMB BMTools: <a href="https://github.com/OpenBMB/BMTools">https://github.com/OpenBMB/BMTools</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18s4y1u7nJ/">清华博士带你搞懂大模型自学工具使用（Toolformer)【论文速读】</a> V 有思维导图<br>1xx. <a href="https://finisky.github.io/toolformer-summary/">使LLM善假于物: Toolformer </a><br>1xx. <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis">Prompt Engineering </a><br>1xx. <a href="https://nakaizura.blog.csdn.net/article/details/130817902">Toolformer and Tool Learning（LLMs如何使用工具）</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)涌现现象</title>
    <url>/www6vHomeAIGC/2023/02/03/gptEmergent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="emergent-abilities">Emergent Abilities</span><a href="#emergent-abilities" class="header-anchor">#</a></h1><ul>
<li>🔗 文章：Emergent Abilities of Large Language Models  (2022.10)  (arxiv.org)</li>
<li>🔑关键词和摘要<ul>
<li>Keywords: LLMs, Emergent Ability, Scaling</li>
<li>abstract<ul>
<li>不可预测</li>
<li>不能从小模型的的性能外推</li>
<li>是否能通过继续扩大模型规模来获得更多涌现能力</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>定义<ul>
<li>通常的涌现现象</li>
<li>大模型的涌现现象<ul>
<li>小模型接近随机</li>
<li><strong>大模型突然出现</strong></li>
<li>相变</li>
</ul>
</li>
<li>实验框架<ul>
<li>performance vs 1. FLOPs, model parameters</li>
<li><input checked disabled type="checkbox"> Training datasets</li>
<li>叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。</li>
</ul>
</li>
<li>实验1<ul>
<li>Few-shot Prompting</li>
<li>测试数据说明:<ul>
<li>A: 三位数加法，两位数乘法</li>
<li>B: [dɪfərənt], 复原 “different,” </li>
<li>C: 从 e l h l o 复原 hello</li>
<li>D: 波斯语问答</li>
<li>E: 针对GPT-3 对抗标的问答</li>
<li>…</li>
</ul>
</li>
<li>结果<ul>
<li>这些 task，以 few-shot 形式展示过以后，都有 emergent</li>
<li>不同模型 emergent scale 不一样</li>
<li>有的 task，只有 540B 的 PaLM  emerge了</li>
</ul>
</li>
</ul>
</li>
<li>实验2<ul>
<li>增强语言模型能力的 emerge 现象</li>
<li>已知的一些大模型技巧在何种规模下发挥作用？<ul>
<li>大模型技巧<ul>
<li>思维链 Chain-of-thought: Let’s think step by step.</li>
<li>指令微调 请写一段XXX的描述</li>
<li>草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6&#x3D;11，进位1”</li>
</ul>
</li>
</ul>
</li>
<li>这些增强语言模型能力的方法都有一定程度的涌现</li>
<li>联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？</li>
</ul>
</li>
</ul>
</li>
<li>讨论<ul>
<li><strong>Emergent 现象的解释</strong><ul>
<li><strong>多步能力说</strong><ul>
<li>每个子能力达到 90%  -&gt; 一无是处</li>
<li>每个子能力达到 95% -&gt; 能完成一些任务了</li>
</ul>
</li>
<li>指标缺陷说</li>
<li>奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降</li>
</ul>
</li>
<li><strong>Emergent 的阈值可能会越来越小</strong><ul>
<li>更干净的数据，更好的训练技巧，更优秀的模型结构都可以是  Emergent阈值变小</li>
</ul>
</li>
<li>未来方向：<ul>
<li>继续扩大 model scale，远未达到上限</li>
<li>一些新结构的 scaling</li>
<li>数据的 scaling</li>
<li>理解 prompt 机制</li>
<li>更前沿的 task，用来指导 emergent</li>
<li>理解 emergence</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>第一次正式提出 emergent 实验</li>
<li><strong>做了充分的实验表明该现象在各种数据集上广泛存在</strong></li>
<li>甚至验证了一些“方法”的涌现</li>
<li>提出了一些解释该现象的观点，并提出质疑</li>
</ul>
</li>
<li>改进点<ul>
<li><strong>还是不知道为啥 emerge</strong></li>
<li>实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1qX4y1i78J/">清华博士带你思考大语言模型LLM的涌现现象（Emergent）</a>  有脑图<br> Emergent Abilities of Large Language Models （<a href="https://arxiv.org/abs/2206.07682%EF%BC%89">https://arxiv.org/abs/2206.07682）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399147&idx=1&sn=6e6d416db50d9708c900ee3b5416bba3">再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Emergent</category>
      </categories>
      <tags>
        <tag>Emergent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)继续Pre-Training</title>
    <url>/www6vHomeAIGC/2023/02/03/gptContinualPretraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="继续-预训练-continual-pre-training-1">继续-预训练 continual pre-training [1]</span><a href="#继续-预训练-continual-pre-training-1" class="header-anchor">#</a></h1><ul>
<li><p>继续预训练的目的<br>为了得到<strong>适应不同行业&#x2F;任务领域</strong>的预训练模型，<strong>提升下游任务的效果</strong></p>
</li>
<li><p>什么时候需要继续预训练？<br><strong>预训练(pre-train)的语料与下游任务(finetune)语料的【数据分布&#x2F;领域差异】大时</strong></p>
</li>
</ul>
<h1><span id="千帆llama-2中文增强技术介绍-postpretrain2">千帆Llama 2中文增强技术介绍-Postpretrain[2]</span><a href="#千帆llama-2中文增强技术介绍-postpretrain2" class="header-anchor">#</a></h1><ul>
<li><p>中文词表构建 +Tokenizer<br>中文词表扩增 29k -&gt; 59k</p>
</li>
<li><p>Embedding<br>在原有Embedding矩阵后追加中文embedding映射</p>
</li>
<li><p>数据配比<br> 中文：英文约1:1</p>
</li>
<li><p>pipeline</p>
<ul>
<li>原始数据集</li>
<li><strong>异常清洗</strong></li>
<li><strong>数据过滤</strong></li>
<li><strong>去重</strong></li>
<li>隐私匿名化</li>
</ul>
</li>
</ul>
<blockquote>
<p>开源大模型预训练语料预处理流程总结： 基于基础规则处理为主 + 基于模型的质量过滤逐步成为趋势</p>
</blockquote>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/545092184">浅谈一下「继续预训练」</a></li>
<li>&lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;<br>1xx. <a href="https://zhuanlan.zhihu.com/p/654463331">如何更好地继续预训练（Continue PreTraining）</a><br>warmup  +  学习率<br>1xx. <a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/120695507">Don’t stop pretraining，继续预训练！</a></li>
</ol>
<p>1xx. 《Investigating Continual Pretraining in Large Language Models: Insights and Implications》<br>    <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583&chksm=83839096b4f41980e8277f34650c2029a45e853adfc2b412ea386952751e44d29e75f0048d12&scene=178&cur_album_id=3343133676745932807#rd">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理-lmdeploy</title>
    <url>/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lmdeploy-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2-10">lmdeploy-推理部署 [10]</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">模型转换</a></li>
<li><a href="#turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">TurboMind 推理+命令行本地对话</a></li>
<li><a href="#turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TurboMind推理+API服务</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="lmdeploy-推理部署-10">lmdeploy-推理部署 [10]</span><a href="#lmdeploy-推理部署-10" class="header-anchor">#</a></h1><h3><span id="模型转换">模型转换</span><a href="#模型转换" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/convert.png" class>

<h3><span id="turbomind-推理命令行本地对话">TurboMind 推理+命令行本地对话</span><a href="#turbomind-推理命令行本地对话" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer.png" class>

<h3><span id="turbomind推理api服务">TurboMind推理+API服务</span><a href="#turbomind推理api服务" class="header-anchor">#</a></h3><ul>
<li>启动服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api.png" class></li>
<li>Client访问服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api-client.png" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="10">
<li><a href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md">lmdeploy 量化部署</a><br>  <a href="https://www.bilibili.com/video/BV1iW4y1A77P/">(5)LMDeploy 大模型量化部署实践</a> V</li>
</ol>
<p>1xx. <a href="https://github.com/www6v/llm-action/tree/main/inference">llm-action  inference</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent-Tools</title>
    <url>/www6vHomeAIGC/2023/01/27/gptAgentTool/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E5%88%86%E7%B1%BB1">分类[1]</a><ul>
<li><a href="#tool-augmented-vs-tool-oriented-kimi-%E6%80%BB%E7%BB%93">Tool-augmented vs. Tool-oriented [kimi 总结]</a></li>
<li><a href="#tool-augmented-learning">Tool-augmented Learning</a></li>
<li><a href="#tool-oriented-learning">Tool-oriented Learning</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><a href="https://arxiv.org/pdf/2304.08354.pdf">Tool Learning with Foundation Models</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/thunlp/ToolLearningPapers">ToolLearningPapers</a> git</p>
</li>
</ul>
<h1><span id="分类1">分类[1]</span><a href="#分类1" class="header-anchor">#</a></h1><h3><span id="tool-augmented-vs-tool-oriented-kimi-总结">Tool-augmented vs. Tool-oriented [kimi 总结]</span><a href="#tool-augmented-vs-tool-oriented-kimi-总结" class="header-anchor">#</a></h3><ol>
<li><p>Tool-augmented Learning（工具增强学习）:</p>
<ul>
<li>这种学习方式指的是在基础模型（如大型预训练语言模型）的基础上，<strong>通过引入外部工具来增强模型的能力</strong>。这些工具可以是任何可以被模型通过某种接口调用的系统或服务，例如搜索引擎、数据库、API等。</li>
<li>工具增强学习的核心在于模型利用这些工具来获取额外的信息或执行特定的任务，从而弥补模型自身知识和能力的不足。</li>
<li>例如，<strong>一个语言模型可能通过调用天气API来获取最新的天气信息，或者通过搜索引擎来找到相关问题的答案</strong>。</li>
</ul>
</li>
<li><p>Tool-oriented Learning（面向工具的学习）:</p>
<ul>
<li>面向工具的学习则更多地关注于模型如何学习和理解如何使用这些工具。这不仅仅是调用工具API那么简单，而是<strong>涉及到模型对工具的深入理解和策略性使用</strong>。</li>
<li>在这种学习模式下，模型可能需要<strong>学习如何组合使用多个工具</strong>，或者在复杂任务中动态调整对工具的使用策略，以实现更高效的问题解决。</li>
<li>例如，模型可能需要学习如何在<strong>规划一次旅行</strong>时，先后调用地图API、航班搜索API和酒店预订API，同时还要根据用户反馈和环境变化动态调整计划。</li>
</ul>
</li>
</ol>
<p>总的来说，Tool-augmented Learning 强调的是通过外部工具来扩展模型的能力，而 Tool-oriented Learning 则更侧重于模型对工具使用的学习和优化。两者都是工具学习（Tool Learning）的重要组成部分，但在实际应用中可能会有不同的实现方式和关注点。</p>
<h3><span id="tool-augmented-learning">Tool-augmented Learning</span><a href="#tool-augmented-learning" class="header-anchor">#</a></h3><ul>
<li>Toolformer   <a href="/www6vHomeAIGC/2023/02/03/gptAgentToolformer/" title="(原理)Toolformer">(原理)Toolformer</a></li>
</ul>
<h3><span id="tool-oriented-learning">Tool-oriented Learning</span><a href="#tool-oriented-learning" class="header-anchor">#</a></h3><ul>
<li>ToolMaker[10]</li>
<li>CREATOR[11]</li>
<li>ToolLLM [12]</li>
<li>Visual ChatGPT[13]</li>
<li>HuggingGPT[13]</li>
<li>Gorilla <a href="/www6vHomeAIGC/2023/04/08/gptAgentToolGorilla/" title="(原理)Gorilla">(原理)Gorilla</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/624459759">大模型工具学习权威综述，BMTools 背后的论文！</a></li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/xixiaoyaoww/article/details/130278978">清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/yZYGqAKIqDfGYF2YUckiiw">回顾大模型在工具使用上的技术总结：兼看图检索增强生成方案-GRAG </a><br>   《Tool Learning with Large Language Models: A Survey》<br>   问题2:关于大模型使用工具的调研整理</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/pPkrHHkmVC29e_c2U8YEGg">一篇大模型Agent工具使用全面研究综述</a><br>    《Tool Learning with Large Language Models: A Survey》</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://zhuanlan.zhihu.com/p/633654195">LLM能够自己制作工具了：详解Large Language Models as Tool Makers</a>  </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1EN4y1q7Zn/">THUNLP成员领读EMNLP大模型工具创造新框架“CREATOR”</a> V 有思维导图 </p>
</li>
<li><p>《TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS》<br><a href="https://zhuanlan.zhihu.com/p/647899563">TOOLLLM：让大型语言模型掌握真实世界的API</a><br><a href="https://github.com/OpenBMB/ToolBench">ToolBench </a> git<br><a href="https://blog.csdn.net/Dbox_boom/article/details/134815624">论文阅读：ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a></p>
</li>
<li><a href="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/" title="(原理)Agent 多模态">(原理)Agent 多模态</a> self</li>
</ol>
<h3><span id="others">Others</span><a href="#others" class="header-anchor">#</a></h3><p>《Augmented Language Models》<br>1xx. <a href="https://blog.csdn.net/qq_39388410/article/details/130798125">Augmented Language Models（增强语言模型）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/611492200">增强语言模型（ALM）之综述篇</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PromptTuning</title>
    <url>/www6vHomeAIGC/2023/01/25/gptPromptTuningPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/646748939">大模型参数高效微调技术实战（二）-Prompt Tuning</a><br><a href="https://zhuanlan.zhihu.com/p/635686756">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a><br><a href="https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb">peft_prompt_tuning_clm.ipynb</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)不可能三角</title>
    <url>/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%921">不可能三角[1]</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">不可能三角</a></li>
<li><a href="#%E5%BC%A5%E8%A1%A5%E6%96%B9%E6%B3%95">弥补方法</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96-%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">其他 不可能三角</a><ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F">分布式系统</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8">分布式存储</a></li>
</ul>
</li>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a><ul>
<li><a href="#pretrain-finetune-%E8%8C%83%E5%BC%8F3">pretrain, finetune 范式[3]</a></li>
<li><a href="#pretrain-prompt-predict-%E8%8C%83%E5%BC%8F3">pretrain, prompt, predict 范式[3]</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#scaling-law10">Scaling Law[10]</a><ul>
<li><a href="#scaling-law">Scaling Law</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F">参数量 vs 数据量</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E6%95%B0%E6%8D%AE%E9%87%8F-1">参数量 vs 数据量</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92-1">不可能三角</a></li>
<li><a href="#scaling-law-1">Scaling Law</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="不可能三角1">不可能三角[1]</span><a href="#不可能三角1" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/impossibleTriangle.JPG" class>

<ul>
<li>预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果</li>
<li>而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的<strong>少样本效果依旧比不过中等模型的精调</strong></li>
</ul>
<h3><span id="弥补方法">弥补方法</span><a href="#弥补方法" class="header-anchor">#</a></h3><ul>
<li><strong>优化size</strong><ul>
<li>对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低</li>
</ul>
</li>
<li><strong>优化few-shot</strong><ul>
<li>对于提升少样本表现，<strong>数据增强</strong>是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限</li>
</ul>
</li>
<li><strong>fine-tuning</strong><ul>
<li>对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA</li>
</ul>
</li>
</ul>
<h1><span id="其他-不可能三角">其他 不可能三角</span><a href="#其他-不可能三角" class="header-anchor">#</a></h1><h3><span id="分布式系统">分布式系统</span><a href="#分布式系统" class="header-anchor">#</a></h3><ul>
<li>CAP理论<ul>
<li>C 一致性</li>
<li>A 可用性</li>
<li>P 分区</li>
</ul>
</li>
</ul>
<h3><span id="分布式存储">分布式存储</span><a href="#分布式存储" class="header-anchor">#</a></h3><ul>
<li>RUM猜想<ul>
<li>Read-overhead </li>
<li>Update-overhead </li>
<li>Memory-overhead</li>
</ul>
</li>
</ul>
<h1><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h1><h3><span id="pretrain-finetune-范式3">pretrain, finetune 范式[3]</span><a href="#pretrain-finetune-范式3" class="header-anchor">#</a></h3><p>第三阶段范式</p>
<h3><span id="pretrain-prompt-predict-范式3">pretrain, prompt, predict 范式[3]</span><a href="#pretrain-prompt-predict-范式3" class="header-anchor">#</a></h3><p>第四阶段范式</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响</p>
<h1><span id="scaling-law10">Scaling Law[10]</span><a href="#scaling-law10" class="header-anchor">#</a></h1><h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/scalingLaw.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/paramVSdataSize.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/computeVSDatasize.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/501381510">预训练模型的下一步？突破Impossible Triangle</a></li>
<li><a href="https://arxiv.org/pdf/2204.06130.pdf">Impossible Triangle: What’s Next for Pre-trained Language Models?</a></li>
<li><a href="https://blog.csdn.net/zandaoguang/article/details/124395479">微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」</a></li>
<li><a href="/www6vHomeAIGC/2023/01/06/gptPromptTuning/" title="(原理)Prompt Tuning">(原理)Prompt Tuning</a> self</li>
</ol>
<h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/667489780">解析大模型中的Scaling Law</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663296750">论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models</a><br>2xx. <a href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/">Training Compute-Optimal Large Language Models 简读 </a></li>
</ol>
<p>2xx. <a href="https://zhuanlan.zhihu.com/p/536053110">【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！</a><br>《Scaling Laws for Neural Language Models》<br>《Training Compute-Optimal Large Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Multi-Agents</title>
    <url>/www6vHomeAIGC/2023/01/21/gptMultiAgents/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="multi-agents-原理">Multi-Agents 原理</span><a href="#multi-agents-原理" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Multi-Agents-10dbfe211084801f882dd0fe42d93eef?pvs=4">(原理)Multi-Agents</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agents</category>
      </categories>
      <tags>
        <tag>Agents</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 论文</title>
    <url>/www6vHomeAIGC/2023/01/20/gptStudyPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#paper">Paper</a></li>
<li><a href="#gpt%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%911">GPT研究方向[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="paper">Paper</span><a href="#paper" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/www6v/paper-reading">paper-reading</a> 李牧大神</p>
<ul>
<li>Transformer  *** <ul>
<li>GPT-4</li>
<li>Instruct GPT *** </li>
<li>GPT, GPT-2, GPT-3 精读  ***</li>
</ul>
</li>
<li>多模态<ul>
<li>CLIP</li>
<li>ViLT</li>
</ul>
</li>
<li>Chain of Thought  ***</li>
</ul>
</li>
<li><p><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a><br>基础篇 - 论文 *** </p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/129508065">LLM&#x2F;ChatGPT与多模态必读论文150篇(已更至第101篇)</a> </p>
</li>
<li><p><a href="https://github.com/zjunlp/LLMAgentPapers">LLMAgentPapers</a> 浙江大学</p>
</li>
<li><p><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a> 浙江大学</p>
</li>
</ul>
<h1><span id="gpt研究方向1">GPT研究方向[1]</span><a href="#gpt研究方向1" class="header-anchor">#</a></h1><ul>
<li>Efficient (PEFT)</li>
<li>Existing stuff(pretrained model)  -应用<br>New directions</li>
<li>Plug-and-play<br> 通用模块组件，能用在各个领域， baseline</li>
<li>Dataset,  evaluation and survey</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1oX4y1d7X6">大模型时代下做科研的四个思路【论文精读·52】</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态</title>
    <url>/www6vHomeAIGC/2023/01/18/gptMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87foundational-models-defining">论文[Foundational Models Defining]</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB-1">基础模型分类 [1]</a><ul>
<li><a href="#textually-prompted-models">textually prompted models</a></li>
<li><a href="#visually-prompted-models">visually prompted models</a></li>
<li><a href="#heterogeneous-models">heterogeneous models</a></li>
</ul>
</li>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87mm-llms">论文[MM-LLMs]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87mllm">论文[MLLM]</a></li>
<li><a href="#arch-32">Arch [3.2]</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B31">类型[3.1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文foundational-models-defining">论文[Foundational Models Defining]</span><a href="#论文foundational-models-defining" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》大学</li>
</ul>
<h1><span id="基础模型分类-1">基础模型分类 [1]</span><a href="#基础模型分类-1" class="header-anchor">#</a></h1><ul>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern.webp" class>
</li>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern1.webp" class></li>
</ul>
<h3><span id="textually-prompted-models">textually prompted models</span><a href="#textually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>contrastive<br>CLIP  双塔</li>
<li>generative<br>Flamingo </li>
<li>hybrid<br>BLIP</li>
<li>conversational<br>GPT-4， miniGPT4, LLaVa</li>
</ul>
<p>传统上，视觉语言模型主要用于需要同时理解视觉和文本模态的任务。然而，随着CLIP展示出的卓越性能，基于<strong>语言监督的模型</strong>在显著上升，并成为主流方法。在本节中，我们专注于探索依赖<strong>语言作为主要监督来源</strong>的方法。这些以文本为提示的模型可以广泛分为三种主要类型：对比、生成和混合方法。</p>
<h3><span id="visually-prompted-models">visually prompted models</span><a href="#visually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>Foundational<br>SAM</li>
</ul>
<h3><span id="heterogeneous-models">heterogeneous  models</span><a href="#heterogeneous-models" class="header-anchor">#</a></h3><h1><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch.webp" class>

<hr>
<h1><span id="论文mm-llms">论文[MM-LLMs]</span><a href="#论文mm-llms" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《MM-LLMs: Recent Advances in MultiModal Large Language Models》  腾讯</p>
</li>
<li><p>开源地址<br><a href="https://mm-llms.github.io/archives/">mm-llms</a> 腾讯</p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/MM-LLMs-Recent-Advances-in-MultiModal-Large-Language-Models-7ee5033df80e4b5394153c6a77cc21a3?pvs=4">解析</a></p>
</li>
</ul>
<hr>
<h1><span id="论文mllm">论文[MLLM]</span><a href="#论文mllm" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/pdf/2306.13549v1">A Survey on Multimodal Large Language Models</a><br> <a href="https://arxiv.org/abs/2306.13549">A Survey on Multimodal Large Language Models</a> 中国科学技术大学   腾讯</p>
</li>
<li><p>开源地址<br><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Repo</a></p>
</li>
</ul>
<h1><span id="arch-32">Arch [3.2]</span><a href="#arch-32" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch2.png" class>

<h1><span id="类型31">类型[3.1]</span><a href="#类型31" class="header-anchor">#</a></h1><ul>
<li>本文将最近具有代表性的MLLM分为4种主要类型：<ul>
<li><strong>多模态指令调整（MIT）</strong></li>
<li>多模态上下文学习（M-ICL）</li>
<li>多模态思想链（M-CoT）</li>
<li><strong>LLM辅助视觉推理（LAVR）</strong>【类似agent】</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol>
<li><p>《Foundational Models Defining a New Era in Vision: A Survey and Outlook》<br> <a href="https://blog.csdn.net/qq_45368632/article/details/132180645">视觉大模型的全面解析</a><br> <a href="https://zhuanlan.zhihu.com/p/655135848">基础模型定义视觉的新时代：综述和展望</a><br> <a href="https://zhuanlan.zhihu.com/p/648578542">万字长文带你全面解读视觉大模型</a></p>
</li>
<li><p>xxx</p>
</li>
<li><p>《A Survey on Multimodal Large Language Models》  v1 v2版本<br>3.1 <a href="https://cloud.tencent.com/developer/article/2322835">MLLM首篇综述 | 一文全览多模态大模型的前世、今生和未来</a>  v1版本<br>3.2 <a href="https://mp.weixin.qq.com/s/V5aiWUYh14q00jAn2O6VKA">多模态大语言模型全面综述：架构，训练，数据，评估，扩展，应用，挑战，机遇</a>  v2版本</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Pre-Training</title>
    <url>/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="pre-training-实战">Pre-Training 实战</span><a href="#pre-training-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Pre-Training-4284cd3148a0430185a9ef14041afc56?pvs=4">(实战)Pre-Training</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Pre-Training</category>
      </categories>
      <tags>
        <tag>Pre-Training</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain  Agent</title>
    <url>/www6vHomeAIGC/2023/01/11/gptLangchainAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="langchain-agent">Langchain Agent</span><a href="#langchain-agent" class="header-anchor">#</a></h1><ul>
<li>Conversational</li>
<li>OpenAI assistants</li>
<li>OpenAI functions</li>
<li>OpenAI Multi Functions Agent</li>
<li>OpenAI tools<br>OpenAI parallel function calling (a.k.a. tool calling)</li>
<li>ReAct<br>ZeroShotReactAgent</li>
<li>Self-ask with search</li>
<li>Structured tool chat</li>
</ul>
<h1><span id="langchain-apps">Langchain Apps</span><a href="#langchain-apps" class="header-anchor">#</a></h1><h3><span id="rag-chroma-private-2">rag-chroma-private [2]</span><a href="#rag-chroma-private-2" class="header-anchor">#</a></h3><p><strong>本地 部署</strong><br>This template performs RAG with no reliance on external APIs.<br>It utilizes <strong>Ollama the LLM, GPT4All for embeddings, and Chroma for the vectorstore</strong>.</p>
<h3><span id="research-assistant-34">research-assistant [3][4]</span><a href="#research-assistant-34" class="header-anchor">#</a></h3><p>This template implements a version of<br>“GPT Researcher” that you can use as a starting point for a <strong>research agent</strong>.</p>
<h1><span id="langgraph5">LangGraph[5]</span><a href="#langgraph5" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://github.com/www6v/langchain-app">Langchain Apps</a> Project Code</li>
<li><a href="https://www.bilibili.com/video/BV1JV411F7Yj/">LangChain Agents 保姆级教程 | 动画演示 讲清 核心模块 Agents | Code 讲解 | Demo 演示</a></li>
<li><a href="https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/">“Research Assistant”: Exploring UXs Besides Chat</a></li>
<li><a href="https://www.youtube.com/watch?v=DjuXACWYkkU">Building a Research Assistant from Scratch</a> </li>
<li><a href="https://blog.langchain.dev/langgraph/">LangGraph</a></li>
<li><a href="https://github.com/www6v/gpt-researcher/">gpt-researcher</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(list)数据集</title>
    <url>/www6vHomeAIGC/2023/01/08/gptDataSet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dataset">DataSet</span><a href="#dataset" class="header-anchor">#</a></h1><ul>
<li><p>综合[平台] </p>
<ul>
<li><a href="http://opendatalab.com/">OpenDataLab</a> [1]<br>上海人工智能实验室<br><strong>数据描述语言  DSDL</strong> + 平台标准数据集</li>
<li><a href="https://www.luge.ai/#/">千言数据集</a><br>百度</li>
</ul>
</li>
<li><p>评测数据集</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405040&idx=1&sn=ad45944e78b5742337158cff80dbd9b3">再看领域微调大模型的主流基座和评测数据集：项目地址及论文指引</a></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18m4y1h7zW/">大模型时代的数据变革 - 如何设计大模型的数据配方、智能数据采集、标注、ETL</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatGLM</title>
    <url>/www6vHomeAIGC/2023/01/06/gptChatGLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://www.bilibili.com/video/BV1ju411T74Y/">第十一课：ChatGLM</a> V<br><a href="https://blog.csdn.net/v_JULY_v/article/details/129880836">ChatGLM两代的部署&#x2F;微调&#x2F;实现：从基座GLM、ChatGLM的LoRA&#x2F;P-Tuning微调、6B源码解读到ChatGLM2的微调与实现</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/625468667">【Instruction Tuning】ChatGLM 微调实战（附源码）</a></p>
<p><a href="https://github.com/www6v/transformers_tasks/blob/main/LLM/chatglm_finetune/readme.md">Finetune ChatGLM-6B</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401516&idx=1&sn=80b3cfecc9f4338b87fcd9bc91ef2465">也看支持32K上下文的ChatGLM2-6B模型：优化点简读及现有开源模型主流训练优化点概述 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ChatGLM</category>
      </categories>
      <tags>
        <tag>ChatGLM</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Instruct Tuning</title>
    <url>/www6vHomeAIGC/2023/01/06/gptInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#in-context-learning-icl-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0">In Context Learning ( ICL ) 上下文学习</a></li>
<li><a href="#instruction-learning-1">Instruction Learning [1]</a><ul>
<li><a href="#instruct-tuning-">Instruct Tuning-</a></li>
<li><a href="#instructgpt">instructGPT</a></li>
<li><a href="#chatgpt">chatGPT</a></li>
</ul>
</li>
<li><a href="#instruction-tuning">Instruction Tuning</a></li>
<li><a href="#limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="in-context-learning-icl-上下文学习">In Context Learning ( ICL ) 上下文学习</span><a href="#in-context-learning-icl-上下文学习" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/ICL.webp" class>

<ul>
<li><strong>in context learning</strong>，大意是在<strong>prompt learning的基础上，将少量有标签样本融入prompt</strong>。</li>
<li>上图的ICL模型可以理解成<strong>有监督、无训练</strong>的<strong>小样本学习</strong>。</li>
<li>但<strong>并非所有ICL都不训练</strong>。比如下图右上角的<strong>FLAN</strong>就是用instruction tuning<strong>训练参数</strong>的。</li>
</ul>
<img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/ICL-tech.webp" class>
<ul>
<li><strong>FLAN</strong>，<strong>既属于 in context learning，也属于 instruction learning</strong></li>
</ul>
<h1><span id="instruction-learning-1">Instruction Learning [1]</span><a href="#instruction-learning-1" class="header-anchor">#</a></h1><h3><span id="instruct-tuning-">Instruct Tuning-</span><a href="#instruct-tuning-" class="header-anchor">#</a></h3><pre><code>FLANv1, FLANv2
</code></pre>
<h3><span id="instructgpt">instructGPT</span><a href="#instructgpt" class="header-anchor">#</a></h3><h3><span id="chatgpt">chatGPT</span><a href="#chatgpt" class="header-anchor">#</a></h3><h1><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/instructTuning.webp" class>

<ul>
<li><p>对于已有的预训练模型，继续在多项任务（B、C、D等）上做训练，在其他任务（A）上做预测。<strong>虽然依然没见过任务A，但是根据对B、C、D等的训练，对A的效果有所提升；</strong> [1]</p>
</li>
<li><p><strong>Instruct Tuning 本质上也是Prompt Tuning</strong> [2]</p>
</li>
<li><p>研究了缩放对指令微调的影响 [3]<br>  与微调指令的任务数量有关，<strong>任务数量越多效果越好</strong><br>  与模型的大小有关，<strong>模型越大效果越好</strong></p>
</li>
<li><p>Prompt vs. Instruction Tuning  [4]<br>  Prompt是去激发语言模型的<strong>补全能力</strong>，比如给出上半句生成下半句、或者做完形填空，都还是像在做language model任务.<br>  而Instruction Tuning则是激发语言模型的<strong>理解能力</strong>，通过给出更明显的指令&#x2F;指示，让模型去理解并做出正确的action<br>  <strong>Prompt tuning</strong>都是针对<strong>一个任务</strong>的，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务，而经过<strong>Instruction Tuning多任务</strong>精调后，可以用于其他任务的zero-shot</p>
</li>
<li><p>Instruction Tuning 指令微调  [4]</p>
<ul>
<li>Self Instruction<ul>
<li>Alpaca &#x3D; LLaMA + Intruction Tuning [2]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="limitation-of-instruction-finetuning-2">Limitation of instruction finetuning [2]</span><a href="#limitation-of-instruction-finetuning-2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptInstructTuning/limitation.JPG" class>
<p>问题1.  开放性问题<br>问题2.  看图</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/619406727">各种tuning的简单逻辑解释</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1cm4y1e7Cc/">第九课：Instruct Tuning</a> *** V</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/646136859">FLANv2：大模型指令微调必看论文</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/408166011">Instruction Tuning｜谷歌Quoc V.Le团队提出又一精调范式</a></p>
</li>
</ol>
<p>1xx. <a href="https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2">June 2023, A Stage Review of Instruction Tuning</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/629461665">【LLM系列之FLAN-T5&#x2F;PaLM】Scaling Instruction-Finetuned Language Models</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/597036814">如何优化大模型的In-Context Learning效果？</a></p>
<p>1xx. <a href="https://nakaizura.blog.csdn.net/article/details/128265846">Instruction Tuning（FLAN、instructGPT、chatGPT）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Instruct-Tuning</category>
      </categories>
      <tags>
        <tag>Instruct-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Prompt Tuning</title>
    <url>/www6vHomeAIGC/2023/01/06/gptPromptTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="npl范式-1">NPL范式 [1]</span><a href="#npl范式-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptPromptTuning/npl4Paragiam.jpg" class title="4种范式">


<h1><span id="prompt-tuning-2">Prompt Tuning [2]</span><a href="#prompt-tuning-2" class="header-anchor">#</a></h1><ul>
<li>🔔 Prompt Tuning<ul>
<li>🔗 文章：The Power of Scale for Parameter-Efficient Prompt Tuning (EMNLP 2021) <a href="https://aclanthology.org/2021.emnlp-main.243/">https://aclanthology.org/2021.emnlp-main.243/</a></li>
<li>🔑关键词和摘要<ul>
<li>Keywords: Large-scale PLMs, Parameter-efficient Tuning, Prompt Tuning</li>
<li>摘要<ul>
<li>Prompt变成可学习的向量，固定PLM，微调Prompt来适配下游任务</li>
<li>PLM参数规模越大，Prompt Tuning的性能和全参数微调越接近</li>
<li>这种基于<strong>Soft Prompt</strong>的Prompt Tuning方法可以看作是<strong>Prefix Tuning的简化版本</strong>（只加在输入上）</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>方法   <ul>
<li>模型示意图：xxx</li>
<li>模型基本思路：<ul>
<li>经典分类：P(Y | X; θ)<ul>
<li>Hard Prompt: P(Y | [P;X] ; θ)<ul>
<li>Soft Prompt: P(Y | [P;X] ; θ; Δ)</li>
</ul>
</li>
</ul>
</li>
<li>Pre-Training<ul>
<li>Fine-Tuning<ul>
<li>Prompt Tuning</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实现细节：<ul>
<li>模型参数量<ul>
<li>参数量：T5 ~ T5-XXL(10B)</li>
<li>预训练：LM Adaptation</li>
</ul>
</li>
<li>Prompt长度：xxx<ul>
<li>1、5、20、100、150</li>
</ul>
</li>
<li>初始化方法：xxx<ul>
<li>随机初始化</li>
<li>使用预设文本的词向量初始化，类似于设计hard prompt，然后将hard prompt转化为soft prompt</li>
<li>使用类别词向量初始化，类似于提供选项</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>实验<ul>
<li>数据集：SuperGLUE</li>
<li>xxx<ul>
<li>Prompt的规模越大，性能相对而言会越好</li>
</ul>
</li>
<li>xxx<ul>
<li>基于语义信息的初始化比随机初始化要好</li>
</ul>
</li>
<li>xxx<ul>
<li>LM Adaptation 对性能提升显著</li>
<li>Prompt Tuning还是需要大模型有较好的文本生成能力</li>
</ul>
</li>
<li>xxx<ul>
<li>模型参数规模越大，Prompt Tuning效果越好</li>
<li>10B参数时与全参数微调性能接近</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点（计算友好）<ul>
<li>大模型的<strong>微调新范式</strong></li>
<li><strong>一个中心模型服务多个下游任务</strong>，<strong>节省参数存储量</strong></li>
<li><strong>无需优化模型参数</strong>，节省优化器的计算量和存储量</li>
<li><strong>只在输入层进行操作</strong>，适合多任务场景下的计算合并</li>
</ul>
</li>
<li>缺点（性能和收敛性存在问题）<ul>
<li>Prompt Tuning的<strong>收敛速度很慢</strong></li>
<li>Prompt Tuning的模型<strong>性能不稳定</strong></li>
<li>Few-shot场景上表现不佳</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="prompt-tuning3">Prompt Tuning[3]</span><a href="#prompt-tuning3" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/06/gptPromptTuning/promptTuning.JPG" class>

<ul>
<li>Allow an <strong>additional k tunable tokens</strong> per downstream task to <strong>be prepended to the input text</strong></li>
<li>No intermediate-layer prefixes or task-specific output layers</li>
<li><strong>Freeze the entire pre-trained model</strong> and <strong>only optimize the embedding layer</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/396098543">[综述]鹏飞大神的Pre-train, Prompt, and Predict [1]</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV18P411E7VK/">清华博后带你轻松吃透Prompt Tuning顶会大模型论文</a> V</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Wg4y1K77R/">第七课：Prompt Tuning</a> ***  V  有ppt</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/395115779">近代自然语言处理技术发展的“第四范式”</a>  Prompt Learning</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/396971490">Prompt范式的缘起｜Pattern-Exploiting Training</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/400790006">Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning</a></p>
<h3><span id="p-tuning-v2">P-tuning v2</span><a href="#p-tuning-v2" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/423306405">清华P-tuning v2、谷歌SPoT｜Prompt可以超过精调了吗？</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Prompt-Tuning</category>
      </categories>
      <tags>
        <tag>Prompt-Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)分布式训练</title>
    <url>/www6vHomeAIGC/2023/01/06/gptTrainParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="分布式训练">分布式训练</span><a href="#分布式训练" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/b23e09b21dee4e7595122d2c5f3943ae?pvs=4">(原理)分布式训练</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战) Lora</title>
    <url>/www6vHomeAIGC/2023/01/05/gptPEFTLora/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="lora-实战">Lora  实战</span><a href="#lora-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PEFT-Lora-10dbfe21108480489a27f07aba286e4f?pvs=4">(实战) Lora </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型 排行榜</title>
    <url>/www6vHomeAIGC/2023/01/04/gptLeaderBoard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%8E%92%E8%A1%8C%E6%A6%9C">排行榜</a></li>
<li><a href="#%E4%B8%AD%E5%9B%BD%E6%8E%92%E8%A1%8C%E6%A6%9C">中国排行榜</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="排行榜">排行榜</span><a href="#排行榜" class="header-anchor">#</a></h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">HuggingFaceH 大模型排行榜</a></p>
<p><a href="https://www.promptingguide.ai/models/collection">LLM Collection</a></p>
<h3><span id="中国排行榜">中国排行榜</span><a href="#中国排行榜" class="header-anchor">#</a></h3><p><a href="https://github.com/www6v/awesome-LLMs-In-China">中国大模型 </a></p>
<ul>
<li>通用 39</li>
<li>金融 25</li>
<li>司法 8</li>
<li>法律 6</li>
<li>医学 13</li>
<li>医疗 24</li>
<li>教育 13</li>
<li>科研 17</li>
<li>工业 23</li>
<li>政务 12</li>
<li>运维 7</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>leaderBoard</category>
      </categories>
      <tags>
        <tag>leaderBoard</tag>
      </tags>
  </entry>
  <entry>
    <title>垂类大模型</title>
    <url>/www6vHomeAIGC/2023/01/04/gptDomain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="垂类大模型">垂类大模型</span><a href="#垂类大模型" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/05ad83c78a264b4f90310923580070a6?pvs=4">垂类大模型</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>NL2SQL</title>
    <url>/www6vHomeAIGC/2023/01/03/gptNL2SQL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://github.com/www6v/NL2SQL">https://github.com/www6v/NL2SQL</a><br>1xx. <a href="https://github.com/www6v/nl2sql-">https://github.com/www6v/nl2sql-</a><br>1xx. <a href="https://blog.langchain.dev/llms-and-sql/">LLMs and SQL</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/640580808">大模型与数据科学：从Text-to-SQL 开始（一）</a> 多款产品</p>
<p>百度千帆-ppt<br>QCon-ppt</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUxNzk5MTU3OQ==&mid=2247487028&idx=1&sn=7b6767878b7f6b891fc69e408f248ef1">语义解析 (Text-to-SQL) 技术研究及应用 上篇 </a><br>1xx. <a href="https://mp.weixin.qq.com/s/5lTLW5OOuRMo2zjbzMxr_Q">语义解析 (Text-to-SQL) 技术研究及应用 下篇 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/670509396">LLM在中文Text2SQL的实践</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/673474672">LLM在中文Text2SQL任务上的优化V2.0</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/670913902">LLM在中文Text2SQL任务上的优化V1.0</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402400&idx=1&sn=fe122657b35f27090aaca9c144d1d23b">也看大模型与数据库查询分析的落地结合：C3 Text2SQL方案及Data-Copilot数据自动化编排机制的实现思想阅读 </a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/668557045">C3: Zero-shot Text-to-SQL with ChatGPT笔记</a><br>1xx. <a href="https://github.com/bigbigwatermalon/C3SQL">C3SQL  </a> git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/Ffm8ooH8je2553IcLkJBmw">大模型Text2SQL主流数据集及可用实践项目：兼看利用大模型进行5W1H新闻要素提取 </a><br>   问题2:关于Text2sql当前的可用项目及数据集<br>   <a href="https://github.com/www6v/Awesome-Text2SQL">https://github.com/www6v/Awesome-Text2SQL</a></p>
<h3><span id="project">Project</span><a href="#project" class="header-anchor">#</a></h3><p><a href="https://github.com/eosphoros-ai/DB-GPT">DB-GPT Repo</a> git<br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487442&idx=1&sn=7889420553058119506bd677298e69d4">开源 Text-to-SQL 工具哪家强？Vanna 让 SQL 小白也能轻松玩转数据分析！</a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487419&idx=1&sn=31fb6947c97793bc58b645444a1b587c">YC孵化的Text-to-SQL未来之星：Defog开源 SQLCoder模型，打造企业级数据分析利器 </a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487328&idx=1&sn=dd20578ab376aaa11fee526d4787ce58">WrenAI：开源Text-to-SQL引擎让 SQL触手可及，数据分析的“GPT”时刻来了？</a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjkwNzY4OA==&mid=2247487260&idx=1&sn=53b5d036c9f2e039d4fc26c8053355b8">Dataherald 核心 Text-to-SQL 引擎全面开源！</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NL2SQL</category>
      </categories>
      <tags>
        <tag>NL2SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>(总结)推理优化</title>
    <url>/www6vHomeAIGC/2023/01/01/gptInference/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E4%BC%98%E5%8C%96">推理 优化</a><ul>
<li><a href="#overview2">overview[2]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-1">模型压缩 [1]</a></li>
<li><a href="#kv-cache">KV Cache</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-优化">推理 优化</span><a href="#推理-优化" class="header-anchor">#</a></h1><h3><span id="overview2">overview[2]</span><a href="#overview2" class="header-anchor">#</a></h3><p>有几种方法可以在内存中<strong>降低推理成本</strong>或&#x2F;和<strong>加快推理速度</strong>。</p>
<ul>
<li>应用各种<strong>并行处理方式</strong>，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。</li>
<li><strong>内存卸载</strong>，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。</li>
<li><strong>智能批处理策略</strong>；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。</li>
<li><strong>网络压缩技术</strong>，如<strong>修剪、量化、蒸馏</strong>。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。</li>
<li>针对目标模型架构的特定改进。许多<strong>架构变化</strong>，特别是针对注意力层的变化，有助于提高Transformer解码速度。</li>
</ul>
<h3><span id="模型压缩-1">模型压缩 [1]</span><a href="#模型压缩-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/01/gptInference/compress.png" class>

<ul>
<li>剪枝（Pruning）</li>
<li>知识蒸馏（Knowledge Distillation，KD）</li>
<li>量化（Quantization）</li>
<li>低秩分解（Low-Rank Factorization）</li>
</ul>
<h3><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/glPPSqHjsnDjC0DZSuuPzA">一文探秘LLM应用开发(13)-模型部署与推理(优化理论) </a> </li>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization </a>  lilianweng</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a> ***<br>1xx. <a href="https://zhuanlan.zhihu.com/p/656485997">大语言模型推理性能优化综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA</title>
    <url>/www6vHomeAIGC/2023/01/01/gptLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="llama">LLaMA</span><a href="#llama" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLaMA-2b55a2a573b549df9f6fc4cc26c2292f?pvs=4">LLaMA</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Agent</title>
    <url>/www6vHomeAIGC/2023/01/01/gptAgentPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E5%BE%AE%E8%B0%83%E7%9A%84agent-function-call12">基于微调的Agent-function call[1][2]</a></li>
<li><a href="#assistant-api-3">Assistant API [3]</a><ul>
<li><a href="#assistant-api%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D">Assistant API功能介绍</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
<li><a href="#lagent-agentlego4">Lagent &amp; AgentLego[4]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="基于微调的agent-function-call12">基于微调的Agent-function call[1][2]</span><a href="#基于微调的agent-function-call12" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/01/gptAgentPractice/dirs.JPG" class>

<img src="/www6vHomeAIGC/2023/01/01/gptAgentPractice/xtuner-agent.png" class>

<h1><span id="assistant-api-3">Assistant API [3]</span><a href="#assistant-api-3" class="header-anchor">#</a></h1><h3><span id="assistant-api功能介绍">Assistant API功能介绍</span><a href="#assistant-api功能介绍" class="header-anchor">#</a></h3><p>从功能实现层面来说，Assistant API是截至目前最完整、性能最强大的AI应用开发API，具体功能如下：</p>
<ul>
<li>首先，Assistant API前所未有的能够<strong>调用OpenAI各模型的各项能力</strong>，包括可以调用Chat系列模型（即GPT系列模型）完成文本对话、调用DALL·E 3进行绘图、调用GPT-4-vision进行图像识别、以及调用Text-to-Speech模型进行语音转文字等，并且支持在一轮对话中调用不同模型；</li>
<li>其次，Assistant API还<strong>内置了代码解释器功能（Code interpreter）和海量文本信息提取功能（Knowledge retrieval）</strong>同时也一如既往支持借助<strong>Function calling</strong>进行模型功能层面拓展，此外，非常重要的是，Assistant API还支持在一轮对话中调用多个工具；</li>
<li>其三，此外对于开发者非常友好的一点是，Assistant API最小运行单元为持久化的线程对象（persistent Threads），因此在实际运行Assistant API时，不仅能可以精确控制每一步的执行过程，同时persistent Threads也会保留每轮对话的核心信息，并且当超出模型接收信息最大上下文限制时能够自动删除早期信息，从而实现对模型短期记忆的合理管理；</li>
<li>其四，Assistant API还能够直<strong>接连接OpenAI在线文档库</strong>，即如果用户将外部文档保存在OpenAI云空间内，则可以在调用Assistant API时实时访问文档库中的任意文件，甚至可以在不同线程中调用不同的文档。而在借助Assistant API的Knowledge retrieval功能，则可以让大模型实时获取这些文件信息，并且合理管理短期记忆；</li>
</ul>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><h1><span id="lagent-amp-agentlego4">Lagent &amp; AgentLego[4]</span><a href="#lagent-amp-agentlego4" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/InternLM/tutorial/blob/main/xtuner/README.md">xtuner 实战</a><br>4【补充】用 MS-Agent 数据集 赋予 LLM 以 Agent 能力</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1yK4y1B75J/">(4)XTuner 大模型单卡低成本微调实战</a> V</p>
</li>
<li><p><a href="https://github.com/www6v/AIGC/tree/master/basic/%E4%B9%9D%E5%A4%A9Hector/Assistant%20API%E8%AF%A6%E8%A7%A3%E4%B8%8EAgent%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98">Assistant API详解与Agent开发实战-九天Hector</a></p>
</li>
<li><p><a href="https://github.com/InternLM/Tutorial/tree/camp2/agent">Lagent &amp; AgentLego 智能体应用搭建</a><br><a href="https://github.com/InternLM/Tutorial/blob/camp2/agent/lagent.md">Lagent：轻量级智能体框架</a><br><a href="https://github.com/InternLM/Tutorial/blob/camp2/agent/agentlego.md">AgentLego：组装智能体“乐高”</a></p>
</li>
</ol>
<p>1xx. <a href="https://qwenlm.github.io/zh/blog/qwen-agent-2405/">使用Qwen-Agent将上下文记忆扩展到百万量级</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)RAG</title>
    <url>/www6vHomeAIGC/2022/12/31/gptRAGPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#data-processing17">Data processing[17]</a></li>
<li><a href="#%E5%8C%BB%E7%96%97%E9%97%AE%E7%AD%94rag20">医疗问答RAG[20]</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#chuck">chuck</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F">数据格式</a></li>
<li><a href="#%E6%94%B9%E5%86%99query">改写query</a></li>
<li><a href="#%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B">召回模型</a></li>
<li><a href="#%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B">排序模型</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F">索引方式</a></li>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#xxx">xxx</a></li>
<li><a href="#%E5%8C%BB%E7%96%97%E9%97%AE%E7%AD%94">医疗问答</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="data-processing17">Data processing[17]</span><a href="#data-processing17" class="header-anchor">#</a></h1><p>长文本   变成   QA pair</p>
<ul>
<li>规则匹配</li>
<li><strong>利用LLM抽取</strong></li>
<li>人工处理</li>
</ul>
<h1><span id="医疗问答rag20">医疗问答RAG[20]</span><a href="#医疗问答rag20" class="header-anchor">#</a></h1><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/31/gptRAGPractice/arch.JPG" class>

<h3><span id="chuck">chuck</span><a href="#chuck" class="header-anchor">#</a></h3><p><strong>段落</strong><br>句子<br>token</p>
<h3><span id="数据格式">数据格式</span><a href="#数据格式" class="header-anchor">#</a></h3><p>{“id”: xxx, “病情描述”: “xxx”,  “治疗方案”: “xxx” }</p>
<h3><span id="改写query">改写query</span><a href="#改写query" class="header-anchor">#</a></h3><ul>
<li>HyDE</li>
<li>RAG Fusion -&gt; Generate Similar query<br>用户的查询不精准，要扩充query, 用大模型改写</li>
</ul>
<h3><span id="召回模型">召回模型</span><a href="#召回模型" class="header-anchor">#</a></h3><ul>
<li><p>bert模型</p>
<ul>
<li><strong>sbert</strong><ul>
<li><strong>2个bert模型</strong>，共享参数，s1,s2向量化后做<strong>相似度</strong>计算</li>
<li><strong>速度快</strong></li>
<li>相似度<br>欧式距离</li>
</ul>
</li>
<li>在百万语料上训练<ul>
<li><strong>语料格式</strong><br>[s1][s2] 0 - 无关<br>[s1][s2] 1-类似</li>
</ul>
</li>
</ul>
</li>
<li><p>根据query, 召回id和value整条记录</p>
</li>
</ul>
<h3><span id="排序模型">排序模型</span><a href="#排序模型" class="header-anchor">#</a></h3><ul>
<li>bert模型<ul>
<li>1个bert模型</li>
<li><strong>速度慢</strong></li>
<li><strong>格式</strong><ul>
<li>query[sep]s2  -&gt; 经过softmax，产生2分类，0-1</li>
</ul>
</li>
<li>也要训练<ul>
<li>同<strong>召回模型训练方式</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="索引方式">索引方式</span><a href="#索引方式" class="header-anchor">#</a></h3><ul>
<li>树索引</li>
<li>知识图谱的索引</li>
</ul>
<h3><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h3><ul>
<li>综合归纳的作用</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="17">
<li>&lt;&lt;大模型结合 RAG 构建客服场景自动问答系统&gt;&gt;  NVIDIA大模型日系列活动</li>
</ol>
<h3><span id="医疗问答">医疗问答</span><a href="#医疗问答" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://www.bilibili.com/video/BV1fW421P7u6?p=5">基于百万语料的医疗RAG项目</a> v</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Retrievers</title>
    <url>/www6vHomeAIGC/2022/12/31/gptRetrievers/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#langchain-retrievers10">Langchain Retrievers[10]</a><ul>
<li><a href="#multiqueryretriever">MultiQueryRetriever</a></li>
<li><a href="#contextual-compression">Contextual compression</a></li>
<li><a href="#ensemble-retriever">Ensemble Retriever</a></li>
<li><a href="#multivector-retriever">MultiVector Retriever</a></li>
<li><a href="#parent-document-retriever">Parent Document Retriever</a></li>
<li><a href="#self-querying">Self-querying</a></li>
</ul>
</li>
<li><a href="#langchian-retriever10">Langchian Retriever[10]</a></li>
<li><a href="#langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="langchain-retrievers10">Langchain Retrievers[10]</span><a href="#langchain-retrievers10" class="header-anchor">#</a></h1><h3><span id="multiqueryretriever">MultiQueryRetriever</span><a href="#multiqueryretriever" class="header-anchor">#</a></h3><p>The MultiQueryRetriever automates the process of prompt tuning by using an LLM to <strong>generate multiple queries from different perspectives for a given user input query</strong>. </p>
<h3><span id="contextual-compression">Contextual compression</span><a href="#contextual-compression" class="header-anchor">#</a></h3><h3><span id="ensemble-retriever">Ensemble Retriever</span><a href="#ensemble-retriever" class="header-anchor">#</a></h3><p>The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and <strong>rerank the results based on the Reciprocal Rank Fusion algorithm</strong>.<br>The most common pattern is to <strong>combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity)</strong>, because their strengths are complementary. It is also known as “hybrid search”.</p>
<h3><span id="multivector-retriever">MultiVector Retriever</span><a href="#multivector-retriever" class="header-anchor">#</a></h3><p>The methods to create multiple vectors per document include:<br>    - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).<br>    - Summary: create a summary for each document, embed that along with (or instead of) the document.<br>    - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.</p>
<h3><span id="parent-document-retriever">Parent Document Retriever</span><a href="#parent-document-retriever" class="header-anchor">#</a></h3><p>chunks of data</p>
<h3><span id="self-querying">Self-querying</span><a href="#self-querying" class="header-anchor">#</a></h3><p>This allows the retriever to not only use the user-input query for <strong>semantic similarity comparison</strong> with the contents of stored documents but to also extract filters from the user query on <strong>the metadata</strong> of stored documents and to execute those filters.</p>
<h1><span id="langchian-retriever10">Langchian Retriever[10]</span><a href="#langchian-retriever10" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Name</th>
<th>Index Type</th>
<th>Uses an LLM</th>
<th>When to Use</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore">Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you are just getting started and looking for something quick and easy.</td>
<td>This is the <strong>simplest method</strong> and the one that is easiest to get started with. It involves creating embeddings for each piece of text.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever">ParentDocument</a></td>
<td>Vectorstore + Document Store</td>
<td>No</td>
<td>If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.</td>
<td>This involves indexing <strong>multiple chunks</strong> for each document. Then you find the  chunks that are most similar in embedding space, but you retrieve the  <strong>whole parent</strong> document and <strong>return</strong> that (rather than individual chunks).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector">Multi Vector</a></td>
<td>Vectorstore + Document Store</td>
<td>Sometimes during indexing</td>
<td>If you are able to extract information from documents that you think is more relevant to index than the text itself.</td>
<td>This involves creating multiple vectors for each document. Each vector could be created in a <strong>myriad of ways</strong> - examples include <strong>summaries of the text</strong> and <strong>hypothetical questions</strong>.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query">Self Query</a></td>
<td>Vectorstore</td>
<td>Yes</td>
<td>If users are asking questions that are better answered by fetching  documents based on metadata rather than similarity with the text.</td>
<td>This uses an LLM to transform user input into two things: (1) a string to  look up semantically, (2) a <strong>metadata filer</strong> to go along with it. This is  useful because oftentimes questions are about the METADATA of documents  (not the content itself).</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression">Contextual Compression</a></td>
<td>Any</td>
<td>Sometimes</td>
<td>If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.</td>
<td>This puts a <strong>post-processing step</strong> on top of another retriever and extracts  only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/time_weighted_vectorstore">Time-Weighted Vectorstore</a></td>
<td>Vectorstore</td>
<td>No</td>
<td>If you have timestamps associated with your documents, and you want to retrieve the most recent ones</td>
<td>This fetches documents based on a combination of semantic similarity (as in  normal vector retrieval) and recency (looking at timestamps of indexed  documents)</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever">Multi-Query Retriever</a></td>
<td>Any</td>
<td>Yes</td>
<td>If users are asking questions that are complex and require multiple pieces of distinct information to respond</td>
<td>This uses an LLM to <strong>generate multiple queries</strong> from the original one. This is useful when the original query needs pieces of information about  multiple topics to be properly answered. By generating multiple queries, we can then fetch documents for each of them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble">Ensemble</a></td>
<td>Any</td>
<td>No</td>
<td>If you have multiple retrieval methods and want to try combining them.</td>
<td>This fetches documents from <strong>multiple retrievers</strong> and then <strong>combines</strong> them.</td>
</tr>
<tr>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder">Long-Context Reorder</a></td>
<td>Any</td>
<td>No</td>
<td>If you are working with a long-context model and noticing that it’s not  paying attention to information in the middle of retrieved documents.</td>
<td>This fetches documents from an underlying retriever, and then reorders them  so that the most similar are near the beginning and end. This is useful  because it’s been shown that for longer context models they sometimes  don’t pay attention to information in the middle of the context window.</td>
</tr>
</tbody></table>
<h1><span id="langchain-vs-llamaindex-1">langchain vs. llamaindex [1]</span><a href="#langchain-vs-llamaindex-1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>langchain</th>
<th>llamaindex</th>
</tr>
</thead>
<tbody><tr>
<td>Ensemble</td>
<td>Hybrid Fusion</td>
</tr>
<tr>
<td>Rewrite-Retrieve-Read</td>
<td>Query Rewriting</td>
</tr>
<tr>
<td></td>
<td>AutoMerging</td>
</tr>
<tr>
<td>ParentDocumentRetrieval</td>
<td>Small-to-Big Retrieval</td>
</tr>
<tr>
<td></td>
<td>Sentence Window Retrieval</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1qe411r78b/">【高级RAG || 原理介绍】Llamaindex 5种高级RAG方法</a> V </li>
<li><a href="https://python.langchain.com/docs/modules/data_connection/retrievers">retrievers</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Retrievers</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>LLMOps</title>
    <url>/www6vHomeAIGC/2022/12/28/gptLLMOps/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li><a href="https://drive.google.com/file/d/1LZXTrRdrloIqAJT6xaNTl4WQd6y95o7K/view">LLMOps: Deployment and Learning in Production</a><br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/">LLMOps: Deployment and Learning in Production</a><br><a href="https://zhuanlan.zhihu.com/p/629589593">[必读] LLM 应用开发全栈指南</a> LLMOps</li>
<li><a href="https://zhuanlan.zhihu.com/p/632026876">了解一下新领域 LLMOps: 大模型运维</a><br><a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations">Understanding LLMOps: Large Language Model Operations</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Fine-Tuning 时机</title>
    <url>/www6vHomeAIGC/2022/12/28/gptFineTuningWhen/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BD%95%E6%97%B6%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%831">何时进行微调[1]</a></li>
<li><a href="#what-4">what [4]</a></li>
<li><a href="#common-use-cases2">Common use cases[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="何时进行微调1">何时进行微调[1]</span><a href="#何时进行微调1" class="header-anchor">#</a></h1><p>语言模型（LLM）可以通过至少两种方式学习新知识：权重更新（例如预训练或微调）或提示（例如检索增强生成，RAG）。模型的权重就像长期记忆，而提示就像短期记忆。这个OpenAI Cookbook给出了一个有用的比喻：当你对模型进行微调时，就像是在离考试还有一周的时候准备复习。当你通过提示（例如检索）向提示中插入知识时，就像是在有开放笔记的考试中。</p>
<p>基于这一点，<strong>不建议使用微调来教授LLM新的知识或事实回忆</strong>；OpenAI的John Schulman在一次讲话中指出，微调可能会<strong>增加虚构</strong>。微调<strong>更适合教授专门的任务</strong>，但应与提示或RAG相对比。正如这里所讨论的，对于具有丰富示例和&#x2F;或缺乏上下文学习能力的LLM来说，微调对于定义明确的任务可能是有帮助的。这篇Anyscale博客很好地总结了这些观点：<strong>微调是为形式而非事实</strong>[3]。</p>
<h1><span id="what-4">what [4]</span><a href="#what-4" class="header-anchor">#</a></h1><p>这是一个很好的问题。我大致将微调类比为人的专业知识：</p>
<ul>
<li><strong>用文字描述一个任务 ~&#x3D; 零样本提示</strong></li>
<li><strong>给出解决任务的示例 ~&#x3D; 少样本提示</strong></li>
<li><strong>允许人们练习任务 ~&#x3D; 微调</strong></li>
</ul>
<p>考虑到这个比喻，令人惊奇的是我们有了可以仅通过提示就能在许多任务上达到高水平准确性的模型，但我也预计达到顶级性能可能需要微调，特别是在具有明确定义的具体任务的应用中，在这些任务中我们可以收集大量数据并在其上进行“练习”。</p>
<p>这可能是一个需要牢记的<strong>粗略图景</strong>。<strong>小型模型</strong>无法进行上下文学习，并且从提示工程中受益甚少，但根据任务的难度，<strong>仍然有可能将它们微调为表现良好的专家</strong>。</p>
<p>需要注意的是，所有这些都还是非常新颖的。</p>


<h1><span id="common-use-cases2">Common use cases[2]</span><a href="#common-use-cases2" class="header-anchor">#</a></h1><p>微调可以改善结果的一些常见<strong>用例</strong>包括：</p>
<ul>
<li><strong>设定风格、语气、格式或其他定性因素</strong></li>
<li><strong>提高生成所需输出的可靠性</strong></li>
<li><strong>纠正无法按照复杂提示要求执行的问题</strong></li>
<li>以特定方式处理许多边缘情况</li>
<li><strong>执行难以用提示清晰表达的新技能或任务</strong></li>
</ul>
<p>从较高层面来看，这些情况下微调更容易实现“<strong>展示而非告诉</strong>”的效果。在接下来的部分中，我们将探讨如何为微调设置数据以及各种示例，这些示例中微调改善了基线模型的性能。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/">Using LangSmith to Support Fine-tuning</a><br>  <a href="https://colab.research.google.com/drive/1tpywvzwOS74YndNXhI8NUaEfPeqOc7ub?usp=sharing&ref=blog.langchain.dev">colab</a>   LANGCHAIN_API_KEY</p>
</li>
<li><p><a href="https://platform.openai.com/docs/guides/fine-tuning">Fine-tuning</a>  openai *** </p>
</li>
<li><p><a href="https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts">Fine tuning is for form, not facts</a> ***</p>
</li>
<li><p><a href="https://twitter.com/karpathy/status/1655994367033884672">Andrej Karpathy twitter</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Fine-Tuning</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)RAG OpenAI案例</title>
    <url>/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#openai-rag-%E6%A1%88%E4%BE%8B3">OpenAI RAG 案例[3]</a><ul>
<li><a href="#query-transformations5">Query Transformations[5]</a></li>
<li><a href="#query-construction-4">Query Construction [4]</a></li>
</ul>
</li>
<li><a href="#advanced-rag">Advanced RAG</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="openai-rag-案例3">OpenAI RAG 案例[3]</span><a href="#openai-rag-案例3" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/openai-rag.jpg" class>

<ol>
<li>retrieval with consine similarity</li>
<li><strong>HyDE retrieval</strong> [5]<br>Fine-tune Embeddings<br><strong>Chunk&#x2F;embedding experiments</strong></li>
<li><strong>Reranking</strong> [6][8]<br>Classification step</li>
<li>Prompt engineering<br><strong>Tool use</strong><br><strong>Query expansion</strong>[5]</li>
</ol>
<h3><span id="query-transformations5">Query Transformations[5]</span><a href="#query-transformations5" class="header-anchor">#</a></h3><ul>
<li><strong>Query expansion</strong><br>Multi-query retriever </li>
<li><strong>HyDE</strong></li>
<li>Step back prompting<br> [抽象prompting]</li>
<li>Rewrite-Retrieve-Read</li>
</ul>
<h3><span id="query-construction-4">Query Construction [4]</span><a href="#query-construction-4" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/structured_data_stacks.jpg" class>

<table>
<thead>
<tr>
<th><strong>Examples</strong></th>
<th><strong>Data source</strong></th>
<th><strong>References</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Text-to-metadata-filter</strong></td>
<td>Vectorstores</td>
<td><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/?ref=blog.langchain.dev#constructing-from-scratch-with-lcel"><strong>Docs</strong></a></td>
</tr>
<tr>
<td><strong>Text-to-SQL</strong></td>
<td>SQL DB</td>
<td><a href="https://python.langchain.com/docs/use_cases/qa_structured/sql?ref=blog.langchain.dev"><strong>Docs</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/llms-and-sql/"><strong>blog</strong></a><strong>,</strong> <a href="https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/"><strong>blog</strong></a></td>
</tr>
</tbody></table>
<ul>
<li>Text-to-metadata-filter [7]</li>
</ul>
<p>A <strong>self-querying</strong> retriever is one that, as the name suggests, has the  ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a <strong>structured query</strong> and then applies that structured query to its underlying  VectorStore. This allows the retriever to not only use the user-input  query for semantic similarity comparison with the contents of stored  documents but to also <strong>extract filters from the user query on the  metadata of stored documents and to execute those filters</strong>.</p>
<h1><span id="advanced-rag">Advanced RAG</span><a href="#advanced-rag" class="header-anchor">#</a></h1><h3><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h3><ul>
<li>离线 index</li>
<li>在线 查询</li>
</ul>
<img src="/www6vHomeAIGC/2022/12/27/gptRAGOpenAI/rag.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://blog.langchain.dev/deconstructing-rag/">Deconstructing RAG</a> ***</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://blog.langchain.dev/applying-openai-rag/">Applying OpenAI’s RAG Strategies</a>   *** </p>
</li>
<li><p><a href="https://blog.langchain.dev/query-construction/">Query Construction</a> ***</p>
</li>
<li><p><a href="https://blog.langchain.dev/query-transformations/">Query Transformations</a></p>
</li>
<li><p><a href="https://txt.cohere.com/rerank/">Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here</a><br><a href="https://github.com/langchain-ai/langchain/tree/master/templates/rag-pinecone-rerank">Rerank</a><br><a href="https://python.langchain.com/docs/integrations/retrievers/cohere-reranker">Cohere Reranker</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/docs/docs/modules/data_connection/retrievers/self_query.ipynb">self_query</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb">RAG Fusion</a><br><a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1">Forget RAG, the Future is RAG-Fusion</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PEFT 概述</title>
    <url>/www6vHomeAIGC/2022/12/20/gptFineTuningPEFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="huggingface-peft中的任务1">Huggingface  PEFT中的任务[1]</span><a href="#huggingface-peft中的任务1" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class TaskType(str, enum.Enum):</span><br><span class="line">    SEQ_CLS = &quot;SEQ_CLS&quot;  # 3. 序列分类任务</span><br><span class="line">    SEQ_2_SEQ_LM = &quot;SEQ_2_SEQ_LM&quot;  # 2. 条件生成任务</span><br><span class="line">    CAUSAL_LM = &quot;CAUSAL_LM&quot;  #  1. 因果语言建模任务</span><br><span class="line">    TOKEN_CLS = &quot;TOKEN_CLS&quot;  #  4. Token 分类任务</span><br><span class="line">    QUESTION_ANS = &quot;QUESTION_ANS&quot;</span><br><span class="line">    FEATURE_EXTRACTION = &quot;FEATURE_EXTRACTION&quot;</span><br></pre></td></tr></table></figure>

<h3><span id="1-因果语言建模任务causal-language-modeling">1. 因果语言建模任务（Causal Language Modeling）</span><a href="#1-因果语言建模任务causal-language-modeling" class="header-anchor">#</a></h3><p>  因果语言建模任务（CLM），在这种建模方法中，模型试图预测给定上下文中的下一个单词，该上下文通常包括在当前单词之前的所有单词。</p>
<h3><span id="2-条件生成任务conditional-generation">2. 条件生成任务（Conditional Generation）</span><a href="#2-条件生成任务conditional-generation" class="header-anchor">#</a></h3><p>  条件生成任务（Conditional Generation），根据给定的输入（可能是文本、图片等）生成符合条件的输出。<br>  条件生成的应用包括但不限于机器翻译、文本摘要、图像描述等。这些任务通常需要模型在输入和输出之间建立复杂的映射关系。</p>
<blockquote>
<p>因果语言建模任务  vs.  条件生成任务<br>  因果语言建模主要关注于生成连贯、自然的文本，而条件生成关注于生成满足特定条件或任务要求的文本。这两种建模方法在某些场景下可能会互相使用和结合，以实现更复杂的自然语言处理任务。</p>
</blockquote>
<h3><span id="3-序列分类任务sequence-classification">3. 序列分类任务（Sequence Classification）</span><a href="#3-序列分类任务sequence-classification" class="header-anchor">#</a></h3><p>  序列分类（Sequence Classification），对整个句子进行分类。如: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关等</p>
<h3><span id="4-token-分类任务token-classification">4. Token 分类任务（Token Classification）</span><a href="#4-token-分类任务token-classification" class="header-anchor">#</a></h3><p>  Token 分类任务（Token Classification），对句子中的每个词进行分类。如: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/651744834">大模型参数高效微调技术实战（一）-PEFT概述</a></li>
<li><a href="https://github.com/www6v/llm-action#llm%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98">LLM微调实战</a> 李国东</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 系列</title>
    <url>/www6vHomeAIGC/2022/12/11/gptFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%9B%E5%8C%96%E6%97%B6%E9%97%B4%E7%BA%BF">进化时间线</a></li>
<li><a href="#gpt1-1">GPT1 [1]</a></li>
<li><a href="#gpt2-1">GPT2 [1]</a><ul>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">核心思想</a></li>
<li><a href="#gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</a></li>
</ul>
</li>
<li><a href="#gpt3-1">GPT3 [1]</a><ul>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">下游任务评估方法</a></li>
<li><a href="#few-shot-vs-fine-tuning">Few-shot vs fine-tuning</a></li>
<li><a href="#gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</a></li>
</ul>
</li>
<li><a href="#instructgpt-1">InstructGPT [1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88">技术方案</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="#chatgpt-%E8%AE%AD%E7%BB%83-3">ChatGPT 训练  [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="进化时间线">进化时间线</span><a href="#进化时间线" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/12/11/gptFamily/family.jpg" class>

<h1><span id="gpt1-1">GPT1 [1]</span><a href="#gpt1-1" class="header-anchor">#</a></h1><ol>
<li>它是最早一批提出在 NLP 任务上使用 <strong>pre-train + fine-tuning 范式</strong>的工作。</li>
<li>GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</li>
<li><strong>预训练模型具有 zero-shot 的能力</strong>，并且能随着预训练的进行不断增强</li>
</ol>
<h1><span id="gpt2-1">GPT2 [1]</span><a href="#gpt2-1" class="header-anchor">#</a></h1><h3><span id="核心思想">核心思想</span><a href="#核心思想" class="header-anchor">#</a></h3><p>当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，<strong>不需要在下游任务微调</strong>。</p>
<h3><span id="gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</span><a href="#gpt-2-vs-gpt-1" class="header-anchor">#</a></h3><ol>
<li><strong>主推 zero-shot</strong>，而 GPT-1 为 pre-train + fine-tuning；</li>
<li>训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；</li>
<li>模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；</li>
<li>模型结构调整，层归一化和参数初始化方式；</li>
<li>训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；</li>
</ol>
<h1><span id="gpt3-1">GPT3 [1]</span><a href="#gpt3-1" class="header-anchor">#</a></h1><h3><span id="下游任务评估方法">下游任务评估方法</span><a href="#下游任务评估方法" class="header-anchor">#</a></h3><p>GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：<br><strong>Zero-shot</strong>：仅使用当前任务的自然语言描述，不进行任何梯度更新；<br><strong>One-shot</strong>：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；<br><strong>Few-shot</strong>：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；</p>
<ul>
<li>Shot[2]<ul>
<li>One-shot</li>
<li>Few-Shot</li>
<li>Zero-Shot</li>
</ul>
</li>
</ul>
<h3><span id="few-shot-vs-fine-tuning">Few-shot vs fine-tuning</span><a href="#few-shot-vs-fine-tuning" class="header-anchor">#</a></h3><p>其中 <strong>Few-shot</strong> 也被称为 <strong>in-context learning</strong>，虽然它与 fine-tuning 一样都需要一些<strong>有监督标注数据</strong>，但是两者的区别是：<br>【本质区别】<br><strong>fine-tuning</strong> 基于标注数据<strong>对模型参数进行更新</strong><br>而<strong>in-context learning</strong>使用标注数据时不做任何的梯度回传, <strong>模型参数不更新</strong></p>
<h3><span id="gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</span><a href="#gpt-3-vs-gpt-2" class="header-anchor">#</a></h3><ol>
<li>效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章；</li>
<li><strong>主推 few-shot</strong>，相比于 GPT-2 的 zero-shot，具有很强的创新性；</li>
<li>模型结构略微变化，采用 <strong>sparse attention</strong> 模块；</li>
<li>海量训练语料 <strong>45TB</strong>（清洗后 570GB），相比于 GPT-2 的 40GB；</li>
<li>海量模型参数，最大模型为 <strong>1750 亿</strong>，GPT-2 最大为 15 亿参数；</li>
</ol>
<h1><span id="instructgpt-1">InstructGPT [1]</span><a href="#instructgpt-1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>有监督微调，</li>
<li>奖励模型训练，</li>
<li>强化学习训练</li>
</ul>
<h3><span id="技术方案">技术方案</span><a href="#技术方案" class="header-anchor">#</a></h3><ul>
<li><p>有监督微调（SFT）<br>本质上来说，<strong>SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3</strong>。但是值得一提的是，这里<strong>标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别</strong>。<br>InstructGPT 在 SFT 中标注的数据，正是为了<strong>消除这种模型预测与用户表达习惯之间的 gap</strong>。在标注过程中，他们<strong>从 GPT-3 的用户真实请求中采样</strong>大量下游任务的描述，然后让<strong>标注人员对任务描述进行续写</strong>，从而得到该问题的高质量回答。</p>
</li>
<li><p>基于人类反馈的强化学习（RLHF）</p>
<img src="/www6vHomeAIGC/2022/12/11/gptFamily/instructGPT.jpg" class></li>
</ul>
<h3><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h3><ol>
<li>解决 GPT-3 的<strong>输出与人类意图</strong>之间的<strong>Align问题</strong>；</li>
<li>让具备丰富世界知识的大模型，<strong>学习“人类偏好”</strong>；</li>
<li>标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠；</li>
<li>InstructGPT 在<strong>真实性</strong>，<strong>丰富度</strong>上表现更好；</li>
<li>InstructGPT 对有害结果的生成控制的更好，但是对于<strong>“偏见”没有明显改善</strong>；</li>
</ol>
<h1><span id="chatgpt-训练-3">ChatGPT 训练  [3]</span><a href="#chatgpt-训练-3" class="header-anchor">#</a></h1><ul>
<li>基于人类反馈的强化学习微调技术 RLHF<ul>
<li>使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型<ul>
<li>Supervised fine-tuning (SFT)<br>&#x3D; Instruction Tuning</li>
</ul>
</li>
<li>训练奖励模型 Reward Model（RM）</li>
<li>使用强化学习算法微调语言模型<ul>
<li>RLHF<br>[本质  基于强化学习, 强化学习算法]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/609716668">GPT &#x2F; GPT-2 &#x2F; GPT-3 &#x2F; InstructGPT 进化之路</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/624793654">Few-Shot, Zero-Shot &amp; One-shot 的通俗理解</a></p>
</li>
<li><p><a href="https://shimo.im/docs/KlkKv4XQDouwWRqd/read">AI 大模型微调训练营大纲</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/VYv8BRgGnp9ZTuXxaSuFwg">万字拆解！追溯ChatGPT各项能力的起源 </a> 符尧</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642282717">[Transformer 101系列] ChatGPT是怎么炼成的?</a> 未</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Advanced RAG</title>
    <url>/www6vHomeAIGC/2022/12/07/gptRAGPerformance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="advanced-rag">Advanced RAG</span><a href="#advanced-rag" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Advanced-RAG-108bfe2110848030b150e8c09baa7232?pvs=4">(原理)Advanced RAG</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Transformer</title>
    <url>/www6vHomeAIGC/2022/11/30/gptTransformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="原理transformer">(原理)Transformer</span><a href="#原理transformer" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Transformer-b1b9836f9c244db3acda7869f64ff860?pvs=4">(原理)Transformer</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>向量数据库</title>
    <url>/www6vHomeAIGC/2022/11/27/gptVectorStore/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93">向量数据库</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F-7">向量数据库-索引方式 [7]</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%953">向量的相似度算法[3]</a><ul>
<li><a href="#%E6%AF%94%E8%BE%834">比较[4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


  
<h1><span id="向量数据库">向量数据库</span><a href="#向量数据库" class="header-anchor">#</a></h1><ul>
<li><p>国产</p>
<ul>
<li>Milvus</li>
<li>Tencent </li>
<li>zilliz cloud</li>
</ul>
</li>
<li><p>国外</p>
<ul>
<li>Pinecone</li>
<li>FAISS<br>[ANN]</li>
<li>Chroma</li>
<li>Weaviate</li>
</ul>
</li>
</ul>
<h1><span id="向量数据库-索引方式-7">向量数据库-索引方式 [7]</span><a href="#向量数据库-索引方式-7" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/27/gptVectorStore/index.jpg" class>

<h1><span id="向量的相似度算法3">向量的相似度算法[3]</span><a href="#向量的相似度算法3" class="header-anchor">#</a></h1><ul>
<li>Cosine Similarity *<br>余弦</li>
<li>Dot Product *</li>
<li>Squared Euclidean (L2-Squared) *<br>欧式距离</li>
<li>Manhattan (L1 Norm or Taxicab Distance) *</li>
<li>Hamming *</li>
<li>ANN</li>
</ul>
<h3><span id="比较4">比较[4]</span><a href="#比较4" class="header-anchor">#</a></h3><table>
<thead>
<tr>
<th>Similarity Metric</th>
<th>Vector properties considered</th>
</tr>
</thead>
<tbody><tr>
<td>Euclidean distance</td>
<td>Magnitudes and direction</td>
</tr>
<tr>
<td>Cosine similarity</td>
<td>Only direction</td>
</tr>
<tr>
<td>Dot product similarity</td>
<td>Magnitudes and direction</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/476025527">云原生向量数据库Milvus扫盲，看完这篇就够了</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/477231485">云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema</a></p>
</li>
<li><p><a href="https://weaviate.io/blog/distance-metrics-in-vector-search?ref=blog.langchain.dev">Distance Metrics in Vector Search</a></p>
</li>
<li><p><a href="https://www.pinecone.io/learn/vector-similarity/">Vector Similarity Explained</a></p>
</li>
<li><p>xxx</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://www.modb.pro/db/1694527960317513728">向量数据库（第 1 部分）：每个数据库有何不同？</a></p>
</li>
</ol>
<p>1xx. <a href="https://cloud.tencent.com/developer/article/2352088">微信向量检索分析一体化数仓探索：OLAP For Embedding</a> *** </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/646832642">Meta向量数据库Faiss介绍</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>向量数据库</category>
      </categories>
      <tags>
        <tag>向量数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>金融大模型</title>
    <url>/www6vHomeAIGC/2022/11/24/gptDomainFinance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%8A%80%E6%9C%AF1">金融大模型 技术[1]</a></li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B">金融大模型</a><ul>
<li><a href="#fingpt-%E5%93%A5%E5%A4%A7-2">FinGPT 哥大 [2]</a></li>
<li><a href="#disc-finllm-4">DISC-FinLLM [4]</a></li>
<li><a href="#%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-3">轩辕金融大模型 [3]</a></li>
<li><a href="#bloomberggpt">BloombergGPT</a></li>
<li><a href="#finbert">FinBERT</a></li>
</ul>
</li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%9C%BA%E6%99%AF">金融场景</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="金融大模型-技术1">金融大模型 技术[1]</span><a href="#金融大模型-技术1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/24/gptDomainFinance/finance.png" class>
<p>A,B  先忽略<br>C - BloombergGPT<br>D - FinGPT</p>
<h1><span id="金融大模型">金融大模型</span><a href="#金融大模型" class="header-anchor">#</a></h1><h3><span id="fingpt-哥大-2">FinGPT   哥大 [2]</span><a href="#fingpt-哥大-2" class="header-anchor">#</a></h3><ul>
<li><p>Resource</p>
<ul>
<li><a href="https://github.com/www6v/FinGPT">Github Repo</a></li>
<li><a href="https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster">FinGPT-Forecaster</a></li>
<li><a href="https://huggingface.co/FinGPT">huggingface</a></li>
<li>五篇paper</li>
</ul>
</li>
<li><p><a href="https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99">Training</a></p>
</li>
</ul>
<h3><span id="disc-finllm-4">DISC-FinLLM [4]</span><a href="#disc-finllm-4" class="header-anchor">#</a></h3><h3><span id="轩辕金融大模型-3">轩辕金融大模型 [3]</span><a href="#轩辕金融大模型-3" class="header-anchor">#</a></h3><ul>
<li>Resource<ul>
<li><a href="https://github.com/Duxiaoman-DI/XuanYuan">XuanYuan</a> git </li>
<li><a href="https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus">数据集</a></li>
</ul>
</li>
</ul>
<p>【大模型(基于BLOOM-176B)转向小模型（XuanYuan-13B-Chat）】</p>
<h3><span id="bloomberggpt">BloombergGPT</span><a href="#bloomberggpt" class="header-anchor">#</a></h3><p>未开源</p>
<h3><span id="finbert">FinBERT</span><a href="#finbert" class="header-anchor">#</a></h3><h1><span id="金融场景">金融场景</span><a href="#金融场景" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《A Survey of Large Language Models in Finance (FinLLMs)》<br> <a href="https://github.com/adlnlp/FinLLMs">FinLLMs</a></li>
<li><a href="https://www.bilibili.com/video/BV1R64y1j76H/">FinGPT开源金融垂类大模型</a> V</li>
<li>&lt;&lt;06【脱敏版】金融行业实战：度小满轩辕金融大模型应用探索与开发实践&gt;&gt;<br> <a href="https://www.bilibili.com/video/BV1G64y1j7Zj/">金融行业实战：度小满轩辕金融大模型应用探索与开发实践</a> V</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404800&idx=2&sn=9c1ad9d8aa8b0725dd6289bc15e177c9">本周大模型代表进展解析:ChatGLM3的特性认识及LoRA专家模组形式的金融领域微调模型实现策略</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400799&idx=1&sn=fb3778d1914849d3b41b047b33ce32a9">ChatGPT能否预测股价走势？大模型应用于金融预测与今日大模型前沿速递</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>金融大模型</category>
      </categories>
      <tags>
        <tag>金融大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Training</title>
    <url>/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#training-pipeline0">Training Pipeline[0]</a><ul>
<li><a href="#%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0-2">设置训练参数 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%87%8F-2">参数量 vs 训练数据量 [2]</a></li>
</ul>
</li>
<li><a href="#pre-training">Pre-training</a><ul>
<li><a href="#pre-training-4">Pre-training [4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="training-pipeline0">Training Pipeline[0]</span><a href="#training-pipeline0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/bigModelTrainingPipeline.jpg" class>

<p><strong>模型训练分为四个阶段</strong> [2]</p>
<ul>
<li>预训练（Pretraining） –&gt;Base model  <ul>
<li>预训练技术<br>预训练本质上是⼀个⽆监督学习过程</li>
</ul>
</li>
<li>监督微调（Supervised Finetuning） –&gt; SFT model<br>核⼼原因还是在于需要“赋予”⼤模型更加定制化的功能</li>
<li>奖励建模（Reward Modeling）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<p><strong>三个角度解析</strong> [2]</p>
<ul>
<li>数据量：<strong>预训练</strong>阶段所需的<strong>数据量很大</strong>，但<strong>质量要求不高</strong>；而<strong>后面的三个阶段</strong>恰恰相反，需要的<strong>数据质量较高</strong>。</li>
<li>训练方法：<strong>预训练和监督微调</strong>的训练方法相同，都是<strong>预测下一个单词</strong>。奖励模型和强化学习的训练方法则不同。<strong>奖励模型</strong>是<strong>二元分类学习</strong>，而<strong>强化学习</strong>则鼓励模型生成奖励模型评分较高的回答。</li>
<li>训练所需资源：预训练阶段的资源消耗巨大，使用数千颗GPU，花费<strong>数月</strong>时间，占总训练时间的99%。后面的三个阶段只需使用数十颗GPU，训练时间约<strong>数天</strong>。</li>
</ul>
<h3><span id="设置训练参数-2">设置训练参数 [2]</span><a href="#设置训练参数-2" class="header-anchor">#</a></h3><p>设置训练参数，如batch-size、learning rate等</p>
<ul>
<li>预训练阶段的<strong>Batch Size非常大</strong>，范围在0.5M到4M之间。</li>
<li><strong>Learning rate设定较小</strong>，且随着网络规模的增大，Learning rate越来越小。</li>
</ul>
<h3><span id="参数量-vs-训练数据量-2">参数量 vs 训练数据量 [2]</span><a href="#参数量-vs-训练数据量-2" class="header-anchor">#</a></h3><p><strong>参数量并不是衡量模型能力的唯一标准，训练数据量也是一个非常重要的因素。</strong><br>LLaMA模型，尽管它的参数量只有650亿，但其性能与参数量为1750亿的GPT-3模型相比也非常优秀。主要原因在于，LLaMA模型的训练数据量达到了1.4万亿，而GPT-3只有3000亿。</p>
<h1><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor">#</a></h1><h3><span id="pre-training-4">Pre-training [4]</span><a href="#pre-training-4" class="header-anchor">#</a></h3><ul>
<li><p>⾃回归与⽣成式</p>
<ul>
<li><strong>⾃回归模型</strong>是⼀种序列模型，它在预测下⼀个输出时，会将之前的所有输出作为输⼊，然后<strong>根据统计规律、结合已经输⼊的样本</strong>，预测下个位置各单词出现的概率，然后输出概率最⼤的单词，类似于完形填空；</li>
<li><strong>⽣成式模型</strong>的预测过程和⾃回归模型类似，都是根据统<br>计规律预测下个单词的概率，所不同的是，<strong>⽣成式模型可以根据之前的样本的<br>概率分布⽣成下⼀个词，⽣成式模型预测时会存在⼀定的随机性；</strong></li>
</ul>
</li>
<li><p>GPT来说，就是⼀个⾃回归⽣成式模型 [4]<br>⼀个⾃回归⽣成式模型在进⾏预测的时候，<strong>会⾸先根据⾃回归模型，在参考到⽬前为⽌<br>已经⽣成的词的情况下确定下⼀个词的概率分布，然后再根据⽣成式的⽅式来根据这个<br>分布⽣成下⼀个词</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a>  from ppt</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://techdiylife.github.io/big-model-training/deepspeed/LLM-state-of-GPT.html">大模型训练入门实战</a>  ***<br><a href="https://karpathy.ai/stateofgpt.pdf">State of GPT</a><br><a href="https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg">State of GPT：大神Andrej揭秘OpenAI大模型原理和训练过程 </a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
<li><p>大模型入门必看教程  九天Hector</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399532&idx=1&sn=31b7bc5a4f3114d8215da0edc2559e47">语言模型预训练基础知识总结：标准数据流pipleline、tokenizer的认识以及常见编码模型范式 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/651316650">从头预训练大模型实践经验</a>  ***<br><a href="https://wandb.ai/site/wp-content/uploads/2023/09/Current-Best-Practices-for-Training-LLMs-from-Scratch-Final.pdf">Current Best Practices for Training LLMs from Scratch</a>  原文</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)PEFT</title>
    <url>/www6vHomeAIGC/2022/11/18/gptFineTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="peft原理">PEFT原理</span><a href="#peft原理" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PEFT-10dbfe2110848028b9afd05f05fdbde6?pvs=4">(原理)PEFT</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>PEFT</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) OpenAI Function Call</title>
    <url>/www6vHomeAIGC/2022/11/16/gptFunctionCall/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#function-call">Function Call</a><ul>
<li><a href="#%E8%B0%83%E7%94%A8%E9%A1%BA%E5%BA%8F-0-12">调用顺序 [0] [1][2]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-2">代码 [2]</a></li>
<li><a href="#goal">goal</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><h3><span id="调用顺序-0-12">调用顺序  [0] [1][2]</span><a href="#调用顺序-0-12" class="header-anchor">#</a></h3><ul>
<li>Function Calling 整个功能的调用顺序大致如下<ul>
<li>声明函数：定义当前函数的名称，描述，以及对应的参数信息，并请求对应的接口；</li>
<li>解析函数参数：接受对应的接口返回，并解析对应的函数参数信息；</li>
<li>执行函数：根据对应的参数信息调用本地函数；</li>
<li>上报结果：将本地函数执行的结果上报给 Chat 接口；</li>
</ul>
</li>
</ul>
<img src="/www6vHomeAIGC/2022/11/16/gptFunctionCall/functioncall1.png" class>

<h3><span id="代码-2">代码 [2]</span><a href="#代码-2" class="header-anchor">#</a></h3><h3><span id="goal">goal</span><a href="#goal" class="header-anchor">#</a></h3><p> The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="http://lihuaxi.xjx100.cn/news/1382737.html">大模型开发(十一)：Chat Completions模型的Function calling功能详解</a> </p>
</li>
<li><p><a href="https://www.duidaima.com/Group/Topic/OtherTools/13709">如何使用Chat Completions接口的函数调用功能</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131892482">OpenAI开发系列（十一）：Function calling功能的实际应用流程与案例解析</a>   代码  流程图<br><a href="https://github.com/www6v/AIGC/tree/master/basic/Function-calling">代码</a>  git</p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131933871">OpenAI开发系列（十三）：利用Function calling功能开发基于大模型的实时天气查询助手</a> 未</p>
</li>
<li><p><a href="https://blog.csdn.net/Lvbaby_/article/details/131912170">OpenAI开发系列（十二）：Function calling功能的流程优化与多轮对话实现</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Function Call</category>
      </categories>
      <tags>
        <tag>Function Call</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Prompt Engineering</title>
    <url>/www6vHomeAIGC/2022/11/10/gptPromptEngineering/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#basic-prompting-2">Basic Prompting [2]</a><ul>
<li><a href="#zero-shot-prompting-3">Zero-Shot Prompting [3]</a></li>
<li><a href="#few-shot-prompting-3">Few-Shot Prompting [3]</a></li>
</ul>
</li>
<li><a href="#cot-2">CoT [2]</a><ul>
<li><a href="#chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</a></li>
<li><a href="#self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</a></li>
<li><a href="#tree-of-thoughts-tot">Tree of Thoughts (ToT)</a></li>
<li><a href="#cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</a></li>
<li><a href="#tips-and-extensions-2">Tips and Extensions   [2]</a></li>
</ul>
</li>
<li><a href="#automatic-prompt-design-2">Automatic Prompt Design [2]</a></li>
<li><a href="#six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</a><ul>
<li><a href="#write-clear-instructions">Write clear instructions</a></li>
<li><a href="#provide-reference-text">Provide reference text</a></li>
<li><a href="#split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</a></li>
<li><a href="#give-the-model-time-to-think">Give the model time to “think”</a></li>
<li><a href="#use-external-tools">Use external tools</a></li>
<li><a href="#test-changes-systematically">Test changes systematically</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E7%82%B9vs-%E7%BC%BA%E7%82%B9">优点vs 缺点</a><ul>
<li><a href="#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="basic-prompting-2">Basic Prompting [2]</span><a href="#basic-prompting-2" class="header-anchor">#</a></h1><h3><span id="zero-shot-prompting-3">Zero-Shot Prompting [3]</span><a href="#zero-shot-prompting-3" class="header-anchor">#</a></h3><h3><span id="few-shot-prompting-3">Few-Shot Prompting [3]</span><a href="#few-shot-prompting-3" class="header-anchor">#</a></h3><h1><span id="cot-2">CoT [2]</span><a href="#cot-2" class="header-anchor">#</a></h1><h3><span id="chain-of-thought-promptingcot-3">Chain-of-Thought Prompting(CoT) [3]</span><a href="#chain-of-thought-promptingcot-3" class="header-anchor">#</a></h3><ul>
<li>Few-shot CoT</li>
<li>Zero-shot COT<br><strong>“Let’s think step by step”</strong></li>
</ul>
<h3><span id="self-consistencycot-sc-3">Self-Consistency(CoT-SC) [3]</span><a href="#self-consistencycot-sc-3" class="header-anchor">#</a></h3><p>The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to <strong>select</strong> the most consistent answer.  </p>
<h3><span id="tree-of-thoughts-tot">Tree of Thoughts (ToT)</span><a href="#tree-of-thoughts-tot" class="header-anchor">#</a></h3><h3><span id="cot-vs-cot-sc-vs-tot-3">CoT vs. CoT-SC vs. ToT  [3]</span><a href="#cot-vs-cot-sc-vs-tot-3" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2022/11/10/gptPromptEngineering/TOT.jpg" class>

<h3><span id="tips-and-extensions-2">Tips and Extensions   [2]</span><a href="#tips-and-extensions-2" class="header-anchor">#</a></h3><p>Self-Ask </p>
<h1><span id="automatic-prompt-design-2">Automatic Prompt Design [2]</span><a href="#automatic-prompt-design-2" class="header-anchor">#</a></h1><ul>
<li>Automatic Chain-of-Thought (Auto-CoT) [3]</li>
</ul>
<h1><span id="six-strategies-for-getting-better-results1">Six strategies for getting better results[1]</span><a href="#six-strategies-for-getting-better-results1" class="header-anchor">#</a></h1><h3><span id="write-clear-instructions">Write clear instructions</span><a href="#write-clear-instructions" class="header-anchor">#</a></h3><p>   清晰的指令</p>
<h3><span id="provide-reference-text">Provide reference text</span><a href="#provide-reference-text" class="header-anchor">#</a></h3><h3><span id="split-complex-tasks-into-simpler-subtasks">Split complex tasks into simpler subtasks</span><a href="#split-complex-tasks-into-simpler-subtasks" class="header-anchor">#</a></h3><pre><code>复杂任务简单化
</code></pre>
<h3><span id="give-the-model-time-to-think">Give the model time to “think”</span><a href="#give-the-model-time-to-think" class="header-anchor">#</a></h3><p>   给模型时间去思考</p>
<h3><span id="use-external-tools">Use external tools</span><a href="#use-external-tools" class="header-anchor">#</a></h3><p>   使用外部工具</p>
<h3><span id="test-changes-systematically">Test changes systematically</span><a href="#test-changes-systematically" class="header-anchor">#</a></h3><h1><span id="优点vs-缺点">优点vs 缺点</span><a href="#优点vs-缺点" class="header-anchor">#</a></h1><h3><span id="优点">优点</span><a href="#优点" class="header-anchor">#</a></h3><p>简单  容易上手</p>
<h3><span id="缺点">缺点</span><a href="#缺点" class="header-anchor">#</a></h3><ul>
<li>上限有限  </li>
<li>模型适配<br>prompt要适配每个模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">Prompt engineering</a>  openai</li>
<li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">Prompt Engineering </a> paper</li>
<li><a href="https://www.promptingguide.ai/techniques">Prompt Engineering Guide</a> guide<br><a href="https://github.com/www6v/Prompt-Engineering-Guide">Prompt-Engineering-Guide </a> *** git</li>
</ol>
<p>1xx.   【社区第十三讲】 老刘说NLP线上交流  *** 很全 </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/682352630">[论文阅读] Prompt Engineering综述</a></p>
<p>1xx. <a href="https://blog.langchain.dev/the-prompt-landscape/">The Prompt Landscape</a>  langchain</p>
<p>1xx. <a href="https://colab.research.google.com/github/comet-ml/comet-llm/blob/main/examples/CometLLM_Prompts.ipynb">CometLLM - suite of LLMOps tools - track and visualize LLM prompts and chains</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/671915693">大模型 PUA 指南：来自 Google Meta Microsoft 等大厂</a> </p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/632369186">NLP（十三）：Prompt Engineering 面面观</a></p>
<p>1xx. <a href="https://github.com/brexhq/prompt-engineering?tab=readme-ov-file"> prompt-engineering</a> git</p>
<p>1xx. <a href="https://finisky.github.io/chain-of-thought-prompting-summary/">Chain-of-Thought Prompting 简读 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399405&idx=1&sn=75cc058ff83467aa6bf107cf69335e71">ChatGPT应用端的Prompt解析：从概念、基本构成、常见任务、构造策略到开源工具与数据集 </a></p>
<p>1xx. <a href="https://github.com/Eladlev/AutoPrompt">AutoPrompt Repo</a> git</p>
<h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="200">
<li><a href="https://mp.weixin.qq.com/s/nXoZJ4xfgihA2mnBQ8EdIQ">运维大模型探索之 Text2PromQL 问答机器人 </a>     架构图， 最后两个重点总结   未</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent</title>
    <url>/www6vHomeAIGC/2022/11/02/gptAgent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="agent-原理">Agent 原理</span><a href="#agent-原理" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Agent-10dbfe211084806fa87cfd37aed482ea?pvs=4">(原理)Agent </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Langchain</title>
    <url>/www6vHomeAIGC/2022/11/02/gptLangchain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#modules">Modules</a><ul>
<li><a href="#main-modules">main modules</a><ul>
<li><a href="#model-io">Model I&#x2F;O</a></li>
<li><a href="#retrieval">Retrieval</a></li>
<li><a href="#agent">Agent</a></li>
</ul>
</li>
<li><a href="#additional-modules">Additional modules</a><ul>
<li><a href="#chains">Chains</a></li>
<li><a href="#memory-10">Memory [10]</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#function-call">Function Call</a></li>
<li><a href="#%E5%BA%94%E7%94%A84">应用[4]</a></li>
<li><a href="#chains-1-89">Chains [1] [8][9]</a></li>
<li><a href="#templates7">Templates[7]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="modules">Modules</span><a href="#modules" class="header-anchor">#</a></h1><h2><span id="main-modules">main modules</span><a href="#main-modules" class="header-anchor">#</a></h2><h3><span id="model-ix2fo">Model I&#x2F;O</span><a href="#model-ix2fo" class="header-anchor">#</a></h3><ul>
<li>Language models  [10]        <ul>
<li>LLM</li>
<li>Chat Model</li>
<li><strong>Embedding</strong></li>
</ul>
</li>
<li>Prompts <ul>
<li>Prompt Template</li>
<li>Few-shot example</li>
<li>Example Selectors [类比选择]<br>关键字  相似度  长度</li>
</ul>
</li>
<li>Output parsers</li>
<li><strong>function call</strong>[2]</li>
</ul>
<h3><span id="retrieval">Retrieval</span><a href="#retrieval" class="header-anchor">#</a></h3><ul>
<li>Document Loaders</li>
<li>Text Splitters</li>
<li><strong>Retrievers</strong>[10]</li>
<li>VectorStores</li>
<li>index</li>
</ul>
<h3><span id="agent">Agent</span><a href="#agent" class="header-anchor">#</a></h3><ul>
<li>Plan-and-execute agents</li>
</ul>
<h2><span id="additional-modules">Additional modules</span><a href="#additional-modules" class="header-anchor">#</a></h2><h3><span id="chains">Chains</span><a href="#chains" class="header-anchor">#</a></h3><ul>
<li>2大类<ul>
<li>Chain interface[Legacy]</li>
<li>LangChain Expression Language (LCEL)<br>LCEL is a declarative way to compose chains.</li>
</ul>
</li>
<li>Foundational<ul>
<li>LLM</li>
<li>Sequential- SequentialChain</li>
<li><strong>Router</strong></li>
<li>Transformation</li>
</ul>
</li>
</ul>
<h3><span id="memory-10">Memory [10]</span><a href="#memory-10" class="header-anchor">#</a></h3><ul>
<li>帮语言模型补充上下文</li>
<li>ConversationBufferMemory</li>
<li>ConversationBufferWindowMemory<br>窗口</li>
<li>ConversationSummaryMemory</li>
<li>VectorStoreRetrieverMemory</li>
</ul>
<h1><span id="function-call">Function Call</span><a href="#function-call" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.base <span class="keyword">import</span> (</span><br><span class="line">    create_openai_fn_chain,</span><br><span class="line">    create_structured_output_chain,[<span class="number">2</span>]</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.citation_fuzzy_match <span class="keyword">import</span> (</span><br><span class="line">    create_citation_fuzzy_match_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.extraction <span class="keyword">import</span> (</span><br><span class="line">    create_extraction_chain,</span><br><span class="line">    create_extraction_chain_pydantic,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.qa_with_structure <span class="keyword">import</span> (</span><br><span class="line">    create_qa_with_sources_chain,</span><br><span class="line">    create_qa_with_structure_chain,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain.chains.openai_functions.tagging <span class="keyword">import</span> (</span><br><span class="line">    create_tagging_chain,</span><br><span class="line">    create_tagging_chain_pydantic,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h1><span id="应用4">应用[4]</span><a href="#应用4" class="header-anchor">#</a></h1><ul>
<li>Question &amp; Answering Using Documents As Context[3]</li>
<li>Extraction[Kor]</li>
<li>Evaluation</li>
<li>Querying Tabular Data[sqlite]</li>
<li>Code Understanding</li>
<li>Interacting with APIs</li>
<li>Chatbots</li>
</ul>
<h1><span id="chains-1-89">Chains [1] [8][9]</span><a href="#chains-1-89" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;stuff&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;map_reduce&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">&quot;refine&quot;</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chain = load_qa_chain(llm, chain_type=<span class="string">&quot;map_rerank&quot;</span>, verbose=<span class="literal">True</span>, return_intermediate_steps=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>链类型</th>
<th>整合方法</th>
<th>优缺点</th>
</tr>
</thead>
<tbody><tr>
<td>stuff</td>
<td>将所有内容放入一个提示中，输入LLM</td>
<td>简单、廉价、效果好&#x2F; 对输入文本有一定token限制</td>
</tr>
<tr>
<td>Map_reduce</td>
<td>每个问题和文本块单独给语言模型，并将答案汇总生成最终结果</td>
<td>输入任意数量文本，且并行处理&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Refine</td>
<td>迭代处理多个文本，基于前一个文档答案构建下一个答案</td>
<td>用于组合信息，依次构建答案&#x2F; 速度慢，费token</td>
</tr>
<tr>
<td>Map_rerank</td>
<td>每个文档单独调用LLM,并要求返回一个得分，然后选择最高的得分</td>
<td>需要告诉模型评分的规则&#x2F; 费token</td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2022/11/02/gptLangchain/chains-type.jpg" class>


<h1><span id="templates7">Templates[7]</span><a href="#templates7" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/gkamradt/langchain-tutorials">https://github.com/gkamradt/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-functioncall.py">functioncall</a></p>
</li>
<li><p><a href="https://github.com/www6v/pyExamples/blob/master/langchain/langchain-qaOnDoc.py">qaOnDoc</a></p>
</li>
<li><p><a href="https://github.com/www6v/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb">LangChain Cookbook Part 2: Use Cases</a><br> 10.公开课</p>
</li>
<li><p><a href="https://github.com/kyrolabs/awesome-langchain">https://github.com/kyrolabs/awesome-langchain</a></p>
</li>
<li><p><a href="https://github.com/Crossme0809/langchain-tutorials">https://github.com/Crossme0809/langchain-tutorials</a></p>
</li>
<li><p><a href="https://github.com/langchain-ai/langchain/blob/master/templates/docs/INDEX.md">Templates</a> *** docs<br><a href="https://templates.langchain.com/">templates</a> webui</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/666656208">吴恩达短课_LangChain</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/651216604">精华笔记：吴恩达 x LangChain 《使用LangChain构建与数据对话的聊天机器人》（下）</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2313918">一文入门最热的LLM应用开发框架LangChain</a> 未</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2331337">大模型LangChain框架基础与使用示例</a> 未</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Langchain</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)RAG</title>
    <url>/www6vHomeAIGC/2022/11/02/gptRAG/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="rag综述">RAG综述</span><a href="#rag综述" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/RAG-108bfe21108480be9c7ee46ff02a1ad6?pvs=4">(综述)RAG</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)大模型</title>
    <url>/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#llms%E7%9A%84%E8%83%8C%E6%99%AF1">LLMs的背景[1]</a><ul>
<li><a href="#scaling-law-of-llms">Scaling law of LLMs</a></li>
<li><a href="#llms%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B">LLMs的涌现能力</a></li>
<li><a href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF">大语言模型的关键技术 ***</a></li>
</ul>
</li>
<li><a href="#pre-training1">Pre-training[1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86">数据收集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">模型训练 ***</a></li>
</ul>
</li>
<li><a href="#adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</a><ul>
<li><a href="#%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98">指令调优 ***</a><ul>
<li><a href="#%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%9E%84%E5%BB%BA">格式化实例的构建</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%AD%96%E7%95%A5">指令微调策略</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%9A%84%E6%95%88%E6%9E%9C">指令微调的效果</a></li>
</ul>
</li>
<li><a href="#%E5%AF%B9%E9%BD%90%E8%B0%83%E4%BC%98">对齐调优</a></li>
<li><a href="#%E9%AB%98%E6%95%88%E8%B0%83%E4%BC%98">高效调优</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="llms的背景1">LLMs的背景[1]</span><a href="#llms的背景1" class="header-anchor">#</a></h1><h3><span id="scaling-law-of-llms">Scaling law of LLMs</span><a href="#scaling-law-of-llms" class="header-anchor">#</a></h3><ul>
<li>KM scaling law</li>
<li>Chinchilla Scaling law</li>
</ul>
<h3><span id="llms的涌现能力">LLMs的涌现能力</span><a href="#llms的涌现能力" class="header-anchor">#</a></h3><ul>
<li>in-context learning</li>
<li>instruction following</li>
<li>step-by-step reasoning</li>
</ul>
<h3><span id="大语言模型的关键技术">大语言模型的关键技术 ***</span><a href="#大语言模型的关键技术" class="header-anchor">#</a></h3><ul>
<li>Scaling</li>
<li>Training</li>
<li>Ability Eliciting</li>
<li>Alignment Tuning</li>
<li>Tool Manipulation</li>
</ul>
<h1><span id="pre-training1">Pre-training[1]</span><a href="#pre-training1" class="header-anchor">#</a></h1><h3><span id="数据收集">数据收集</span><a href="#数据收集" class="header-anchor">#</a></h3><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><h3><span id="模型训练">模型训练 ***</span><a href="#模型训练" class="header-anchor">#</a></h3><ul>
<li><p>优化设置</p>
<ul>
<li>Batch Training</li>
<li>Learning Rate</li>
<li>Optimizer</li>
<li>Stabilizing the Training</li>
</ul>
</li>
<li><p>可扩展的训练技巧</p>
<ul>
<li>3D并行<br>数据并行 +  流水线并行 + 张量并行</li>
<li>ZeRO</li>
<li>混合精度训练</li>
<li>总体训练建议</li>
</ul>
</li>
</ul>
<h1><span id="adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</span><a href="#adaptation-tuning-of-llms1" class="header-anchor">#</a></h1><h3><span id="指令调优">指令调优 ***</span><a href="#指令调优" class="header-anchor">#</a></h3><p>本质上，指令微调是在<strong>自然语言格式的实例（instance）集合上</strong>微调预训练后的 LLM 的方法 [62]。</p>
<p>指令微调后，LLM 可以展现出<strong>泛化到未见过任务</strong>的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。</p>
<h5><span id="格式化实例的构建">格式化实例的构建</span><a href="#格式化实例的构建" class="header-anchor">#</a></h5><ul>
<li>格式化已有数据集</li>
<li>格式化人类需求</li>
<li>构建实例的关键因素<ul>
<li><strong>增加指令</strong></li>
<li><strong>设计格式</strong></li>
</ul>
</li>
</ul>
<p>总的来说，指令<strong>多样性似乎比实例数量更重要</strong></p>
<h5><span id="指令微调策略">指令微调策略</span><a href="#指令微调策略" class="header-anchor">#</a></h5><ul>
<li><p><strong>平衡数据分布</strong><br>一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。<br>此外，根据最近的研究发现 [64, 99]，<strong>提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例</strong>通常可以带来<strong>性能提升</strong>。</p>
</li>
<li><p>结合指令微调和预训练<br>为了使微调过程更加有效和稳定，OPT-IML [99] 在<strong>指令微调期间加入了预训练数据</strong>，这可以看作是对模型的正则化（regularization）。</p>
</li>
</ul>
<p>具体而言，GLM-130B [97] 和 Galactica [34] 将<strong>指令格式数据集作为预训练语料库的一小部分来预训练 LLM</strong>，这有可能同时获得预训练和指令微调的优势。</p>
<h5><span id="指令微调的效果">指令微调的效果</span><a href="#指令微调的效果" class="header-anchor">#</a></h5><ul>
<li>性能改进<br>最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，**表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]**。 【普适性】</li>
</ul>
<p>此外，**经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]**。</p>
<ul>
<li>任务泛化性<br>todo</li>
</ul>
<h3><span id="对齐调优">对齐调优</span><a href="#对齐调优" class="header-anchor">#</a></h3><h3><span id="高效调优">高效调优</span><a href="#高效调优" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://aibox.ruc.edu.cn/docs/2023-08/cb9badcb213f4c8b89d00d579eed4a4c.pdf">大语言模型综述</a> 中文  v10<br>  <a href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf">大语言模型综述</a> 中文<br>  <a href="https://github.com/www6v/LLMSurvey">LLMSurvey Repo</a>  git<br>  <a href="https://zhuanlan.zhihu.com/p/630203554">[论文]大语言模型综述</a><br>  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400817&idx=1&sn=c1ed1c9c87bf2526e02d21d84429c5cf">详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结</a><br>  <a href="https://zhuanlan.zhihu.com/p/662673023">大模型综述-A Survey of Large Language Models</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408221&idx=1&sn=2874583ed668ae0b89889c81a4ab8d79">值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/381282229">43页预训练模型综述（清华、复旦、人大）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT  学习资源</title>
    <url>/www6vHomeAIGC/2022/08/01/gptStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E7%A8%8B">工程</a></li>
<li><a href="#%E8%AF%BE%E7%A8%8B">课程</a><ul>
<li><a href="#%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4">极客时间</a></li>
<li><a href="#%E7%9F%A5%E4%B9%8E">知乎</a></li>
<li><a href="#%E6%B8%85%E5%8D%8E">清华</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6">百度</a></li>
<li><a href="#%E4%B9%9D%E5%A4%A9">九天</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E4%B8%9A%E7%95%8C">工业界</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="工程">工程</span><a href="#工程" class="header-anchor">#</a></h1><ul>
<li><a href="https://github.com/www6v/openai-cookbook">openai-cookbook</a><br><a href="https://cookbook.openai.com/">cookbook.openai</a></li>
</ul>
<h1><span id="课程">课程</span><a href="#课程" class="header-anchor">#</a></h1><h3><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h3><ul>
<li><a href="https://shimo.im/docs/47kgM6NewnSO613V">尚硅谷×极客时间《AI 大模型实战训练营》大纲</a> </li>
<li><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a> </li>
<li><a href="https://w.1yb.co/KqBR58E">《AI 大模型微调训练营》大纲</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541101">ChatGPT 从 0 到 1</a>  基础</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541201">ChatGPT 和预训练模型实战课</a></li>
</ul>
<h3><span id="知乎">知乎</span><a href="#知乎" class="header-anchor">#</a></h3><ul>
<li><a href="https://agiclass.feishu.cn/docx/DDzxdQZBooXw9Jx4DdWcLZjLnHd">《AI 大模型全栈工程师》课程表（第 02 期） </a>  </li>
<li><a href="https://www.zhihu.com/people/dou-hong-jian-44/posts">AI Box专栏</a>  中国人大  AI ***<br>大模型survey</li>
</ul>
<h3><span id="清华">清华</span><a href="#清华" class="header-anchor">#</a></h3><ul>
<li><a href="https://www.zhihu.com/education/video-course/1545850719483392000">【清华 NLP X OpenBMB】大模型公开课｜带你从入门到实战</a>  V ***</li>
</ul>
<h3><span id="百度">百度</span><a href="#百度" class="header-anchor">#</a></h3><p><a href="https://cloud.baidu.com/qianfandev/topic/267956">《大模型应用实践》实训营</a></p>
<h3><span id="九天">九天</span><a href="#九天" class="header-anchor">#</a></h3><p><a href="https://appze9inzwc2314.pc.xiaoe-tech.com/p/t_pc/goods_pc_detail/goods_detail/p_64467371e4b0cf39e6c0c026?fromH5=true&entry_type=2002&share_type=5&type=3&entry=2">大模型技术实战课 </a></p>
<h1><span id="工业界">工业界</span><a href="#工业界" class="header-anchor">#</a></h1><p><a href="https://rocketmq-learning.com/">rocketmq-learning 社区</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning</title>
    <url>/www6vHomeAIGC/2022/06/11/aiDeepLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="deep-learning">Deep Learning</span><a href="#deep-learning" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Deep-Learning-10dbfe21108480b9affbf52a3b5bb13e?pvs=4">Deep Learning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/www6vHomeAIGC/2022/06/07/aiMachineLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="机器学习算法">机器学习算法</span><a href="#机器学习算法" class="header-anchor">#</a></h2><ul>
<li><p>监督式学习</p>
<ul>
<li><p>Linear Models</p>
<ul>
<li>逻辑回归 (Logistic Regression)<br><strong>离散</strong><br>逻辑回归其实是一个分类算法而不是回归算法。</li>
<li>线性回归 (Linear Regression)<br><strong>连续</strong></li>
</ul>
</li>
<li><p>Nearest Neighbors</p>
<ul>
<li>K邻近算法，KNN</li>
</ul>
</li>
<li><p>决策树 Decision Trees</p>
</li>
<li><p>Support Vector Machines, SVM [2]</p>
<ul>
<li>可分 <ul>
<li>线性可分</li>
<li>线性不可分</li>
</ul>
</li>
<li>超平面<ul>
<li>低纬升到高纬</li>
</ul>
</li>
</ul>
</li>
<li><p>Naive Bayes</p>
</li>
<li><p>随机森林</p>
</li>
</ul>
</li>
<li><p>无监督式学习</p>
<ul>
<li>关联规则 </li>
<li>K-means聚类算法<br>质心（centroids），距离</li>
</ul>
</li>
<li><p>强化学习</p>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>Classification<br>Identifying which category an object belongs to.</li>
<li>Regression<br>  Predicting a continuous-valued attribute associated with an object.</li>
<li>Clustering<br>  Automatic grouping of similar objects into sets.  </li>
<li>Dimensionality reduction<br>  Reducing the number of random variables to consider.</li>
</ul>
<img src="/www6vHomeAIGC/2022/06/07/aiMachineLearning/scikit-learn.png" class title="scikit-learn overview">



<h2><span id="按学习模型划分-4">按学习模型划分 [4]</span><a href="#按学习模型划分-4" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/479973669">【机器学习算法】10种常见机器学习算法+Python代码</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/b8227eac1fa6">机器学习–有监督–支持向量机SVM</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/supervised_learning.html">Supervised learning</a></p>
</li>
<li><p><a href="https://blog.csdn.net/hustlei/article/details/121803226">人工智能导论(6)——机器学习(Machine Learning)</a> ***</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-工具和应用</title>
    <url>/www6vHomeAIGC/2022/05/09/gpt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#platform">Platform</a></li>
<li><a href="#tools-mix">Tools &amp; Mix</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE">思维导图</a></li>
<li><a href="#%E8%A7%86%E9%A2%91">视频</a></li>
<li><a href="#%E8%8B%B1%E8%AF%AD">英语</a></li>
</ul>
</li>
<li><a href="#%E5%AE%A2%E6%88%B7%E7%AB%AF">客户端</a></li>
<li><a href="#chrome-plugin">Chrome plugin</a></li>
<li><a href="#%E5%88%9B%E4%B8%9A">创业</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="platform">Platform</span><a href="#platform" class="header-anchor">#</a></h1><ul>
<li>国外<br><a href="https://poe.com/ChatGPT">Poe</a> ***</li>
<li>国内<br><a href="https://saas.edu360.cn/system/chatgpt">实战云</a> gpt3.5  gpt4<br><a href="https://www.feijix.com/n/y0BnXI">ChatGPT使用指南！</a>   ***<br><a href="https://www.1888ai.com/base/chat">灵犀百通</a>  gpt3.5<br><a href="https://gpt.91chat-ai.cn/chat">ChatGpt PLUS</a><br><a href="https://yiyan.baidu.com/">文心一言</a></li>
</ul>
<h1><span id="tools-amp-mix">Tools &amp; Mix</span><a href="#tools-amp-mix" class="header-anchor">#</a></h1><ul>
<li><p>GPT学习宝典</p>
<ul>
<li>聚合<ul>
<li><a href="https://gpt.candobear.com/toolbox">GPT  工具箱</a></li>
</ul>
</li>
<li>教程<ul>
<li><a href="https://gpt.candobear.com/courses">学习资料</a></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe">极客时间 AIGC 知识库</a> *** </p>
<ul>
<li>聚合<ul>
<li><a href="https://gp477l8icq.feishu.cn/wiki/M1uCwFNjkiAGC7k30TaclZqknPh">AI工具大全</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/RpabwPG9niFEu9kwJAQcAGxenDg">AI主流工具精选</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/VJ9ewqfOgiyrbQksbyLcrODtnkb">AI经典项目</a></li>
<li><a href="https://gp477l8icq.feishu.cn/wiki/QVV6w3XstiR7hlkK53Bc8f9DnMf">AI导航站</a></li>
</ul>
</li>
<li><a href="https://longalong.feishu.cn/wiki/wikcneAKpN3u473N7J9EAC4Ga0b">应用与变现案例</a></li>
</ul>
</li>
<li><p><a href="https://www.ailookme.com/">AI 工具箱</a>  *** </p>
</li>
<li><p><a href="https://gptdoc.sparkai.chat/">ChatGPT Tutorial 101</a></p>
</li>
</ul>
<h1><span id="应用">应用</span><a href="#应用" class="header-anchor">#</a></h1><h3><span id="思维导图">思维导图</span><a href="#思维导图" class="header-anchor">#</a></h3><p><a href="https://albus.org/">albus</a></p>
<h3><span id="视频">视频</span><a href="#视频" class="header-anchor">#</a></h3><p><a href="https://b.jimmylv.cn/">BibiGPT</a><br><a href="https://crucible.docnavigator.in/">Youtube tools</a></p>
<h3><span id="英语">英语</span><a href="#英语" class="header-anchor">#</a></h3><p><a href="https://callannie.ai/signin">callannie</a></p>
<h1><span id="客户端">客户端</span><a href="#客户端" class="header-anchor">#</a></h1><ul>
<li>ChatGPT 客户端<br> windows， mac</li>
</ul>
<h1><span id="chrome-plugin">Chrome plugin</span><a href="#chrome-plugin" class="header-anchor">#</a></h1><ul>
<li><p>WebChatGPT[instatlled]</p>
</li>
<li><p>AIPRM for ChatGPT[instatlled]</p>
</li>
<li><p>ChatGPT Sidebar<br>要注册账号, 需要api token</p>
</li>
<li><p>ChatHub  [instatlled]<br> chatgpt + new bing<br><a href="https://github.com/chathub-dev/chathub">ChatHub </a></p>
</li>
<li><p>OpenAI Translator<br><a href="https://github.com/yetone/openai-translator">openai-translator</a><br>要注册账号, 需要api token</p>
</li>
</ul>
<h1><span id="创业">创业</span><a href="#创业" class="header-anchor">#</a></h1><ul>
<li><a href="https://gpt3demo.com/map">GPT-3 Demo</a><ul>
<li>聊天机器人</li>
<li>代码辅助</li>
<li>写作应用</li>
<li>游戏</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能-学习资源</title>
    <url>/www6vHomeAIGC/2022/01/22/aiStudyResouce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="书籍">书籍</span><a href="#书籍" class="header-anchor">#</a></h2><ul>
<li>理论<ul>
<li>《人工智能的数据基础》</li>
<li>《统计学习方法》 v2</li>
<li>The Hundred-Page Machine Learning Book - 入门</li>
<li>西瓜书 ***</li>
<li>花书</li>
</ul>
</li>
<li>框架<ul>
<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, v3 - 入门</li>
<li>Deep Learning with PyTorch - 入门</li>
<li>Deep Learning with Python - v2 - keras库</li>
</ul>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>浙大 - 吴浩基   ***</li>
<li>周志华 《机器学习初步》 *** </li>
<li>Coursera吴恩达- 《machine learning》+ 笔记  入门  *** </li>
<li><a href="https://www.bilibili.com/video/av79340208/">Machine Learning A-Z Hands-On Python &amp; R In Data Science</a>  Udemy 入门  ***</li>
<li><a href="https://www.bilibili.com/video/BV1KB4y1E73v">【人工智能系列】【中文】机器学习A-Z Machine Learning in Chinese(前7部分)</a></li>
<li><a href="https://www.bilibili.com/video/BV1jF411A7VF/">[Coursera公开课] [机器学习专项课程1&#x2F;4] 机器学习基础：案例研究</a>  ***</li>
<li><a href="https://www.bilibili.com/video/BV1Bg411Z77N">聚类算法：层次聚类、k-means 聚类、k-medoids 聚类、密度聚类</a>  ***</li>
</ul>
<h2><span id="深度学习">深度学习</span><a href="#深度学习" class="header-anchor">#</a></h2><ul>
<li>《动手学深度学习- 第二版 -pyTorch》  ***<br>b站有视频课<br><a href="http://zh.d2l.ai/index.html">《动手学深度学习》</a> 在线<br>电子书+jupternote代码</li>
<li>《神经网络和深度学习 》 复旦  </li>
<li>Coursera吴恩达《深度学习》 + 笔记  ***</li>
<li>老唐 ***</li>
<li>莫烦Python </li>
<li>北京大学 TensorFlow 2.0</li>
<li>算法可视化  ***</li>
<li>李宏毅 台湾</li>
</ul>
<h2><span id="nlp-amp-大模型">NLP &amp; 大模型</span><a href="#nlp-amp-大模型" class="header-anchor">#</a></h2><ul>
<li><p><a href="https://www.zhihu.com/education/video-course/1546509363711614976">沈向洋带你读论文——CV &amp; NLP 专题</a> V </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1C14y147dp">2022年首发！B站讲的最好的【NLP自然语言处理】保姆级教程！</a>  V  有实践  *** </p>
</li>
<li><p>MLNLP第六期学术研讨会开始报名</p>
</li>
</ul>
<h2><span id="知识图谱">知识图谱</span><a href="#知识图谱" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1VT411G7Y6?p=6">【国家级精品课】浙江大学教授（新全44集）知识图谱公开课分享</a>  ***</li>
</ul>
<h2><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h2><ul>
<li>极客时间<ul>
<li>《AI 技术内参》  洪亮劼   全 ***</li>
<li>《机器学习 40 讲》  王天一 </li>
<li>《人工智能基础课》  王天一<br> 机器学习，深度学习</li>
<li>《成为AI产品经理》  刘海丰 京东   </li>
<li>《零基础实战机器学习》 黄佳  ***</li>
<li>《PyTorch深度学习实战》方远 大厂</li>
<li><a href="https://time.geekbang.org/course/intro/100023001?tab=catalog">TensorFlow 快速入门与实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/315">TensorFlow 2 项目进阶实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/100046401">NLP 实战高手课</a></li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>深度学习在CTR预估的应用   张俊林</li>
<li>深度学习在图像理解中的应用  熊鹏飞</li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>知识图谱技术实践  邵蓥侠</li>
</ul>
</li>
<li>极客训练营<br>-《机器学习训练营1期》  视频课</li>
</ul>
<h2><span id="中国大学mooc">中国大学MOOC</span><a href="#中国大学mooc" class="header-anchor">#</a></h2><ul>
<li>中国大学MOOC <a href="https://www.icourse163.org/learn/HIT-1206320802?tid=1468208513#/learn/announce">深度学习基础</a>   哈尔滨工业大学</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/FUDAN-1205806833">深度学习及其应用</a>   复旦</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/ZUCC-1206146808">深度学习应用开发-TensorFlow实践</a>  浙大城市学院</li>
</ul>
<h2><span id="培训">培训</span><a href="#培训" class="header-anchor">#</a></h2><ul>
<li><a href="http://bit.baidu.com/">百度技术培训中心</a>  *** 认证， 自动驾驶，人工智能培训  </li>
<li><a href="http://bit.baidu.com/courseRouteDetail?id=111">人工智能学习路线</a>  百度技术培训中心</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能 知识点</title>
    <url>/www6vHomeAIGC/2022/01/22/aiOverview/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="人工智能引论知识点">人工智能引论知识点</span><a href="#人工智能引论知识点" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2022/01/22/aiOverview/ai-overview.png" class>

<blockquote>
<p>62个知识点，9个高阶知识点(研究生课程)</p>
</blockquote>
<h2><span id="美国k12-ai知识点">美国K12 AI知识点</span><a href="#美国k12-ai知识点" class="header-anchor">#</a></h2><ul>
<li>智能感知</li>
<li>表示和推理</li>
<li>机器学习</li>
<li>自然交互能力</li>
<li>对社会的影响</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1Wa41157U4?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">吴飞教授解读：人工智能知识点全景图：迈向智能+时代蓝皮书</a> video<br><a href="https://www.163.com/dy/article/HFAFUJPM051193U6.html">人工智能知识点全景图：迈向智能+时代蓝皮书</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>AI 应用场景</title>
    <url>/www6vHomeAIGC/2021/08/11/ai/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E4%B8%8E%E8%A1%8C%E4%B8%9A-2">应用与行业 [2]</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-4">计算机视觉 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">应用场景</a></li>
<li><a href="#%E8%AF%AD%E9%9F%B3%E6%8A%80%E6%9C%AF-4">语音技术 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1">应用场景</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-4">自然语言处理 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2">应用场景</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2021/08/11/ai/ai.png" class title="AI">

<ul>
<li>人工智能的三个层面<ul>
<li>计算智能<br>能算能存</li>
<li>感知智能<br>能听会说， 能看会认</li>
<li>认知智能<br>能理解，会思考</li>
</ul>
</li>
</ul>
<h2><span id="应用与行业-2">应用与行业 [2]</span><a href="#应用与行业-2" class="header-anchor">#</a></h2><ul>
<li>健康码<img src="/www6vHomeAIGC/2021/08/11/ai/ai-hangye.png" class title="行业"></li>
</ul>
<h2><span id="计算机视觉-4">计算机视觉 [4]</span><a href="#计算机视觉-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>图像分类<ul>
<li>计算机视觉的核心问题<ul>
<li>细粒度图像分类</li>
</ul>
</li>
<li>人脸识别<ul>
<li>身份确认</li>
<li>身份查找</li>
</ul>
</li>
</ul>
</li>
<li>图像重建</li>
<li>目标检测<ul>
<li>物体定位</li>
<li>热门方向，领域<ul>
<li>在无人驾驶领域很重要</li>
<li>机器人导航</li>
<li>智能视频监控</li>
<li>工业检查</li>
</ul>
</li>
<li>关键问题<ul>
<li>小目标 高精度检测</li>
<li>多类别物体检测</li>
</ul>
</li>
</ul>
</li>
<li>图像搜索</li>
<li>图像分割<ul>
<li>核心问题</li>
<li>三类(逐层递进)<ul>
<li>语义分割</li>
<li>实例分割</li>
<li>全景分割</li>
</ul>
</li>
<li>应用场景</li>
</ul>
</li>
<li>目标跟踪</li>
</ul>
<h2><span id="语音技术-4">语音技术 [4]</span><a href="#语音技术-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>语音识别<ul>
<li>语音转文字</li>
<li>应用<ul>
<li>智能音响</li>
<li>语音输入发</li>
</ul>
</li>
</ul>
</li>
<li>语音合成 <ul>
<li>文字转语音 TTS</li>
<li>应用<ul>
<li>人机交互</li>
<li>语音客服</li>
<li>虚拟偶像-腾讯AI主播 艾灵</li>
</ul>
</li>
</ul>
</li>
<li>声纹识别<ul>
<li>微信的声音锁功能</li>
</ul>
</li>
</ul>
<h2><span id="自然语言处理-4">自然语言处理 [4]</span><a href="#自然语言处理-4" class="header-anchor">#</a></h2><ul>
<li>人工智能的最高境界</li>
</ul>
<h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>文本分类<ul>
<li>新闻分类  </li>
<li>邮件自动回复，垃圾邮件</li>
<li>客服聊天情感分析</li>
<li>内容审核</li>
</ul>
</li>
<li>机器翻译<ul>
<li>在线多语言翻译</li>
<li>会议中的语音同传</li>
<li>翻译机</li>
<li>跨语言检索</li>
</ul>
</li>
<li>知识图谱<ul>
<li>认知智能</li>
</ul>
</li>
<li>对话系统<ul>
<li>任务导向 - 问答系统</li>
<li>非任务导向 - 聊天机器人</li>
</ul>
</li>
<li>信息检索</li>
<li>文本生成<ul>
<li>写作机器人</li>
</ul>
</li>
</ul>
<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li>机器可以看   -  计算机视觉 </li>
<li>机器可以听   -  语音技术</li>
<li>机器可以理解 -  自然语言处理</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://cloud.tencent.com/edu/learning/course-3460-61199">腾讯云人工智能从业者认证线上培训课程</a> </p>
<ol>
<li>1.1  </li>
<li>1.2 </li>
<li>1.4 未</li>
<li>1.5</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>应用场景</category>
      </categories>
      <tags>
        <tag>应用场景</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-Code</title>
    <url>/www6vHomeAIGC/2021/05/28/gptPromptCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E5%85%B7">工具</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%AE%80%E5%8D%95%E4%BB%BB%E5%8A%A1-1">代码相关-简单任务 [1]</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3-%E7%B9%81%E7%90%90%E5%B7%A5%E4%BD%9C-1">代码相关- 繁琐工作 [1]</a></li>
<li><a href="#%E8%BF%90%E7%BB%B4-ops-4">运维 Ops [4]</a><ul>
<li><a href="#%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80-vs-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80">编程语言 vs 自然语言</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="工具">工具</span><a href="#工具" class="header-anchor">#</a></h1><ul>
<li>Copilot *** - 收费</li>
<li>AWS CodeWhispter</li>
<li>Cursor ***</li>
<li>tabnine 免费</li>
<li>Code Llama  - 开源</li>
</ul>
<h1><span id="代码相关-简单任务-1">代码相关-简单任务 [1]</span><a href="#代码相关-简单任务-1" class="header-anchor">#</a></h1><ul>
<li><p><strong>注释</strong><br> 你作为一名程序员，请解释一下下面这段代码</p>
</li>
<li><p><strong>防御性编程</strong><br> 请为这段代码增加防御性编程的功能</p>
</li>
<li><p>写单元测试 </p>
</li>
<li><p><strong>时间复杂度  time complexity</strong><br> 这段代码的时间复杂度是多少</p>
</li>
<li><p>流程图<br> 画出redis master和slave之间同步的流程图</p>
</li>
<li><p>Writing shell script</p>
</li>
<li><p>Writing git commands<br>一个分支中的代码合并到另一个分支中</p>
</li>
<li><p><strong>Improve code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  How do i improve this code?</span><br><span class="line">  fruits = [<span class="string">&quot;apple&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;cherry&quot;</span>]</span><br><span class="line">  newlist = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> fruits:</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&quot;a&quot;</span> <span class="keyword">in</span> x:</span><br><span class="line">    newlist.append(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(newlist)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Translating Code</strong> 代码转换</p>
<ul>
<li>Convert this Python code to Javascript    </li>
<li>请把下面这段python代码转换成Java代码</li>
</ul>
</li>
</ul>
<h1><span id="代码相关-繁琐工作-1">代码相关- 繁琐工作 [1]</span><a href="#代码相关-繁琐工作-1" class="header-anchor">#</a></h1><ul>
<li><p>Building API</p>
<ul>
<li>I need an API built with express.js to return the list of products. Each product should have attributes like ID, title, description, price and imageUrl</li>
<li>modify the code and  retrieve the products from a MongoDB database</li>
<li>use TypeScript in this code</li>
<li>Generate this API using Python and FastAPI</li>
</ul>
</li>
<li><p><strong>Generating Dummy Data</strong></p>
<ul>
<li>Generate dummy data for a table called customers. Each customer should have an ID, first name, last name and city.</li>
<li>I don’t need a Javascript. Just give the data.</li>
<li>Create a Python class for storing these objects.</li>
</ul>
</li>
<li><p><strong>SQL</strong></p>
<ul>
<li>write a SQL query to generate a table called products with these columns：<br>ID（int）<br>title（string）<br>category(int)</li>
<li>write a query to retrieve the top 5 customers in Shanghai</li>
<li>Revise this query and join the customers table with the orders table to find out how much each cumster has spent. Then pick the top 5 who have spent the most.</li>
</ul>
</li>
<li><p>正则  [2]</p>
</li>
<li><p>CronJob [2]</p>
</li>
<li><p>K8s</p>
</li>
</ul>
<h1><span id="运维-ops-4">运维 Ops [4]</span><a href="#运维-ops-4" class="header-anchor">#</a></h1><h2><span id="编程语言-vs-自然语言">编程语言 vs 自然语言</span><a href="#编程语言-vs-自然语言" class="header-anchor">#</a></h2><table>
<thead>
<tr>
<th>语言类型</th>
<th>执行原理</th>
</tr>
</thead>
<tbody><tr>
<td>C++语言</td>
<td>C++语言 –&gt; 编译器&#x2F;链接器 –&gt; 既定任务</td>
</tr>
<tr>
<td>Java语言</td>
<td>Java语言 –&gt; 编译器&#x2F;虚拟机 –&gt; 既定任务</td>
</tr>
<tr>
<td>Python语言</td>
<td>Python语言 –&gt; 解释器 –&gt; 既定任务</td>
</tr>
<tr>
<td>人类自然语言</td>
<td>人类自然语言 –&gt; LLMs –&gt; 各种后端组件 –&gt; 既定任务</td>
</tr>
</tbody></table>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1Z84y1G7nY/">【ChatGPT】面向程序员的ChatGPT使用教程38种方式来提升生产力</a> V</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a><br>03, 04, 06<br><a href="https://github.com/www6v/AICoder">AICoder</a> git</li>
<li><a href="https://cloud.tencent.com/developer/article/2207540">ChatGPT 帮我跑了一个完整的 DevOps 流水线，离了个大谱…</a><br><a href="https://github.com/www6v/AICoder/tree/master/Cursor/">Gin on K8s</a> git</li>
<li><a href="https://www.promptops.com/">PromptOps</a>    </li>
<li><a href="https://www.geeksforgeeks.org/chatgpt-prompts-for-software-developers/">Top 20 ChatGPT Prompts For Software Developers</a> 未</li>
</ol>
<pre><code>

</code></pre>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt-How to use</title>
    <url>/www6vHomeAIGC/2021/05/26/gptPrompt/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B9%94%E5%93%88%E9%87%8C%E6%B2%9F%E9%80%9A%E8%A7%86%E7%AA%97-4-%E8%B1%A1%E9%99%90">乔哈里沟通视窗-4 象限</a><ul>
<li><a href="#%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93gpt%E7%9F%A5%E9%81%93">你不知道，GPT知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B9%9F%E7%9F%A5%E9%81%93">你知道，GPT也知道</a></li>
<li><a href="#%E4%BD%A0%E7%9F%A5%E9%81%93gpt%E4%B8%8D%E7%9F%A5%E9%81%93">你知道，GPT不知道</a></li>
<li><a href="#%E4%BD%A0%E5%92%8Cgpt%E9%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93">你和GPT都不知道</a></li>
</ul>
</li>
<li><a href="#%E8%BE%BE%E5%85%8B%E6%95%88%E5%BA%94">达克效应</a><ul>
<li><a href="#%E6%A3%80%E9%AA%8C%E8%87%AA%E5%B7%B1%E8%AE%A4%E7%9F%A5%E8%83%BD%E5%8A%9B%E6%B0%B4%E5%B9%B3%E6%8F%90%E9%97%AE%E5%8F%A5%E5%BC%8F">检验自己认知&#x2F;能力水平提问句式</a></li>
</ul>
</li>
<li><a href="#%E7%9F%A5%E9%81%93%E5%81%9A%E5%88%B0">知道做到</a></li>
<li><a href="#%E8%A7%92%E8%89%B2%E5%85%B3%E7%B3%BB">角色关系</a></li>
<li><a href="#%E9%80%9A%E7%94%A8">通用</a><ul>
<li><a href="#%E6%B2%9F%E9%80%9A%E6%A8%A1%E5%BC%8F">沟通模式</a></li>
<li><a href="#%E5%BD%92%E7%BA%B3">归纳</a></li>
<li><a href="#%E6%80%9D%E7%BB%B4%E9%93%BE">思维链</a></li>
<li><a href="#%E6%B2%A1%E5%95%A5%E7%94%A8">没啥用</a></li>
</ul>
</li>
<li><a href="#prompt-how-to-use">Prompt - How to use</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="乔哈里沟通视窗-4-象限">乔哈里沟通视窗-4 象限</span><a href="#乔哈里沟通视窗-4-象限" class="header-anchor">#</a></h1><h3><span id="你不知道gpt知道">你不知道，GPT知道</span><a href="#你不知道gpt知道" class="header-anchor">#</a></h3><p>1、元问题：我想了解xxxx，我应该向你问哪些问题？<br>2、请给我列出xxx领域&#x2F;行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。<br>3、请详细介绍一下elon musk的主要生平事迹。请详细介绍一下tesla这家企业的发展历程。</p>
<h3><span id="你知道gpt也知道">你知道，GPT也知道</span><a href="#你知道gpt也知道" class="header-anchor">#</a></h3><p>检验认知：<br>1、对于xxx主题&#x2F;技能，你认为哪些是我必须理解和掌握的核心要点？<br>2、我理解的xxx是这样的，你觉得我的理解对吗？<br>3、我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？<br>4、我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？</p>
<p>扩充认知：<br>1、我知道xxx的概念，我想知道更多关于xxx的信息。<br>2、我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？<br>3、我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？<br>4、我想要在xxx领域有所创新，你能提供一些启发或想法吗？<br>5、我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？<br>6、我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？<br>7、（背景信息xxx），我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。<br>8、我是一个xx新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？</p>
<h3><span id="你知道gpt不知道">你知道，GPT不知道</span><a href="#你知道gpt不知道" class="header-anchor">#</a></h3><p>介绍背景现象之后可以向gpt发问，你怎么看待这种现象？可能的原因有哪些？这会对xxx产生什么样的影响？你觉得xxx应该怎么做？</p>
<h3><span id="你和gpt都不知道">你和GPT都不知道</span><a href="#你和gpt都不知道" class="header-anchor">#</a></h3><p>如果xxx，这对社会会产生什么影响？</p>
<h1><span id="达克效应">达克效应</span><a href="#达克效应" class="header-anchor">#</a></h1><h3><span id="检验自己认知x2f能力水平提问句式">检验自己认知&#x2F;能力水平提问句式</span><a href="#检验自己认知x2f能力水平提问句式" class="header-anchor">#</a></h3><p>1、为了测试我对xxx的理解程度，你会问我什么问题来检验我的水平，最少10个。<br>2、我是xx领域的专家，你会问我哪些问题来检验我的专业水平？<br>3、追问一句，这些我都懂，还有更专业更细更深的问题吗？<br>4、你问我答的游戏</p>
<p>扩展自己能力边界的提问句式我已经很精通xxx了，我想知道我是否还有需要学习的地方？然后不停的问，还有呢还有呢？</p>
<h1><span id="知道做到">知道做到</span><a href="#知道做到" class="header-anchor">#</a></h1><p>让GPT完成具体任务<br>1、我想做xxx，你能给我提供什么帮助？<br>2、我想要你做xxx，我应该给你输入什么信息？<br>3、直接下指令</p>
<h1><span id="角色关系">角色关系</span><a href="#角色关系" class="header-anchor">#</a></h1><ul>
<li>模拟虚拟人物</li>
<li>模拟名人</li>
<li>模拟一段关系</li>
<li>模拟多个具体的人</li>
<li>模拟多类人</li>
</ul>
<h1><span id="通用">通用</span><a href="#通用" class="header-anchor">#</a></h1><h3><span id="沟通模式">沟通模式</span><a href="#沟通模式" class="header-anchor">#</a></h3><p>prompt &#x3D;  定义角色+背景信息+任务目标+输出要求</p>
<h3><span id="归纳">归纳</span><a href="#归纳" class="header-anchor">#</a></h3><ul>
<li>使用markdown格式写富爸爸穷爸爸的思维导图，以代码格式输出</li>
<li>以脑图的方式归纳上文</li>
<li>请用提纲的方式来归纳上文</li>
<li>请用简练的列提纲的方式来归纳上文</li>
</ul>
<h3><span id="思维链">思维链</span><a href="#思维链" class="header-anchor">#</a></h3><h3><span id="没啥用">没啥用</span><a href="#没啥用" class="header-anchor">#</a></h3><p>请用简单的语句来归纳上文，归纳的语句可以生成脑图<br>请用金字塔思维的方式来简单的归纳上文</p>
<h1><span id="prompt-how-to-use">Prompt - How to use</span><a href="#prompt-how-to-use" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://learnprompting.org/zh-Hans/docs/intro">Learn Prompting</a> *** **</p>
</li>
<li><p><a href="https://www.aishort.top/">Chatgpt ShortCut</a><br><a href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT Shortcut </a></p>
</li>
<li><p><a href="https://prompts.chat/"> Awesome ChatGPT Prompts</a>  </p>
</li>
<li><p><a href="https://snackprompt.com/">snackprompt.com</a> ***</p>
</li>
<li><p><a href="https://flowgpt.com/">flowgpt</a> ***</p>
</li>
<li><p><a href="https://prompthero.com/">prompthero</a></p>
</li>
<li><p><a href="https://publicprompts.art/">publicprompts</a></p>
</li>
<li><p><a href="https://learningprompt.wiki/">https://learningprompt.wiki/</a> prompt 学习教程</p>
</li>
<li><p><a href="https://gpt.candobear.com/prompt">Prompt 大全</a></p>
</li>
<li><p><a href="https://gp477l8icq.feishu.cn/wiki/ZYkUwbXgzi5eqYkHl0MceNrSnhb">提示指令库</a>  汇总</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://www.bilibili.com/video/BV1Lg4y1c7fk/">学完这个视频，简历加一条：熟练掌握ChatGPT解决复杂问题｜ChatGPT使用教程</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>prompt</category>
      </categories>
      <tags>
        <tag>prompt</tag>
      </tags>
  </entry>
</search>
