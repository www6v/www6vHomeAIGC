<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MLSys  汇总</title>
    <url>/www6vHomeAIGC/2023/12/11/gptInferSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="inference">Inference *</span><a href="#inference" class="header-anchor">#</a></h2><ul>
<li>框架<ul>
<li><a href="/www6vHomeAIGC/2023/03/21/gptInferFramework/" title="(原理)推理-框架">(原理)推理-框架</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/" title="(实战)推理-lmdeploy">(实战)推理-lmdeploy</a> </li>
<li>vLLM<ul>
<li><strong><a href="/www6vHomeAIGC/2023/05/31/gptInfervLLM/" title="(原理) vLLM">(原理) vLLM</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/" title="(实战) vLLM">(实战) vLLM</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/12/02/gptInfervLLMCode/" title="(源码)vLLM">(源码)vLLM</a></strong></li>
</ul>
</li>
<li><strong><a href="/www6vHomeAIGC/2023/06/02/gptInferTensorRT/" title="(原理|实战) TensorRT-LLM">(原理|实战) TensorRT-LLM</a></strong> </li>
<li>Ray<ul>
<li><a href="/www6vHomeAIGC/2023/06/11/gptInferRay/" title="(原理)推理 Ray">(原理)推理 Ray</a>   </li>
<li><a href="/www6vHomeAIGC/2023/06/16/gptInferRayPractice/" title="(实战)推理 Ray">(实战)推理 Ray</a></li>
</ul>
</li>
</ul>
</li>
<li>优化<ul>
<li><a href="/www6vHomeAIGC/2023/01/01/gptInference/" title="(总结)推理优化">(总结)推理优化</a></li>
<li><strong><a href="/www6vHomeAIGC/2023/08/14/gptInferenceSurvey/" title="(综述)推理优化">(综述)推理优化</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2024/09/11/gptInferenceSurvey1/" title="(综述)推理优化">(综述)推理优化</a></strong> </li>
<li>系统层优化<ul>
<li>Attention Opt.<ul>
<li>FlashAttention    <ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/13/gptFlashAttention/" title="(原理)Flash Attention">(原理)Flash Attention</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/15/gptInferFlashAttention2/" title="(原理)FlashAttention2">(原理)FlashAttention2</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/06/gptInferFlashDecoding/" title="(原理)Flash Decoding">(原理)Flash Decoding</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>SpeculativeDecoding<ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/06/gptInferSpeculativeDecoding/" title="Speculative Decoding">Speculative Decoding</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/21/gptInferSpeculativeDecodingSurvey/" title="(Survey)Speculative Decoding">(Survey)Speculative Decoding</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/12/gptInferMedusa/" title="(原理)Medusa">(原理)Medusa</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/12/gptInferEagle/" title="EAGLE">EAGLE</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/12/gptInferSpecInfer/" title="SpecInfer">SpecInfer</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/18/gptInferSpeculativeDecodingvLLM/" title="(vLLM)Speculative Decoding">(vLLM)Speculative Decoding</a></strong></li>
</ul>
</li>
<li>Memory Management<ul>
<li>KVCache<ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/01/gptInferKVCache/" title="(原理) KV Cache">(原理) KV Cache</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheOptimize/" title="(原理)KV Cache 优化">(原理)KV Cache 优化</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2024/11/15/gptInferKVCacheRadixAttention/" title="(原理|实战)RadixAttention">(原理|实战)RadixAttention</a></strong> </li>
<li>Compress  <ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheQuantization/" title="(原理)KV Cache 量化">(原理)KV Cache 量化</a></strong> Quantization</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>系统层优化<ul>
<li>Batch<ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/18/gptInferContinuousBatching/" title="Continuous Batching">Continuous Batching</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/18/gptInferChunkedPrefill/" title="Chunked Prefill">Chunked Prefill</a></strong></li>
</ul>
</li>
<li>PD 分离<ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/05/gptInferDistServe/" title="DistServe">DistServe</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/19/gptInferMooncake/" title="(原理)Mooncake">(原理)Mooncake</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/30/gptInferLlumnix/" title="(原理)Llumnix">(原理)Llumnix</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>模型层优化 <ul>
<li>模型压缩<ul>
<li>量化<ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/21/gptQuantizationSurvey/" title="(Survey)Quantization">(Survey)Quantization</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/02/19/gptQuantization/" title="(原理)量化">(原理)量化</a></strong> </li>
<li>PTQ<ul>
<li><strong><a href="/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/" title="(原理)PTQ-Weight Only">(原理)PTQ-Weight Only</a></strong> </li>
<li>Weight Only <ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/12/gptQuantizationGPTQ/" title="(原理|实战)GPTQ">(原理|实战)GPTQ</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/10/12/gptQuantizationAWQ/" title="(原理|实战)AWQ">(原理|实战)AWQ</a></strong></li>
</ul>
</li>
<li>Weight&amp;Activation<ul>
<li><strong><a href="/www6vHomeAIGC/2023/10/12/gptQuantizationInt8/" title="(原理|实战)LLM.int8()">(原理|实战)LLM.int8()</a></strong>              </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/14/gptQuantizationSmoothQuant/" title="(原理)SmoothQuant">(原理)SmoothQuant</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/14/gptQuantizationFP8/" title="(原理)FP8">(原理)FP8</a></strong></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/" title="(实战)量化">(实战)量化</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>Sparse Attention<ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/02/gptInferKVCacheStreamingLLM/" title="(原理)Streaming LLM">(原理)Streaming LLM</a></strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>其他<ul>
<li><strong><a href="/www6vHomeAIGC/2023/03/30/gptTemperature/" title="推理常见参数">推理常见参数</a></strong></li>
</ul>
</li>
</ul>
<h2><span id="training">Training  *</span><a href="#training" class="header-anchor">#</a></h2><ul>
<li>分布式 <ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/06/gptTrainParallelism/" title="(原理)分布式训练">(原理)分布式训练</a></strong></li>
<li>DP<ul>
<li><strong><a href="/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/" title="(原理) Deepspeed Zero">(原理) Deepspeed Zero</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/03/25/gptTrainDistributedPractice/" title="(实战)DeepSpeed Training">(实战)DeepSpeed Training</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/09/19/gptTrainDDP/" title="(原理|实战)DDP">(原理|实战)DDP</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/19/gptTrainFSDP/" title="(原理|实战)FSDP">(原理|实战)FSDP</a></strong></li>
</ul>
</li>
<li>TP <ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainTensorParallelism/" title="(原理)张量并行(TP)">(原理)张量并行(TP)</a></strong></li>
</ul>
</li>
<li>PP     <ul>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainPipelineParallelism/" title="(原理|实战)流水线并行(PP)">(原理|实战)流水线并行(PP)</a></strong></li>
</ul>
</li>
<li>混合并行<ul>
<li><strong><a href="/www6vHomeAIGC/2023/11/26/gptTrainMegatron/" title="(原理)Megatron">(原理)Megatron</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/26/gptTrainHybridParallel/" title="(原理)混合并行">(原理)混合并行</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>低精度<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/16/gptLowPrecision/" title="低精度训练">低精度训练</a></strong>    </li>
<li><strong><a href="/www6vHomeAIGC/2024/02/01/gptPrecision/" title="(原理|实战)混合精度">(原理|实战)混合精度</a></strong></li>
</ul>
</li>
</ul>
<h2><span id="llops">LLOps</span><a href="#llops" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/05/24/gptLLamaFactory/" title="LLama-Factory">LLama-Factory</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/07/01/gptGPUComputing/" title="显存估算">显存估算</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/12/23/gptGPUMetrics/" title="GPU 指标&amp;监控">GPU 指标&amp;监控</a></strong>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/26/gptLLMOpsPaaS/" title="LLM PaaS">LLM PaaS</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/05/23/gptGPU/" title="GPU 算力">GPU 算力</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/28/gptLLMOps/" title="LLMOps">LLMOps</a></li>
</ul>
<h2><span id="mlsys">MLSys</span><a href="#mlsys" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/11/21/gptMLSysCUDA/" title="CUDA 编程">CUDA 编程</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/08/gptTrainCommunication/" title="(原理)通信原语">(原理)通信原语</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/10/01/gptParameterServer/" title="(原理|实战)Parameter Server">(原理|实战)Parameter Server</a></strong></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC 汇总</title>
    <url>/www6vHomeAIGC/2023/11/16/gptSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#basic">Basic</a></li>
<li><a href="#deeplearning">DeepLearning</a></li>
<li><a href="#nlp">NLP</a></li>
<li><a href="#model">Model *</a></li>
<li><a href="#training">Training *</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#%E5%9E%82%E7%B1%BB%E6%A8%A1%E5%9E%8B">垂类模型</a></li>
<li><a href="#study">Study</a></li>
<li><a href="#research">Research</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="basic">Basic</span><a href="#basic" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2021/08/11/ai/" title="AI 应用场景">AI 应用场景</a> </li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiOverview/" title="人工智能 知识点">人工智能 知识点</a></li>
<li><a href="/www6vHomeAIGC/2022/06/07/aiMachineLearning/" title="Machine Learning">Machine Learning</a></li>
</ul>
<h2><span id="deeplearning">DeepLearning</span><a href="#deeplearning" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2022/06/11/aiDeepLearning/" title="Deep Learning">Deep Learning</a></strong></li>
<li><strong><a href="/www6vHomeAIGC/2023/03/28/gptPytorch/" title="(实战)PyTorch">(实战)PyTorch</a></strong></li>
</ul>
<h2><span id="nlp">NLP</span><a href="#nlp" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/05/gptNLPTask/" title="NLP &amp; LLM">NLP &amp; LLM</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/02/18/gptDocSimilarity/" title="短文本相似度">短文本相似度</a>  </li>
<li><a href="/www6vHomeAIGC/2023/05/28/gptDialogue/" title="多轮对话">多轮对话</a></li>
</ul>
<h2><span id="model">Model *</span><a href="#model" class="header-anchor">#</a></h2><ul>
<li>Backbone <ul>
<li><strong><a href="/www6vHomeAIGC/2022/11/30/gptTransformer/" title="(原理)Transformer">(原理)Transformer</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/19/gptSelfAttention/" title="(原理)Self-Attention">(原理)Self-Attention</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/11/19/gptModelGQA/" title="(原理)GQA">(原理)GQA</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/02/16/gptTransformerCode/" title="(实战)Transformer">(实战)Transformer</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/18/gptEmbedding/" title="(原理)Embedding">(原理)Embedding</a></li>
</ul>
</li>
<li>Foundation Models<ul>
<li><a href="/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/" title="(综述)大模型">(综述)大模型</a></li>
<li><a href="/www6vHomeAIGC/2023/02/17/gptLargeModel/" title="大模型">大模型</a> </li>
<li><a href="/www6vHomeAIGC/2022/12/11/gptFamily/" title="GPT 系列">GPT 系列</a>  </li>
<li><a href="/www6vHomeAIGC/2023/01/06/gptChatGLM/" title="ChatGLM">ChatGLM</a>    </li>
<li>LLaMA<ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/01/gptLlama/" title="LLaMA">LLaMA</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/" title="LLaMA 家族">LLaMA 家族</a>   </li>
<li><strong><a href="/www6vHomeAIGC/2024/09/04/gptLlama3-1/" title="Llama3.1">Llama3.1</a></strong></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/01/04/gptLeaderBoard/" title="大模型 排行榜">大模型 排行榜</a></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/12/03/gptScalingLaw/" title="Scaling Law">Scaling Law</a> * </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptEmergent/" title="(原理)涌现现象">(原理)涌现现象</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/06/gptHallucination/" title="(原理)幻觉问题">(原理)幻觉问题</a> * </li>
<li><a href="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/" title="(原理)不可能三角">(原理)不可能三角</a>    </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptEval/" title="测评">测评</a></li>
</ul>
<h2><span id="training">Training  *</span><a href="#training" class="header-anchor">#</a></h2><ul>
<li>训练<ul>
<li><a href="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/" title="(原理)Training">(原理)Training</a></li>
<li><strong><a href="/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/" title="(实战)Pre-Training">(实战)Pre-Training</a></strong> </li>
<li><a href="/www6vHomeAIGC/2023/02/03/gptContinualPretraining/" title="(原理|实战)继续Pre-Training">(原理|实战)继续Pre-Training</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/21/gptChineseLlama/" title="(实战)Chinese-LLaMA PT+SFT">(实战)Chinese-LLaMA PT+SFT</a></li>
</ul>
</li>
</ul>
<h2><span id="data">Data</span><a href="#data" class="header-anchor">#</a></h2><ul>
<li>List<ul>
<li><a href="/www6vHomeAIGC/2023/01/08/gptDataSet/" title="(list)数据集">(list)数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/" title="(List) Pretrain 数据集">(List) Pretrain 数据集</a> </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/" title="(List)SFT数据集">(List)SFT数据集</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/" title="(survey)多模态  数据集">(survey)多模态  数据集</a></li>
</ul>
</li>
<li>DataProcess<ul>
<li><a href="/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/" title="(Survey)Dataset">(Survey)Dataset</a> </li>
<li><a href="/www6vHomeAIGC/2023/02/05/gptDataProcess/" title="(Survey)数据处理">(Survey)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/" title="(实战)数据处理">(实战)数据处理</a>  </li>
<li><a href="/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/" title="(原理|实战)Data  Annotation">(原理|实战)Data  Annotation</a></li>
</ul>
</li>
<li>Data Management<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataManagement/" title="(Survey)Data Management">(Survey)Data Management</a>  </li>
<li>Pretrain  <ul>
<li><a href="/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/" title="(质量过滤)RefinedWeb, Textbooks">(质量过滤)RefinedWeb, Textbooks</a>  </li>
<li><a href="/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/" title="Tokenizer">Tokenizer</a></li>
</ul>
</li>
<li>SFT <ul>
<li>Data Quality<ul>
<li>Instruction Quality<ul>
<li><a href="/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/" title="(原理)LIMA, LESS">(原理)LIMA, LESS</a></li>
</ul>
</li>
<li>Instruction Diversity<ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/21/gptSelfInstruct/" title="(原理)SELF-INSTRUCT">(原理)SELF-INSTRUCT</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/09/27/gptDataSelfQA/" title="(原理|实战)Self-QA">(原理|实战)Self-QA</a></strong></li>
</ul>
</li>
<li>Instruction Complexity  <ul>
<li><a href="/www6vHomeAIGC/2023/03/18/gptDataWizard/" title="(原理)Wizard">(原理)Wizard</a></li>
</ul>
</li>
</ul>
</li>
<li>Task composition<ul>
<li><strong><a href="/www6vHomeAIGC/2023/02/06/gptDatasetSFT/" title="(原理)SFT 数据组合">(原理)SFT 数据组合</a></strong></li>
</ul>
</li>
</ul>
<ul>
<li><strong><a href="/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/" title="(原理)SFT Scaling">(原理)SFT Scaling</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/05/05/gptDataSelection/" title="(原理)Data Selection">(原理)Data Selection</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="垂类模型">垂类模型</span><a href="#垂类模型" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2023/01/04/gptDomain/" title="垂类大模型">垂类大模型</a></strong> </li>
<li><a href="/www6vHomeAIGC/2022/11/24/gptDomainFinance/" title="金融大模型">金融大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2023/02/07/gptDomainMed/" title="医疗大模型">医疗大模型</a>   </li>
<li><a href="/www6vHomeAIGC/2024/02/07/gptDomainLaw/" title="法律大模型">法律大模型</a></li>
</ul>
<h2><span id="study">Study</span><a href="#study" class="header-anchor">#</a></h2><ul>
<li><a href="/www6vHomeAIGC/2022/08/01/gptStudy/" title="GPT  学习资源">GPT  学习资源</a></li>
<li><a href="/www6vHomeAIGC/2022/01/22/aiStudyResouce/" title="人工智能-学习资源">人工智能-学习资源</a></li>
</ul>
<h2><span id="research">Research</span><a href="#research" class="header-anchor">#</a></h2><ul>
<li><strong><a href="/www6vHomeAIGC/2024/02/11/gptPaperTools/" title="科研-工具">科研-工具</a></strong> </li>
<li><a href="/www6vHomeAIGC/2023/01/20/gptStudyPaper/" title="GPT 论文">GPT 论文</a></li>
<li><a href="/www6vHomeAIGC/2023/02/25/gptSurveyList/" title="Survey List">Survey List</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/04/gptAgentPaper/" title="Paper-Agent">Paper-Agent</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision 汇总</title>
    <url>/www6vHomeAIGC/2023/07/23/gptVisionSummary/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#multimodal">Multimodal *</a><ul>
<li><a href="#survey">Survey</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3">视觉理解</a></li>
<li><a href="#%E7%94%9F%E6%88%90">生成</a></li>
<li><a href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83llmlmm">端到端训练LLM(LMM)</a></li>
<li><a href="#multimodal-agent">Multimodal Agent*</a></li>
</ul>
</li>
<li><a href="#vision">Vision</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="multimodal">Multimodal *</span><a href="#multimodal" class="header-anchor">#</a></h1><h3><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> </li>
<li><a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a></li>
<li><a href="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/" title="多模态 系列">多模态 系列</a></li>
</ul>
<h3><span id="视觉理解">视觉理解</span><a href="#视觉理解" class="header-anchor">#</a></h3><ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/02/gptMultimodalEncoder/" title="Vision Encoder">Vision Encoder</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/05/gptMultimodalConnector/" title="(原理)Connector">(原理)Connector</a></strong>  	</li>
<li>Segmentation<br>+ <a href="/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/" title="(原理|实战) SAM">(原理|实战) SAM</a></li>
</ul>
<h3><span id="生成">生成</span><a href="#生成" class="header-anchor">#</a></h3><ul>
<li>Diffusion<ul>
<li><strong><a href="/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/" title="(原理)Diffusion">(原理)Diffusion</a></strong>   </li>
<li><a href="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/" title="(实战)Diffusion">(实战)Diffusion</a>  </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/01/gptDiffusionXL/" title="(原理)SD XL">(原理)SD XL</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/27/gptDiffusionunCLIP/" title="(原理)unCLIP">(原理)unCLIP</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/08/01/gptDiffusionGuidance/" title="(原理)Guidance">(原理)Guidance</a></strong></li>
</ul>
</li>
<li>Controllable  <ul>
<li><strong><a href="/www6vHomeAIGC/2023/07/29/gptDiffusionControllable/" title="(综述)Controllable">(综述)Controllable</a></strong> </li>
<li><strong><a href="/www6vHomeAIGC/2023/07/17/gptDiffusionControllableWork/" title="(Work|实战)Controllable">(Work|实战)Controllable</a></strong></li>
<li>Spatial Control<ul>
<li>Dense control<ul>
<li>single<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionControlNet/" title="(原理|实战)ControlNet">(原理|实战)ControlNet</a></strong></li>
</ul>
</li>
<li>multi <ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionT2IAdapter/" title="(原理|实战)T2I-Adapter">(原理|实战)T2I-Adapter</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>layout&#x2F;box<ul>
<li>GLIGEN</li>
<li>Reco</li>
</ul>
</li>
</ul>
</li>
<li>Style control<ul>
<li>subject-driven<ul>
<li>concept customization [fine-tuning]<ul>
<li><strong><a href="/www6vHomeAIGC/2023/07/06/gptDiffusionFineTuning/" title="(work|实战) fine-tuning">(work|实战) fine-tuning</a></strong>   </li>
<li><strong><a href="/www6vHomeAIGC/2024/07/17/gptDiffusionDreamBooth/" title="(原理|实战)DreamBooth">(原理|实战)DreamBooth</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>image-driven<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionIPAdapter/" title="(原理|实战)IP-Adapter">(原理|实战)IP-Adapter</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>Sementic Control</li>
<li>其他<ul>
<li><strong><a href="/www6vHomeAIGC/2023/08/22/gptDiffusionReferenceNet/" title="(原理|实战)ReferenceNet">(原理|实战)ReferenceNet</a></strong></li>
</ul>
</li>
</ul>
</li>
<li>editing<ul>
<li><a href="/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/" title="(综述)Image Editing">(综述)Image Editing</a>   </li>
<li><strong><a href="/www6vHomeAIGC/2023/07/27/gptDiffusionImageEditWork/" title="(Work|实战)Image Editing">(Work|实战)Image Editing</a></strong></li>
</ul>
</li>
<li>人像生图<ul>
<li><strong><a href="/www6vHomeAIGC/2024/08/03/gptMultimodalIDCreate/" title="人像生图">人像生图</a></strong></li>
</ul>
</li>
<li><a href="/www6vHomeAIGC/2023/07/21/gptDiffusionDiT/" title="(原理|实战)DiT">(原理|实战)DiT</a></li>
</ul>
<h3><span id="端到端训练llmlmm">端到端训练LLM(LMM)</span><a href="#端到端训练llmlmm" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/" title="(图生文)BLIP-2, Flamingo">(图生文)BLIP-2, Flamingo</a> </li>
<li><strong><a href="/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/" title="(原理|实战) LLaVa 演化">(原理|实战) LLaVa 演化</a></strong>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/" title="(原理|实战)MiniGPT4">(原理|实战)MiniGPT4</a>    </li>
<li>Train  *<ul>
<li><a href="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/" title="(原理)多模态预训练 概述">(原理)多模态预训练 概述</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/" title="(综述)多模态InstructTuning">(综述)多模态InstructTuning</a></li>
</ul>
</li>
</ul>
<h3><span id="multimodal-agent">Multimodal Agent*</span><a href="#multimodal-agent" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/" title="(原理)Agent 多模态">(原理)Agent 多模态</a>  </li>
<li><a href="/www6vHomeAIGC/2023/03/05/gptAgentWeb/" title="(原理)Web Agent">(原理)Web Agent</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/" title="Agent - UI-assistants">Agent - UI-assistants</a></li>
</ul>
<h1><span id="vision">Vision</span><a href="#vision" class="header-anchor">#</a></h1><ul>
<li><a href="/www6vHomeAIGC/2023/07/25/gptVisionTask/" title="CV 任务">CV 任务</a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>汇总</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)RadixAttention</title>
    <url>/www6vHomeAIGC/2024/11/15/gptInferKVCacheRadixAttention/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="radixattention">RadixAttention</span><a href="#radixattention" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/RadixAttention-105bfe211084807581eccf952ba3bb59?pvs=4">(原理|实战)RadixAttention</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)推理优化</title>
    <url>/www6vHomeAIGC/2024/09/11/gptInferenceSurvey1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Towards-Efficient-Generative-Large-Language-Model-Serving-A-Survey-from-Algorithms-to-Systems-c1914500c33f4446ac7fbe8848354d91?pvs=4">Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Llama3.1</title>
    <url>/www6vHomeAIGC/2024/09/04/gptLlama3-1/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="llama31">Llama3.1</span><a href="#llama31" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLaMA3-1-c2124d23077241afa9d92eb9a54a043a?pvs=4">Llama3.1</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>人像生图</title>
    <url>/www6vHomeAIGC/2024/08/03/gptMultimodalIDCreate/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="instantid">InstantID</span><a href="#instantid" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><strong>InstantID</strong> 小红书</p>
</li>
<li><p>开源地址<br><a href="https://github.com/InstantID/InstantID">InstantID</a></p>
</li>
<li><p>Project page<br><a href="https://instantid.github.io/">Project page</a></p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/InstantID-0b96b4a8d12340a1900995ca7f33ef05?pvs=4">InstantID</a></p>
</li>
</ul>
<hr>
<h1><span id="photomaker">PhotoMaker</span><a href="#photomaker" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br>PhotoMaker  腾讯</p>
</li>
<li><p>开源地址<br><a href="https://github.com/TencentARC/PhotoMaker">Repo git</a> git</p>
</li>
<li><p>Project page<br><a href="https://photo-maker.github.io/">Project page</a>   </p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/PhotoMaker-f4b3e96a9ed046838b7255e026bd1abf?pvs=4">解析</a></p>
</li>
</ul>
<hr>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>【InstantID : ipAdaptor +controlnet,  image Contoll的思路】<br>【photomaker: image  Edit 的思路】</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DreamBooth</title>
    <url>/www6vHomeAIGC/2024/07/17/gptDiffusionDreamBooth/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dreambooth">DreamBooth</span><a href="#dreambooth" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DreamBooth-d591acda472a49c7a189009a53addd24?pvs=4">(原理|实战)DreamBooth</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态 系列</title>
    <url>/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#stage1-%E6%A8%A1%E5%9D%97%E7%8B%AC%E7%AB%8B2">Stage1: 模块独立[2]</a><ul>
<li><a href="#model">model</a></li>
</ul>
</li>
<li><a href="#stage2-%E6%A8%A1%E5%9D%97%E5%85%B1%E4%BA%AB2">Stage2: 模块共享[2]</a><ul>
<li><a href="#model-1">model</a></li>
</ul>
</li>
<li><a href="#stage3-%E8%8C%83%E5%BC%8F%E7%BB%9F%E4%B8%802">Stage3: 范式统一[2]</a><ul>
<li><a href="#model-2">model</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93-1">总结 [1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#overview">Overview</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="stage1-模块独立2">Stage1: 模块独立[2]</span><a href="#stage1-模块独立2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/stage1.webp" class>

<h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>CLIP</li>
<li>ViLT</li>
<li>ALBEF</li>
</ul>
<h1><span id="stage2-模块共享2">Stage2: 模块共享[2]</span><a href="#stage2-模块共享2" class="header-anchor">#</a></h1><h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>VLMO</li>
<li>BLIP</li>
<li>BLIP2</li>
<li>BEiTv3</li>
</ul>
<h1><span id="stage3-范式统一2">Stage3: 范式统一[2]</span><a href="#stage3-范式统一2" class="header-anchor">#</a></h1><h3><span id="model">model</span><a href="#model" class="header-anchor">#</a></h3><ul>
<li>Unified-IO</li>
<li>Uni-Perceiver</li>
<li>PaLi</li>
</ul>
<h1><span id="总结-1">总结 [1]</span><a href="#总结-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2024/04/04/gptMultimodalSeries/multimodal.webp" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/653902791">多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/643969218">[Transformer 101系列] 多模态的大一统之路</a>  ***</p>
</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/qq_52038588/article/details/133893013">多模态论文串讲</a> ***<br>   <a href="https://blog.csdn.net/qq_40168949/article/details/130374733">多模态论文串讲：ALBEF &amp; VLMo &amp; BLIP &amp; CoCa &amp; Beit V3</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409338&idx=1&sn=5445ff1e9bedc561393b6da63fdf71f9">图生文多模态大模型开源项目回顾：兼看20240307大模型进展早报</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662889725">图文多模态大模型综述</a></p>
<p>1xx. <a href="https://huyenchip.com/2023/10/10/multimodal.html">Multimodality and Large Multimodal Models (LMMs)</a><br>   <a href="https://baoyu.io/translations/lmm/multimodality-and-large-multimodal-models">多模态和多模态大模型 (LMM)[译]</a>  CLIP Flamingo</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/667942680">写在多模态征服一切之前（未来数据和模型应该是什么样的？）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)量化</title>
    <url>/www6vHomeAIGC/2024/03/22/gptQuantizationPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="量化实战">量化实战</span><a href="#量化实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/1213172a99a949ceba7e8e2164710a77?pvs=4">(实战)量化-推理</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(质量过滤)RefinedWeb, Textbooks</title>
    <url>/www6vHomeAIGC/2024/02/27/gptDataRefinedWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="动机1">动机[1]</span><a href="#动机1" class="header-anchor">#</a></h1><ul>
<li>作者执着证明网页数据好于专有数据<ul>
<li>网页数据的量级比公开数据大的多，仅用专有数据模型模型训练不到最佳效果</li>
<li>专有数据处理起来很麻烦</li>
<li>大部分专有数据其实在网页数据中也能找到</li>
</ul>
</li>
</ul>
<p>作者认为要想模型训练的大、耗费的人力少就不得不重新<strong>将网页数据精细化</strong>利用起来。</p>
<h1><span id="结论1">结论[1]</span><a href="#结论1" class="header-anchor">#</a></h1><ul>
<li>作者证明了仅用<strong>web数据</strong>如果经过恰当的<strong>清洗和过滤</strong>，可以获得超过使用了专有数据模型的效果。</li>
</ul>
<h1><span id="文本处理pipeline1">文本处理Pipeline[1]</span><a href="#文本处理pipeline1" class="header-anchor">#</a></h1><h3><span id="目标语言识别">目标语言识别</span><a href="#目标语言识别" class="header-anchor">#</a></h3><h3><span id="规则过滤">规则过滤</span><a href="#规则过滤" class="header-anchor">#</a></h3><h3><span id="通过机器学习方法过滤出高质量语料库">通过机器学习方法过滤出高质量语料库</span><a href="#通过机器学习方法过滤出高质量语料库" class="header-anchor">#</a></h3><h3><span id="去重deduplication">去重（Deduplication）</span><a href="#去重deduplication" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="refinedweb">RefinedWeb</span><a href="#refinedweb" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/641013454">数据为王：大模型预训练中的数据处理及思考—The RefinedWeb Dataset for Falcon LLM论文解读</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401484&idx=1&sn=c49b5ca5fc962ca757d3a082b74f037a">“超越LLama 65B”的Falcon40B语言模型为什么好：再看精细化的数据清洗的重要性 </a><br>   RefinedWeb Dataset for Falcon,   Falcon采用bloom架构</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402104&idx=1&sn=7d4924b2a5a840e4ff3de43299248b1d">再谈大模型的预训数据清洗与微调数据生成：RedPajama数据处理框架与entity-centric指令生成方法解读 </a><br>    llama数据的复现项目SlimPajama</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/637996787">【Falcon Paper】我们是靠洗数据洗败 LLaMA 的！</a> 未</p>
<h3><span id="textbooks-数量-gtscaling-law">Textbooks   数量-&gt;scaling law</span><a href="#textbooks-数量-gtscaling-law" class="header-anchor">#</a></h3><p>1xx. <a href="https://finisky.github.io/textbooks-are-all-you-need-summary/">数据为王: Textbooks Are All You Need </a>   以小博大  打破传统语言模型缩放定律<br>1xx. <a href="https://zhuanlan.zhihu.com/p/673021932">Textbooks Are All You Need II: phi-1.5 technical report 精读与翻译</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/672066480">小模型的惊人能力: Phi-2</a></p>
<h3><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403821&idx=1&sn=7b96e0db09f05888078019cd20bc8390">再看多语种大模型预训数据如何清洗：兼论文档结构信息对大模型问答的重要性及实现思路 </a><br>二、再看训练数据集如何清洗：多语种开源训练数据集CulturaX</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-工具</title>
    <url>/www6vHomeAIGC/2024/02/11/gptPaperTools/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="科研-工具">科研-工具</span><a href="#科研-工具" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/115bfe21108480ce88fdcb7b9f9dbc4e?pvs=4">科研-工具</a></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>法律大模型</title>
    <url>/www6vHomeAIGC/2024/02/07/gptDomainLaw/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="法律大模型">法律大模型</span><a href="#法律大模型" class="header-anchor">#</a></h3><ul>
<li>ChatLaw </li>
<li>LawGPT_zh</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402872&idx=1&sn=0649e8f7490e057680cff1be16157209">再看法律领域微调模型及外挂知识库问答优化方案：从引入关键词、领域嵌入到知识库细化、意图识别及知识增强项目案例 </a></p>
<p>1xx. <a href="https://finisky.github.io/lawyer-llama-summary/">训练中文垂类大模型：Lawyer LLaMA </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)混合精度</title>
    <url>/www6vHomeAIGC/2024/02/01/gptPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="混合精度">混合精度</span><a href="#混合精度" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/d27bdf000e7c42eabd288f9d036ea5e7?pvs=4">(原理|实战)混合精度</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Precision</category>
      </categories>
      <tags>
        <tag>Precision</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 指标&amp;监控</title>
    <url>/www6vHomeAIGC/2023/12/23/gptGPUMetrics/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="gpu-指标amp监控">GPU 指标&amp;监控</span><a href="#gpu-指标amp监控" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/165bfe21108480e29fa1d06e4852a6e0?pvs=4">GPU 指标&amp;监控</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling Law</title>
    <url>/www6vHomeAIGC/2023/12/03/gptScalingLaw/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="scaling-law10">Scaling Law[10]</span><a href="#scaling-law10" class="header-anchor">#</a></h1><h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/12/03/gptScalingLaw/scalingLaw.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/12/03/gptScalingLaw/paramVSdataSize.jpg" class>

<h3><span id="参数量-vs-数据量">参数量 vs 数据量</span><a href="#参数量-vs-数据量" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/12/03/gptScalingLaw/computeVSDatasize.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="scaling-law">Scaling Law</span><a href="#scaling-law" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/667489780">解析大模型中的Scaling Law</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/663296750">论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models</a><br>2xx. <a href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/">Training Compute-Optimal Large Language Models 简读 </a></li>
</ol>
<p>2xx. <a href="https://zhuanlan.zhihu.com/p/536053110">【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！</a><br>《Scaling Laws for Neural Language Models》<br>《Training Compute-Optimal Large Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>model</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>(源码)vLLM</title>
    <url>/www6vHomeAIGC/2023/12/02/gptInfervLLMCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="vllm">vLLM</span><a href="#vllm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vLLM-10bbfe2110848006b1f9d51397008e89?pvs=4">(源码)vLLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Llumnix</title>
    <url>/www6vHomeAIGC/2023/11/30/gptInferLlumnix/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="llumnix">Llumnix</span><a href="#llumnix" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Llumnix-14407e4f4d2f4a908edcf73b6990b4f0?pvs=4">(原理)Llumnix</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)混合并行</title>
    <url>/www6vHomeAIGC/2023/11/26/gptTrainHybridParallel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="混合并行">混合并行</span><a href="#混合并行" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/111bfe2110848026bf8ff0d2c779e3ed?pvs=4">(原理)混合并行</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Megatron</title>
    <url>/www6vHomeAIGC/2023/11/26/gptTrainMegatron/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="megatron">Megatron</span><a href="#megatron" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Megatron-10bbfe21108480a3890ae6388bcdf834?pvs=4">(原理)Megatron</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA 编程</title>
    <url>/www6vHomeAIGC/2023/11/21/gptMLSysCUDA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="cuda-编程">CUDA 编程</span><a href="#cuda-编程" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/CUDA-145bfe21108480eea4ffd58735a564f6?pvs=4">CUDA 编程</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)GQA</title>
    <url>/www6vHomeAIGC/2023/11/19/gptModelGQA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://candied-skunk-1ca.notion.site/GQA-984685a6ee2441a0879cc0e18066a641?pvs=4">(原理)GQA</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>model</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Self-Attention</title>
    <url>/www6vHomeAIGC/2023/11/19/gptSelfAttention/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="self-attention">Self-Attention</span><a href="#self-attention" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Self-Attention-142bfe21108480f6a0c0cdbf9262a7e3?pvs=4">(原理)Self-Attention</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>model</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>(vLLM)Speculative Decoding</title>
    <url>/www6vHomeAIGC/2023/11/18/gptInferSpeculativeDecodingvLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="vllm-speculative-decoding">(vLLM) Speculative Decoding</span><a href="#vllm-speculative-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vLLM-Speculative-Decoding-81c9a87ab53947c19c7ba22bcaee5125?pvs=4">(vLLM) Speculative Decoding</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)FlashAttention2</title>
    <url>/www6vHomeAIGC/2023/11/15/gptInferFlashAttention2/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="flash-attention2">Flash Attention2</span><a href="#flash-attention2" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Flash-Attention2-110bfe21108480f782fcc7258d860ccc?pvs=4">(原理)Flash Attention2</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Infer</category>
      </categories>
      <tags>
        <tag>Infer</tag>
      </tags>
  </entry>
  <entry>
    <title>SpecInfer</title>
    <url>/www6vHomeAIGC/2023/11/12/gptInferSpecInfer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="specinfer">SpecInfer</span><a href="#specinfer" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SpecInfer-8b75821e44384cecba4541f7aa758adb?pvs=4">SpecInfer</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>EAGLE</title>
    <url>/www6vHomeAIGC/2023/11/12/gptInferEagle/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="eagle">EAGLE</span><a href="#eagle" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/EAGLE-126bfe2110848058aeb0ed8c2d06319b?pvs=4">EAGLE</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Medusa</title>
    <url>/www6vHomeAIGC/2023/11/12/gptInferMedusa/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="medusa">Medusa</span><a href="#medusa" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Medusa-31e27ea0a51d4c818c804a654a3c839a?pvs=4">Medusa</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Quantization</title>
    <url>/www6vHomeAIGC/2023/10/21/gptQuantizationSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="quantization">Quantization</span><a href="#quantization" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/A-Survey-of-Low-bit-Large-Language-Models-Basics-Systems-and-Algorithms-116bfe2110848045b3e0c165318ec16b?pvs=4">A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Speculative Decoding</title>
    <url>/www6vHomeAIGC/2023/10/21/gptInferSpeculativeDecodingSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="speculative-decoding">Speculative Decoding</span><a href="#speculative-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Unlocking-Efficiency-in-Large-Language-Model-Inference-A-Comprehensive-Survey-of-Speculative-Decodi-117bfe2110848059977ece3df3f9791d?pvs=4">(Survey)Speculative Decoding </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Mooncake</title>
    <url>/www6vHomeAIGC/2023/10/19/gptInferMooncake/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="mooncake">Mooncake</span><a href="#mooncake" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Mooncake-d7d1506860df44bca7a2f880a1352285?pvs=4">Mooncake</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)FP8</title>
    <url>/www6vHomeAIGC/2023/10/14/gptQuantizationFP8/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="fp8">FP8</span><a href="#fp8" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/FP8-12c3b58d744e43299dad0badc397f592?pvs=4">(原理)FP8</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SmoothQuant</title>
    <url>/www6vHomeAIGC/2023/10/14/gptQuantizationSmoothQuant/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="smoothquant">SmoothQuant</span><a href="#smoothquant" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SmoothQuant-11dbfe21108480fc83f8ea2a495092b7?pvs=4">(原理)SmoothQuant</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)AWQ</title>
    <url>/www6vHomeAIGC/2023/10/12/gptQuantizationAWQ/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="awq">AWQ</span><a href="#awq" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/AWQ-11dbfe211084806396a0fc0f87983446?pvs=4">(原理|实战)AWQ</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)GPTQ</title>
    <url>/www6vHomeAIGC/2023/10/12/gptQuantizationGPTQ/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="gptq">GPTQ</span><a href="#gptq" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/GPTQ-11dbfe21108480cb8510ed7d85a64371?pvs=4">(原理|实战)GPTQ</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)LLM.int8()</title>
    <url>/www6vHomeAIGC/2023/10/12/gptQuantizationInt8/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="llmint8">LLM.int8()</span><a href="#llmint8" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLM-int8-11dbfe21108480d0b6adc341aed9ec3d?pvs=4">(原理|实战)LLM.int8()</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Flash Decoding</title>
    <url>/www6vHomeAIGC/2023/10/06/gptInferFlashDecoding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="flash-decoding">Flash Decoding</span><a href="#flash-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Flash-Decoding-110bfe2110848085aa6dde60217c486a?pvs=4">(原理)Flash Decoding</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Infer</category>
      </categories>
      <tags>
        <tag>Infer</tag>
      </tags>
  </entry>
  <entry>
    <title>Speculative Decoding</title>
    <url>/www6vHomeAIGC/2023/10/06/gptInferSpeculativeDecoding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="speculative-decoding">Speculative Decoding</span><a href="#speculative-decoding" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Speculative-Decoding-117bfe2110848060a00efd475a0abbac?pvs=4">Speculative Decoding</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>DistServe</title>
    <url>/www6vHomeAIGC/2023/10/05/gptInferDistServe/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="distserve">DistServe</span><a href="#distserve" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DistServe-dd4ae7040b78496f9a60c0291941922b?pvs=4">DistServe</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Parameter Server</title>
    <url>/www6vHomeAIGC/2023/10/01/gptParameterServer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="parameter-server">Parameter Server</span><a href="#parameter-server" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Parameter-Server-b7ba4d2c8de44339997cff6697f51df5?pvs=4">(原理|实战)Parameter Server</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MLSys</category>
      </categories>
      <tags>
        <tag>MLSys</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Self-QA</title>
    <url>/www6vHomeAIGC/2023/09/27/gptDataSelfQA/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="self-qa">Self-QA</span><a href="#self-qa" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Self-QA-10ebfe21108480999eadec23ffadb6fe?pvs=4">(原理|实战)Self-QA</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Data</category>
      </categories>
      <tags>
        <tag>Data</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM PaaS</title>
    <url>/www6vHomeAIGC/2023/09/26/gptLLMOpsPaaS/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llm-paas">LLM PaaS</span><a href="#llm-paas" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/9ddf2032d70b4722ad34a48cb305d80b?pvs=4">LLM PaaS</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)FSDP</title>
    <url>/www6vHomeAIGC/2023/09/19/gptTrainFSDP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="fsdp">FSDP</span><a href="#fsdp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/FSDP-2fc101f3b7ac4796b74d5e9287ff8210?pvs=4">FSDP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DDP</title>
    <url>/www6vHomeAIGC/2023/09/19/gptTrainDDP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ddp">DDP</span><a href="#ddp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/DDP-3576fea6254a42c2ab520f5e6c4ccb86?pvs=4">DDP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>Chunked Prefill</title>
    <url>/www6vHomeAIGC/2023/09/18/gptInferChunkedPrefill/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="chunked-prefill">Chunked Prefill</span><a href="#chunked-prefill" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/chunked-prefill-102bfe21108480a7af99fee1f56fd5af?pvs=4">Chunked Prefill</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Continuous Batching</title>
    <url>/www6vHomeAIGC/2023/09/18/gptInferContinuousBatching/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="continuous-batching">Continuous Batching</span><a href="#continuous-batching" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Continuous-batching-3ce74a6d992944fba6314e21b3c3ec22">Continuous Batching</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)通信原语</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainCommunication/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="通信原语">通信原语</span><a href="#通信原语" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/1b545b71538a4c5cb14e5771da8b4835?pvs=4">(原理)通信原语</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)张量并行(TP)</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainTensorParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="张量并行tp">张量并行(TP)</span><a href="#张量并行tp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/TP-35acabf325004c16b9ce93f82cb175c2?pvs=4">(原理)张量并行(TP)</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)流水线并行(PP)</title>
    <url>/www6vHomeAIGC/2023/09/08/gptTrainPipelineParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="流水线并行pp">流水线并行(PP)</span><a href="#流水线并行pp" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PP-f528f9a456184d1db006808039c0d2ee?pvs=4">(原理|实战)流水线并行(PP)</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)KV Cache 量化</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheQuantization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache-量化">KV Cache 量化</span><a href="#kv-cache-量化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Quantization-c6fa20cf425a4211af150b4987711f47?pvs=4">KV Cache 量化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Streaming LLM</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheStreamingLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="streaming-llm">Streaming LLM</span><a href="#streaming-llm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/StreamingLLM-5141d463ddf84b4783c369459c71eec8?pvs=4">Streaming LLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)KV Cache 优化</title>
    <url>/www6vHomeAIGC/2023/09/02/gptInferKVCacheOptimize/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache-优化">KV Cache 优化</span><a href="#kv-cache-优化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/KV-cache-bd0a35015c9845bd8e17d5c902dba152?pvs=4">(原理)KV cache优化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)unCLIP</title>
    <url>/www6vHomeAIGC/2023/08/27/gptDiffusionunCLIP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="unclip">unCLIP</span><a href="#unclip" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/unCLIP-7603f64564a54cb4af08a1cf38c890e1?pvs=4">(原理)unCLIP</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ReferenceNet</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionReferenceNet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="referencenet">ReferenceNet</span><a href="#referencenet" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/ReferenceNet-22a14d3a669e4d7f8c99409e34252349?pvs=4">(原理|实战)ReferenceNet</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)IP-Adapter</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionIPAdapter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="ip-adapter">IP-Adapter</span><a href="#ip-adapter" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/IP-Adapter-40dfd1f30d38456b8776a90871716c73?pvs=4">(原理|实战)IP-Adapter</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)T2I-Adapter</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionT2IAdapter/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="t2i-adapter">T2I-Adapter</span><a href="#t2i-adapter" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/T2I-Adapter-26eef5080f084dacb6e89d643d31e53d?pvs=4">(原理|实战)T2I-Adapter</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ControlNet</title>
    <url>/www6vHomeAIGC/2023/08/22/gptDiffusionControlNet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="controlnet">ControlNet</span><a href="#controlnet" class="header-anchor">#</a></h1><p>  <a href="https://candied-skunk-1ca.notion.site/ControlNet-76c3d54a18424e20a25a3e4a8a71b8e3?pvs=4">(原理|实战)ControlNet</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>低精度训练</title>
    <url>/www6vHomeAIGC/2023/08/16/gptLowPrecision/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="低精度训练">低精度训练</span><a href="#低精度训练" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/cb067e2d0bc545d898cd43dd1091c8b3?pvs=4">低精度训练</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>低精度</category>
      </categories>
      <tags>
        <tag>低精度</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)推理优化</title>
    <url>/www6vHomeAIGC/2023/08/14/gptInferenceSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/A-Survey-on-Efficient-Inference-for-Large-Language-Models-22145473188e437881bf566241492bea?pvs=4">A Survey on Efficient Inference for Large Language Models</a> 翻译</p>
<p><a href="https://candied-skunk-1ca.notion.site/A-Survey-on-Efficient-Inference-for-Large-Language-Models-135bfe2110848034bf45ea8e5d1d2fdb?pvs=4">A Survey on Efficient Inference for Large Language Models</a> 总结</p>
<h1><span id="inference-papers">Inference Papers</span><a href="#inference-papers" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Inference-Papers-bd22ef1d8c274d6f9951c394a95ff427?pvs=4">Inference Papers</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Connector</title>
    <url>/www6vHomeAIGC/2023/08/05/gptMultimodalConnector/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="connector">Connector</span><a href="#connector" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Connector-6db25052e8e542c29e43f503fd572475?pvs=4">Connector</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Encoder</title>
    <url>/www6vHomeAIGC/2023/08/02/gptMultimodalEncoder/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="vision-encoder">Vision Encoder</span><a href="#vision-encoder" class="header-anchor">#</a></h1><h3><span id="multimodal-learning"><strong>multimodal learning</strong></span><a href="#multimodal-learning" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/" title="(原理)CLIP">(原理)CLIP</a>  </li>
<li><a href="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/" title="(实战)CLIP">(实战)CLIP</a></li>
</ul>
<h3><span id="supervised-learning"><strong>Supervised Learning</strong></span><a href="#supervised-learning" class="header-anchor">#</a></h3><ul>
<li><a href="/www6vHomeAIGC/2023/03/01/gptMultimodalVit/" title="(原理|实战)ViT, ViLT">(原理|实战)ViT, ViLT</a></li>
</ul>
<h3><span id="self-distillation"><strong>self-distillation</strong></span><a href="#self-distillation" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/DINO-ac5b90014a01494ea1311f4d24af38dd?pvs=4">DINO</a> </li>
<li><a href="https://candied-skunk-1ca.notion.site/DINOv2-b3d258bbb20f42bbac0ef6ca6f093f9d?pvs=4">DINOv2</a></li>
</ul>
<h3><span id="auto-encoding"><strong>Auto-encoding</strong></span><a href="#auto-encoding" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/VQ-VAE-e56af23289844662b653be10667bf239?pvs=4">VQVAE</a></li>
</ul>
<h3><span id="masked-modeling">Masked Modeling</span><a href="#masked-modeling" class="header-anchor">#</a></h3><ul>
<li>MAE</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Guidance</title>
    <url>/www6vHomeAIGC/2023/08/01/gptDiffusionGuidance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="guidance">Guidance</span><a href="#guidance" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Guidance-bcb0b5a85b5f454fa84875eaeb518983?pvs=4">Guidance</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SD XL</title>
    <url>/www6vHomeAIGC/2023/08/01/gptDiffusionXL/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="sdxl">SDXL</span><a href="#sdxl" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SDXL-0446aba46c8e400d8583fde17d8df264?pvs=4">SDXL</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)Controllable</title>
    <url>/www6vHomeAIGC/2023/07/29/gptDiffusionControllable/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> <a href="http://doi.org/10.1007/s11390-024-3814-0">A Survey of Multimodal Controllable Diffusion Models</a></li>
</ul>
<h1><span id="论文解析">论文解析</span><a href="#论文解析" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/A-Survey-of-Multimodal-Controllable-Diffusion-Models-b736974658bd4ad79f4128690b0cfb3a?pvs=4">论文解析</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(Work|实战)Image Editing</title>
    <url>/www6vHomeAIGC/2023/07/27/gptDiffusionImageEditWork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#prompt-to-prompt">Prompt-to-Prompt</a></li>
<li><a href="#instructpix2pix">InstructPix2Pix</a></li>
<li><a href="#mgie">MGIE</a></li>
<li><a href="#pix2pix-zero">pix2pix-zero</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="prompt-to-prompt">Prompt-to-Prompt</span><a href="#prompt-to-prompt" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/Prompt-to-Prompt-3ae01e342c6b4b41adc58c6ec5233020">解析</a></li>
</ul>
<h1><span id="instructpix2pix">InstructPix2Pix</span><a href="#instructpix2pix" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/InstructPix2Pix-aedf2e9b6acd48fab6928f717065288c?pvs=4">解析</a></li>
</ul>
<h1><span id="mgie">MGIE</span><a href="#mgie" class="header-anchor">#</a></h1><ul>
<li>解析<br><a href="https://candied-skunk-1ca.notion.site/MGIE-d6bfedcc92ae42a48a64ac199ce2aa14?pvs=4">解析</a></li>
</ul>
<h1><span id="pix2pix-zero">pix2pix-zero</span><a href="#pix2pix-zero" class="header-anchor">#</a></h1><ul>
<li>论文解析<br><a href="https://candied-skunk-1ca.notion.site/pix2pix-zero-f79b1c40cde44f40aeeb8eb34482aa28?pvs=4">论文解析</a></li>
</ul>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><ul>
<li><p>Prompt-to-Prompt<br>train-free，察觉到了attention map的妙用</p>
</li>
<li><p>pix2pix-zero<br>察觉到了attention map的妙用  </p>
</li>
<li><p>InstructPix2Pix<br>trainable，training数据基于Prompt-to-Prompt </p>
</li>
<li><p>MGIE<br>基于LMM</p>
</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>CV 任务</title>
    <url>/www6vHomeAIGC/2023/07/25/gptVisionTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="分类-1">分类 [1]</span><a href="#分类-1" class="header-anchor">#</a></h1><h3><span id="image-level">image-level</span><a href="#image-level" class="header-anchor">#</a></h3><ul>
<li><p>image recognition</p>
</li>
<li><p>(Retrieval)image-text retrieval</p>
</li>
<li><p>Caption(image captioning) </p>
</li>
<li><p>VQA(visual question answering)</p>
</li>
</ul>
<h3><span id="region-level">region-level</span><a href="#region-level" class="header-anchor">#</a></h3><ul>
<li><p>Object Detection object detection</p>
<ul>
<li>DETR -&gt; DINO -&gt; Grounding DINO</li>
</ul>
</li>
<li><p>dense caption</p>
</li>
<li><p>phrase grounding</p>
</li>
</ul>
<h3><span id="pixel-level">pixel-level</span><a href="#pixel-level" class="header-anchor">#</a></h3><ul>
<li>Segmentation<ul>
<li>generic segmetation</li>
<li>referring segmetation</li>
</ul>
</li>
</ul>
<h1><span id="其他">其他</span><a href="#其他" class="header-anchor">#</a></h1><ul>
<li><p>对比</p>
<ul>
<li>[CNN  更深的网络]</li>
<li>[transformer 没有局限]</li>
</ul>
</li>
<li><p>CV任务</p>
<ul>
<li>分类（Classification）</li>
<li>检测（Detection）</li>
<li>分割（Segmentation）</li>
<li>跟踪（Tracking）</li>
<li>行为识别（Action Recognition）</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>[<a href="https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036">CVPR Tutorial Talk] Towards General Vision Understanding Interface</a><br><a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf">pdf</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Vision</category>
      </categories>
      <tags>
        <tag>Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)DiT</title>
    <url>/www6vHomeAIGC/2023/07/21/gptDiffusionDiT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a> </p>
</li>
<li><p>开源地址<br> <a href="https://www.wpeebles.com/DiT">Scalable Diffusion Models with Transformers</a> git<br> <a href="https://github.com/facebookresearch/DiT">DiT Repo</a>  git</p>
</li>
</ul>
<h1><span id="arch">Arch</span><a href="#arch" class="header-anchor">#</a></h1><p><img src="https://www.wpeebles.com/images/DiT/block.png" alt="DiT"></p>
<h1><span id="code10">code[10]</span><a href="#code10" class="header-anchor">#</a></h1><ul>
<li>DiT<br><a href="https://github.com/www6v/mnist-dits/blob/main/dit.py">dit</a> git</li>
<li>DiTBlock<br><a href="https://github.com/www6v/mnist-dits/blob/main/dit_block.py">dit_block</a> git</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://www.bilibili.com/video/BV12E421T7Zi/">AI大讲堂：文生视频谁能敌？专业拆解【DiT模型】</a> V<br>   DiT原文: <a href="https://arxiv.org/abs/2212.09748">https://arxiv.org/abs/2212.09748</a><br>   Code: <a href="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a><br>   Huggingface: <a href="https://huggingface.co/spaces/wpeebles/DiT">https://huggingface.co/spaces/wpeebles/DiT</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV12J4m1379T/">14步手搓sora!Diffusion Transformer, DiT工作原理</a>  V </p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.bilibili.com/video/BV13K421h79z/">【Sora重要技术】复现DiT（Diffusion Transformer）模型</a> V ***<br>   <a href="https://github.com/owenliang/mnist-dits">dits Repo</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(Work|实战)Controllable</title>
    <url>/www6vHomeAIGC/2023/07/17/gptDiffusionControllableWork/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>




<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/controllable-763edf3a43b94e03a1ff0faee9ac41c2?pvs=4">ControlNet + t2i_adapter + Custom Diffusion </a>  diffusers</li>
</ul>
<h1><span id="总结metaso">总结[metaso]</span><a href="#总结metaso" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th></th>
<th><strong>功能定位</strong></th>
<th><strong>性能与效率</strong></th>
<th><strong>应用场景</strong></th>
<th>总结</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ControlNet</strong></td>
<td>主要用于对图像生成过程中的<strong>特定部分</strong>进行控制</td>
<td><strong>较大</strong>且可能需要较多计算资源</td>
<td>适用于需要对图像特定区域进行<strong>精细控制</strong>的场景</td>
<td>【ControlNet  <strong>结构控制</strong>， image2image】</td>
</tr>
<tr>
<td><strong>T2I-Adapter</strong></td>
<td>专注于将<strong>文本提示转换为图像</strong></td>
<td><strong>较小</strong>且更高效，适合资源受限的环境</td>
<td>适用于需要<strong>从文本描述生成图像</strong>的场景</td>
<td>【T2I-Adapter  <strong>多种控制</strong>, text2image】</td>
</tr>
<tr>
<td><strong>IP-Adapter</strong></td>
<td>用于<strong>分析图像提示并提取特征</strong>，再将其用于图像生成</td>
<td>在图像质量和对齐方面表现优异</td>
<td>适用于需要结合图像和文本提示进行<strong>复杂图像生成</strong>的场景</td>
<td>【IP-Adapter <strong>风格特征控制</strong>,  text2image|image2image 】</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Diffusion</title>
    <url>/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#api-based">API-based</a><ul>
<li><a href="#diffusion-models-class-%E5%AE%98%E6%96%B9%E8%AF%BE%E7%A8%8B">diffusion-models-class [官方课程]</a></li>
<li><a href="#diffusers-%E9%87%8D%E7%82%B9pipeline-10">diffusers 重点pipeline [10]</a></li>
</ul>
</li>
<li><a href="#ui-based">UI-based</a><ul>
<li><a href="#stable-diffusion-webui">stable-diffusion-webui</a></li>
<li><a href="#comfyui">comfyui</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#api-based-1">API-based</a></li>
<li><a href="#ui-based-1">UI-based</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="api-based">API-based</span><a href="#api-based" class="header-anchor">#</a></h1><h3><span id="diffusion-models-class-官方课程">diffusion-models-class [官方课程]</span><a href="#diffusion-models-class-官方课程" class="header-anchor">#</a></h3><ul>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-1-An-Introduction-to-Diffusion-Models-f0ee4c8bc4914ef8961b48241064b2b7?pvs=4">Unit 1: An Introduction to Diffusion Models</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-2-Fine-Tuning-Guidance-and-Conditioning-27180b80a58e4bd2860019c4237a8532?pvs=4">Unit 2: Fine-Tuning, Guidance and Conditioning</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-3-Stable-Diffusion-a8770ac5b0214c2f9cfce878812b5bf8?pvs=4">Unit 3: Stable Diffusion</a></li>
<li><a href="https://candied-skunk-1ca.notion.site/Unit-4-Going-Further-with-Diffusion-Models-e997fe47d4e64069bba59ac81b7a4718?pvs=4">Unit 4: Going Further with Diffusion Models</a></li>
</ul>
<h3><span id="diffusers-重点pipeline-10">diffusers 重点pipeline [10]</span><a href="#diffusers-重点pipeline-10" class="header-anchor">#</a></h3><ul>
<li>controlnet 【controllable】</li>
<li>dreambooth 【fine tuning】</li>
<li>instruct_pix2pix 【image edit】</li>
</ul>
<h1><span id="ui-based">UI-based</span><a href="#ui-based" class="header-anchor">#</a></h1><h3><span id="stable-diffusion-webui">stable-diffusion-webui</span><a href="#stable-diffusion-webui" class="header-anchor">#</a></h3><ul>
<li><p>stable-diffusion-webui-colab[11]<br>没试过，colab要充值</p>
</li>
<li><p>stable-diffusion-webui   on   阿里serverless [12]  </p>
<img src="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/webui.jpg" class></li>
</ul>
<h3><span id="comfyui">comfyui</span><a href="#comfyui" class="header-anchor">#</a></h3><ul>
<li>ComfyUI  on  阿里serverless[13]<img src="/www6vHomeAIGC/2023/07/09/gptMultimodalDiffusionPractice/comfyUI.jpg" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="api-based">API-based</span><a href="#api-based" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://github.com/huggingface/diffusers/tree/main/examples">Repo diffusers</a> git</li>
</ol>
<h3><span id="ui-based">UI-based</span><a href="#ui-based" class="header-anchor">#</a></h3><ol start="11">
<li><p><a href="https://www.bilibili.com/video/BV1QS421A7zF/">可白嫖且很香—轻轻松松在colab上部署Stable Diffusion大模型！</a> V<br> <a href="https://github.com/camenduru/stable-diffusion-webui-colab">stable-diffusion-webui-colab Repo</a> git<br> <a href="https://github.com/camenduru/stable-diffusion-webui-colab/tree/drive">Install the WebUI Colab to Google Drive </a> git 运行这3个脚本</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV19u411x7Bk/">超详细云端部署Stable Diffusion教程！</a> V<br>【用FC的应用模版部署】【3个月免费的serverless+NAS】</p>
</li>
<li><p><a href="https://alidocs.dingtalk.com/i/p/x9JOGOjr65om4QLAy0mVPNbMnOEE8z89">函数计算 ComfyUI 使用文档</a><br>用 ComfyUI 自制“粘土滤镜<br>【用FC的应用模版部署】【3个月免费的serverless+NAS】</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(work|实战) fine-tuning</title>
    <url>/www6vHomeAIGC/2023/07/06/gptDiffusionFineTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="fine-tuning">Fine-tuning</span><a href="#fine-tuning" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/work-fine-tuning-71037b41f5c04bd8adf2cfc1c5881bfe?pvs=4">Fine-tuning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)Image Editing</title>
    <url>/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%911">图像编辑[1]</a><ul>
<li><a href="#%E5%A4%A7%E7%B1%BB">大类</a></li>
<li><a href="#approaches">APPROACHES</a></li>
</ul>
</li>
<li><a href="#%E8%AE%BA%E6%96%872">论文[2]</a></li>
<li><a href="#survey2">Survey[2]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2402.17525">《Diffusion Model-Based Image Editing: A Survey》</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">Repo</a> git</p>
</li>
</ul>
<h1><span id="图像编辑1">图像编辑[1]</span><a href="#图像编辑1" class="header-anchor">#</a></h1><h3><span id="大类">大类</span><a href="#大类" class="header-anchor">#</a></h3><ul>
<li>从图片编辑的任务方面可以被分为3个大类<ul>
<li>语义编辑semantic editing </li>
<li>风格编辑stylistic editing</li>
<li>结构编辑structural editing</li>
</ul>
</li>
</ul>
<h3><span id="approaches">APPROACHES</span><a href="#approaches" class="header-anchor">#</a></h3><ul>
<li><p>TRAINING-BASED APPROACHES</p>
<ul>
<li>InstructPix2Pix</li>
</ul>
</li>
<li><p>TESTING-TIME FINETUNING APPROACHES</p>
</li>
<li><p>TRAINING AND FINETUNING FREE APPROACHES</p>
</li>
</ul>
<hr>
<h1><span id="论文2">论文[2]</span><a href="#论文2" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工</p>
</li>
<li><p>开源地址<br> <a href="https://github.com/xinchengshuai/Awesome-Image-Editing">A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models</a></p>
</li>
</ul>
<h1><span id="survey2">Survey[2]</span><a href="#survey2" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/07/06/gptDiffusionImageEdit/survey.png" class>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol>
<li><p>《Diffusion Model-Based Image Editing: A Survey》<br><a href="https://blog.csdn.net/huzimu_/article/details/136547375">论文阅读：Diffusion Model-Based Image Editing: A Survey</a><br><a href="https://mp.weixin.qq.com/s/MFbCt0XfOf9fV0YbdkmR6g">基于扩散模型的图像编辑：首篇综述</a></p>
</li>
<li><p>《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》<br><a href="https://www.jiqizhixin.com/articles/2024-06-28-14">300多篇相关研究，复旦、南洋理工最新多模态图像编辑综述论文</a></p>
</li>
</ol>
<p>1xx. 《LLMs Meet Multimodal Generation and Editing: A Survey》 *<br>Image Generation， Image Editing<br><a href="https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation">Repo</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Diffusion</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
      </tags>
  </entry>
  <entry>
    <title>显存估算</title>
    <url>/www6vHomeAIGC/2023/07/01/gptGPUComputing/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="显存估算">显存估算</span><a href="#显存估算" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/e4e6bd5f7c43430fa2e805c5a2777308?pvs=4">显存估算</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent - UI-assistants</title>
    <url>/www6vHomeAIGC/2023/06/30/gptAgentMultimodalApp/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="app-agent">App Agent</span><a href="#app-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/677071947">AppAgent源码分析&amp;思考</a><br><a href="https://github.com/mnotgod96/AppAgent">https://github.com/mnotgod96/AppAgent</a><br><a href="https://icoz69.github.io/">https://icoz69.github.io/</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/681424409">【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent</a><br>   <a href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a></p>
<p>1xx. <a href="https://github.com/OpenAdaptAI/OpenAdapt">https://github.com/OpenAdaptAI/OpenAdapt</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Diffusion</title>
    <url>/www6vHomeAIGC/2023/06/29/gptMultimodalDiffusion/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="diffusion-原理">Diffusion 原理</span><a href="#diffusion-原理" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Diffusion-8cdcc8079d7d42c2a193ae6baf06246e?pvs=4">(原理)Diffusion</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)CLIP</title>
    <url>/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#clip-training-9">CLIP Training [9]</a></li>
<li><a href="#simple-demo10">Simple Demo[10]</a></li>
<li><a href="#open_clip11">open_clip[11]</a></li>
<li><a href="#chinese-clip">Chinese-CLIP</a><ul>
<li><a href="#%E6%96%B9%E6%B3%9520">方法[20]</a></li>
<li><a href="#demo21">demo[21]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
<li><a href="#chinese-clip-1">Chinese-CLIP</a></li>
<li><a href="#xxx">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="clip-training-9">CLIP Training [9]</span><a href="#clip-training-9" class="header-anchor">#</a></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - ResNet or Vision Transformer</span><br><span class="line"># text_encoder - CBOW or Text Transformer</span><br><span class="line"># I[n, h, w, c] - minibatch of aligned images</span><br><span class="line"># T[n, l] - minibatch of aligned texts</span><br><span class="line"># W_i[d_i, d_e] - learned proj of image to embed</span><br><span class="line"># W_t[d_t, d_e] - learned proj of text to embed</span><br><span class="line"># t - learned temperature parameter</span><br><span class="line"># extract feature representations of each modality</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 1、图像/文字数据过image/text encoder，提取单模态特征</span><br><span class="line"># 每张图片对应一个基本特征I_i</span><br><span class="line"># 每张文字对应一个基本特征T_i</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_f = image_encoder(I) #[n, d_i]</span><br><span class="line">T_f = text_encoder(T) #[n, d_t]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 2. 图像/文字的基本特征过多模态Embedding，提取多模态特征</span><br><span class="line"># 同时对这两个多模态特征做Layer Norm</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1) # [n, d_i] * [d_i, d_e] = [n, d_e]</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # [n, d_t] * [d_t, d_e] = [n, d_e]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 3、计算图片-文字向量的余弦相似度</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t) # [n, n]</span><br><span class="line"></span><br><span class="line"># -------------------------------------------------</span><br><span class="line"># 4、计算Loss</span><br><span class="line"># -------------------------------------------------</span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t)/2</span><br></pre></td></tr></table></figure>



<ul>
<li>CLIP分为<strong>按行计算Loss</strong>和<strong>按列计算Loss</strong></li>
<li><strong>按行计算Loss</strong>，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字。</li>
<li><strong>按列计算Loss</strong>，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片。</li>
<li><strong>最后将这两个Loss相加取平均</strong>，代表我们在模型优化过程中<strong>考虑了“图片-&gt;文字”和“文字-&gt;图片”的双向关系</strong>。</li>
</ul>
<h1><span id="simple-demo10">Simple Demo[10]</span><a href="#simple-demo10" class="header-anchor">#</a></h1><p>【基于clip on  resnet,   数据集为mnist中的&lt;数字文本，数字图片&gt;对】</p>
<h1><span id="open_clip11">open_clip[11]</span><a href="#open_clip11" class="header-anchor">#</a></h1><ul>
<li>Training CLIP</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m training.main \</span><br><span class="line">    --save-frequency <span class="number">1</span> \</span><br><span class="line">    --zeroshot-frequency <span class="number">1</span> \</span><br><span class="line">    --report-to tensorboard \</span><br><span class="line">    --train-data=<span class="string">&quot;/path/to/train_data.csv&quot;</span>  \      <span class="comment"># 训练数据 </span></span><br><span class="line">    --val-data=<span class="string">&quot;/path/to/validation_data.csv&quot;</span>  \   <span class="comment"># 验证数据</span></span><br><span class="line">    --csv-img-key filepath \</span><br><span class="line">    --csv-caption-key title \</span><br><span class="line">    --imagenet-val=/path/to/imagenet/root/val/ \</span><br><span class="line">    --warmup <span class="number">10000</span> \      <span class="comment">#</span></span><br><span class="line">    --batch-size=<span class="number">128</span> \    <span class="comment">#</span></span><br><span class="line">    --lr=<span class="number">1e-3</span> \           <span class="comment">#</span></span><br><span class="line">    --wd=<span class="number">0.1</span> \       </span><br><span class="line">    --epochs=<span class="number">30</span> \         <span class="comment">#</span></span><br><span class="line">    --workers=<span class="number">8</span> \</span><br><span class="line">    --model RN50          <span class="comment"># 模型</span></span><br></pre></td></tr></table></figure>

<h1><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h1><h3><span id="方法20">方法[20]</span><a href="#方法20" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/06/25/gptMultimodalCLIPPractice/chinese-clip.webp" class>

<p>我们的核心方法在于把训练分为<strong>两阶段</strong>（如上图所示），<strong>第一阶段</strong>和LiT是一致的，<strong>冻结图像塔</strong>，<strong>让文本塔表示接近图像塔表示</strong>。当训练继续但下游精度不能再产生显著提升，即下游零样本检索的精度，我们就把训练切换到<strong>第二阶段</strong>，即<strong>解除图像塔的参数冻结，继续用contrastive tuning预训练</strong>，同样直到下游精度没有显著提升。<strong>后者的意义在于让图像塔能拟合中文世界的图像数据的分布，学习中文世界的知识</strong>。更多实验参数欢迎查看论文的附录部分。</p>
<h3><span id="demo21">demo[21]</span><a href="#demo21" class="header-anchor">#</a></h3><p>代码都看过</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 图片库特征抽取代码</span></span><br><span class="line">python3 extract_embeddings.py </span><br><span class="line"><span class="comment"># 图片特征在faiss向量数据库建立索引   </span></span><br><span class="line">python3 build_index.py</span><br><span class="line"><span class="comment"># 可视化应用界面 </span></span><br><span class="line">streamlit run app.py</span><br></pre></td></tr></table></figure>

<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><ol start="9">
<li><p><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV13K421v7Ar/">【多模态】复现OpenAI的CLIP模型</a> V<br><a href="https://github.com/owenliang/mnist-clip">mnist-clip Repo</a> git</p>
</li>
<li><p><a href="https://github.com/mlfoundations/open_clip">open_clip Repo</a> git<br><a href="https://colab.research.google.com/drive/1TEUe2j2oXi-sKiteGYUhsCtdvXocI24w#scrollTo=YPHN7PJgKOzb">Interacting with open_clip</a></p>
</li>
</ol>
<h3><span id="chinese-clip">Chinese-CLIP</span><a href="#chinese-clip" class="header-anchor">#</a></h3><ol start="20">
<li><p><a href="https://zhuanlan.zhihu.com/p/580546929">中文CLIP模型卷土重来，这次加量不加价！</a> 论文</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/680405647">AIGC之图片生成——基于clip内容检索</a><br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu/tree/main/APP_example/clip_retrieval">clip_retrieval</a> git<br><a href="https://github.com/liangwq/Chatglm_lora_multi-gpu">demos Repo</a>  readme有解释</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/539374033">【已重新开源】CLIP的中文副本？说不定有惊喜呢</a></p>
<p>1xx. <a href="https://github.com/www6v/Chinese-CLIP">Chinese-CLIP Repo</a> git</p>
<p>1xx. <a href="https://modelscope.cn/studios/iic/chinese_clip_applications/summary">中文CLIP文到图搜索应用</a> demo</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><p>1xx. langchain 中有CLIP的实现</p>
<p>1xx. <a href="https://github.com/jina-ai/clip-as-service">GitHub - jina-ai&#x2F;clip-as-service: Scalable embedding, reasoning, ranking for images and sentences with CLIP</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/16/gptInferRayPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a><ul>
<li><a href="#%E7%8E%AF%E5%A2%83">环境</a></li>
<li><a href="#%E5%AE%9E%E6%88%981">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%98320">实战3[20]</a></li>
<li><a href="#%E5%AE%9E%E6%88%984">实战4</a></li>
</ul>
</li>
<li><a href="#monitor40">monitor[40]</a><ul>
<li><a href="#ray-dashboard41">Ray Dashboard[41]</a></li>
<li><a href="#ray-logging">Ray logging</a></li>
<li><a href="#built-in-ray-serve-metrics">Built-in Ray Serve metrics</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%AE%9E%E6%88%981-1">实战1</a></li>
<li><a href="#%E5%AE%9E%E6%88%982-1">实战2</a></li>
<li><a href="#%E5%AE%9E%E6%88%983">实战3</a></li>
<li><a href="#%E5%AE%9E%E6%88%984-1">实战4</a></li>
<li><a href="#monitor">monitor</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h1><h3><span id="环境">环境</span><a href="#环境" class="header-anchor">#</a></h3><p>modelscope  GPU</p>
<h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ul>
<li><p>脚本[1]</p>
</li>
<li><p>遇到的异常[2]</p>
</li>
</ul>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ul>
<li><p>脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## 变更模型名字</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## import &#x27;modelscope&#x27; package</span></span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>异常[11]</p>
</li>
</ul>
<h3><span id="实战320">实战3[20]</span><a href="#实战320" class="header-anchor">#</a></h3><ul>
<li>脚本<br>vllm   0.2.3 -&gt; 报异常<br>vllm  0.3.3 -&gt; 报另一个异常</li>
</ul>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ul>
<li><p>脚本 [30]</p>
</li>
<li><p>异常 [31]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 运行这个命令报异常</span><br><span class="line">python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h1><span id="monitor40">monitor[40]</span><a href="#monitor40" class="header-anchor">#</a></h1><h3><span id="ray-dashboard41">Ray Dashboard[41]</span><a href="#ray-dashboard41" class="header-anchor">#</a></h3><h3><span id="ray-logging">Ray logging</span><a href="#ray-logging" class="header-anchor">#</a></h3><p>Loki  grafana</p>
<h3><span id="built-in-ray-serve-metrics">Built-in Ray Serve metrics</span><a href="#built-in-ray-serve-metrics" class="header-anchor">#</a></h3><p>Prometheus </p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="实战1">实战1</span><a href="#实战1" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://docs.ray.io/en/master/serve/tutorials/vllm-example.html">Serve a Large Language Model with vLLM</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/60750288/invalid-device-id-when-using-pytorch-dataparallel">Invalid device id when using pytorch dataparallel！</a>  运行时碰到的异常</p>
</li>
</ol>
<h3><span id="实战2">实战2</span><a href="#实战2" class="header-anchor">#</a></h3><ol start="10">
<li><p><a href="https://github.com/vllm-project/vllm/blob/main/examples/offline_inference_distributed.py">examples&#x2F;offline_inference_distributed.py</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device</a></p>
</li>
</ol>
<h3><span id="实战3">实战3</span><a href="#实战3" class="header-anchor">#</a></h3><ol start="20">
<li><a href="https://github.com/asprenger/ray_vllm_inference">Ray vLLM Interence</a></li>
</ol>
<p>1xx. <a href="https://github.com/ray-project/langchain-ray/tree/main">GitHub - ray-project&#x2F;langchain-ray: Examples on how to use LangChain and Ray</a> git</p>
<h3><span id="实战4">实战4</span><a href="#实战4" class="header-anchor">#</a></h3><ol start="30">
<li><p><a href="https://blog.csdn.net/engchina/article/details/135455197">在甲骨文云上用 Ray +Vllm 部署 Mixtral 8*7B 模型_mixtral 8x7b 部署-CSDN博客</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zh515858237/article/details/135262401">报错:RuntimeError: CUDA error: no kernel image is available for execution on the device-CSDN博客</a></p>
</li>
</ol>
<h3><span id="monitor">monitor</span><a href="#monitor" class="header-anchor">#</a></h3><ol start="40">
<li><p><a href="https://docs.ray.io/en/master/serve/monitoring.html">Monitor Your Application</a></p>
</li>
<li><p><a href="https://docs.ray.io/en/master/ray-observability/getting-started.html">Ray Dashboard </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Flash Attention</title>
    <url>/www6vHomeAIGC/2023/06/13/gptFlashAttention/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="flash-attention">Flash Attention</span><a href="#flash-attention" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Flash-Attention-2e424082ae3a46b8b1ddd24ead847dd9?pvs=4">(原理)Flash Attention</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>FlashAttention</category>
      </categories>
      <tags>
        <tag>FlashAttention</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战) vLLM</title>
    <url>/www6vHomeAIGC/2023/06/12/gptInfervLLMPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="vllm-实战">vLLM 实战</span><a href="#vllm-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vllm-a35a50c4cd2c4875a8de173575275217?pvs=4">(实战) vLLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理 Ray</title>
    <url>/www6vHomeAIGC/2023/06/11/gptInferRay/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="architecture-overview">Architecture Overview</span><a href="#architecture-overview" class="header-anchor">#</a></h1><h3><span id="application-concepts-1">Application concepts [1]</span><a href="#application-concepts-1" class="header-anchor">#</a></h3><ul>
<li>Task - A remote function invocation. </li>
<li>Object - An application value.</li>
<li>Actor - a stateful worker process (an instance of a <code>@ray.remote</code> class).</li>
<li>Driver - The program root, or the “main” program.</li>
<li>Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment.</li>
</ul>
<h3><span id="design-1">Design [1]</span><a href="#design-1" class="header-anchor">#</a></h3><ul>
<li>Components<ul>
<li>One or more worker processes</li>
<li>A raylet. <ul>
<li>scheduler</li>
<li>object store</li>
</ul>
</li>
<li>head node<ul>
<li>Global Control Service (GCS)</li>
<li>driver process(es)</li>
<li>cluster-level services</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="spark-vs-ray10">Spark vs. Ray[10]</span><a href="#spark-vs-ray10" class="header-anchor">#</a></h1><ul>
<li><p>总的来说，Ray和Spark的主要差别在于他们的<strong>抽象层次</strong>。<strong>Spark</strong>对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。<strong>Ray</strong>的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，<strong>Ray揭示和暴露了并行，而Spark抽象和隐藏了并行</strong>。</p>
</li>
<li><p>就架构而言，<strong>Spark</strong>采用<strong>BSP模型</strong>，是无副作用的，而<strong>Ray</strong>本质上是一个<strong>RPC 框架+Actor框架+对象存储</strong>。</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://developer.volcengine.com/articles/7241442880106004536">基于 Ray 的大规模离线推理</a> 字节<br>   <a href="https://mp.weixin.qq.com/s/mU2RymHIHj8mJiDWBUAdWA">字节跳动基于 Ray 的大规模离线推理</a></p>
<p>1xx. <a href="https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.eg7m6lz2y48u">Ray Design Patterns</a> 查看-&gt;模式</p>
<p>1xx. <a href="https://blog.csdn.net/2401_83124266/article/details/136428395">大模型训练部署利器–开源分布式计算框架Ray原理介绍</a></p>
<h3><span id="spark-vs-ray">Spark vs. Ray</span><a href="#spark-vs-ray" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.zhihu.com/question/432813259/answer/2335473370">加州大学伯克利分校为何能连续孵化出 Mesos,Spark,Alluxio,Ray 等重量级开源项目?</a> 孙挺Sunt</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/junerli/article/details/138476201">分布式领域计算模型及Spark&amp;Ray实现对比</a></p>
<h3><span id="internal">Internal</span><a href="#internal" class="header-anchor">#</a></h3><ol>
<li><a href="https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.iyrm5j2gcdoq">Ray v2 Architecture</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/111340572">Ray 分布式计算框架介绍</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/344736949">Ray 1.0 架构解读</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) TensorRT-LLM</title>
    <url>/www6vHomeAIGC/2023/06/02/gptInferTensorRT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="tensorrt-llm">TensorRT-LLM</span><a href="#tensorrt-llm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/TensorRT-LLM-11dbfe2110848030b7d5f27b9e9bda76?pvs=4">(原理|实战) TensorRT-LLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) KV Cache</title>
    <url>/www6vHomeAIGC/2023/06/01/gptInferKVCache/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/KV-Cache-52168038d1874bce9d5cf68c5930f5c1?pvs=4">(原理) KV Cache</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) vLLM</title>
    <url>/www6vHomeAIGC/2023/05/31/gptInfervLLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="vllm">vLLM</span><a href="#vllm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/vLLM-ccb00d32fef14f92b0f7ab4c1c1db390?pvs=4">(原理) vLLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>infer</category>
      </categories>
      <tags>
        <tag>infer</tag>
      </tags>
  </entry>
  <entry>
    <title>多轮对话</title>
    <url>/www6vHomeAIGC/2023/05/28/gptDialogue/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D">传统的多轮对话</a><ul>
<li><a href="#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D12">多轮对话[1][2]</a></li>
<li><a href="#%E9%9A%BE%E7%82%B93">难点[3]</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B-2">基础能力 [2]</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Ellm%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D4">基于LLM的多轮对话[4]</a><ul>
<li><a href="#types">Types</a></li>
<li><a href="#evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="传统的多轮对话">传统的多轮对话</span><a href="#传统的多轮对话" class="header-anchor">#</a></h1><h3><span id="多轮对话12">多轮对话[1][2]</span><a href="#多轮对话12" class="header-anchor">#</a></h3><ul>
<li>NLU<ul>
<li>意图(intent)分类 [3]   </li>
<li>槽位抽取</li>
</ul>
</li>
<li>DM<br>DST + DP(Policy)</li>
<li>NLG</li>
</ul>
<blockquote>
<p>多轮-上下文</p>
</blockquote>
<h3><span id="难点3">难点[3]</span><a href="#难点3" class="header-anchor">#</a></h3><ul>
<li>上下文信息丢失</li>
<li>指代词识别</li>
</ul>
<h3><span id="基础能力-2">基础能力 [2]</span><a href="#基础能力-2" class="header-anchor">#</a></h3><ul>
<li>意图识别</li>
<li>情绪识别</li>
</ul>
<h1><span id="基于llm的多轮对话4">基于LLM的多轮对话[4]</span><a href="#基于llm的多轮对话4" class="header-anchor">#</a></h1><h3><span id="types">Types</span><a href="#types" class="header-anchor">#</a></h3><ul>
<li><p>Task-oriented dialogue system<br> NLU -&gt; DST -&gt; DPL-&gt; NLG</p>
</li>
<li><p>open-domain dialogue system</p>
</li>
</ul>
<h3><span id="evolution-of-lm-based-dialogue-system">evolution of LM-based dialogue system</span><a href="#evolution-of-lm-based-dialogue-system" class="header-anchor">#</a></h3><ul>
<li><p>Fusion within Task-oriented dialogue system(task)</p>
<ul>
<li>task<ul>
<li>NLU  DST  DPL【可有可无】</li>
<li>NLG【保留】</li>
</ul>
</li>
<li><strong>end2end Task-oriented DS的出现</strong></li>
</ul>
</li>
<li><p>fusion between TOD and ODD(data)</p>
<ul>
<li>TOD -&gt; ODD<br>Q-TOD</li>
<li>ODD -&gt; TOD<br>UniDS</li>
</ul>
</li>
<li><p>fusion between dialogue modal and language model(model)<br><strong>LLM as DM</strong><br>【LLM本身有对话的能力，需要被激发出来】【instruction tuning】<br>【chat模型是做价值观对齐】</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1dt4y1S7kn/">自然语言处理：多轮对话在工业中的应用-贪心学院</a> *** V</li>
<li><a href="https://www.bilibili.com/video/BV1vZ4y147Qv/">1-人-人对话数据驱动的多轮对话技术探索与实践-孙超博</a> V 美团</li>
<li><a href="https://www.bilibili.com/video/BV1Yt4y1S75w/">人工智能如何在多轮对话中进行意图理解——祝凯华</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1Mb4y137yB/">基于大模型对话系统的前世今生</a>  V<br>《An Survey of the Evolution of Language Model-Based Dialogue Systems》</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247489281&idx=1&sn=0273bf49530a93df16ecf5cb5fbc8f65">前沿重器[37] | 大模型对任务型对话的作用研究</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>对话</category>
      </categories>
      <tags>
        <tag>对话</tag>
      </tags>
  </entry>
  <entry>
    <title>LLama-Factory</title>
    <url>/www6vHomeAIGC/2023/05/24/gptLLamaFactory/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llama-factory">LLama-Factory</span><a href="#llama-factory" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLama-Factory-b6e286a1e9054c5399eebc5ffaeac82e?pvs=4">LLama-Factory</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLamaFactory</category>
      </categories>
      <tags>
        <tag>LLamaFactory</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 算力</title>
    <url>/www6vHomeAIGC/2023/05/23/gptGPU/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="gpu算力">GPU算力</span><a href="#gpu算力" class="header-anchor">#</a></h1><h3><span id="免费1">免费[1]</span><a href="#免费1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/free.JPG" class>

<ul>
<li>modelscope 100小时 GPU</li>
</ul>
<h3><span id="专业收费2">专业收费[2]</span><a href="#专业收费2" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/05/23/gptGPU/cost.JPG" class>


<h1><span id="显卡">显卡</span><a href="#显卡" class="header-anchor">#</a></h1><ul>
<li><p>显卡天梯榜<br> <a href="https://topic.expreview.com/GPU">显卡天梯榜</a></p>
</li>
<li><p>显卡<br>显卡 &#x3D; GPU +  显存</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1fC4y1N7qV/">5种在线GPU算力资源白嫖指南</a> V</li>
<li><a href="https://www.bilibili.com/video/BV1q5411z7HM/">5种专业在线GPU算力资源白嫖指南</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV1Pv4y1f7VV/">【PyTorch深度学习】01 GPU购买与白嫖指南</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Data Selection</title>
    <url>/www6vHomeAIGC/2023/05/05/gptDataSelection/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#ifd1">IFD[1]</a></li>
<li><a href="#mods2">MoDS[2]</a></li>
<li><a href="#deita-3">DEITA [3]</a><ul>
<li><a href="#%E5%A4%8D%E6%9D%82%E6%80%A7%E8%AF%84%E5%88%86">复杂性评分</a></li>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%AF%84%E5%88%86">质量评分</a></li>
<li><a href="#%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%AD%9B%E9%80%89">多样性筛选</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="ifd1">IFD[1]</span><a href="#ifd1" class="header-anchor">#</a></h1><ul>
<li>三个步骤<ul>
<li>Learning from Brief Experience<br>利用少量进行进行<strong>模型初学</strong> </li>
<li>Evaluating Based on Experience<br>利用初学模型计算原始数据中所有<strong>IFD指标</strong><ul>
<li>算法<ul>
<li>条件回答分数（ Conditioned Answer Score，CAS）</li>
<li>直接答案分数（Direct Answer Score，DAS）</li>
<li>指令跟随难度（Instruction-Following Difficulty，IFD）分数</li>
</ul>
</li>
</ul>
</li>
<li>Retraining from Self-Guided Experience<br>利用<strong>樱桃数据</strong>进行模型<strong>重训练</strong></li>
</ul>
</li>
</ul>
<h1><span id="mods2">MoDS[2]</span><a href="#mods2" class="header-anchor">#</a></h1><ul>
<li><p>质量筛选<br>采用OpenAssistant的<strong>reward-model</strong>-debertav3-large-v2模型（一个基于<strong>DeBERTa架构</strong>设计的奖励模型）对数据进行<strong>质量打分</strong>。</p>
</li>
<li><p>多样性筛选<br>为了避免所选质量数据高度相似，通过<strong>K-Center-Greedy算法</strong>进行数据筛选，在最大化多样性的情况下，使指令数据集最小。<br>在该步骤中，采用<strong>BERT模型</strong>为指令数据生成句向量来计算不同数据之间的距离。</p>
</li>
<li><p>必要性筛选</p>
</li>
</ul>
<h1><span id="deita-3">DEITA [3]</span><a href="#deita-3" class="header-anchor">#</a></h1><h3><span id="复杂性评分">复杂性评分</span><a href="#复杂性评分" class="header-anchor">#</a></h3><ul>
<li>复杂性评估的方法  <ul>
<li>Random Selection：随机选择样本。</li>
<li>Instruction Length：按照指令的长度计算复杂性。</li>
<li><strong>Perplexity</strong>：通过预训练模型计算回复的困惑度作为复杂性指标，困惑值越大意味着数据样本越难。</li>
<li><strong>Direct Scoring</strong>：利用ChaGPT给指令的复杂性打分。</li>
<li>Instruction Node：利用ChatGPT将指令转换成语义树，通过树的节点数作为复杂性指标。</li>
<li><strong>Instag Complexity</strong>：利用ChatGPT对部分数据进行打标签，再训练一个Llama模型，再利用训练后的Llama模型对全量数据预测，标签越多说明数据约复杂。</li>
<li><strong>IFD</strong>：指令跟随难度作为复杂性指标。</li>
</ul>
</li>
</ul>
<p>DEITA评估复杂性的方法，主要先对一个小规模种子数据集（2k）进行数据复杂性<strong>扩展</strong>，再利<strong>用ChatGPT对扩展数据进行打分</strong>，并<strong>训练一个Llama1-7B的模型</strong>，最后利用训练后的模型对数据的打分作为复杂性评估指标。</p>
<h3><span id="质量评分">质量评分</span><a href="#质量评分" class="header-anchor">#</a></h3><ul>
<li>质量评估的方法有<ul>
<li>Random Selection：随机选择样本。</li>
<li>Response Length：采用输出长度作为质量评估指标。</li>
<li>Direct Scoring：利用ChatGPT直接评估对特定指令输出结果的准确性。</li>
</ul>
</li>
</ul>
<p>DEITA评估质量的方法，<strong>与评估复杂性方法一致</strong>。先对一个小规模种子数据集（2k，与复杂性数据一致）进行数据质量扩展，再利用ChatGPT对扩展数据进行打分并训练一个Llama1-7B的模型，最后利用训练后的模型对数据的打分作为质量评估指标。</p>
<p><strong>数据质量扩展</strong>，通过特殊的提示词利用ChatGPT对数据的回复部分进行改写，主要是增强回复的有用性、相关性、丰富深度、创造力和提供额外的细节描述。</p>
<h3><span id="多样性筛选">多样性筛选</span><a href="#多样性筛选" class="header-anchor">#</a></h3><p>多样性筛选方法，首先将数据池中的数据按照复杂性和质量的综合得分（复杂性分数*质量分数）进行降序<strong>排序</strong>；<br>然后按顺序逐个取出样本数据x ，<strong>计算x 与筛选池中相邻最近的样本之间距离值</strong>，其中，数据利用Llama1-13B模型进行向量表征，距离计算采用<strong>余弦相似度</strong>。<br>如果<strong>距离值小于 r时</strong>，认为该样本与筛选池中数据相似程度不高，可以<strong>纳入筛选池</strong>；否则<strong>不纳入筛选池</strong>。当筛选池中样本数达到规定样本个数，完成多样性筛选。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/658128530">如何从数据集中自动识别高质量的指令数据-IFD指标的使用</a><br>《From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning》<br>ChatLaw就这么训的</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/671183709">大模型微调技巧 | 高质量指令数据筛选方法-MoDS</a><br>《MoDS: Model-oriented Data Selection for Instruction Tuning》<br> 质量筛选， 多样性筛选，必要性筛选   </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/675928711">DEITA-大模型指令微调的数据高效筛选方法</a></p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/687339776">DEITA：融合复杂度、质量、多样性的高效数据筛选</a><br>   复杂度、质量、多样性</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报 </a><br>《A Survey on Data Selection for Language Models》</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Data Selection</category>
      </categories>
      <tags>
        <tag>Data Selection</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)LIMA, LESS</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataSFTQuality/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lima-1kimi">LIMA [1][kimi]</a></li>
<li><a href="#less-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-10">LESS 核心思想 [10]</a></li>
<li><a href="#less10kimi">LESS[10][kimi]</a><ul>
<li><a href="#%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95">实验方法：</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论：</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#lima">LIMA</a></li>
<li><a href="#less">LESS</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="lima-1kimi">LIMA [1][kimi]</span><a href="#lima-1kimi" class="header-anchor">#</a></h1><p>LIMA（Less Is More for Alignment）的实验通过一系列设计精良的步骤来探究数据质量、多样性以及数量对模型性能的影响，从而得出了<strong>提高数据质量和增加提示多样性比单纯增加数据量更能提升模型性能的结论</strong>。以下是<strong>实验方法</strong>的关键步骤：</p>
<ol>
<li><p><strong>精心策划的微调数据</strong>：LIMA模型在<strong>1000个</strong>精心策划的提示和回复上进行了<strong>微调</strong>，这些数据被设计为模拟真实用户与AI助手的交互。</p>
</li>
<li><p><strong>消融实验</strong>：通过消融实验，研究者们观察了在增加数据量的同时不增加提示多样性时，模型性能的提升是否有限；而在优化数据质量时，性能是否有显著提升。</p>
</li>
<li><p><strong>数据构造</strong>：研究者从Stack Exchange、wikiHow和Pushshift Reddit数据集收集数据，并进行了<strong>质量和多样性</strong>的控制。这些数据集被用来构造训练样本，以确保输入的多样性和输出的一致性。</p>
</li>
<li><p><strong>质量与多样性的对比</strong>：研究者比较了经过质量过滤的Stack Exchange数据和同质化的wikiHow数据对模型性能的影响。结果显示，更<strong>多样化的Stack Exchange数据在性能上优于同质化的wikiHow数据</strong>。 【多样化】</p>
</li>
<li><p><strong>数量的对比</strong>：研究者对从Stack Exchange抽取的指数级增加的训练集进行了测试，发现<strong>训练集的翻倍并没有改善响应质量</strong>，从而说明单纯增加数据量并不一定能提升性能。【数量】</p>
</li>
<li><p><strong>质量控制的实验</strong>：研究者还比较了未经过任何质量或风格过滤的Stack Exchange数据集与经过过滤的数据集上训练的模型性能，发现<strong>过滤后</strong>的数据集上训练的模型性能<strong>更优</strong>。【质量】</p>
</li>
<li><p><strong>人类评估</strong>：为了评估LIMA模型的性能，研究者进行了人类偏好研究，将LIMA的输出与其他几个基线模型的输出进行比较，并让人群工作者选择他们更喜欢的输出。</p>
</li>
</ol>
<p>通过这些实验步骤，LIMA的研究得出了<strong>数据质量和提示多样性对于提升模型性能的重要性远超过单纯增加数据量的结论</strong>。这些发现支持了“浅层对齐假说”，即模型在预训练阶段已经学习到了几乎所有知识和能力，而微调过程主要是学习与人类交互的风格和格式。</p>
<ul>
<li><p>总结 [1]</p>
<p>消融实验显示，<strong>当扩大数据量而不同时扩大提示多样性时，收益会大大减少，而在优化数据质量时，收益会大大增加</strong><br>【<strong>数量</strong> &lt;–&gt; <strong>多样性</strong>  <strong>质量</strong>】</p>
</li>
</ul>
<h1><span id="less-核心思想-10">LESS 核心思想 [10]</span><a href="#less-核心思想-10" class="header-anchor">#</a></h1><p>通过仅给出<strong>少数体现特定能力的示例</strong>，从大量指令数据集中<strong>有效地选择5%有影响力的数据</strong>用于目标指令微调，结果优于全量数据集进行微调，并且所选子集在不同模型参数规模和不同模型系列中仍然普遍有效。</p>
<h1><span id="less10kimi">LESS[10][kimi]</span><a href="#less10kimi" class="header-anchor">#</a></h1><p>LESS（Selecting Influential Data for Targeted Instruction Tuning）的实验方法和相应的结论如下：</p>
<h3><span id="实验方法">实验方法：</span><a href="#实验方法" class="header-anchor">#</a></h3><ol>
<li><p><strong>热身训练（Warmup Training）</strong>：使用LoRA（Low-Rank Adaptation）技术对预训练模型进行热身训练，以适应特定的数据分布。</p>
</li>
<li><p><strong>梯度数据存储（Gradient Data Store）</strong>：构建了一个具有投影低维梯度特征的梯度数据存储，该存储可以重复用于不同的目标任务。</p>
</li>
<li><p><strong>数据选择算法</strong>：利用数据存储和算法选择与体现特定能力的少数示例最相似的训练数据点。?</p>
</li>
<li><p><strong>模型训练</strong>：使用选择的数据子集来训练目标模型。</p>
</li>
<li><p><strong>评估</strong>：在不同的下游任务上评估LESS选择的数据子集的性能，包括MMLU、TYDIQA和BBH数据集。</p>
</li>
</ol>
<h3><span id="结论">结论：</span><a href="#结论" class="header-anchor">#</a></h3><ol>
<li><p><strong>LESS的有效性</strong>：LESS<strong>在不同的模型中都是有效的</strong>，能够在多个评估数据集上提高性能。</p>
</li>
<li><p><strong>数据子集的性能</strong>：<strong>使用LESS选择的5%的数据通常优于使用完整数据集进行训练的结果</strong>。这表明完整数据集可能包含与特定目标任务无关或有害的数据点。</p>
</li>
<li><p><strong>数据的可转移性</strong>：使用较小模型选择的数据可以提高较大模型和不同模型系列的性能，证明了LESS选择的数据具有高度的可转移性。</p>
</li>
<li><p><strong>与其他方法的比较</strong>：LESS是唯一一致有效的方法，相较于其他基线方法（如随机选择、BM25、DSIR、RDS）表现出更好的性能。</p>
</li>
<li><p><strong>计算成本</strong>：LESS的计算成本较高，但由于其有效性，这一成本是合理的。</p>
</li>
<li><p><strong>定性分析</strong>：LESS选择的数据能够体现预期下游应用所需的推理技能，而不是仅仅基于表面形式线索。</p>
</li>
<li><p><strong>局限性</strong>：LESS需要热身训练阶段，这增加了计算负载。此外，使用补全Token的平均梯度可能导致性能问题。还有，最小化验证损失并不总能提高任务性能，且数据选择中的线性度假设是LESS的一个限制。</p>
</li>
</ol>
<p>总体而言，LESS通过选择与目标任务高度相关的数据点，能够在指令微调中实现高效的性能提升，尽管存在一些局限性和计算成本。</p>
<p>【总结:  少量有<strong>质量</strong>的数据  优于  全量数据 】 【数据选择算法】</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="lima">LIMA</span><a href="#lima" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/c50HrOfKOqgqGPVRHf6EpA">大模型微调究竟需要多少数据：从三个现有代表工作看几组结论及一点思考 </a><br>  <strong>指令格式的多样性</strong><br>  《LIMA: Less Is More for Alignment》<br>  《MAYBE ONLY 0.5% DATA IS NEEDED》</li>
</ol>
<p>1xx. <a href="https://blog.csdn.net/jinniulema/article/details/133915276">【论文笔记】LIMA: Less Is More for Alignment</a></p>
<h3><span id="less">LESS</span><a href="#less" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/686007325">LESS：仅选择5%有影响力的数据优于全量数据集进行目标指令微调</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/686687923">LESS 实践：用少量的数据进行目标指令微调</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Data Management</title>
    <url>/www6vHomeAIGC/2023/04/27/gptDataManagement/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%863">数据管理[3]</a><ul>
<li><a href="#pretraining-of-llm">Pretraining of LLM</a></li>
<li><a href="#sft">SFT</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据管理3">数据管理[3]</span><a href="#数据管理3" class="header-anchor">#</a></h1><h3><span id="pretraining-of-llm">Pretraining of LLM</span><a href="#pretraining-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
<ul>
<li><strong>Scaling Laws</strong></li>
<li>Data Repetition</li>
</ul>
</li>
<li><p>Data Quality</p>
<ul>
<li>Deduplication<ul>
<li>N-gram和Hash技术<br><strong>MinHash算法</strong></li>
<li>神经网络方法</li>
<li>语义去重<br>SemDeDup</li>
</ul>
</li>
<li>Quality Filtering   <ul>
<li><strong>分类器</strong></li>
<li><strong>启发式规则</strong></li>
<li>阈值过滤<br>例如基于困惑度（Perplexity）</li>
</ul>
</li>
<li>Diversity &amp; Age<ul>
<li><strong>数据多样性（Diversity）</strong></li>
<li>数据时效性（Age）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="sft">SFT</span><a href="#sft" class="header-anchor">#</a></h3><ul>
<li><p>Data Quantity</p>
</li>
<li><p>Data Quality</p>
<ul>
<li>Instruction Quality<br>Instruction Mining,  LIMA</li>
<li><strong>Instruction Diversity</strong><br><strong>Self-Instruct</strong>,  <strong>#InsTag</strong>， Alpaca</li>
<li><strong>Instruction Complexity</strong><br><strong>WizardLM</strong>,  <strong>#InsTag</strong>, <strong>Evol-Instruct</strong></li>
<li>Prompt Design</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="3">
<li>《Data Management For Large Language Models: A Survey》huawei<br> <a href="https://blog.csdn.net/weixin_60760661/article/details/136058893">大模型的数据管理——论文精读</a><br> <a href="https://github.com/www6v/data_management_LLM">Data Management for LLM</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DataManagement</category>
      </categories>
      <tags>
        <tag>DataManagement</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT Scaling</title>
    <url>/www6vHomeAIGC/2023/04/26/gptDataSFTScaling/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="sft-scaling">SFT Scaling</span><a href="#sft-scaling" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SFT-Scaling-36916e81271a4c1d963d9f357b919508?pvs=4">(原理)SFT Scaling</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)Data  Annotation</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataProcessAnnotation/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h3><p>1xx.  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408650&idx=2&sn=ef8424969be749489188ebd810800f08">如何利用大模型进行数据标注与知识蒸馏：兼看ActiveRAG上下文去噪的大模型RAG问答范式</a><br>   大模型用于数据标注<br>   《Large Language Models for Data Annotation: A Survey》<br>1xx. <a href="https://mp.weixin.qq.com/s/U3kWk_jPaeBzloOhUJ5DXQ">LLM数据标注技术调研：定义、框架、提示、反馈、评价、挑战、机遇 </a> 翻译<br>   《Large Language Models for Data Annotation: A Survey》</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399919&idx=1&sn=66fc1dfdba57744a80c6869b8cf941af">ChatGPT用于数据标注是否可行：基于推特分类、生成内容排序任务的代表性实验报告介绍 </a></p>
<h3><span id="framework">Framework</span><a href="#framework" class="header-anchor">#</a></h3><p>1xx.  AutoLabel - 自助数据标注 </p>
<p>1xx. <a href="https://opendatalab.github.io/labelU/">LabelU 介绍</a><br>   <a href="https://github.com/opendatalab/labelU">LabelU Repo</a> git 上海人工智能实验室</p>
<p>1xx. <a href="https://developer.aliyun.com/article/1311807">InsTag：大语言模型监督微调数据标签标注工具</a>  有相关的paper<br>   <a href="https://www.modelscope.cn/studios/lukeminglkm/instagger_demo/summary">InsTag指令打标工具</a> demo</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(List) Pretrain 数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDataSetPretrainList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="pretrain数据集">Pretrain数据集</span><a href="#pretrain数据集" class="header-anchor">#</a></h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/641187337">LLM大模型数据集之谜</a> Pretrain数据集</li>
<li><a href="https://github.com/brightmart/nlp_chinese_corpus">大规模中文自然语言处理语料</a></li>
<li><a href="https://github.com/Glanvery/LLM-Travel/blob/main/LLM_Pretrain_Datasets.md">开源的可用于LLM Pretrain数据集</a> Pretrain数据集</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399359&idx=1&sn=502c65376e14b20a7dc1ceb35c62141d">大规模语言模型训练必备数据集-The Pile：涵盖22类、800GB的多样性文本数据集概述 </a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>3、RLFH强化与预训练数据集 </li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403405&idx=1&sn=cb53c35efda2b771b4c1f289ae97c1d3">大模型研发必备：两大开源可用且清洗过的中文文本语料库及大模型FLOPS、参数量快速估计工具推荐 </a>           书生·万卷1.0 ,    wudao数据集</li>
<li><a href="https://mp.weixin.qq.com/s/JDkKlD9IKvagCYucPey6UQ">大规模中文开源文本训练数据集的几点启发：兼看两个知识图谱与大模型的练手竞赛 </a><br> 万卷数据集   wudao数据集  MVBNC数据集  OpenNewsArchive</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(List)SFT数据集</title>
    <url>/www6vHomeAIGC/2023/04/24/gptDatasetSFTList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="sft数据集12">SFT数据集[1][2]</span><a href="#sft数据集12" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402424&idx=1&sn=e2d26821b6e9a5a2871e0ddbca565c30">大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析</a><br>1、通用指令微调数据</p>
</li>
<li><p><a href="https://github.com/chaoswork/sft_datasets">开源SFT数据集整理</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Embedding</title>
    <url>/www6vHomeAIGC/2023/04/18/gptEmbedding/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#example-1">example [1]</a></li>
<li><a href="#embedding-%E4%BB%B7%E5%80%BC-2">Embedding 价值 [2]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8-2">应用 [2]</a></li>
<li><a href="#%E5%A4%A9%E6%A2%AF%E6%A6%9C">天梯榜</a></li>
<li><a href="#example3">example[3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

 

<h1><span id="example-1">example [1]</span><a href="#example-1" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong>:   t-SNE  </li>
<li>K-Means 聚类</li>
<li>文本搜索  相似度搜索</li>
</ul>
<h1><span id="embedding-价值-2">Embedding 价值 [2]</span><a href="#embedding-价值-2" class="header-anchor">#</a></h1><ul>
<li><strong>降维</strong><br>将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</li>
<li>捕捉语义信息<br>Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。</li>
<li>泛化能力<br>由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示</li>
</ul>
<h1><span id="应用-2">应用 [2]</span><a href="#应用-2" class="header-anchor">#</a></h1><ul>
<li>语义表示和语义相似度</li>
<li>词语关系和类比推理</li>
<li>上下文理解</li>
<li>文本分类和情感分析</li>
<li>机器翻译和生成模型</li>
</ul>
<h1><span id="天梯榜">天梯榜</span><a href="#天梯榜" class="header-anchor">#</a></h1><p>  <a href="https://huggingface.co/spaces/mteb/leaderboard">mteb&#x2F;leaderboard</a></p>
<h1><span id="example3">example[3]</span><a href="#example3" class="header-anchor">#</a></h1><ul>
<li>m3e模型</li>
<li>bge模型</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://github.com/www6v/openai-quickstart/blob/main/openai_api/embedding.ipynb">embedding</a> git</p>
</li>
<li><p>《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding</p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/135311471">一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge</a></p>
</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Hk4y1X7aG/">如何选取RAG中的embedding模型</a> v ***<br>   <a href="https://huggingface.co/spaces/mteb/leaderboard">huggingface embedding模型排行榜</a><br>   <a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence Bert</a><br>   <a href="https://github.com/blackinkkkxi/RAG_langchain">Demo Repo</a>  git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/qIh07eU8_lYL2gBVzTFzKA">引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案</a><br>   <a href="https://github.com/xlang-ai/instructor-embedding">Repo</a> git</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648406715&idx=1&sn=a680597afdb7d5439a11302c7911795f">也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 </a>        《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/676589001">如何提高LLMs的文本表征(Text Embedding)能力?</a><br>    《Improving Text Embeddings with Large Language Models》</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1ex4y1S7u5/?p=2">文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速）</a>   V</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Embedding</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>(survey)多模态  数据集</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetMulitmodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#survey0">Survey[0]</a></li>
<li><a href="#pre-training%E6%95%B0%E6%8D%AE%E9%9B%86">Pre-training数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86">SFT数据集</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
<li><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86">预训练数据集</a></li>
<li><a href="#sft%E6%95%B0%E6%8D%AE%E9%9B%86-1">SFT数据集</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="survey0">Survey[0]</span><a href="#survey0" class="header-anchor">#</a></h1><ul>
<li>Pre-training</li>
<li>Adaptation</li>
</ul>
<h1><span id="pre-training数据集">Pre-training数据集</span><a href="#pre-training数据集" class="header-anchor">#</a></h1><ul>
<li><p>LAION[1]<br><a href="https://laion.ai/projects/">LAION</a></p>
</li>
<li><p>wukong[1]<br><a href="https://zhuanlan.zhihu.com/p/473794131">[论文]中文多模态数据集WuKong &amp; FILIP &amp; LiT-tuning</a><br><a href="https://zhuanlan.zhihu.com/p/551622338">Wukong：一亿规模的中文跨模态预训练基准</a></p>
</li>
<li><p>MMDialog<br><a href="https://zhuanlan.zhihu.com/p/584894471">百万量级的多模态对话数据集来了，153万张图片4000多主题</a> </p>
</li>
<li><p>OBELISC[2]</p>
</li>
<li><p>ShareGPT4V[3]<br>opensource</p>
</li>
</ul>
<h1><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h1><ul>
<li>LAMM</li>
<li>MultiIntruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol start="0">
<li><a href="https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw">多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 </a><br>《A Survey of Multimodal Large Language Model from A Data-centric Perspective》</li>
</ol>
<h3><span id="预训练数据集">预训练数据集</span><a href="#预训练数据集" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/686757824">多模态数据集收集</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/670149958">[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents</a><br>从网页文档里得到的数据集</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/669485001">超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能</a><br><a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">ShareGPT4V</a> git</p>
</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/527182857">多模态预训练数据集</a></p>
<p>1xx. <a href="https://opendatalab.org.cn/">OpenDataLab</a></p>
<h3><span id="sft数据集">SFT数据集</span><a href="#sft数据集" class="header-anchor">#</a></h3><p>1xx. <a href="https://datac.blog.csdn.net/article/details/135434897">【LMM 015】LAMM：多模态指令微调数据集，框架和基准</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678489834">[NeurIPS2023] LAMM：多模态指令微调数据集、框架、评测基准</a></p>
<p>1xx. <a href="https://www.bilibili.com/video/BV12p4y1M7RV/">Talk | ACL’23 杰出论文，MultiIntruct：通过多模态指令集微调提升VLM的零样本学习</a><br>1xx. <a href="https://blog.csdn.net/qq_45978862/article/details/132008907">【ACL2023】MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)Dataset</title>
    <url>/www6vHomeAIGC/2023/04/01/gptDatasetSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《Datasets for Large Language Models: A Comprehensive Survey》</p>
</li>
<li><p>开源地址<br> <a href="https://github.com/lmmlzn/Awesome-LLMs-Datasets">Awesome-LLMs-Datasets</a> git</p>
</li>
</ul>
<h1><span id="微调数据">微调数据</span><a href="#微调数据" class="header-anchor">#</a></h1><h3><span id="微调数据的构造方式">微调数据的构造方式</span><a href="#微调数据的构造方式" class="header-anchor">#</a></h3><ul>
<li>人工生成的数据集(HG)</li>
<li>模型构建的数据集(MC)<ul>
<li>Alpaca</li>
<li>BELLE</li>
<li>Self-Instruct</li>
<li>ShareGPT</li>
<li>Wizard</li>
</ul>
</li>
<li>现有数据集的收集和改进(CI) </li>
<li>使用多种方法创建的数据集</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409066&idx=1&sn=54e68bbbd45b4cc5bef8fd446fa187f8">大模型训练数据集(从预训到强化)全面综述：兼看20240229大模型早报 </a>***</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>推理常见参数</title>
    <url>/www6vHomeAIGC/2023/03/30/gptTemperature/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="推理常见参数">推理常见参数</span><a href="#推理常见参数" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/4200b90adfd246ab93bfb1b330aa1bb2?pvs=4">推理常见参数</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Temperature</category>
      </categories>
      <tags>
        <tag>Temperature</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)PyTorch</title>
    <url>/www6vHomeAIGC/2023/03/28/gptPytorch/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="pytorch-实战">PyTorch 实战</span><a href="#pytorch-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/PyTorch-bae29b5883fd45f7a20c97918382da12?pvs=4">(实战)PyTorch</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)PTQ-Weight Only</title>
    <url>/www6vHomeAIGC/2023/03/26/gptQuantizationWeight/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="weight-only">Weight Only</span><a href="#weight-only" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Weight-Only-ab8e3953cef144a3aef44716d4068216?pvs=4">Weight Only</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)DeepSpeed Training</title>
    <url>/www6vHomeAIGC/2023/03/25/gptTrainDistributedPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="deepspeed-training">DeepSpeed Training</span><a href="#deepspeed-training" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/98541b7f8be2493eb1deda3629677d26?pvs=4">DeepSpeed Training</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Deepspeed</category>
      </categories>
      <tags>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理) Deepspeed Zero</title>
    <url>/www6vHomeAIGC/2023/03/23/gptTrainZeroDeepspeed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="deepspeed-zero">Deepspeed Zero</span><a href="#deepspeed-zero" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Zero-Deepspeed-85c9344a27624649a36b66f3e2e4c4d1?pvs=74">(原理) Deepspeed Zero</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Zero</category>
      </categories>
      <tags>
        <tag>Zero</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)推理-框架</title>
    <url>/www6vHomeAIGC/2023/03/21/gptInferFramework/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E6%A1%86%E6%9E%B61">推理 框架[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-框架1">推理 框架[1]</span><a href="#推理-框架1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/21/gptInferFramework/inference.jpg" class>

<ul>
<li><p>inference execute engine(server)<br>vLLM，TensorRT， deepspeed</p>
</li>
<li><p>inference execute engine(pc&#x2F;edge 移动端)<br> llama.cpp<br> mlc-llm<br> ollama</p>
</li>
<li><p>inference Server<br>Triton Server,  Ray</p>
</li>
<li><p>Chat Server [2]<br>FastChat, XInference,  modelscope  SWIFT</p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzA5MTIxNTY4MQ==&scene=1&album_id=2959126655292211206">探秘LLM应用开发</a>   8-19</p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2422454">LLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference&#x2F;FastChat等框架]</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142079&idx=1&sn=07d9033203c0064408fe0af33d1f9414">一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461142012&idx=1&sn=dafb0b676cdf6d41fd9bd54f9b6a82d3">一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/659792625">大模型推理框架概述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)数据处理</title>
    <url>/www6vHomeAIGC/2023/03/19/gptDataProcessPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a><ul>
<li><a href="#%E5%8D%83%E5%B8%86llama-2%E4%B8%AD%E6%96%87%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B2%BE%E7%AE%80">数据精简</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">数据配比</a></li>
</ul>
</li>
<li><a href="#%E5%BA%A6%E5%B0%8F%E6%BB%A1%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B31">度小满轩辕金融大模型[31]</a><ul>
<li><a href="#%E9%80%9A%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%B5%81%E6%B0%B4%E7%BA%BF">通用的数据清洗流水线</a></li>
<li><a href="#%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%9C%80%E4%BD%B3%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94">增量预训练 最佳数据配比</a></li>
<li><a href="#%E6%9E%84%E9%80%A0%E9%80%9A%E7%94%A8%E5%92%8C%E9%87%91%E8%9E%8D%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE">构造通用和金融指令数据</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B-1">案例</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h1><h2><span id="千帆llama-2中文增强技术介绍-sft30">千帆Llama 2中文增强技术介绍-SFT[30]</span><a href="#千帆llama-2中文增强技术介绍-sft30" class="header-anchor">#</a></h2><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><ul>
<li>Self-instruct</li>
<li>wizard [20]</li>
</ul>
<h3><span id="数据精简">数据精简</span><a href="#数据精简" class="header-anchor">#</a></h3><ul>
<li>低质量过滤</li>
<li>相似数据过滤</li>
</ul>
<h3><span id="数据配比">数据配比</span><a href="#数据配比" class="header-anchor">#</a></h3><ul>
<li>领域数据</li>
<li>多语言数据</li>
</ul>
<h2><span id="度小满轩辕金融大模型31">度小满轩辕金融大模型[31]</span><a href="#度小满轩辕金融大模型31" class="header-anchor">#</a></h2><h3><span id="通用的数据清洗流水线">通用的数据清洗流水线</span><a href="#通用的数据清洗流水线" class="header-anchor">#</a></h3><ul>
<li>文本抽取<ul>
<li>多来源数据收集</li>
<li>正文提取</li>
</ul>
</li>
<li>数据清洗<ul>
<li>规则过滤</li>
<li>模型过滤</li>
</ul>
</li>
<li>去重与校验<ul>
<li>MinHashLSH</li>
<li>质量校验</li>
</ul>
</li>
</ul>
<h3><span id="增量预训练-最佳数据配比">增量预训练 最佳数据配比</span><a href="#增量预训练-最佳数据配比" class="header-anchor">#</a></h3><ul>
<li><p><strong>英文数据  vs 中文数据</strong><br><strong>1  :  3</strong></p>
</li>
<li><p>中文数据中的  <strong>通用数据 vs 金融数据</strong><br>从 9:1 变成  <strong>4:1</strong></p>
<ul>
<li>通用领域指令数据<br> 8大类 50小类</li>
<li>金融领域指令数据<br> 4大类 20小类</li>
</ul>
</li>
</ul>
<h3><span id="构造通用和金融指令数据">构造通用和金融指令数据</span><a href="#构造通用和金融指令数据" class="header-anchor">#</a></h3>
<ul>
<li>Self-Instruct</li>
<li>Evol-Instruct</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="案例">案例</span><a href="#案例" class="header-anchor">#</a></h3><ol start="30">
<li>《千帆增强版 Llama 2》 百度 有ppt</li>
<li>《金融行业实战：度小满轩辕金融大模型应用探索与开发实践》 百度  有ppt</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399885&idx=1&sn=4f49c5148715c38aa4eaee3080435f17">也看大模型训练语料如何清洗：Common Crawl概述、代表性清洗方案及代码实现          </a> 代码<br>   <a href="https://zhuanlan.zhihu.com/p/610659484?utm_id=0">GPT-3 训练语料 Common Crawl 处理流程</a></p>
<p>1xx. <a href="https://github.com/alibaba/data-juicer/blob/main/README_ZH.md">Data-Juicer: 为大语言模型提供更高质量、更丰富、更易“消化”的数据</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Wizard</title>
    <url>/www6vHomeAIGC/2023/03/18/gptDataWizard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#wizard-%E6%96%B9%E6%B3%95">Wizard 方法</a><ul>
<li><a href="#%E8%87%AA%E5%8A%A8%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96-1">自动指令数据进化 [1]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F-%E5%A4%9A%E6%A0%B7%E6%80%A7-%E5%A4%8D%E6%9D%82%E5%BA%A6">质量-&gt; 多样性, 复杂度</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="wizard-方法">Wizard 方法</span><a href="#wizard-方法" class="header-anchor">#</a></h1><h3><span id="自动指令数据进化-1">自动指令数据进化 [1]</span><a href="#自动指令数据进化-1" class="header-anchor">#</a></h3><p>1）指令进化</p>
<ul>
<li>In-Depth Evolving 提示 [深度]<ul>
<li>五种类型的提示来增强指令<br>增加约束 + 深化 + 具体化 + 增加推理步骤 + 复杂化输入</li>
<li>核心部分<br><strong>In-Depth Evolving的提示的核心部分是 “你的目标是将一个给定的提示改写成更复杂的版本，使那些著名的人工智能系统（如ChatGPT和GPT4）更难处理。但改写后的提示必须是合理的，能被人理解，并能被人回应”</strong></li>
</ul>
</li>
<li>In-Breadth Evolving提示 [广度]<ul>
<li>目的<br>旨在提高<strong>主题覆盖率</strong>、<strong>技能覆盖率</strong>和整体数据集的<strong>多样性</strong></li>
</ul>
</li>
</ul>
<p>2）响应生成</p>
<p>3）消除进化<br>   即过滤未能进化的指令</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="质量-gt-多样性-复杂度">质量-&gt; 多样性, 复杂度</span><a href="#质量-gt-多样性-复杂度" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401462&idx=1&sn=764f0302918174cea29ae22ac5760033">如何构造复杂多样的微调指令数据：WizardLM复杂指令构造思想与实验分析工作总结 </a><br> <a href="https://github.com/nlpxucan/WizardLM">WizardLM</a> git</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)多模态</title>
    <url>/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#overview-0">overview [0]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3-1">视觉理解 [1]</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90-1">视觉生成 [1]</a><ul>
<li><a href="#human-alignments-in-visual-generation-10">Human Alignments in Visual Generation  [10]</a><ul>
<li><a href="#spatial-controllable-t2i-generation">spatial controllable T2I generation</a></li>
<li><a href="#text-based-editing">text-based editing</a></li>
<li><a href="#text-promots-following">text promots following</a></li>
<li><a href="#concept-customization">concept customization</a></li>
</ul>
</li>
<li><a href="#text-to-image-generation-%E6%8A%80%E6%9C%AF%E6%B5%81%E6%B4%BE4%E7%B1%BB">Text-to-Image Generation  技术流派（4类）</a></li>
</ul>
</li>
<li><a href="#%E7%BB%9F%E4%B8%80%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B2">统一的视觉模型[2]</a></li>
<li><a href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%AD%E7%BB%83llm2">端到端的方式训练LLM[2]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent3">多模态 Agent[3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BF%BB%E8%AF%91">翻译</a></li>
<li><a href="#%E8%A7%A3%E8%AF%BB">解读</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023   - microsoft</p>
</li>
<li><p>开源地址<br><a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings">Computer Vision in the Wild (CVinW)</a></p>
</li>
</ul>
<h1><span id="overview-0">overview [0]</span><a href="#overview-0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/overview.jpeg" class>

<h1><span id="视觉理解-1">视觉理解 [1]</span><a href="#视觉理解-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding.png" class>

<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/understanding-method.png" class>


<h1><span id="视觉生成-1">视觉生成 [1]</span><a href="#视觉生成-1" class="header-anchor">#</a></h1><h2><span id="human-alignments-in-visual-generation-10">Human Alignments in Visual Generation  [10]</span><a href="#human-alignments-in-visual-generation-10" class="header-anchor">#</a></h2><p>四种alignment的方式</p>
<h3><span id="spatial-controllable-t2i-generation">spatial controllable T2I generation</span><a href="#spatial-controllable-t2i-generation" class="header-anchor">#</a></h3><ul>
<li><p><strong>结合位置分布的文字描述</strong>（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于<strong>对位置要求比较高的创意设计（海报等）</strong></p>
<ul>
<li><p>直接讲原来clip那种image-level的text description升级为基于区域的text description</p>
<ul>
<li><strong>reco</strong></li>
<li>gligen</li>
</ul>
</li>
<li><p>将box描述变为spatial condition</p>
<ul>
<li><strong>controlnet</strong></li>
</ul>
</li>
<li><p>无需fintinue，直接变为inference-guide</p>
<ul>
<li>universal guidance for diffusion model</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="text-based-editing">text-based editing</span><a href="#text-based-editing" class="header-anchor">#</a></h3><ul>
<li><p><strong>给一张图和对应的修改文字，输出要求的图</strong>，常用于ps等产品</p>
<ul>
<li><p>diffusion process manipulations</p>
<ul>
<li><strong>promot2promot</strong></li>
</ul>
</li>
<li><p>text instruction editing</p>
<ul>
<li><strong>InstructPix2Pix</strong></li>
</ul>
</li>
<li><p>Editing with external pre-trained models</p>
</li>
</ul>
</li>
</ul>
<h3><span id="text-promots-following">text promots following</span><a href="#text-promots-following" class="header-anchor">#</a></h3><ul>
<li><strong>直接给文字描述，生成对应的图</strong>，这个是<strong>目前常见文生图产品的交互方式</strong>，常用于c端或者b端用户图像内容生成。但其对<strong>更细节的控制存在一定的难度</strong><ul>
<li>Inference-time manipulation</li>
<li>StructureDiffusion</li>
<li><strong>Attend-and-Excite</strong></li>
<li>Model tuning to follow text prompt</li>
<li>ddpo</li>
</ul>
</li>
</ul>
<h3><span id="concept-customization">concept customization</span><a href="#concept-customization" class="header-anchor">#</a></h3><ul>
<li><p>给一张图，提取图片中的关键内容，做<strong>各种风格（背景&#x2F;动作）变换</strong>，更用于<strong>不那么精细的广义产品</strong>，比2的运用范围更加广义</p>
<ul>
<li><p>Concept Customization</p>
<ul>
<li><strong>Textual Inversion</strong></li>
<li><strong>[DreamBooth]</strong></li>
</ul>
</li>
<li><p>Multi-concept customization</p>
<ul>
<li>Custom Diffusion</li>
</ul>
</li>
<li><p>Customization without test-time finetuning</p>
<ul>
<li>SuTI</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/align.jpg" class>
<img src="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/generation.png" class>

<h2><span id="text-to-image-generation-技术流派4类">Text-to-Image Generation  技术流派（4类）</span><a href="#text-to-image-generation-技术流派4类" class="header-anchor">#</a></h2><ul>
<li>Generative adversarial networks (GAN)</li>
<li>Variational autoencoder (VAE)</li>
<li>Discrete image token prediction</li>
<li><strong>Diffusion model</strong></li>
</ul>
<h1><span id="统一的视觉模型2">统一的视觉模型[2]</span><a href="#统一的视觉模型2" class="header-anchor">#</a></h1><h1><span id="端到端的方式训练llm2">端到端的方式训练LLM[2]</span><a href="#端到端的方式训练llm2" class="header-anchor">#</a></h1><h1><span id="多模态-agent3">多模态 Agent[3]</span><a href="#多模态-agent3" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="翻译">翻译</span><a href="#翻译" class="header-anchor">#</a></h3><p>《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》  </p>
<ol start="0">
<li><p><a href="https://blog.csdn.net/qq_41185868/article/details/133594461">AGI之MFM：《Multimodal Foundation Models: From Specialists to General-Purpose Assistants多模态基础模型：从专家到通用助</a> 翻译</p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594554">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之视觉理解、视觉生成</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133594624">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之统一的视觉模型、加持LLMs的大型多模态模型</a></p>
</li>
<li><p><a href="https://yunyaniu.blog.csdn.net/article/details/133606408">AGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之与LLM协同工作的多模态智能体、结论和研究趋势</a></p>
</li>
</ol>
<h3><span id="解读">解读</span><a href="#解读" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://zhuanlan.zhihu.com/p/669757416">大模型系列04 -文本图像生成</a></li>
</ol>
<p>1xx.   <a href="https://blog.csdn.net/qq_41200212/article/details/134663233">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>  </p>
<p>1xx.  对应第二章节<br>  <a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Zhengyuan_Tutorial_T2I2023.pdf">《Alignments in Text-to-Image Generation》</a><br>   <a href="https://www.bilibili.com/video/BV14P411v7Un/">[CVPR2023 Tutorial Talk] Alignments in Text-to-Image Generation</a> V</p>
<p>1xx. 对应第三章节<br><a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf">《From Specialist to Generalist:<br>Towards General Vision Understanding Interface》</a><br>  <a href="https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036">[CVPR Tutorial Talk] Towards General Vision Understanding Interface</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(图生文)BLIP-2, Flamingo</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalBlip/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#blip-2">BLIP-2</a><ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5-1">两阶段的训练策略 [1]</a></li>
<li><a href="#%E6%9E%B6%E6%9E%843">架构[3]</a></li>
<li><a href="#code-2">code [2]</a></li>
</ul>
</li>
<li><a href="#flamingo1">Flamingo[1]</a><ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#blip2">blip2</a></li>
<li><a href="#flamingo">Flamingo</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="blip-2">BLIP-2</span><a href="#blip-2" class="header-anchor">#</a></h1><h3><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h3><p>用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，<strong>只训练Qformer</strong>，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练</p>
<h3><span id="两阶段的训练策略-1">两阶段的训练策略 [1]</span><a href="#两阶段的训练策略-1" class="header-anchor">#</a></h3><p>BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。</p>
<ul>
<li>第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(<strong>ITC</strong>)，Image-grounded Text Generation(<strong>ITG</strong>)，Image-Text Matching(<strong>ITM</strong>)让Qformer学会如何从<strong>视觉编码器中抽取文本相关的特征</strong>。</li>
<li>第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。</li>
</ul>
<h3><span id="架构3">架构[3]</span><a href="#架构3" class="header-anchor">#</a></h3><ul>
<li><strong>两个阶段训练</strong><ul>
<li>阶段一<br>获得高质量的 <strong>图文对齐向量表征</strong><br>通过<strong>ITC ITM  ITG 三个损失函数</strong>获得了很好的图片文本 <strong>对齐向量表征能力</strong>，仅训练<strong>Qformer</strong>中很少的参数<br>【ITM:  image-text 是否是匹配的 |    image 和text 都能相互看到】<br>【ITG: image生成text |    image 能全看到, text只能逐个的看】<br>【ITC: image和text的对比学习, 对比学习分类分错了的  送入ITM 负样本 |  image和 text  之间是不能看到的】</li>
<li>阶段二<br>通过向量表征进行<strong>文字生成</strong></li>
</ul>
</li>
</ul>
<h3><span id="code-2">code [2]</span><a href="#code-2" class="header-anchor">#</a></h3><h1><span id="flamingo1">Flamingo[1]</span><a href="#flamingo1" class="header-anchor">#</a></h1><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><p>它在Frozen模型的基础上做进一步的改进，不同点主要有两个：一是使用了更大的LLMs，二是<strong>冻结视觉编码器</strong>，引入<strong>perceiver resampler</strong>和<strong>XAttn-Dense</strong>两个适配单元作为可训练的模块。</p>
<ul>
<li>perceiver resampler：<br>  类似DETR，通过设计多个Perceiver Resampler来生成<strong>64个固定长度的tokens</strong>，主要作用在于可以<strong>从图像中提取固定长度的特征向量</strong>，能够解决图像甚至多帧视频的<strong>feature map不一致的问题</strong>。【图像和文本对齐】</li>
<li>XAttn-Dense：在每一层LLM上都会增加<strong>corss- attention</strong>以入到<strong>LLM中与视觉向量进行交互</strong>，<strong>融合多模态信息</strong>。【融合】</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="blip2">blip2</span><a href="#blip2" class="header-anchor">#</a></h3><ol>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130757157?spm=1001.2014.3001.5502">基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）</a></p>
</li>
<li><p><a href="https://github.com/www6v/LAVIS/tree/main/projects/blip2">blip2</a> git<br><a href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">blip2_instructed_generation</a> git 运行过</p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1Ek4y1G74J">强推！科大讯飞和中科院终于把多模态大模型讲明白了，CLIP、blip、blip2三种模型原理一口气学完</a> V ***</p>
</li>
</ol>
<p>1xx.  <a href="https://www.bilibili.com/video/BV18u4y137ZV/">AI论文精读之多模态大模型BLIP-2</a> V</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践</a> *</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/606364639">BLIP2：下一代多模态模型的雏形</a></p>
<h3><span id="flamingo">Flamingo</span><a href="#flamingo" class="header-anchor">#</a></h3><p>1xx. <a href="https://www.bilibili.com/video/BV1pu411G7ce">[论文速览]Flamingo: a Visual Language Model for Few-Shot Learning[2204.14198]</a> V<br>1xx.  <a href="https://zhuanlan.zhihu.com/p/511517344">DeepMind出手！多模态小样本打败精调</a><br>1xx. <a href="https://github.com/Luodian/Otter">Otter  on OpenFlamingo</a> git<br>1xx. <a href="https://github.com/mlfoundations/open_flamingo">open_flamingo</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态InstructTuning</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalInstructTuning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</a><ul>
<li><a href="#single-turn">Single-turn</a></li>
<li><a href="#multi-turn">Multi-turn</a></li>
</ul>
</li>
<li><a href="#vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</a><ul>
<li><a href="#annotation-adaption">Annotation Adaption</a></li>
<li><a href="#self-instruct">Self-Instruct</a></li>
</ul>
</li>
<li><a href="#high-quality-vlit-data2">High-Quality VLIT Data[2]</a><ul>
<li><a href="#correctness">Correctness</a></li>
<li><a href="#diversity">Diversity</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#method-12">Method [1][2]</a><ul>
<li><a href="#annotation-adaption-si">Annotation Adaption-&gt; SI</a></li>
<li><a href="#self-instruct-aa">Self-Instruct -&gt; AA</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="datasets-for-visual-instruction-tuning1">Datasets for Visual Instruction Tuning[1]</span><a href="#datasets-for-visual-instruction-tuning1" class="header-anchor">#</a></h1><h3><span id="single-turn">Single-turn</span><a href="#single-turn" class="header-anchor">#</a></h3><ul>
<li><p>MiniGPT-4<br><strong>MiniGPT-4</strong> [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 <strong>randomly selects 5000 images from the Conceptual Caption dataset</strong> [38], [39] and prompts its <strong>pre-trained VLM model</strong> to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.</p>
</li>
<li><p>MultiInstruct<br>MultiInstruct [43] build a comprehensive instruction dataset that covers 62 diverse multimodal tasks from 10 broad categories, such VQA, Image-text matching, grounded generation, and so on. These tasks include 34 existing tasks derived from 21 public dataset and 28 new tasks extended from them. Each task is equipped with 5 instruction templates to prompt the model to perform the specific task.</p>
</li>
</ul>
<h3><span id="multi-turn">Multi-turn</span><a href="#multi-turn" class="header-anchor">#</a></h3><ul>
<li>LLaVA<br><strong>LLaVA-Instruct-158k</strong> [9] contains 158 image-text instruction data, including <strong>58k conversation data</strong> asking about the visual content of the image,<strong>23k description data</strong>, and <strong>77k complex reasoning data</strong> where the question may involve multi-step reasoning process.</li>
</ul>
<h1><span id="vlit-data-construction-strategy2">VLIT Data Construction Strategy[2]</span><a href="#vlit-data-construction-strategy2" class="header-anchor">#</a></h1><h3><span id="annotation-adaption">Annotation Adaption</span><a href="#annotation-adaption" class="header-anchor">#</a></h3><ul>
<li>MiniGPT-4</li>
</ul>
<h3><span id="self-instruct">Self-Instruct</span><a href="#self-instruct" class="header-anchor">#</a></h3><ul>
<li>LLaVA</li>
</ul>
<h1><span id="high-quality-vlit-data2">High-Quality VLIT Data[2]</span><a href="#high-quality-vlit-data2" class="header-anchor">#</a></h1><h3><span id="correctness">Correctness</span><a href="#correctness" class="header-anchor">#</a></h3><h3><span id="diversity">Diversity</span><a href="#diversity" class="header-anchor">#</a></h3><h3><span id="complexity">Complexity</span><a href="#complexity" class="header-anchor">#</a></h3><h1><span id="method-12">Method [1][2]</span><a href="#method-12" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>Method</th>
<th>Training Paradigm[2]</th>
<th>Vision Encoder</th>
<th>Language Encoder</th>
<th>Inst[2]</th>
<th>Tuning Data</th>
</tr>
</thead>
<tbody><tr>
<td>MiniGPT-4</td>
<td>FA → VLIT</td>
<td>EvaCLIP ViT</td>
<td>Vicuna</td>
<td>AA</td>
<td>CC3M, CC12M, SBU, LAION 400M, MiniGPT-3.5K</td>
</tr>
<tr>
<td>MiniGPT-v2</td>
<td></td>
<td>EVA</td>
<td>LLaMA2-chat</td>
<td>AA+SI</td>
<td>LAION, CC3M, SBU, GRIT-20M, COCO caption, Text Captions, RefCOCO, RefCOCO+, RefCOCOg, GQA, VQA-v2, OCR-VQA, OKVQA, AOK-VQA, Flickr30k Dataset, Unnatural Instruction Dataset</td>
</tr>
<tr>
<td>LLaVa</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td>SI</td>
<td>CC3M Concept-balanced 595K, LLaVA-Instruct-158K</td>
</tr>
<tr>
<td>LLaVA-1.5</td>
<td>FA → VLIT</td>
<td>CLIP ViT</td>
<td>Vicuna</td>
<td></td>
<td>LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG</td>
</tr>
<tr>
<td>MultiInstruct</td>
<td>VLIT</td>
<td>OFA</td>
<td>OFA</td>
<td>AA</td>
<td>VQAv2, Visual7w, GQA, OK-VQA, Visual Genome, MSCOCO, RefCOCO, COCO-Text, TDIUC, IQA, VAW, MOCHEG, WikiHow</td>
</tr>
<tr>
<td>Otter</td>
<td></td>
<td>CLIP ViT</td>
<td>MPT</td>
<td>SI</td>
<td>MIMIC-IT</td>
</tr>
<tr>
<td>LAMM</td>
<td>VLIT</td>
<td>CLIP ViT-L&#x2F;14</td>
<td>Vicuna</td>
<td>SI</td>
<td>Language-Assisted Multi-Modal Instruction-Tuning Dataset</td>
</tr>
<tr>
<td>Qwen-VL</td>
<td>FA → VLIT(Multi-Task Tuning)</td>
<td>ViT</td>
<td>Qwen-7B</td>
<td></td>
<td>LAION-en&amp;zh, DataComp, Coyo, CC12M&amp;3M, SBU, COCO, In-house Data, GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D, GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg, SynthDoG-en&amp;zh, Common Crawl pdf&amp;HTML</td>
</tr>
<tr>
<td>CogVLM</td>
<td>FA → VLIT</td>
<td>EVA2-CLIP-E</td>
<td>Vicuna-7Bv-1.5</td>
<td></td>
<td>VQAv2, TextVQA</td>
</tr>
<tr>
<td>StableLLaVA</td>
<td>FA → VLIT</td>
<td>CLIP-ViT-L&#x2F;14</td>
<td>LLaMA</td>
<td>AA</td>
<td>Synthesized Image-Dialogue Dataset</td>
</tr>
</tbody></table>
<h3><span id="annotation-adaption-gt-si">Annotation Adaption-&gt; SI</span><a href="#annotation-adaption-gt-si" class="header-anchor">#</a></h3><h3><span id="self-instruct-gt-aa">Self-Instruct -&gt; AA</span><a href="#self-instruct-gt-aa" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p>《Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey》 ***  第4 5章  南洋大学 </p>
</li>
<li><p>《Vision-Language Instruction Tuning: A Review and Analysis》 ***  第2 3 4 5章   腾讯</p>
</li>
<li><p>《Instruction Tuning for Large Language Models: A Survey》 第5章</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)MiniGPT4</title>
    <url>/www6vHomeAIGC/2023/03/15/gptMultimodalMinigpt4/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#introduction1">INTRODUCTION[1]</a></li>
<li><a href="#method1">METHOD[1]</a><ul>
<li><a href="#first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</a></li>
<li><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</a></li>
<li><a href="#second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E5%8E%9F%E7%90%86">原理</a></li>
<li><a href="#%E5%AE%9E%E6%88%98">实战</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="introduction1">INTRODUCTION[1]</span><a href="#introduction1" class="header-anchor">#</a></h1><p>MiniGPT-4 增加了一个<strong>投影层</strong>，将<strong>编码的视觉特征与 Vicuna 语言模型对齐</strong>，并<strong>冻结了所有其他视觉和语言组件</strong></p>
<h1><span id="method1">METHOD[1]</span><a href="#method1" class="header-anchor">#</a></h1><ul>
<li><p>图 1</p>
</li>
<li><p>MiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，</p>
<ul>
<li>使用 <strong>Vicuna作为语言解码器</strong>，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。</li>
<li>视觉感知方：采用与 <strong>BLIP-2</strong> 相同的<strong>视觉编码器</strong>，<strong>ViT Backbone</strong>及其预先训练好的 <strong>Q-Former</strong>。<br>语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。</li>
</ul>
</li>
</ul>
<h3><span id="first-pretraining-stage">FIRST <strong>PRETRAINING</strong> STAGE</span><a href="#first-pretraining-stage" class="header-anchor">#</a></h3><ul>
<li><p>第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。</p>
</li>
<li><p>Traditional alignment method [2]</p>
<ul>
<li>Input: Image</li>
<li>Output: Caption</li>
<li>Training Objective: Maximize the likelihood of GT captions</li>
<li>Training Dataset 组合数据集 [postprocessed by BLIP] <ul>
<li>Conceptual Caption</li>
<li>SBU </li>
<li>LAION</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><span id="curating-a-high-quality-alignment-dataset-for-vision-language-domain">CURATING A <strong>HIGH-QUALITY ALIGNMENT DATASET</strong> FOR VISION-LANGUAGE DOMAIN.</span><a href="#curating-a-high-quality-alignment-dataset-for-vision-language-domain" class="header-anchor">#</a></h3><ul>
<li>Create a dataset with detailed, human-perfered descriptions[2][1]<ul>
<li>model  generates descriptions<br>在初始阶段，我们使用从第一个预训练阶段得到的模型来<strong>生成输入图像的描述</strong>。      </li>
<li>polishing and filtering by chatgpt<br>上述自动生成的图片说明包含<strong>噪音或不连贯的描述</strong>，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了<strong>ChatGPT</strong>，通过以下提示对描述进行<strong>修补</strong></li>
<li>further polishing and filtering by rules &amp; human<br>完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。</li>
</ul>
</li>
</ul>
<h3><span id="second-stage-finetuning">SECOND-STAGE <strong>FINETUNING</strong></span><a href="#second-stage-finetuning" class="header-anchor">#</a></h3><ul>
<li>第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。</li>
</ul>
<p>【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】  [2]</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="原理">原理</span><a href="#原理" class="header-anchor">#</a></h3><ol>
<li><a href="https://datac.blog.csdn.net/article/details/135399033">【LMM 009】MiniGPT-4：使用 Vicuna 增强视觉语言理解能力的多模态大模型</a> *** </li>
<li><a href="https://www.bilibili.com/video/BV1n24y1F7kv/">MiniGPT-4、表格推理、代码生成、生成式推理-来自斯坦福、北大、阿卜杜拉、达摩院的四位论文一作思辨大模型</a> V<br>1xx. <a href="https://www.bilibili.com/video/BV12Q4y1b7nY/">miniGPT4：多模态图文理解训练</a> V<br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400402&idx=1&sn=efd84698e6a207b2035995ec2e255417">MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 </a><br>1xx. <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d282e4b007b201a34052">使用大型语言模型为MiniGPT-4构建视觉语言理解能力</a> V</li>
</ol>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/627671257">大杀器，多模态大模型MiniGPT-4入坑指南</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) LLaVa 演化</title>
    <url>/www6vHomeAIGC/2023/03/14/gptMultimodalLlava/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llava-演化">LLaVa 演化</span><a href="#llava-演化" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/LLaVa-cef875377c394636a64cf57edbb0026e?pvs=4">(原理|实战) LLaVa 演化</a></li>
</ul>
<h1><span id="llava-实战">LLaVa 实战</span><a href="#llava-实战" class="header-anchor">#</a></h1><ul>
<li><a href="https://candied-skunk-1ca.notion.site/LLaVa-0bf9f127dc7c41e796050bcb8f7fb1b3?pvs=4">(实战) LLaVa </a></li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Web Agent</title>
    <url>/www6vHomeAIGC/2023/03/05/gptAgentWeb/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="web-scenarios-1">web scenarios [1]</span><a href="#web-scenarios-1" class="header-anchor">#</a></h1><p>在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。</p>
<p>通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。</p>
<p>为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。</p>
<p>Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs. </p>
<p>Agents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping [392] and search engine retrieval [90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management [391], agents often face challenges in performance. </p>
<p>In order to enable successful interactions between agents and more realistic web pages, some researchers [393; 394] have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web [389] combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [388] in real-world scenarios and extract valuable information. Furthermore, WebGum [390] empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive understanding of web pages.</p>
<h1><span id="papers-2">papers [2]</span><a href="#papers-2" class="header-anchor">#</a></h1><p><strong>In web scenarios</strong></p>
<ul>
<li>[2023&#x2F;10] <strong>OpenAgents: An Open Platform for Language Agents in the Wild.</strong> <em>XLang Lab (The University of Hong Kong) arXiv.</em> [<a href="https://arxiv.org/abs/2310.10634">paper</a>] [<a href="https://docs.xlang.ai/">project page</a>] [<a href="https://github.com/xlang-ai/OpenAgents">code</a>] [<a href="https://chat.xlang.ai/">demo</a>]  ***</li>
<li>[2023&#x2F;07] <strong>WebArena: A Realistic Web Environment for Building Autonomous Agents.</strong> <em>Shuyan Zhou (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.13854">paper</a>] [<a href="https://webarena.dev/">code</a>] *</li>
<li>[2023&#x2F;07] <strong>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.</strong> <em>Izzeddin Gur (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.12856">paper</a>]</li>
<li>[2023&#x2F;06] <strong>SYNAPSE: Leveraging Few-Shot Exemplars for<br>Human-Level Computer Control.</strong> <em>Longtao Zheng (Nanyang Technological University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.07863">paper</a>] [<a href="https://github.com/ltzheng/synapse">code</a>] *</li>
<li>[2023&#x2F;06] <strong>Mind2Web: Towards a Generalist Agent for the Web.</strong> <em>Xiang Deng (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.06070">paper</a>] [<a href="https://osu-nlp-group.github.io/Mind2Web/">code</a>] ***</li>
<li>[2023&#x2F;05] <strong>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</strong> <em>Hiroki Furuta (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11854">paper</a>]</li>
<li>[2023&#x2F;03] <strong>Language Models can Solve Computer Tasks.</strong> <em>Geunwoo Kim (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17491">paper</a>] [<a href="https://github.com/posgnu/rci-agent">code</a>]</li>
<li>[2022&#x2F;07] <strong>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.01206">paper</a>] [<a href="https://webshop-pnlp.github.io/">code</a>] *</li>
<li>[2021&#x2F;12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano (OpenAI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332">paper</a>]</li>
<li>[2023&#x2F;05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> <em>Wangchunshu Zhou (AIWaves) et al. arXiv.</em> [<a href="https://arxiv.org/pdf/2309.07870.pdf">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]  ***</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《The Rise and Potential of Large Language Model Based Agents: A Survey》、</li>
<li><a href="https://github.com/woooodyy/llm-agent-paper-list">The Rise and Potential of Large Language Model Based Agents: A Survey</a> git<br>1xx. <a href="https://sites.google.com/view/mm-webnav/">Multimodal Web Navigation with Instruction-Finetuned Foundation Models</a><br>1xx. <a href="https://hub.baai.ac.cn/view/28104">Google DeepMind｜具备规划长程上下文理解和程序合成能力的真实世界WebAgent</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/662146234">LLMs-Agent 论文: WebAgent, 2023, Izzeddin Gur et al., Google DeepMind.</a><br>1xx. <a href="https://github.com/www6v/OpenAgents">OpenAgents</a></li>
</ol>
<h3><span id="web-agent">web  agent</span><a href="#web-agent" class="header-anchor">#</a></h3><p>1xx. <a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>agent</category>
      </categories>
      <tags>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper-Agent</title>
    <url>/www6vHomeAIGC/2023/03/04/gptAgentPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<p>1xx. <a href="https://github.com/zjunlp/LLMAgentPapers">LLM Agents Papers</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/662506575">28 篇最新论文解读 LLMs-based Agents</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/661741663">ICLR’24 Agent论文合集：RL-based、LLM-based 前沿研究汇总</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/665282216">ICLR’24 大语言模型智能体最新研究进展丨智能体应用篇</a><br>1xx. <a href="https://mp.weixin.qq.com/s/GGRWQJ-eBvHerB9H9JPCjg">ICLR’24 大语言模型智能体最新研究进展丨智能体能力篇 </a><br>   <a href="https://mp.weixin.qq.com/s/eYnZY1GFWMKdU_Z57iSEJg">ICLR’24 大语言模型智能体最新研究进展 </a><br>1xx. <a href="https://github.com/www6v/LLM-Agents-Papers"> LLM-Agents-Papers</a></p>
<p>1xx. <a href="https://www.bilibili.com/read/cv27126779/">AGI新突破！52篇论文尽览大模型Agent最新研究进展！</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a>  ***</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)多模态预训练 概述</title>
    <url>/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83">多模态预训练</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84transformer">架构Transformer</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">模型 - 自监督学习</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1">下游任务</a><ul>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83">多模态下游任务-模型微调</a></li>
</ul>
</li>
<li><a href="#%E6%9B%B4%E5%A4%A7%E6%9B%B4%E5%BC%BA%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">更大更强的多模态预训练模型</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="overview">Overview</span><a href="#overview" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/04/gptMultimodalPretrain/overview.png" class>


<h1><span id="多模态预训练">多模态预训练</span><a href="#多模态预训练" class="header-anchor">#</a></h1><h2><span id="数据集">数据集</span><a href="#数据集" class="header-anchor">#</a></h2><ul>
<li>大规模无标注</li>
<li>内容杂  噪音多</li>
</ul>
<h2><span id="架构transformer">架构Transformer</span><a href="#架构transformer" class="header-anchor">#</a></h2><ul>
<li><p>基于transformer encoder-理解任务<br>单流 - vl-bert  UNITER<br>双流 - ViLBERT， CLIP（双流结构，对比学习）</p>
</li>
<li><p>基于transformer decoder-生成任务<br>DALL-E  （VQVAE+GPT,  Text-to-Image Generation）<br>现在都用 → SD 扩散模型</p>
</li>
<li><p>基于encoder+decoder-理解+生成<br>文本的decoder</p>
</li>
</ul>
<ol>
<li>encoder + decoder 串行,  交叉注意力</li>
<li>encoder + decoder 并行</li>
</ol>
<h2><span id="模型-自监督学习">模型 - 自监督学习</span><a href="#模型-自监督学习" class="header-anchor">#</a></h2><ul>
<li><p>模态内掩码学习<br>文本 语音 视觉自身token级别mask</p>
</li>
<li><p>模态间掩码学习<br>不同模态信息的相互预测<br>mask视觉， 输出对应文本</p>
</li>
<li><p>模态间匹配学习<br>匹配与否的分类问题 - 正负样本(二分类)<br>对比学习 - 模态间的图文匹配对</p>
</li>
</ul>
<h1><span id="下游任务">下游任务</span><a href="#下游任务" class="header-anchor">#</a></h1><h2><span id="多模态下游任务-模型微调">多模态下游任务-模型微调</span><a href="#多模态下游任务-模型微调" class="header-anchor">#</a></h2><ul>
<li><p>模型微调</p>
<ul>
<li>p+ finetune（全参数）</li>
<li>p+ prompt-tuning</li>
<li>p+ adaptor-tuning</li>
<li>p+ lora</li>
</ul>
</li>
<li><p>多模态下游任务</p>
<ul>
<li>理解： text&#x2F;audio&#x2F;visual 内容生成</li>
<li>生成： 跨模态 检索&#x2F;问答&#x2F;推理</li>
</ul>
</li>
</ul>
<h1><span id="更大更强的多模态预训练模型">更大更强的多模态预训练模型</span><a href="#更大更强的多模态预训练模型" class="header-anchor">#</a></h1><ul>
<li><strong>强大的语言模型</strong></li>
<li>更大的视觉模型</li>
<li>更大规模的预训练数据</li>
<li>更多模态形式的数据</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV13P411q7tH/">中科院刘静：多模态预训练的进展回顾与展望</a>  V</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)ViT, ViLT</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalVit/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/pdf/2010.11929">AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a> </p>
</li>
<li><p>开源地址<br><a href="https://github.com/google-research/vision_transformer">vision_transformer</a> git</p>
</li>
</ul>
<h1><span id="model">model</span><a href="#model" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/03/01/gptMultimodalVit/model.png" class>


<h1><span id="代码1">代码[1]</span><a href="#代码1" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="vit">ViT</span><a href="#vit" class="header-anchor">#</a></h3><ol>
<li><a href="https://www.bilibili.com/video/BV1fH4y1H7mV/">【Sora重要技术】复现ViT（Vision Transformer）模型</a> V<br>  <a href="https://github.com/owenliang/mnist-vit">mnist-vit Repo</a> git</li>
</ol>
<p>1xx. <a href="https://www.bilibili.com/video/BV1Uu411o7oY">VIT (Vision Transformer) 模型论文+代码(源码)从零详细解读，看不懂来打我</a> V</p>
<p>1xx. <a href="https://blog.csdn.net/qq_43449643/article/details/135623953">详解VIT（Vision Transformer)模型原理, 代码级讲解</a><br>   <a href="https://github.com/yangyunfeng-cyber/Useful-DL-Projects-for-Exercise/blob/main/VIT/vit_model.py">VIT Repo</a> git  ***</p>
<p>1xx. <a href="https://www.bilibili.com/video/BV1xm4y1b7Pw/">ViT｜ Vision Transformer ｜理论 + 代码</a>  V<br>   <a href="https://65d8gk.axshare.com/">PPT</a></p>
<h3><span id="vilt">ViLT</span><a href="#vilt" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/369733979">ViLT：最简单的多模态Transformer</a><br>1xx. <a href="https://github.com/dandelin/vilt">ViLT</a> git<br>1xx. <a href="https://blog.csdn.net/qq_42030496/article/details/134641704">ViLT 论文精读【论文精读】</a><br>   <a href="https://www.bilibili.com/video/BV14r4y1j74y/">ViLT 论文精读【论文精读】</a> V<br>1xx. <a href="https://blog.csdn.net/m0_56722835/article/details/125071550">多模态ViLT模型下游任务微调原理及代码</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ViT</category>
      </categories>
      <tags>
        <tag>ViT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战) SAM</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="论文1">论文[1]</span><a href="#论文1" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br><a href="https://arxiv.org/abs/2304.02643">Segment Anything Model论文</a></p>
</li>
<li><p>开源地址<br> <a href="https://github.com/facebookresearch/segment-anything">Segment Anything Model模型源码</a> Git</p>
</li>
<li><p>官网<br> <a href="https://segment-anything.com/">Segment Anything Model官网</a><br><a href="https://segment-anything.com/demo">Segment Anything Model官网demo网页端</a></p>
</li>
</ul>
<h1><span id="模型2">模型[2]</span><a href="#模型2" class="header-anchor">#</a></h1><h3><span id="image-encoder">image encoder</span><a href="#image-encoder" class="header-anchor">#</a></h3><h3><span id="prompt-encoder">prompt encoder</span><a href="#prompt-encoder" class="header-anchor">#</a></h3><h3><span id="mask-decoder">mask decoder</span><a href="#mask-decoder" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/03/01/gptMultimodalSAM/mask%20decoder.webp" class>

<p>在prompt embeddings中插入一个可学习的token，用于docoder的输出。<br>（1）prompt toekns+output tokens进行self attn,<br>（2）用得到的token和image embedding进行 cross attn（token作为Q）<br>（3）point-wise MLP 更新token<br>（4）用image embedding和（3）的token进行cross atten（image embedding作为Q）<br>重复上述步骤2次，再将attn再通过残差进行连接，最终输出masks和iou scores。</p>
<h1><span id="sam应用3">SAM应用[3]</span><a href="#sam应用3" class="header-anchor">#</a></h1><ul>
<li>图像分割</li>
<li>目标检测</li>
<li>图像修复( image inpainting)</li>
<li><strong>模型微调</strong></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://blog.csdn.net/weixin_44386956/article/details/130262260">【模型解读】【代码复现】Segment Anything Model(SAM)</a> *</li>
<li><a href="https://zhuanlan.zhihu.com/p/620355474">【论文解读】MetaAi SAM(Segment Anything) 分割一切</a> ***<br>最下面有应用</li>
<li><a href="https://zhuanlan.zhihu.com/p/630529550">Segment Anything(sam)项目整理汇总</a> ***<br>1xx. <a href="https://www.bilibili.com/video/BV1aV4y1d7gC/">【Segment Anything 模型深度解构】GPT时代，干翻计算机视觉第一步！</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)CLIP</title>
    <url>/www6vHomeAIGC/2023/03/01/gptMultimodalCLIP/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#clip-%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%81%9A%E4%BA%86%E5%93%AA%E4%BA%9B%E4%BA%8B%E6%83%85elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</a></li>
<li><a href="#clip-zero-shot%E6%8E%A8%E7%90%861">CLIP Zero-shot推理[1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E5%B1%80%E9%99%90">局限</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="clip-在训练过程中做了哪些事情elmo1">CLIP 在训练过程中做了哪些事情？[Elmo][1]</span><a href="#clip-在训练过程中做了哪些事情elmo1" class="header-anchor">#</a></h1><p>在训练过程中，CLIP（Contrastive Language-Image Pre-training）模型主要进行了以下几个步骤：</p>
<ol>
<li><strong>数据预处理</strong> : CLIP 使用了一个大规模的数据集，包含 4 亿个 “图像 - 文本” 对，这些数据需要进行清洗和预处理，以便于模型学习。</li>
<li><strong>特征提取</strong> : 通过 Text Encoder 和 Image Encoder 分别对文本和图像进行特征提取。Text Encoder 通常是基于 Transformer 的模型，而 Image Encoder 可以是基于 CNN（卷积神经网络）或者 VIT（Vision Transformer）的模型。</li>
<li><strong>对比学习</strong> : CLIP 采用对比学习的策略，通过对比正确的图像 - 文本对与错误的图像 - 文本对，使得模型能够学习到正确对的特征表示与其他对的区分。具体来说，CLIP 通过 InfoNCE 损失函数来最大化正确对的相似度，同时最小化错误对的相似度。</li>
<li><strong>特征空间对齐</strong> : 通过对比学习，CLIP 将图像和文本的特征映射到一个共享的多模态特征空间中，使得图像特征和文本特征可以直接进行相似度比较。</li>
<li><strong>参数优化</strong> : 通过反向传播和梯度下降等方法，不断调整模型参数，以最小化损失函数，从而优化模型性能。</li>
<li><strong>Zero-shot 推理能力的培养</strong> : 在训练过程中，CLIP 学习了如何通过文本提示（prompts）来进行 zero-shot 的图像分类，即在没有直接观测到的类别标签下，通过文本描述来识别图像内容。</li>
<li><strong>模型评估与调整</strong> : 通过在验证集上的评估，调整模型结构和超参数，以提高模型的泛化能力和性能。</li>
</ol>
<p>通过这些训练步骤，CLIP 能够学习到一个强大的多模态特征表示，使其能够在多种视觉任务上进行 zero-shot 或 few-shot 的推理。</p>
<h1><span id="clip-zero-shot推理1">CLIP Zero-shot推理[1]</span><a href="#clip-zero-shot推理1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>首先，我们创建一个<strong>标签全集</strong>，如图中（2）所示，并得到每一个<strong>标签的特征向量</strong></li>
<li>然后，我们取一张图片，如图中（3）所示，过Image Encoder后得到该<strong>图片的特征向量</strong></li>
<li>最后，计算图片向量和文字向量间的<strong>相似度</strong>，取相似度最高的那条label即可。</li>
</ul>
<h3><span id="局限">局限</span><a href="#局限" class="header-anchor">#</a></h3><p>  当你喂给CLIP一张图时，不管这张图片它是否有见过，CLIP都不会生成一个全新的标签，而是去全集标签中找一个最相似的给你。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/660476765">关于多模态经典之作CLIP，还有哪些细节是你不知道的</a> 代码</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/493489688">神器CLIP：连接文本和图像，打造可迁移的视觉模型</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/486857682">【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision</a> ***</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/646790176">CLIP 模型解读</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/521151393">详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50&#x2F;101</a></p>
<p>1xx. <a href="https://blog.csdn.net/lsb2002/article/details/132275132">openai多模态大模型：clip详解及实战</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/555314976">CLIP：多模态领域革命者</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>Tokenizer</title>
    <url>/www6vHomeAIGC/2023/02/26/gptTrainTokenizer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h3><span id="tokenizer-分词">tokenizer 分词</span><a href="#tokenizer-分词" class="header-anchor">#</a></h3><ul>
<li>单词分词法</li>
<li>单字分词法</li>
<li>子词分词法<br>BPE [GPT系列], WordPiece</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://zhuanlan.zhihu.com/p/630696264">大模型词表扩充必备工具SentencePiece</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/458452872">NLP（二）：浅谈分词</a><br>1xx. <a href="https://www.bilibili.com/video/BV1vN411p7t2/">https://www.bilibili.com/video/BV1vN411p7t2/</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400849&idx=1&sn=58006756cccde4d06d273df59e2c8dd8">开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Tokenizer</category>
      </categories>
      <tags>
        <tag>Tokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey List</title>
    <url>/www6vHomeAIGC/2023/02/25/gptSurveyList/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="awesome-llm-survey">Awesome-LLM-Survey</span><a href="#awesome-llm-survey" class="header-anchor">#</a></h1><ul>
<li><a href="#awesome-llm-survey">Awesome-LLM-Survey</a><ul>
<li><p><a href="#general-survey">General Survey</a> *** </p>
</li>
<li><p><a href="#training-of-llm">Training of LLM</a></p>
<ul>
<li><a href="#instruction-tuning">Instruction Tuning</a> *** </li>
<li><a href="#human-alignment-for-llm">Human Alignment for LLM</a> *</li>
</ul>
</li>
<li><p><a href="#prompt-of-llm">Prompt of LLM</a></p>
<ul>
<li><a href="#chain-of-thought-for-llm">Chain of Thought for LLM</a> *** </li>
<li><a href="#prompt-engineering-for-llm">Prompt Engineering for LLM</a> * </li>
<li><a href="#retrieval-augmented-llm">Retrieval-Augmented LLM</a> ***</li>
</ul>
</li>
<li><p><a href="#challenge-of-llm">Challenge of LLM</a></p>
<ul>
<li><p><a href="#hallucination-in-llm">Hallucination in LLM</a> ***</p>
</li>
<li><p><a href="#compression-for-llm">Compression for LLM</a> ***</p>
</li>
<li><p><a href="#evaluation-of-llm">Evaluation of LLM</a></p>
</li>
<li><p><a href="#reasoning-with-llm">Reasoning with LLM</a></p>
</li>
<li><p><a href="#long-context-for-llm">Long-Context for LLM</a></p>
</li>
<li><p><a href="#factuality-in-llm">Factuality in LLM</a></p>
</li>
<li><p><a href="#knowledge-for-llm">Knowledge for LLM</a></p>
</li>
<li><p><a href="#self-correction-for-llm">Self-Correction for LLM</a></p>
</li>
<li><p><a href="#tool-using-of-llm">Tool Using of LLM</a> ***</p>
</li>
<li><p><a href="#agent-of-llm">Agent of LLM</a> ***</p>
</li>
<li><p><a href="#efficiency-of-llm">Efficiency of LLM</a> *** </p>
</li>
<li><p><a href="#data-of-llm">Data of LLM</a> ***</p>
</li>
<li><p><a href="#continual-learning-of-llm">Continual Learning of LLM</a></p>
</li>
</ul>
</li>
<li><p><a href="#mulitmodal-of-llm">Mulitmodal of LLM</a></p>
<ul>
<li><a href="#visual-llm">Visual LLM</a></li>
</ul>
</li>
<li><p><a href="#llm-for-domain-application">LLM for Domain Application</a></p>
<ul>
<li><a href="#llm-for-health">LLM for Health</a></li>
<li><a href="#llm-for-finance">LLM for Finance</a> ***</li>
<li><a href="#llm-for-education">LLM for Education</a></li>
<li><a href="#llm-for-law">LLM for Law</a></li>
<li><a href="#llm-for-mental-health">LLM for Mental Health</a></li>
</ul>
</li>
<li><p><a href="#llm-for-downstream-tasks">LLM for Downstream Tasks</a></p>
<ul>
<li><p><a href="#llm-for-recommendation">LLM for Recommendation</a></p>
</li>
<li><p><a href="#llm-for-information-retrieval">LLM for Information Retrieval</a></p>
</li>
<li><p><a href="#llm-for-software-engineering">LLM for Software Engineering</a></p>
</li>
<li><p><a href="#llm-for-time-series">LLM for Time Series</a></p>
</li>
<li><p><a href="#detection-of-llms-generated-content">Detection of LLMs-Generated Content</a></p>
</li>
<li><p><a href="#llm-for-information-extraction">LLM for Information Extraction</a></p>
</li>
</ul>
</li>
<li><p><a href="#star-history">Star History</a></p>
</li>
</ul>
</li>
</ul>
<hr>
<h2><span id="general-survey">General Survey</span><a href="#general-survey" class="header-anchor">#</a></h2><ul>
<li><p>Challenges and Applications of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.10169">[paper]</a> *** </p>
</li>
<li><p>A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT, 2023.02 <a href="https://arxiv.org/abs/2302.09419">[paper]</a> ***</p>
</li>
<li><p>A Survey of Large Language Models, 2023.11 <a href="https://arxiv.org/abs/2303.18223">[paper]</a><a href="https://github.com/RUCAIBox/LLMSurvey">[project]</a>  ***</p>
</li>
<li><p>A Comprehensive Overview of Large Language Models *</p>
</li>
<li><p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</p>
</li>
<li><p>Pre-Trained Models: Past, Present and Future</p>
</li>
<li><p>A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4, 2023.10 <a href="https://arxiv.org/pdf/2310.12321.pdf">[paper]</a></p>
</li>
<li><p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, 2023.04  <a href="https://arxiv.org/abs/2304.13712">[paper]</a><a href="https://github.com/Mooler0410/LLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects, 2023.12 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.23589741.v4">[paper]</a> <a href="https://github.com/anas-zafar/LLM-Survey">[project]</a></p>
</li>
<li><p>The future of gpt: A taxonomy of existing chatgpt research, current challenges, and possible future directions, 2023.04 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4413921">[paper]</a></p>
</li>
<li><p>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges, 2023.10 <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.24171183.v1">[paper]</a></p>
</li>
<li><p>Understanding LLMs: A Comprehensive Overview from Training to Inference, 2024.01 <a href="https://arxiv.org/pdf/2401.02038.pdf">[paper]</a></p>
</li>
</ul>
<hr>
<h2><span id="training-of-llm">Training of LLM</span><a href="#training-of-llm" class="header-anchor">#</a></h2><h3><span id="instruction-tuning">Instruction Tuning</span><a href="#instruction-tuning" class="header-anchor">#</a></h3><ul>
<li>Are Prompts All the Story? No. A Comprehensive and Broader View of Instruction Learning, 2023.03 <a href="https://arxiv.org/pdf/2303.10475.pdf">[paper]</a> <a href="https://github.com/RenzeLou/awesome-instruction-learning">[project]</a></li>
<li>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a></li>
<li>Instruction Tuning for Large Language Models: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.10792">[paper]</a>  ***</li>
</ul>
<h3><span id="human-alignment-for-llm">Human Alignment for LLM</span><a href="#human-alignment-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>AI Alignment: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19852">[paper]</a><a href="https://www.alignmentsurvey.com/">[project]</a></p>
</li>
<li><p>Large Language Model Alignment: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.15025">[paper]</a></p>
</li>
<li><p>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Model, 2023.08 <a href="https://arxiv.org/abs/2308.12014">[paper]</a><a href="https://github.com/ValueCompass/Alignment-Goal-Survey">[project]</a></p>
</li>
<li><p>Aligning Large Language Models with Human: A Survey, 2023.07 <a href="https://arxiv.org/abs/2307.12966">[paper]</a><a href="https://github.com/GaryYufei/AlignLLMHumanSurvey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="prompt-of-llm">Prompt of LLM</span><a href="#prompt-of-llm" class="header-anchor">#</a></h2><h3><span id="chain-of-thought-for-llm">Chain of Thought for LLM</span><a href="#chain-of-thought-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
<li><p>A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future, 2023.09 <a href="https://arxiv.org/abs/2309.06256">[paper]</a><a href="https://github.com/zchuz/CoT-Reasoning-Survey">[project]</a></p>
</li>
<li><p>Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents, 2023.11 <a href="https://arxiv.org/pdf/2311.11797.pdf">[paper]</a> <a href="https://github.com/Zoeyyao27/CoT-Igniting-Agent">[project]</a></p>
</li>
</ul>
<h3><span id="prompt-engineering-for-llm">Prompt Engineering for LLM</span><a href="#prompt-engineering-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Prompting Frameworks for Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12785.pdf">[paper]</a><a href="https://github.com/lxx0628/Prompting-Framework-Survey">[project]</a></p>
</li>
<li><p>Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review, 2023.10 <a href="https://arxiv.org/pdf/2310.14735.pdf">[paper]</a></p>
</li>
<li><p>Towards Better Chain-of-Thought Prompting Strategies: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.04959.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="retrieval-augmented-llm">Retrieval-Augmented LLM</span><a href="#retrieval-augmented-llm" class="header-anchor">#</a></h3><ul>
<li><p>Retrieving Multimodal Information for Augmented Generation: A Survey  *** </p>
</li>
<li><p>Retrieval-Augmented Generation for AI-Generated Content: A Survey *** </p>
</li>
<li><p>A Survey on Retrieval-Augmented Text Generation, 2022.02 <a href="https://arxiv.org/abs/2202.01110">[paper]</a></p>
</li>
<li><p>Retrieval-Augmented Generation for Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.10997.pdf">[paper]</a> <a href="https://github.com/Tongji-KGLLM/RAG-Survey">[project]</a> ***</p>
</li>
</ul>
<hr>
<h2><span id="challenge-of-llm">Challenge of LLM</span><a href="#challenge-of-llm" class="header-anchor">#</a></h2><h3><span id="hallucination-in-llm">Hallucination in LLM</span><a href="#hallucination-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.07914">[paper]</a></p>
</li>
<li><p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, 2023.11 <a href="https://arxiv.org/pdf/2311.05232">[paper]</a><a href="https://github.com/LuckyyySTA/Awesome-LLM-hallucination">[project]</a> ***</p>
</li>
<li><p>A Survey of Hallucination in “Large” Foundation Models, 2023.09  <a href="https://arxiv.org/paper/2309.05922">[paper]</a><a href="https://github.com/vr25/hallucination-foundation-model-survey">[project]</a></p>
</li>
<li><p>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models, 2023.09 <a href="https://arxiv.org/abs/2309.01219">[paper]</a><a href="https://arxiv.org/abs/2309.01219">[project]</a></p>
</li>
<li><p>Cognitive Mirage: A Review of Hallucinations in Large Language Models, 2023.09 <a href="https://arxiv.org/paper/2309.06794.paper">[paper]</a><a href="https://github.com/hongbinye/Cognitive-Mirage-Hallucinations-in-LLMs">[project]</a></p>
</li>
<li><p>Augmenting LLMs with Knowledge: A survey on hallucination prevention, 2023.09 <a href="https://arxiv.org/pdf/2309.16459.pdf">[paper]</a></p>
</li>
<li><p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models, 2024.01 <a href="https://arxiv.org/pdf/2401.01313.pdf">[paper]</a></p>
</li>
<li><p>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment, 2023.08 <a href="https://arxiv.org/abs/2308.05374">[paper]</a></p>
</li>
</ul>
<h3><span id="compression-for-llm">Compression for LLM</span><a href="#compression-for-llm" class="header-anchor">#</a></h3><ul>
<li>A Survey on Model Compression for Large Language Models, 2023.08 <a href="https://arxiv.org/abs/2308.07633">[paper]</a>  ***</li>
<li>A Comprehensive Survey of Compression Algorithms for Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.15347.pdf">paper</a>]</li>
</ul>
<h3><span id="evaluation-of-llm">Evaluation of LLM</span><a href="#evaluation-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Evaluating Large Language Models: A Comprehensive Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.19736.pdf">[paper]</a><a href="https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers">[project]</a> ***</p>
</li>
<li><p>A Survey on Evaluation of Large Language Models, 2023.07 <a href="https://arxiv.org/abs/2307.03109">[paper]</a><a href="https://llm-eval.github.io/">[project]</a> ***</p>
</li>
</ul>
<h3><span id="reasoning-with-llm">Reasoning with LLM</span><a href="#reasoning-with-llm" class="header-anchor">#</a></h3><ul>
<li><p>Reasoning with Language Model Prompting: A Survey, 2022.12 <a href="https://arxiv.org/abs/2212.09597">[paper]</a><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">[project]</a></p>
</li>
<li><p>A Survey of Reasoning with Foundation Models, 2023.12 [[papaer]] (<a href="https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)">https://arxiv.org/pdf/2312.11562.pdf)[[project]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)</a> ***</p>
</li>
</ul>
<h3><span id="long-context-for-llm">Long-Context for LLM</span><a href="#long-context-for-llm" class="header-anchor">#</a></h3><ul>
<li>Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.12351">[paper]</a></li>
<li>Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding, 2023.12 <a href="https://arxiv.org/abs/2312.17044">[paper]</a></li>
</ul>
<h3><span id="factuality-in-llm">Factuality in LLM</span><a href="#factuality-in-llm" class="header-anchor">#</a></h3><ul>
<li><p>A Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity, 2023.10 <a href="https://arxiv.org/abs/2310.07521">[paper]</a><a href="https://github.com/wangcunxiang/LLM-Factuality-Survey">[project]</a></p>
</li>
<li><p>Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models, 2023.10 <a href="https://arxiv.org/pdf/2310.16570.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="knowledge-for-llm">Knowledge for LLM</span><a href="#knowledge-for-llm" class="header-anchor">#</a></h3><ul>
<li><p>Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges, 2023.11 <a href="https://arxiv.org/pdf/2311.15766">[paper]</a></p>
</li>
<li><p>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications, 2023.11 <a href="https://arxiv.org/pdf/2311.05876.pdf">[paper]</a></p>
</li>
<li><p>Knowledge Editing for Large Language Models: A Survey, 2023.10 <a href="https://arxiv.org/pdf/2310.16218.pdf">[paper]</a></p>
</li>
<li><p>Editing Large Language Models: Problems, Methods, and Opportunities, 2023.05 <a href="https://arxiv.org/abs/2305.13172">[paper]</a><a href="https://github.com/zjunlp/EasyEdit">[project]</a></p>
</li>
<li><p>Building trust in conversational ai: A comprehensive review and solution architecture for explainable, privacy-aware systems using llms and knowledge graph, 2023.08 <a href="https://arxiv.org/pdf/2308.13534.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="self-correction-for-llm">Self-Correction for LLM</span><a href="#self-correction-for-llm" class="header-anchor">#</a></h3><ul>
<li>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies, 2023.08 <a href="https://arxiv.org/abs/2308.03188">[paper]</a><a href="https://github.com/teacherpeterpan/self-correction-llm-papers">[project]</a></li>
</ul>
<h3><span id="tool-using-of-llm">Tool Using of LLM</span><a href="#tool-using-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Foundation Models for Decision Making: Problems, Methods, and Opportunities, 2023.03 <a href="https://arxiv.org/abs/2303.04129">[paper]</a></p>
</li>
<li><p>Augmented Language Models: a Survey, 2023.02 <a href="https://arxiv.org/abs/2302.07842">[paper]</a> ***</p>
</li>
<li><p>Tool Learning with Foundation Models</p>
</li>
</ul>
<h3><span id="agent-of-llm">Agent of LLM</span><a href="#agent-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Understanding the planning of LLM agents: A survey, 2024 </p>
</li>
<li><p>A Survey on Large Language Model based Autonomous Agents, 2023.08 <a href="https://arxiv.org/abs/2308.11432">[paper]</a><a href="https://github.com/Paitesanshi/LLM-Agent-Survey">[project]</a> ***</p>
</li>
<li><p>The Rise and Potential of Large Language Model Based Agents: A Survey, 2023.09 <a href="https://arxiv.org/abs/2309.07864">[paper]</a><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">[project]</a> ***</p>
</li>
<li><p>Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives, 2023.12 <a href="https://arxiv.org/pdf/2312.11970.pdf">[paper]</a></p>
</li>
</ul>
<h3><span id="efficiency-of-llm">Efficiency of LLM</span><a href="#efficiency-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning ***</p>
</li>
<li><p>The Power of Scale for Parameter-Efficient Prompt Tuning</p>
</li>
<li><p>Efficient Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03863">[paper]</a><a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">[project]</a> ***</p>
</li>
<li><p>The Efficiency Spectrum of Large Language Models: An Algorithmic Survey, 2023.12 <a href="https://arxiv.org/pdf/2310.10844.pdf">[paper]</a><a href="https://github.com/tding1/Efficient-LLM-Survey">[project]</a></p>
</li>
<li><p>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment, 2023.12 <a href="https://arxiv.org/pdf/2312.12148.pdf">[paper]</a></p>
</li>
<li><p>A Survey on Hardware Accelerators for Large Language Models, 2024.01 [<a href="https://arxiv.org/pdf/2401.09890.pdf">paper</a>]</p>
</li>
</ul>
<h3><span id="data-of-llm">Data of LLM</span><a href="#data-of-llm" class="header-anchor">#</a></h3><ul>
<li><p>Data Management For Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.01700">[paper]</a><a href="https://github.com/ZigeW/data_management_LLM">[project]</a></p>
</li>
<li><p>Data-centric Artificial Intelligence: A Survey</p>
</li>
</ul>
<h3><span id="continual-learning-of-llm">Continual Learning of LLM</span><a href="#continual-learning-of-llm" class="header-anchor">#</a></h3><ul>
<li>Continual Learning with Pre-Trained Models: A Survey, 2024.01 <a href="https://arxiv.org/pdf/2401.16386">[paper]</a> <a href="https://github.com/sun-hailong/LAMDA-PILOT">[project]</a></li>
</ul>
<hr>
<h2><span id="mulitmodal-of-llm">Mulitmodal of LLM</span><a href="#mulitmodal-of-llm" class="header-anchor">#</a></h2><h3><span id="visual-llm">Visual LLM</span><a href="#visual-llm" class="header-anchor">#</a></h3><ul>
<li><p>An Empirical Study of Training End-to-End Vision-and-Language Transformers, 2022.03 *** microsoft +</p>
</li>
<li><p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants, 2023.09 *** microsoft +</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 ***  大学 +</p>
</li>
<li><p>MM-LLMs: Recent Advances in MultiModal Large Language Models, 2024.02 *** 腾讯  +</p>
</li>
<li><p>Vision-Language Instruction Tuning: A Review and Analysis, 2023,11 <a href="https://arxiv.org/abs/2311.08172">[paper]</a><a href="https://github.com/palchenli/VL-Instruction-Tuning">[project]</a> *** + </p>
</li>
<li><p>How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model, 2023.11 <a href="https://arxiv.org/pdf/2311.07594.pdf">[paper]</a> *</p>
</li>
<li><p>A Survey on Multimodal Large Language Models, 2023.06  <a href="https://arxiv.org/abs/2306.13549">[paper]</a> <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">[project]</a> ***</p>
</li>
<li><p>Multimodal Large Language Models: A Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.13165.pdf">[paper]</a> **</p>
</li>
<li><p>Large Language Models Meet Computer Vision: A Brief Survey, 2023.11 <a href="https://arxiv.org/pdf/2311.16673.pdf">[paper]</a> *</p>
</li>
<li><p>Foundational Models Defining a New Era in Vision: A Survey and Outlook, 2023.07 <a href="https://arxiv.org/pdf/2307.13721.pdf">[paper]</a><a href="https://github.com/awaisrauf/Awesome-CV-Foundational-Models">[project]</a> ***  + </p>
</li>
<li><p>Video Understanding with Large Language Models: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17432.pdf">[paper]</a> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">[project]</a></p>
</li>
</ul>
<hr>
<h2><span id="llm-for-domain-application">LLM for Domain Application</span><a href="#llm-for-domain-application" class="header-anchor">#</a></h2><h3><span id="domain">domain</span><a href="#domain" class="header-anchor">#</a></h3><ul>
<li>Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey</li>
</ul>
<h3><span id="llm-for-health">LLM for Health</span><a href="#llm-for-health" class="header-anchor">#</a></h3><ul>
<li><p>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge, 2023.11 <a href="https://arxiv.org/pdf/2311.05112">[paper]</a><a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">[project]</a></p>
</li>
<li><p>Large Language Models Illuminate a Progressive Pathway to Artificial  Healthcare Assistant: A Review, 2023.10 <a href="https://arxiv.org/pdf/2311.01918">[paper]</a><a href="https://github.com/mingze-yuan/Awesome-LLM-Healthcare">[project]</a></p>
</li>
<li><p>Large AI Models in Health Informatics: Applications, Challenges, and the Future, 2023.03 <a href="https://arxiv.org/abs/2303.11568">[paper]</a><a href="https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models">[project]</a></p>
</li>
<li><p>A SWOT (Strengths, Weaknesses, Opportunities, and Threats) Analysis of ChatGPT in the Medical Literature: Concise Review, 2023.11 <a href="https://www.jmir.org/2023/1/e49368/PDF">[paper]</a></p>
</li>
<li><p>ChatGPT in Healthcare: A Taxonomy and Systematic Review, 2023.03 <a href="https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1">[paper]</a></p>
</li>
</ul>
<h3><span id="llm-for-finance">LLM for Finance</span><a href="#llm-for-finance" class="header-anchor">#</a></h3><ul>
<li><p>Large Language Models in Finance: A Survey, 2023.09 <a href="https://arxiv.org/abs/2311.10723">[paper]</a></p>
</li>
<li><p>A Survey of Large Language Models in Finance (FinLLMs) ***</p>
</li>
</ul>
<h3><span id="llm-for-education">LLM for Education</span><a href="#llm-for-education" class="header-anchor">#</a></h3><ul>
<li>ChatGPT and Beyond: The Generative AI Revolution in Education, 2023.11 <a href="https://arxiv.org/pdf/2311.15198">[paper]</a></li>
</ul>
<h3><span id="llm-for-law">LLM for Law</span><a href="#llm-for-law" class="header-anchor">#</a></h3><ul>
<li>Large Language Models in Law: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.03718">[paper]</a></li>
</ul>
<h3><span id="llm-for-mental-health">LLM for Mental Health</span><a href="#llm-for-mental-health" class="header-anchor">#</a></h3><ul>
<li>A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement, 2023.10 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10601652/">[paper]</a></li>
<li>Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects, 2023.12 <a href="https://arxiv.org/pdf/2312.04578.pdf">[paper]</a></li>
<li>Large Language Models in Mental Health Care: a Scoping Review, 2024.01 <a href="https://arxiv.org/pdf/2401.02984.pdf">[paper]</a></li>
</ul>
<hr>
<h2><span id="llm-for-downstream-tasks">LLM for Downstream Tasks</span><a href="#llm-for-downstream-tasks" class="header-anchor">#</a></h2><h3><span id="llm-for-recommendation">LLM for Recommendation</span><a href="#llm-for-recommendation" class="header-anchor">#</a></h3><ul>
<li>User Modeling in the Era of Large Language Models: Current Research and Future Directions, 2023.12 <a href="https://doi.org/10.48550/arXiv.2312.11518">[paper]</a><a href="https://github.com/TamSiuhin/LLM-UM-Reading">[project]</a></li>
<li>A Survey on Large Language Models for Personalized and Explainable  Recommendations, 2023.11 <a href="https://arxiv.org/pdf/2311.12338">[paper]</a></li>
<li>Large Language Models for Generative Recommendation: A Survey and Visionary Discussions, 2023.09 <a href="https://arxiv.org/abs/2309.01157">[paper]</a></li>
<li>A Survey on Large Language Models for Recommendation, 2023.08 <a href="https://arxiv.org/abs/2305.19860">[paper]</a><a href="https://github.com/WLiK/LLM4Rec-Awesome-Papers">[project]</a></li>
<li>How Can Recommender Systems Benefit from Large Language Models: A Survey, 2023.06 <a href="https://arxiv.org/abs/2306.05817">[paper]</a><a href="https://github.com/CHIANGEL/Awesome-LLM-for-RecSys">[project]</a></li>
</ul>
<h3><span id="llm-for-information-retrieval">LLM for Information Retrieval</span><a href="#llm-for-information-retrieval" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Information Retrieval: A Survey, 2023.08 <a href="https://arxiv.org/abs/2308.07107">[paper]</a><a href="https://github.com/RUC-NLPIR/LLM4IR-Survey">[project]</a></li>
</ul>
<h3><span id="llm-for-software-engineering">LLM for Software Engineering</span><a href="#llm-for-software-engineering" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Software Engineering: Survey and Open Problems, 2023.10 <a href="https://arxiv.org/abs/2310.03533">[paper]</a></li>
<li>Large Language Models for Software Engineering: A Systematic Literature Review, 2023.08 <a href="https://arxiv.org/abs/2308.10620">[paper]</a></li>
</ul>
<h3><span id="llm-for-time-series">LLM for Time Series</span><a href="#llm-for-time-series" class="header-anchor">#</a></h3><ul>
<li>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook, 2023.10 <a href="https://arxiv.org/abs/2310.10196">[paper]</a><a href="https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM">[project]</a></li>
</ul>
<h3><span id="detection-of-llms-generated-content">Detection of LLMs-Generated Content</span><a href="#detection-of-llms-generated-content" class="header-anchor">#</a></h3><ul>
<li>A Survey on Detection of LLMs-Generated Content, 2023.10 <a href="https://arxiv.org/abs/2310.15654">[paper]</a><a href="https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection">[project]</a></li>
<li>A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions, 2023.10 <a href="https://arxiv.org/pdf/2310.14724.pdf">[paper]</a><br><a href="https://github.com/NLP2CT/LLM-generated-Text-Detection">[project]</a></li>
<li>Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text, 2023.09 <a href="https://arxiv.org/pdf/2309.07689.pdf">[paper]</a></li>
<li></li>
</ul>
<h3><span id="llm-for-information-extraction">LLM for Information Extraction</span><a href="#llm-for-information-extraction" class="header-anchor">#</a></h3><ul>
<li>Large Language Models for Generative Information Extraction: A Survey, 2023.12 <a href="https://arxiv.org/pdf/2312.17617.pdf">[paper]</a> <a href="https://github.com/quqxui/Awesome-LLM4IE-Papers">[project]</a></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p><a href="https://github.com/www6v/Awesome-LLM-Survey">Awesome-LLM-Survey</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA 家族</title>
    <url>/www6vHomeAIGC/2023/02/24/gptLlamaFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="llama-家族1">LLaMA 家族[1]</span><a href="#llama-家族1" class="header-anchor">#</a></h1><table>
<thead>
<tr>
<th>项目</th>
<th>描述</th>
<th>数据集</th>
</tr>
</thead>
<tbody><tr>
<td>LLaMa</td>
<td>基座模型</td>
<td>公开可用的数据集(1T token)</td>
</tr>
<tr>
<td>Stanford Alpaca</td>
<td>结合英文语料通过Self Instruct方式微调LLaMA 7B</td>
<td>Self Instruct from davinci-003 API(52K)</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune)</td>
<td>用户共享对话(70K sample)</td>
</tr>
<tr>
<td>BELLE</td>
<td>结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA</td>
<td></td>
</tr>
<tr>
<td>Chinese-LLaMA&#x2F;Chinese-Alpaca</td>
<td>通过中文数据预训练&#x2F;指令微调LLaMA</td>
<td></td>
</tr>
<tr>
<td>姜子牙系列模型Ziya-LLaMA-13B-v1</td>
<td>基于LLaMA-13B的中英文模型</td>
<td></td>
</tr>
<tr>
<td>ChatLLaMA(英文版)</td>
<td>LLaMA的RLHF版</td>
<td></td>
</tr>
<tr>
<td>ColossalChat</td>
<td>通过self-instruct技术指令微调LLaMA且加上RLHF</td>
<td></td>
</tr>
</tbody></table>
<img src="/www6vHomeAIGC/2023/02/24/gptLlamaFamily/llama2-famaly.jpg" class>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="家族">家族</span><a href="#家族" class="header-anchor">#</a></h3><ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/129709105">LLaMA的解读与其微调：Alpaca-LoRA&#x2F;Vicuna&#x2F;BELLE&#x2F;中文LLaMA&#x2F;姜子牙&#x2F;LLaMA 2</a> ***</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485019&idx=1&sn=e3417472c0c1f98aede498fbe905e1a0&">我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/618695885">NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究</a></p>
<p>1xx. &lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;  v</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402185&idx=2&sn=55901b89381e27aedee56c69041f6af8">近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 </a>    llama-2-7b-32k -  LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。</p>
<h3><span id="实战">实战</span><a href="#实战" class="header-anchor">#</a></h3><p>1xx.  <a href="https://zhuanlan.zhihu.com/p/618321077">从0到1复现斯坦福羊驼（Stanford Alpaca 7B）</a><br>    GPUs: 8 卡 A800 80GB GPUs</p>
<h3><span id="汉化">汉化</span><a href="#汉化" class="header-anchor">#</a></h3><p>1xx.  <a href="https://www.bilibili.com/video/BV1Np4y1j783/">掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享</a> v</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Chinese-LLaMA PT+SFT</title>
    <url>/www6vHomeAIGC/2023/02/21/gptChineseLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81-%E6%A8%A1%E5%9E%8B-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87">代码、模型、数据集准备</a><ul>
<li><a href="#%E4%BB%A3%E7%A0%81%E5%87%86%E5%A4%87-5">代码准备 [5]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D-%E5%8F%8A-tokenizer-%E5%87%86%E5%A4%87-4">模型权重 及 Tokenizer 准备 [4]</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87-3">数据集准备 [3]</a></li>
</ul>
</li>
<li><a href="#%E8%AF%8D%E8%A1%A8%E6%89%A9%E5%85%85">词表扩充</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82">模型训练细节</a><ul>
<li><a href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E9%A2%84%E8%AE%AD%E7%BB%83">第二阶段预训练</a></li>
<li><a href="#%E5%B0%86-lora-%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将 LoRA 权重与基础模型合并</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E7%B2%BE%E8%B0%83">指令精调</a></li>
<li><a href="#%E5%B0%86%E5%A4%9A%E4%B8%AAlora%E6%9D%83%E9%87%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6">将多个LoRA权重与基础模型合并</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86">模型推理</a></li>
<li><a href="#%E7%BB%93%E8%AF%AD">结语</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="环境搭建">环境搭建</span><a href="#环境搭建" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install transformers==4.28.1 sentencepiece==0.1.97 google protobuf deepspeed -i https://pypi.tuna.tsinghua.ed</span></span><br><span class="line">u.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/huggingface/peft.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git checkout 13e53fc</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install torch==1.13.1</span></span><br></pre></td></tr></table></figure>
<h1><span id="代码-模型-数据集准备">代码、模型、数据集准备</span><a href="#代码-模型-数据集准备" class="header-anchor">#</a></h1><h3><span id="代码准备-5">代码准备 [5]</span><a href="#代码准备-5" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3e2f2529</span></span><br><span class="line">git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意: 一定要用 commitid &#x3D;3e2f2529的代码， 用最新代码会有很多异常</p>
</blockquote>
<h3><span id="模型权重-及-tokenizer-准备-4">模型权重 及 Tokenizer 准备 [4]</span><a href="#模型权重-及-tokenizer-准备-4" class="header-anchor">#</a></h3><h3><span id="数据集准备-3">数据集准备 [3]</span><a href="#数据集准备-3" class="header-anchor">#</a></h3><h1><span id="词表扩充">词表扩充</span><a href="#词表扩充" class="header-anchor">#</a></h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python3 merge_tokenizers.py \</span></span><br><span class="line"><span class="language-bash">  --llama_tokenizer_dir /root/internLM/model/skyline2006/llama-7b \</span></span><br><span class="line"><span class="language-bash">  --chinese_sp_model_file /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/chinese_sp.model</span></span><br></pre></td></tr></table></figure>

<h1><span id="模型训练细节">模型训练细节</span><a href="#模型训练细节" class="header-anchor">#</a></h1><h3><span id="第二阶段预训练">第二阶段预训练</span><a href="#第二阶段预训练" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改运行脚本run_pt.sh</span><br><span class="line"></span><br><span class="line">lr=2e-4</span><br><span class="line">lora_rank=8</span><br><span class="line">lora_alpha=32</span><br><span class="line">lora_trainable=&quot;q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj&quot;</span><br><span class="line">modules_to_save=&quot;embed_tokens,lm_head&quot;</span><br><span class="line">lora_dropout=0.05</span><br><span class="line"></span><br><span class="line">pretrained_model=/root/internLM/model/skyline2006/llama-7b #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  #</span><br><span class="line">dataset_dir=/root/internLM/shu-master/books  #</span><br><span class="line">data_cache=/root/cache/books #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/output_dir #</span><br><span class="line">RANDOM=100 #</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>具体执行过程如下所示：<br>sh run_pt.sh </p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/3.png" class>

<p>模型输出文件：</p>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/result.png" class>


<h3><span id="将-lora-权重与基础模型合并">将 LoRA 权重与基础模型合并</span><a href="#将-lora-权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python merge_llama_with_chinese_lora.py \</span><br><span class="line">    --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">    --tokenizer_path /root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf  \</span><br><span class="line">    --lora_model /root/internLM/llamazh/output_dir/lora/ \</span><br><span class="line">    --output_type huggingface \</span><br><span class="line">    --output_dir /root/internLM/llamazh/pt_merged/book-merge-hf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并LLaMA和LoRA后的权重</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ll -h /root/internLM/llamazh/pt_merged/book-merge-hf</span></span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 10:48 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 10:47 ../</span><br><span class="line">-rw-r--r-- 1 root root  598 Feb 23 10:47 config.json</span><br><span class="line">-rw-r--r-- 1 root root  132 Feb 23 10:47 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9.3G Feb 23 10:48 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3.6G Feb 23 10:48 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root  27K Feb 23 10:48 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root  411 Feb 23 10:47 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root 741K Feb 23 10:47 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root  727 Feb 23 10:47 tokenizer_config.json</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">原始llama权重</span></span><br><span class="line">ll -h /root/internLM/model/skyline2006/llama-7b</span><br><span class="line">total 13G</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 21 20:04 ./</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 21 19:38 ../</span><br><span class="line">-rw-r--r-- 1 root root   43 Feb 21 19:38 .mdl</span><br><span class="line">-rw------- 1 root root 3.6K Feb 21 20:04 .msc</span><br><span class="line">-rw-r--r-- 1 root root   36 Feb 21 20:04 .mv</span><br><span class="line">-rw------- 1 root root  11K Feb 21 19:39 LICENSE</span><br><span class="line">-rw------- 1 root root 9.0K Feb 21 20:04 README.md</span><br><span class="line">-rw------- 1 root root  22M Feb 21 19:38 alpaca_data.json</span><br><span class="line">-rw------- 1 root root  427 Feb 21 19:38 config.json</span><br><span class="line">-rw------- 1 root root  302 Feb 21 19:39 configuration.json</span><br><span class="line">-rw------- 1 root root 1.2K Feb 21 19:39 default_offload_opt_param.json</span><br><span class="line">-rw------- 1 root root  124 Feb 21 19:39 generation_config.json</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:40 pytorch_model-00001-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:42 pytorch_model-00002-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:44 pytorch_model-00003-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:47 pytorch_model-00004-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:50 pytorch_model-00005-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:51 pytorch_model-00006-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:53 pytorch_model-00007-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00008-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:55 pytorch_model-00009-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00010-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00011-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:56 pytorch_model-00012-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00013-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:57 pytorch_model-00014-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:58 pytorch_model-00015-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00016-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00017-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00018-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 19:59 pytorch_model-00019-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00020-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00021-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:00 pytorch_model-00022-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00023-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00024-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00025-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:01 pytorch_model-00026-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00027-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00028-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:02 pytorch_model-00029-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00030-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00031-of-00033.bin</span><br><span class="line">-rw------- 1 root root 387M Feb 21 20:03 pytorch_model-00032-of-00033.bin</span><br><span class="line">-rw------- 1 root root 501M Feb 21 20:04 pytorch_model-00033-of-00033.bin</span><br><span class="line">-rw------- 1 root root  25K Feb 21 20:04 pytorch_model.bin.index.json</span><br><span class="line">-rw------- 1 root root    2 Feb 21 20:04 special_tokens_map.json</span><br><span class="line">-rw------- 1 root root 489K Feb 21 20:04 tokenizer.model</span><br><span class="line">-rw------- 1 root root  141 Feb 21 20:04 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h3><span id="指令精调">指令精调</span><a href="#指令精调" class="header-anchor">#</a></h3><p>修改模型精调脚本run_sft.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pretrained_model=/root/internLM/llamazh/pt_merged/book-merge-hf  #</span><br><span class="line">chinese_tokenizer_path=/root/internLM/Chinese-LLaMA-Alpaca-main/scripts/merge_tokenizer/merged_tokenizer_hf #</span><br><span class="line">dataset_dir=/root/internLM/Chinese-LLaMA-Alpaca-main/data #</span><br><span class="line">per_device_train_batch_size=1</span><br><span class="line">per_device_eval_batch_size=1</span><br><span class="line">training_steps=100</span><br><span class="line">gradient_accumulation_steps=1</span><br><span class="line">output_dir=/root/internLM/llamazh/sft_output  #</span><br><span class="line">#peft_model=path/to/peft/model/dir</span><br><span class="line">validation_file=/root/internLM/llm-action-main/train/chinese-llama-alpaca/alpaca_eval.json  #</span><br><span class="line">RANDOM=1000</span><br><span class="line">deepspeed_config_file=ds_zero2_no_offload.json</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; sh run_sft.sh </span><br></pre></td></tr></table></figure>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-2.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result1.png" class>
<img src="/www6vHomeAIGC/2023/02/21/gptChineseLlama/sft-result2.png" class>


<p>模型输出文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -al -h /root/internLM/llamazh/sft_output/lora</span></span><br><span class="line">total 819M</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Feb 23 15:29 .</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Feb 23 15:29 ..</span><br><span class="line">-rw-r--r-- 1 root root  501 Feb 23 15:29 adapter_config.json</span><br><span class="line">-rw-r--r-- 1 root root 819M Feb 23 15:29 adapter_model.bin</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="将多个lora权重与基础模型合并">将多个LoRA权重与基础模型合并</span><a href="#将多个lora权重与基础模型合并" class="header-anchor">#</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python merge_llama_with_chinese_lora.py \</span><br><span class="line">     --base_model /root/internLM/model/skyline2006/llama-7b \</span><br><span class="line">     --tokenizer_path /root/internLM/llamazh/output_dir,/root/internLM/llamazh/sft_output \</span><br><span class="line">     --lora_model /root/internLM/llamazh/output_dir/lora/,/root/internLM/llamazh/sft_output/lora \</span><br><span class="line">     --output_type huggingface \</span><br><span class="line">     --output_dir /root/internLM/llamazh/all_output</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ll  /root/internLM/llamazh/all_output</span><br><span class="line">total 13449132</span><br><span class="line">drwxr-xr-x 2 root root       4096 Feb 23 16:38 ./</span><br><span class="line">drwxr-xr-x 7 root root       4096 Feb 23 16:38 ../</span><br><span class="line">-rw-r--r-- 1 root root         21 Feb 23 16:38 added_tokens.json</span><br><span class="line">-rw-r--r-- 1 root root        598 Feb 23 16:38 config.json</span><br><span class="line">-rw-r--r-- 1 root root        132 Feb 23 16:38 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 9943340890 Feb 23 16:38 pytorch_model-00001-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root 3827767515 Feb 23 16:38 pytorch_model-00002-of-00002.bin</span><br><span class="line">-rw-r--r-- 1 root root      26788 Feb 23 16:38 pytorch_model.bin.index.json</span><br><span class="line">-rw-r--r-- 1 root root        435 Feb 23 16:38 special_tokens_map.json</span><br><span class="line">-rw-r--r-- 1 root root     757958 Feb 23 16:38 tokenizer.model</span><br><span class="line">-rw-r--r-- 1 root root        747 Feb 23 16:38 tokenizer_config.json</span><br></pre></td></tr></table></figure>

<h1><span id="模型推理">模型推理</span><a href="#模型推理" class="header-anchor">#</a></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py \</span><br><span class="line">     --base_model /root/internLM/llamazh/all_output \</span><br><span class="line">     --with_prompt \</span><br><span class="line">     --interactive</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ python inference_hf.py      --base_model /root/internLM/llamazh/all_output      --with_prompt      --<span class="keyword">in</span></span><br><span class="line">teractive</span><br><span class="line">Loading checkpoint shards: <span class="number">100</span>%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| <span class="number">2</span>/<span class="number">2</span> [<span class="number">00</span>:<span class="number">26</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">13.09</span>s/it]</span><br><span class="line">Vocab of the base model: <span class="number">49954</span></span><br><span class="line">Vocab of the tokenizer: <span class="number">49954</span></span><br><span class="line">Start inference <span class="keyword">with</span> instruction mode.</span><br><span class="line">=====================================================================================</span><br><span class="line">+ 该模式下仅支持单轮问答，无多轮对话能力。</span><br><span class="line">+ 如要进行多轮对话，请使用llama.cpp或llamachat工具。</span><br><span class="line">-------------------------------------------------------------------------------------</span><br><span class="line">+ This mode only supports single-turn QA.</span><br><span class="line">+ If you want to experience multi-turn dialogue, please use llama.cpp <span class="keyword">or</span> llamachat.</span><br><span class="line">=====================================================================================</span><br><span class="line">Input:who are you？</span><br><span class="line">Response:  I am <span class="number">10</span> years old, my name <span class="keyword">is</span> Lilly.</span><br></pre></td></tr></table></figure>

<h1><span id="结语">结语</span><a href="#结语" class="header-anchor">#</a></h1><p>整个训练流程:<br>词表扩充+预训练(继续预训练)  -&gt;  输出lora模型<br>指令精调sft   -&gt;  输出lora模型<br>合并2个lora模型，在进行推理</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/631360711">中文LLaMA&amp;Alpaca大语言模型词表扩充+预训练+指令精调</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/">Chinese-LLaMA-Alpaca</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki">中文文档</a><br><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC">预训练脚本</a></li>
<li><a href="https://github.com/shjwudp/shu">继续预训练 DataSet</a></li>
<li><a href="https://www.modelscope.cn/models/skyline2006/llama-7b/summary">llama-7b</a> 基础模型</li>
<li><a href="https://github.com/www6v/AIGC/tree/master/chinese-llama-alpaca">chinese-llama-alpaca</a>  git 代码以这个为主<br><a href="https://github.com/www6v/llm-action/tree/main/train/chinese-llama-alpaca">chinese-llama-alpaca</a> 参考这个代码，有很多遗漏的文件，都补齐了，已提交到AIGC</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SELF-INSTRUCT</title>
    <url>/www6vHomeAIGC/2023/02/21/gptSelfInstruct/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="self-instruct">SELF-INSTRUCT</span><a href="#self-instruct" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SELF-INSTRUCT-10dbfe21108480adb3c9c6b4d13b57d0?pvs=4">(原理)SELF-INSTRUCT</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>STF</category>
      </categories>
      <tags>
        <tag>STF</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Agent 多模态</title>
    <url>/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#survey">Survey</a><ul>
<li><a href="#%E7%B1%BB%E5%9E%8B-i%E6%97%A0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E9%97%AD%E6%BA%90-llms-%E4%BD%9C%E4%B8%BA%E8%A7%84%E5%88%92%E5%99%A8">类型 I：无长期记忆的闭源 LLMs 作为规划器。</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B-ii%E6%97%A0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E5%BE%AE%E8%B0%83-llms-%E4%BD%9C%E4%B8%BA%E8%A7%84%E5%88%92%E5%99%A8">类型 II：无长期记忆的微调 LLMs 作为规划器。</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B-iv%E5%85%B7%E6%9C%89%E6%9C%AC%E5%9C%B0%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E8%A7%84%E5%88%92%E5%99%A8">类型 IV：具有本地长期记忆的规划器。</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent1">多模态 Agent[1]</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81-agent10">多模态 Agent[10]</a><ul>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a></li>
<li><a href="#works">works</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
<li><a href="#xxx">xxx</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%80%81agent">多模态Agent</a></li>
<li><a href="#xxx-1">xxx</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>



<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/abs/2402.15116">《Large Multimodal Agents: A Survey》</a> </p>
</li>
<li><p>开源地址<br> <a href="https://github.com/jun0wanan/awesome-large-multimodal-agents">Repo</a> git</p>
</li>
</ul>
<h1><span id="survey">Survey</span><a href="#survey" class="header-anchor">#</a></h1><h3><span id="类型-i无长期记忆的闭源-llms-作为规划器">类型 I：无长期记忆的闭源 LLMs 作为规划器。</span><a href="#类型-i无长期记忆的闭源-llms-作为规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2303.04671.pdf"><strong>Visual ChatGPT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2303.11381.pdf"><strong>MM-REACT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2303.08128.pdf"><strong>ViperGPT</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2303.17580.pdf"><strong>HuggingGPT</strong></a>  ***</p>
<p><a href="https://arxiv.org/pdf/2304.09842.pdf"><strong>Chameleon</strong></a> ***</p>
<p><a href="https://arxiv.org/pdf/2311.00571.pdf"><strong>LLaVA-Interactive</strong></a> ***</p>
<p><a href="https://arxiv.org/pdf/2401.01614"><strong>SeeAct</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2310.01415.pdf"><strong>GPT-Driver</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2401.16158.pdf"><strong>Mobile-Agent</strong></a> </p>
<h3><span id="类型-ii无长期记忆的微调-llms-作为规划器">类型 II：无长期记忆的微调 LLMs 作为规划器。</span><a href="#类型-ii无长期记忆的微调-llms-作为规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2306.08640.pdf"><strong>LLaVA-Plus</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2305.18752.pdf"><strong>GPT4Tools</strong></a> </p>
<h3><span id="类型-iv具有本地长期记忆的规划器">类型 IV：具有本地长期记忆的规划器。</span><a href="#类型-iv具有本地长期记忆的规划器" class="header-anchor">#</a></h3><p><a href="https://arxiv.org/pdf/2311.05997.pdf"><strong>JARV IS-1</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2312.13771.pdf"><strong>AppAgent</strong></a> </p>
<p><a href="https://arxiv.org/pdf/2307.07162.pdf"><strong>DLAH</strong></a> </p>
<h1><span id="多模态-agent1">多模态 Agent[1]</span><a href="#多模态-agent1" class="header-anchor">#</a></h1><ul>
<li><p>核心组件</p>
<ul>
<li><strong>感知</strong>组件关注处理多模态信息</li>
<li><strong>规划器</strong>负责推理和制定计划</li>
<li><strong>行动</strong>组件执行计划</li>
<li><strong>记忆</strong>组件则涉及长期和短期记忆</li>
</ul>
</li>
<li><p>四种类型</p>
<ul>
<li>无长期记忆的闭源 LLMs 作为规划器</li>
<li>无长期记忆的微调 LLMs 作为规划器</li>
<li>具有间接长期记忆的规划器 </li>
<li>具有本地长期记忆的规划器</li>
</ul>
</li>
<li><p>多智能体协作</p>
<ul>
<li>讨论了 LMAs 如何通过协作框架共同实现共同目标。</li>
</ul>
</li>
</ul>
<h1><span id="多模态-agent10">多模态 Agent[10]</span><a href="#多模态-agent10" class="header-anchor">#</a></h1><h3><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/tasks.JPG" class>

<ul>
<li><p>MM-ReAct </p>
</li>
<li><p>HuggingGPT[21, 22] </p>
</li>
<li><p>Chameleon</p>
</li>
<li><p>Visual ChatGPT [20]</p>
</li>
</ul>
<h3><span id="works">works</span><a href="#works" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/21/gptAgentMultimodal/works.jpg" class>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488499&idx=1&sn=ac8c5092ddc8fd724965d12aff3f9392">2024年大型多模态智能体(Large Multimodal Agents)综述：组件, 分类，协作，评估，应用，展望</a> ***</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/678203245">智体AI在多模态交互领域的综述（上）</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/678222381">智体AI在多模态交互领域的综述（下）</a></p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="10">
<li><a href="https://www.bilibili.com/video/BV1mM411X7Zn/">多模态 Agents：用大模型语言模型串联多模态专家</a> V</li>
</ol>
<h3><span id="多模态agent">多模态Agent</span><a href="#多模态agent" class="header-anchor">#</a></h3><p>1xx. <a href="/www6vHomeAIGC/2023/01/18/gptMultimodal/" title="(综述)多模态">(综述)多模态</a> self<br>1xx. <a href="/www6vHomeAIGC/2023/03/16/gptMultimodalSurvey/" title="(Survey)多模态">(Survey)多模态</a> self</p>
<h3><span id="xxx">xxx</span><a href="#xxx" class="header-anchor">#</a></h3><ol start="20">
<li><p>《Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models》<br><a href="https://github.com/chenfei-wu/TaskMatrix">Visual ChatGPT</a> git</p>
</li>
<li><p><a href="https://nakaizura.blog.csdn.net/article/details/130856470">LLMs的自动化工具系统（HuggingGPT、AutoGPT、WebGPT、WebCPM）</a>  </p>
</li>
<li><p><a href="https://github.com/microsoft/JARVIS">HuggingGPT</a> git<br><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb">hugginggpt in langchain</a> git<br><a href="https://github.com/camille-vanhoffelen/langchain-huggingGPT">langchain-huggingGPT</a> git</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/VXe6CHI_29Rw8xaOjfbqOQ">Visual Programming——实现通用人工智能的另一种方式 </a> 2022  best paper</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)量化</title>
    <url>/www6vHomeAIGC/2023/02/19/gptQuantization/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="量化">量化</span><a href="#量化" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/9e2982aada064c5895ae2f862f1d33c3?pvs=4">量化</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>量化</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>短文本相似度</title>
    <url>/www6vHomeAIGC/2023/02/18/gptDocSimilarity/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87">论文</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model">Model</a><ul>
<li><a href="#classification-objective-function">Classification Objective Function</a></li>
<li><a href="#regression-objective-function">Regression Objective Function</a></li>
<li><a href="#triplet-objective-function">Triplet Objective Function</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="论文">论文</span><a href="#论文" class="header-anchor">#</a></h1><ul>
<li>论文地址<br><a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence Bert</a></li>
</ul>
<h1><span id="introduction">Introduction</span><a href="#introduction" class="header-anchor">#</a></h1><p>而作者提出了Sentence-BERT网络结构来解决bert模型的不足。简单通俗地讲，就是借鉴孪生网络模型的框架，将不同的句子输入到两个bert模型中（但这两个bert模型是参数共享的，也可以理解为是同一个bert模型），获取到每个句子的句子表征向量；而最终获得的句子表征向量，可以用于语义相似度计算，也可以用于无监督的聚类任务。对于同样的10000个句子，我们想要找出最相似的句子对，只需要计算10000次，需要大约5秒就可计算完全。</p>
<h1><span id="model">Model</span><a href="#model" class="header-anchor">#</a></h1><h3><span id="classification-objective-function">Classification Objective Function</span><a href="#classification-objective-function" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/18/gptDocSimilarity/sbert1.jpg" class>


<h3><span id="regression-objective-function">Regression Objective Function</span><a href="#regression-objective-function" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/18/gptDocSimilarity/sbert2.jpg" class>


<h3><span id="triplet-objective-function">Triplet Objective Function</span><a href="#triplet-objective-function" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/113133510">Sentence-Bert论文笔记</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/111414376">短文本相似度算法研究</a><br>1xx. <a href="https://www.bilibili.com/video/BV13h4y1a7z6/">SentenceBert模型：文本语义去重</a> v<br>1xx. <a href="https://zhuanlan.zhihu.com/p/659682364">Sentence-BERT（SBERT）模型介绍及Sentence Transformers库的使用</a><br>1xx. <a href="https://cloud.tencent.com/developer/article/2330969">大型语言模型：SBERT — 句子BERT</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/113017752">传统方法TF-IDF解决短文本相似度问题</a><br>1xx. <a href="https://zhuanlan.zhihu.com/p/113224707">传统方法BM25解决短文本相似度问题</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>短文本相似度</category>
      </categories>
      <tags>
        <tag>短文本相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型</title>
    <url>/www6vHomeAIGC/2023/02/17/gptLargeModel/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="http://arthurchiao.art/blog/llm-practical-guide-zh/">[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）</a>   实战<br>1xx. <a href="https://zhuanlan.zhihu.com/p/597586623">通向AGI之路：大型语言模型（LLM）技术精要</a> *** </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403847&idx=1&sn=9af731e9f8418a2d869f5464530c8bd6">必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 </a> 12个综述</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Transformer</title>
    <url>/www6vHomeAIGC/2023/02/16/gptTransformerCode/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://github.com/www6v/AIGC/blob/master/transformer/transformer.ipynb">transformer.ipynb</a> git<br>   <a href="https://www.bilibili.com/video/BV1nc411y7m4/">Transformer代码实现</a></p>
<p>1xx. <a href="https://paperswithcode.com/method/transformer">Transformer</a><br>   <a href="https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201">transformer.py</a> git</p>
<p>1xx. <a href="http://arthurchiao.art/blog/transformers-from-scratch-zh/">[译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）</a> V, github<br>    Transformers from scratch</p>
<p>1xx. <a href="https://blog.csdn.net/v_JULY_v/article/details/130090649">从零实现Transformer的简易版与强大版：从300多行到3000多行</a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/398039366">Transformer源码详解（Pytorch版本）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>医疗大模型</title>
    <url>/www6vHomeAIGC/2023/02/07/gptDomainMed/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h3><span id="医疗大模型">医疗大模型</span><a href="#医疗大模型" class="header-anchor">#</a></h3><ul>
<li>LLaMA<ul>
<li>ChatDoctor  </li>
<li>华驼&#x2F;本草  哈工大</li>
<li>PMC-LLaMA 上海交大</li>
</ul>
</li>
<li>ChatGLM-6B<ul>
<li>ChatGLM-Med  哈工大</li>
<li>DoctorGLM</li>
<li>明医 (MING)  MedicalGPT-zh  上海交通大学</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402886&idx=1&sn=0552d60744645a84d13bb0cef57f321c">再看23个医疗领域微调大模型集合：兼看CareLlama医疗模型的一些实践经验与开放医疗数据 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402589&idx=1&sn=3ba9d50fad433adeb8dd6c623b06c42d">大模型遇上心理健康咨询：MeChat、QiaoBan、SoulChat、MindChat四大心理健康领域微调模型总结 </a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402638&idx=1&sn=b9329498806e2b93b2d6817a17941bff">大模型常见错误、反馈的来源及自我修正方法：兼论两个有趣的同名中医微调垂域模型 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>垂类大模型</category>
      </categories>
      <tags>
        <tag>垂类大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>测评</title>
    <url>/www6vHomeAIGC/2023/02/07/gptEval/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402223&idx=1&sn=f2ec30cd04600129bb90bc9c81413d95">一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 </a><br>1xx. <a href="https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese">https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese</a><br>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648402549&idx=1&sn=07a8af1db44df6125939c5c9e90f6267">如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403046&idx=1&sn=0a9b612e9790c0bf49d5cede8fda365c">大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 </a> 二、CEVAL榜单评测中能够得到一些启示<br><a href="https://github.com/hkust-nlp/ceval/blob/main/resources/tutorial.md">1. C-Eval 数据集评测简明教程</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403295&idx=1&sn=126c949d0a00eb85b4a3a6b0106f55a6&poc_token=HApExGWjou7N5NVcTKmJpWt9LZ8ul6wynjV5VHnQ">大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 </a>   CEVAl</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>eval</category>
      </categories>
      <tags>
        <tag>eval</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)SFT 数据组合</title>
    <url>/www6vHomeAIGC/2023/02/06/gptDatasetSFT/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="sft-数据组合">SFT 数据组合</span><a href="#sft-数据组合" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/SFT-0f98cbb2b48d46a182a19ed0ee9fc719?pvs=4">(原理)SFT 数据组合</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>SFT</category>
      </categories>
      <tags>
        <tag>SFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)幻觉问题</title>
    <url>/www6vHomeAIGC/2023/02/06/gptHallucination/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="幻觉-vs-事实性1">幻觉 vs 事实性[1]</span><a href="#幻觉-vs-事实性1" class="header-anchor">#</a></h1><p><strong>幻觉</strong>主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于”生成与某些来源相关的无意义或不真实的内容”。这与<strong>事实性问题</strong>不同，后者强调模型学习、获取和利用事实性知识的能力。</p>
<p>举例说明两者的<strong>区别</strong>：</p>
<p>如果一个LLM在被要求创作”一个关于兔子和狼交朋友的童话故事”时，创作出了一个关于”兔子和狗交朋友”的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。<br>如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是<strong>幻觉</strong>，而<strong>不是事实性问题</strong>。<br>例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是<strong>幻觉</strong>。</p>
<p>相反，如果LLM避免给出直接答案，而是说”我不知道”，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是<strong>事实性问题</strong>，而<strong>不是幻觉</strong>。</p>
<p>此外，值得注意的是，<strong>幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的</strong>。</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404394&idx=1&sn=d7cfcf2cd9aa6756d3cbff938f5f4cf2">再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403998&idx=1&sn=400cc902434bc04df508a55e192d2455">再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 </a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405983&idx=2&sn=95dc9c7a12bed99b63c775d4b90519d8">也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 </a></p>
<p>1xx. <a href="https://arxiv.org/abs/2309.01219">大模型幻觉综述</a><br>   <a href="https://arxiv.org/abs/2309.05922">大模型幻觉综述</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405791&idx=2&sn=d7dada69e6d5ab5fba1333d234b947ef">网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 </a> 大模型幻觉综述</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403602&idx=1&sn=f2365b05630094f8d0de7ff784abe233">大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介</a> 大模型微调遗忘</p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403341&idx=1&sn=86cdaaf2c3a73439d2591a2f3dd0b9e0">值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642648601">大模型的幻觉问题调研: LLM Hallucination Survey</a><br>   <a href="https://mp.weixin.qq.com/s?__biz=MzU5NDg2MjgxMg==&mid=2247485189&idx=1&sn=95d6eb333dde007f262a2955b90bc7ec">人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） </a><br>   <a href="https://mp.weixin.qq.com/s/eGMwNz0F1dQsNDnsLNYr8Q">大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二）</a></p>
<p>1xx. <a href="https://mp.weixin.qq.com/s/N7NOsLHr8HYCMp5XGCBDjg">LLM之幻觉（一）：大语言模型幻觉解决方案综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Hallucination</category>
      </categories>
      <tags>
        <tag>Hallucination</tag>
      </tags>
  </entry>
  <entry>
    <title>(Survey)数据处理</title>
    <url>/www6vHomeAIGC/2023/02/05/gptDataProcess/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-pipeline">数据处理 pipeline</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%80%9A%E7%94%A8-1">数据处理[通用] [1]</a><ul>
<li><a href="#%E8%B4%A8%E9%87%8F%E8%BF%87%E6%BB%A4">质量过滤</a></li>
<li><a href="#%E5%86%97%E4%BD%99%E5%8E%BB%E9%99%A4">冗余去除</a></li>
<li><a href="#%E8%AF%8D%E5%85%83%E5%88%87%E5%88%86">词元切分</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%862">数据处理[2]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%87%E8%AE%B0">数据标记</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">数据准备</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA">数据增强</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-1">数据增强</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F">数据质量</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="数据处理-pipeline">数据处理 pipeline</span><a href="#数据处理-pipeline" class="header-anchor">#</a></h1><h2><span id="数据处理通用-1">数据处理[通用] [1]</span><a href="#数据处理通用-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/data_process.png" class>

<h3><span id="质量过滤">质量过滤</span><a href="#质量过滤" class="header-anchor">#</a></h3><ul>
<li>基于<strong>分类器</strong>的方法</li>
<li>基于<strong>启发 式</strong>的方法</li>
</ul>
<h3><span id="冗余去除">冗余去除</span><a href="#冗余去除" class="header-anchor">#</a></h3><p>可以在<strong>句子级</strong>、<strong>文档级</strong>和<strong>数据集级</strong>等不同粒度上去重<br>在实践中应该 共同使用这三个级别的去重</p>
<h3><span id="词元切分">词元切分</span><a href="#词元切分" class="header-anchor">#</a></h3><ul>
<li>BPE</li>
<li>WordPiece</li>
<li>Unigram 词元分析</li>
</ul>
<h2><span id="数据处理2">数据处理[2]</span><a href="#数据处理2" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2023/02/05/gptDataProcess/pipeline.webp" class>
<h3><span id="数据标记">数据标记</span><a href="#数据标记" class="header-anchor">#</a></h3><ul>
<li>包标签</li>
<li>半监督标签</li>
<li>主动学习</li>
<li>数据编程</li>
<li>远程监督</li>
</ul>
<h3><span id="数据准备">数据准备</span><a href="#数据准备" class="header-anchor">#</a></h3><h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《大规模语言模型》 </li>
<li>《Data-centric Artificial Intelligence: A Survey》 大学<br><a href="https://zhuanlan.zhihu.com/p/620890799">Data-centric Artificial Intelligence: A Survey</a><br> <a href="https://cloud.tencent.com/developer/article/2359824">机器学习数据工程的概述</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/639207933">大模型时代下数据的重要性</a> 综述</p>
<p>1xx. <a href="https://hub.baai.ac.cn/view/28740">大模型研发核心：数据工程、自动化评估及与知识图谱的结合</a><br>   <a href="https://mp.weixin.qq.com/s/SvDnQD886E3DBtw8k9asgg">大模型研发核心：数据工程、自动化评估及与知识图谱的结合 </a></p>
<p>1xx. <a href="https://blog.csdn.net/qq_16949707/article/details/133875958">符尧：别卷大模型训练了，来卷数据吧！【干货十足】</a> 看最后的5个结论 </p>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247488088&idx=1&sn=f401a5a12e7b3727a15abbcff1a0ec51">合成数据(Synthetic data)微调大语言模型实战指南：背景、方案、案例、代码、评估 </a></p>
<ol start="50">
<li><a href="gptInstructTuning.md">Go to Page</a>   self</li>
</ol>
<h3><span id="数据增强">数据增强</span><a href="#数据增强" class="header-anchor">#</a></h3><p>1xx. <a href="https://zhuanlan.zhihu.com/p/420295576">哈工大｜15种NLP数据增强方法总结与对比</a></p>
<h3><span id="数据质量">数据质量</span><a href="#数据质量" class="header-anchor">#</a></h3><p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403976&idx=1&sn=694db5e2b3085b1610e8d19daa93a474">再看大模型预训数据质量如何评估：困惑度、错误L2范数和记忆化三种度量方法的效果对比分析研究</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataProcess</category>
      </categories>
      <tags>
        <tag>dataProcess</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP &amp; LLM</title>
    <url>/www6vHomeAIGC/2023/02/05/gptNLPTask/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="nlp-amp-llm">NLP &amp; LLM</span><a href="#nlp-amp-llm" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/NLP-LLM-10dbfe21108480ae8b5cc825540816b0?pvs=4">NLP &amp; LLM</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)涌现现象</title>
    <url>/www6vHomeAIGC/2023/02/03/gptEmergent/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="emergent-abilities">Emergent Abilities</span><a href="#emergent-abilities" class="header-anchor">#</a></h1><ul>
<li>🔗 文章：Emergent Abilities of Large Language Models  (2022.10)  (arxiv.org)</li>
<li>🔑关键词和摘要<ul>
<li>Keywords: LLMs, Emergent Ability, Scaling</li>
<li>abstract<ul>
<li>不可预测</li>
<li>不能从小模型的的性能外推</li>
<li>是否能通过继续扩大模型规模来获得更多涌现能力</li>
</ul>
</li>
</ul>
</li>
<li>⚙️研究设计和结论<ul>
<li>定义<ul>
<li>通常的涌现现象</li>
<li>大模型的涌现现象<ul>
<li>小模型接近随机</li>
<li><strong>大模型突然出现</strong></li>
<li>相变</li>
</ul>
</li>
<li>实验框架<ul>
<li>performance vs 1. FLOPs, model parameters</li>
<li><input checked disabled type="checkbox"> Training datasets</li>
<li>叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。</li>
</ul>
</li>
<li>实验1<ul>
<li>Few-shot Prompting</li>
<li>测试数据说明:<ul>
<li>A: 三位数加法，两位数乘法</li>
<li>B: [dɪfərənt], 复原 “different,” </li>
<li>C: 从 e l h l o 复原 hello</li>
<li>D: 波斯语问答</li>
<li>E: 针对GPT-3 对抗标的问答</li>
<li>…</li>
</ul>
</li>
<li>结果<ul>
<li>这些 task，以 few-shot 形式展示过以后，都有 emergent</li>
<li>不同模型 emergent scale 不一样</li>
<li>有的 task，只有 540B 的 PaLM  emerge了</li>
</ul>
</li>
</ul>
</li>
<li>实验2<ul>
<li>增强语言模型能力的 emerge 现象</li>
<li>已知的一些大模型技巧在何种规模下发挥作用？<ul>
<li>大模型技巧<ul>
<li>思维链 Chain-of-thought: Let’s think step by step.</li>
<li>指令微调 请写一段XXX的描述</li>
<li>草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6&#x3D;11，进位1”</li>
</ul>
</li>
</ul>
</li>
<li>这些增强语言模型能力的方法都有一定程度的涌现</li>
<li>联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？</li>
</ul>
</li>
</ul>
</li>
<li>讨论<ul>
<li><strong>Emergent 现象的解释</strong><ul>
<li><strong>多步能力说</strong><ul>
<li>每个子能力达到 90%  -&gt; 一无是处</li>
<li>每个子能力达到 95% -&gt; 能完成一些任务了</li>
</ul>
</li>
<li>指标缺陷说</li>
<li>奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降</li>
</ul>
</li>
<li><strong>Emergent 的阈值可能会越来越小</strong><ul>
<li>更干净的数据，更好的训练技巧，更优秀的模型结构都可以是  Emergent阈值变小</li>
</ul>
</li>
<li>未来方向：<ul>
<li>继续扩大 model scale，远未达到上限</li>
<li>一些新结构的 scaling</li>
<li>数据的 scaling</li>
<li>理解 prompt 机制</li>
<li>更前沿的 task，用来指导 emergent</li>
<li>理解 emergence</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>📚论文贡献<ul>
<li>优点<ul>
<li>第一次正式提出 emergent 实验</li>
<li><strong>做了充分的实验表明该现象在各种数据集上广泛存在</strong></li>
<li>甚至验证了一些“方法”的涌现</li>
<li>提出了一些解释该现象的观点，并提出质疑</li>
</ul>
</li>
<li>改进点<ul>
<li><strong>还是不知道为啥 emerge</strong></li>
<li>实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://www.bilibili.com/video/BV1qX4y1i78J/">清华博士带你思考大语言模型LLM的涌现现象（Emergent）</a>  有脑图<br> Emergent Abilities of Large Language Models （<a href="https://arxiv.org/abs/2206.07682%EF%BC%89">https://arxiv.org/abs/2206.07682）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399147&idx=1&sn=6e6d416db50d9708c900ee3b5416bba3">再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 </a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Emergent</category>
      </categories>
      <tags>
        <tag>Emergent</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理|实战)继续Pre-Training</title>
    <url>/www6vHomeAIGC/2023/02/03/gptContinualPretraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="继续-预训练-continual-pre-training-1">继续-预训练 continual pre-training [1]</span><a href="#继续-预训练-continual-pre-training-1" class="header-anchor">#</a></h1><ul>
<li><p>继续预训练的目的<br>为了得到<strong>适应不同行业&#x2F;任务领域</strong>的预训练模型，<strong>提升下游任务的效果</strong></p>
</li>
<li><p>什么时候需要继续预训练？<br><strong>预训练(pre-train)的语料与下游任务(finetune)语料的【数据分布&#x2F;领域差异】大时</strong></p>
</li>
</ul>
<h1><span id="千帆llama-2中文增强技术介绍-postpretrain2">千帆Llama 2中文增强技术介绍-Postpretrain[2]</span><a href="#千帆llama-2中文增强技术介绍-postpretrain2" class="header-anchor">#</a></h1><ul>
<li><p>中文词表构建 +Tokenizer<br>中文词表扩增 29k -&gt; 59k</p>
</li>
<li><p>Embedding<br>在原有Embedding矩阵后追加中文embedding映射</p>
</li>
<li><p>数据配比<br> 中文：英文约1:1</p>
</li>
<li><p>pipeline</p>
<ul>
<li>原始数据集</li>
<li><strong>异常清洗</strong></li>
<li><strong>数据过滤</strong></li>
<li><strong>去重</strong></li>
<li>隐私匿名化</li>
</ul>
</li>
</ul>
<blockquote>
<p>开源大模型预训练语料预处理流程总结： 基于基础规则处理为主 + 基于模型的质量过滤逐步成为趋势</p>
</blockquote>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/545092184">浅谈一下「继续预训练」</a></li>
<li>&lt;&lt;千帆增强版 Llama 2-提升大模型对话指令遵循能力&gt;&gt;<br>1xx. <a href="https://zhuanlan.zhihu.com/p/654463331">如何更好地继续预训练（Continue PreTraining）</a><br>warmup  +  学习率<br>1xx. <a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/120695507">Don’t stop pretraining，继续预训练！</a></li>
</ol>
<p>1xx. 《Investigating Continual Pretraining in Large Language Models: Insights and Implications》<br>    <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583&chksm=83839096b4f41980e8277f34650c2029a45e853adfc2b412ea386952751e44d29e75f0048d12&scene=178&cur_album_id=3343133676745932807#rd">值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)推理-lmdeploy</title>
    <url>/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#lmdeploy-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2-10">lmdeploy-推理部署 [10]</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">模型转换</a></li>
<li><a href="#turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">TurboMind 推理+命令行本地对话</a></li>
<li><a href="#turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TurboMind推理+API服务</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="lmdeploy-推理部署-10">lmdeploy-推理部署 [10]</span><a href="#lmdeploy-推理部署-10" class="header-anchor">#</a></h1><h3><span id="模型转换">模型转换</span><a href="#模型转换" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/convert.png" class>

<h3><span id="turbomind-推理命令行本地对话">TurboMind 推理+命令行本地对话</span><a href="#turbomind-推理命令行本地对话" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer.png" class>

<h3><span id="turbomind推理api服务">TurboMind推理+API服务</span><a href="#turbomind推理api服务" class="header-anchor">#</a></h3><ul>
<li>启动服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api.png" class></li>
<li>Client访问服务<img src="/www6vHomeAIGC/2023/02/02/gptInferFrameworkPractice/infer-api-client.png" class></li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="10">
<li><a href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md">lmdeploy 量化部署</a><br>  <a href="https://www.bilibili.com/video/BV1iW4y1A77P/">(5)LMDeploy 大模型量化部署实践</a> V</li>
</ol>
<p>1xx. <a href="https://github.com/www6v/llm-action/tree/main/inference">llm-action  inference</a> git</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)不可能三角</title>
    <url>/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%921">不可能三角[1]</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">不可能三角</a></li>
<li><a href="#%E5%BC%A5%E8%A1%A5%E6%96%B9%E6%B3%95">弥补方法</a></li>
</ul>
</li>
<li><a href="#%E5%85%B6%E4%BB%96-%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92">其他 不可能三角</a><ul>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F">分布式系统</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8">分布式存储</a></li>
</ul>
</li>
<li><a href="#%E8%8C%83%E5%BC%8F">范式</a><ul>
<li><a href="#pretrain-finetune-%E8%8C%83%E5%BC%8F3">pretrain, finetune 范式[3]</a></li>
<li><a href="#pretrain-prompt-predict-%E8%8C%83%E5%BC%8F3">pretrain, prompt, predict 范式[3]</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92-1">不可能三角</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="不可能三角1">不可能三角[1]</span><a href="#不可能三角1" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/25/gptImpossibleTriangle/impossibleTriangle.JPG" class>

<ul>
<li>预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果</li>
<li>而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的<strong>少样本效果依旧比不过中等模型的精调</strong></li>
</ul>
<h3><span id="弥补方法">弥补方法</span><a href="#弥补方法" class="header-anchor">#</a></h3><ul>
<li><strong>优化size</strong><ul>
<li>对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低</li>
</ul>
</li>
<li><strong>优化few-shot</strong><ul>
<li>对于提升少样本表现，<strong>数据增强</strong>是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限</li>
</ul>
</li>
<li><strong>fine-tuning</strong><ul>
<li>对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA</li>
</ul>
</li>
</ul>
<h1><span id="其他-不可能三角">其他 不可能三角</span><a href="#其他-不可能三角" class="header-anchor">#</a></h1><h3><span id="分布式系统">分布式系统</span><a href="#分布式系统" class="header-anchor">#</a></h3><ul>
<li>CAP理论<ul>
<li>C 一致性</li>
<li>A 可用性</li>
<li>P 分区</li>
</ul>
</li>
</ul>
<h3><span id="分布式存储">分布式存储</span><a href="#分布式存储" class="header-anchor">#</a></h3><ul>
<li>RUM猜想<ul>
<li>Read-overhead </li>
<li>Update-overhead </li>
<li>Memory-overhead</li>
</ul>
</li>
</ul>
<h1><span id="范式">范式</span><a href="#范式" class="header-anchor">#</a></h1><h3><span id="pretrain-finetune-范式3">pretrain, finetune 范式[3]</span><a href="#pretrain-finetune-范式3" class="header-anchor">#</a></h3><p>第三阶段范式</p>
<h3><span id="pretrain-prompt-predict-范式3">pretrain, prompt, predict 范式[3]</span><a href="#pretrain-prompt-predict-范式3" class="header-anchor">#</a></h3><p>第四阶段范式</p>
<h1><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h1><p>根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响</p>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="不可能三角">不可能三角</span><a href="#不可能三角" class="header-anchor">#</a></h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/501381510">预训练模型的下一步？突破Impossible Triangle</a></li>
<li><a href="https://arxiv.org/pdf/2204.06130.pdf">Impossible Triangle: What’s Next for Pre-trained Language Models?</a></li>
<li><a href="https://blog.csdn.net/zandaoguang/article/details/124395479">微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」</a></li>
<li><a href="gptPromptTuning.md">Go to Page</a>  self</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 论文</title>
    <url>/www6vHomeAIGC/2023/01/20/gptStudyPaper/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#paper">Paper</a></li>
<li><a href="#gpt%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%911">GPT研究方向[1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="paper">Paper</span><a href="#paper" class="header-anchor">#</a></h1><ul>
<li><p><a href="https://github.com/www6v/paper-reading">paper-reading</a> 李牧大神</p>
<ul>
<li>Transformer  *** <ul>
<li>GPT-4</li>
<li>Instruct GPT *** </li>
<li>GPT, GPT-2, GPT-3 精读  ***</li>
</ul>
</li>
<li>多模态<ul>
<li>CLIP</li>
<li>ViLT</li>
</ul>
</li>
<li>Chain of Thought  ***</li>
</ul>
</li>
<li><p><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a><br>基础篇 - 论文 *** </p>
</li>
<li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/129508065">LLM&#x2F;ChatGPT与多模态必读论文150篇(已更至第101篇)</a> </p>
</li>
<li><p><a href="https://github.com/zjunlp/LLMAgentPapers">LLMAgentPapers</a> 浙江大学</p>
</li>
<li><p><a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a> 浙江大学</p>
</li>
</ul>
<h1><span id="gpt研究方向1">GPT研究方向[1]</span><a href="#gpt研究方向1" class="header-anchor">#</a></h1><ul>
<li>Efficient (PEFT)</li>
<li>Existing stuff(pretrained model)  -应用<br>New directions</li>
<li>Plug-and-play<br> 通用模块组件，能用在各个领域， baseline</li>
<li>Dataset,  evaluation and survey</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV1oX4y1d7X6">大模型时代下做科研的四个思路【论文精读·52】</a></li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/673788545">AI Agent &amp; 大模型经典论文推荐</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)多模态</title>
    <url>/www6vHomeAIGC/2023/01/18/gptMultimodal/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%AE%BA%E6%96%87foundational-models-defining">论文[Foundational Models Defining]</a></li>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB-1">基础模型分类 [1]</a><ul>
<li><a href="#textually-prompted-models">textually prompted models</a></li>
<li><a href="#visually-prompted-models">visually prompted models</a></li>
<li><a href="#heterogeneous-models">heterogeneous models</a></li>
</ul>
</li>
<li><a href="#%E6%9E%B6%E6%9E%84-1">架构 [1]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87mm-llms">论文[MM-LLMs]</a></li>
<li><a href="#%E8%AE%BA%E6%96%87mllm">论文[MLLM]</a></li>
<li><a href="#arch-32">Arch [3.2]</a></li>
<li><a href="#%E7%B1%BB%E5%9E%8B31">类型[3.1]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#survey">survey</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="论文foundational-models-defining">论文[Foundational Models Defining]</span><a href="#论文foundational-models-defining" class="header-anchor">#</a></h1><ul>
<li>论文地址<br> 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》大学</li>
</ul>
<h1><span id="基础模型分类-1">基础模型分类 [1]</span><a href="#基础模型分类-1" class="header-anchor">#</a></h1><ul>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern.webp" class>
</li>
<li><p>分类</p>
<img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/pattern1.webp" class></li>
</ul>
<h3><span id="textually-prompted-models">textually prompted models</span><a href="#textually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>contrastive<br>CLIP  双塔</li>
<li>generative<br>Flamingo </li>
<li>hybrid<br>BLIP</li>
<li>conversational<br>GPT-4， miniGPT4, LLaVa</li>
</ul>
<p>传统上，视觉语言模型主要用于需要同时理解视觉和文本模态的任务。然而，随着CLIP展示出的卓越性能，基于<strong>语言监督的模型</strong>在显著上升，并成为主流方法。在本节中，我们专注于探索依赖<strong>语言作为主要监督来源</strong>的方法。这些以文本为提示的模型可以广泛分为三种主要类型：对比、生成和混合方法。</p>
<h3><span id="visually-prompted-models">visually prompted models</span><a href="#visually-prompted-models" class="header-anchor">#</a></h3><ul>
<li>Foundational<br>SAM</li>
</ul>
<h3><span id="heterogeneous-models">heterogeneous  models</span><a href="#heterogeneous-models" class="header-anchor">#</a></h3><h1><span id="架构-1">架构 [1]</span><a href="#架构-1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch.webp" class>

<hr>
<h1><span id="论文mm-llms">论文[MM-LLMs]</span><a href="#论文mm-llms" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> 《MM-LLMs: Recent Advances in MultiModal Large Language Models》  腾讯</p>
</li>
<li><p>开源地址<br><a href="https://mm-llms.github.io/archives/">mm-llms</a> 腾讯</p>
</li>
<li><p>解析<br><a href="https://candied-skunk-1ca.notion.site/MM-LLMs-Recent-Advances-in-MultiModal-Large-Language-Models-7ee5033df80e4b5394153c6a77cc21a3?pvs=4">解析</a></p>
</li>
</ul>
<hr>
<h1><span id="论文mllm">论文[MLLM]</span><a href="#论文mllm" class="header-anchor">#</a></h1><ul>
<li><p>论文地址<br> <a href="https://arxiv.org/pdf/2306.13549v1">A Survey on Multimodal Large Language Models</a><br> <a href="https://arxiv.org/abs/2306.13549">A Survey on Multimodal Large Language Models</a> 中国科学技术大学   腾讯</p>
</li>
<li><p>开源地址<br><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Repo</a></p>
</li>
</ul>
<h1><span id="arch-32">Arch [3.2]</span><a href="#arch-32" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2023/01/18/gptMultimodal/arch2.png" class>

<h1><span id="类型31">类型[3.1]</span><a href="#类型31" class="header-anchor">#</a></h1><ul>
<li>本文将最近具有代表性的MLLM分为4种主要类型：<ul>
<li><strong>多模态指令调整（MIT）</strong></li>
<li>多模态上下文学习（M-ICL）</li>
<li>多模态思想链（M-CoT）</li>
<li><strong>LLM辅助视觉推理（LAVR）</strong>【类似agent】</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="survey">survey</span><a href="#survey" class="header-anchor">#</a></h3><ol>
<li><p>《Foundational Models Defining a New Era in Vision: A Survey and Outlook》<br> <a href="https://blog.csdn.net/qq_45368632/article/details/132180645">视觉大模型的全面解析</a><br> <a href="https://zhuanlan.zhihu.com/p/655135848">基础模型定义视觉的新时代：综述和展望</a><br> <a href="https://zhuanlan.zhihu.com/p/648578542">万字长文带你全面解读视觉大模型</a></p>
</li>
<li><p>xxx</p>
</li>
<li><p>《A Survey on Multimodal Large Language Models》  v1 v2版本<br>3.1 <a href="https://cloud.tencent.com/developer/article/2322835">MLLM首篇综述 | 一文全览多模态大模型的前世、今生和未来</a>  v1版本<br>3.2 <a href="https://mp.weixin.qq.com/s/V5aiWUYh14q00jAn2O6VKA">多模态大语言模型全面综述：架构，训练，数据，评估，扩展，应用，挑战，机遇</a>  v2版本</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>(实战)Pre-Training</title>
    <url>/www6vHomeAIGC/2023/01/15/gptLargeModelTrainingPractice/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="pre-training-实战">Pre-Training 实战</span><a href="#pre-training-实战" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Pre-Training-4284cd3148a0430185a9ef14041afc56?pvs=4">(实战)Pre-Training</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Pre-Training</category>
      </categories>
      <tags>
        <tag>Pre-Training</tag>
      </tags>
  </entry>
  <entry>
    <title>(list)数据集</title>
    <url>/www6vHomeAIGC/2023/01/08/gptDataSet/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="dataset">DataSet</span><a href="#dataset" class="header-anchor">#</a></h1><ul>
<li><p>综合[平台] </p>
<ul>
<li><a href="http://opendatalab.com/">OpenDataLab</a> [1]<br>上海人工智能实验室<br><strong>数据描述语言  DSDL</strong> + 平台标准数据集</li>
<li><a href="https://www.luge.ai/#/">千言数据集</a><br>百度</li>
</ul>
</li>
<li><p>评测数据集</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648405040&idx=1&sn=ad45944e78b5742337158cff80dbd9b3">再看领域微调大模型的主流基座和评测数据集：项目地址及论文指引</a></li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="https://www.bilibili.com/video/BV18m4y1h7zW/">大模型时代的数据变革 - 如何设计大模型的数据配方、智能数据采集、标注、ETL</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>dataset</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatGLM</title>
    <url>/www6vHomeAIGC/2023/01/06/gptChatGLM/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<p><a href="https://www.bilibili.com/video/BV1ju411T74Y/">第十一课：ChatGLM</a> V<br><a href="https://blog.csdn.net/v_JULY_v/article/details/129880836">ChatGLM两代的部署&#x2F;微调&#x2F;实现：从基座GLM、ChatGLM的LoRA&#x2F;P-Tuning微调、6B源码解读到ChatGLM2的微调与实现</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/625468667">【Instruction Tuning】ChatGLM 微调实战（附源码）</a></p>
<p><a href="https://github.com/www6v/transformers_tasks/blob/main/LLM/chatglm_finetune/readme.md">Finetune ChatGLM-6B</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401516&idx=1&sn=80b3cfecc9f4338b87fcd9bc91ef2465">也看支持32K上下文的ChatGLM2-6B模型：优化点简读及现有开源模型主流训练优化点概述 </a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>ChatGLM</category>
      </categories>
      <tags>
        <tag>ChatGLM</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)分布式训练</title>
    <url>/www6vHomeAIGC/2023/01/06/gptTrainParallelism/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="分布式训练">分布式训练</span><a href="#分布式训练" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/b23e09b21dee4e7595122d2c5f3943ae?pvs=4">(原理)分布式训练</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型 排行榜</title>
    <url>/www6vHomeAIGC/2023/01/04/gptLeaderBoard/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B">大模型</a><ul>
<li><a href="#%E6%8E%92%E8%A1%8C%E6%A6%9C">排行榜</a></li>
<li><a href="#%E4%B8%AD%E5%9B%BD%E6%8E%92%E8%A1%8C%E6%A6%9C">中国排行榜</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="大模型">大模型</span><a href="#大模型" class="header-anchor">#</a></h1><h3><span id="排行榜">排行榜</span><a href="#排行榜" class="header-anchor">#</a></h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">HuggingFaceH 大模型排行榜</a></p>
<p><a href="https://www.promptingguide.ai/models/collection">LLM Collection</a></p>
<h3><span id="中国排行榜">中国排行榜</span><a href="#中国排行榜" class="header-anchor">#</a></h3><p><a href="https://github.com/www6v/awesome-LLMs-In-China">中国大模型 </a></p>
<ul>
<li>通用 39</li>
<li>金融 25</li>
<li>司法 8</li>
<li>法律 6</li>
<li>医学 13</li>
<li>医疗 24</li>
<li>教育 13</li>
<li>科研 17</li>
<li>工业 23</li>
<li>政务 12</li>
<li>运维 7</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>leaderBoard</category>
      </categories>
      <tags>
        <tag>leaderBoard</tag>
      </tags>
  </entry>
  <entry>
    <title>垂类大模型</title>
    <url>/www6vHomeAIGC/2023/01/04/gptDomain/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h1><span id="垂类大模型">垂类大模型</span><a href="#垂类大模型" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/05ad83c78a264b4f90310923580070a6?pvs=4">垂类大模型</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(总结)推理优化</title>
    <url>/www6vHomeAIGC/2023/01/01/gptInference/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E6%8E%A8%E7%90%86-%E4%BC%98%E5%8C%96">推理 优化</a><ul>
<li><a href="#overview2">overview[2]</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-1">模型压缩 [1]</a></li>
<li><a href="#kv-cache">KV Cache</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a><ul>
<li><a href="#%E7%BB%BC%E8%BF%B0">综述</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="推理-优化">推理 优化</span><a href="#推理-优化" class="header-anchor">#</a></h1><h3><span id="overview2">overview[2]</span><a href="#overview2" class="header-anchor">#</a></h3><p>有几种方法可以在内存中<strong>降低推理成本</strong>或&#x2F;和<strong>加快推理速度</strong>。</p>
<ul>
<li>应用各种<strong>并行处理方式</strong>，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。</li>
<li><strong>内存卸载</strong>，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。</li>
<li><strong>智能批处理策略</strong>；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。</li>
<li><strong>网络压缩技术</strong>，如<strong>修剪、量化、蒸馏</strong>。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。</li>
<li>针对目标模型架构的特定改进。许多<strong>架构变化</strong>，特别是针对注意力层的变化，有助于提高Transformer解码速度。</li>
</ul>
<h3><span id="模型压缩-1">模型压缩 [1]</span><a href="#模型压缩-1" class="header-anchor">#</a></h3><img src="/www6vHomeAIGC/2023/01/01/gptInference/compress.png" class>

<ul>
<li>剪枝（Pruning）</li>
<li>知识蒸馏（Knowledge Distillation，KD）</li>
<li>量化（Quantization）</li>
<li>低秩分解（Low-Rank Factorization）</li>
</ul>
<h3><span id="kv-cache">KV Cache</span><a href="#kv-cache" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><h3><span id="综述">综述</span><a href="#综述" class="header-anchor">#</a></h3><ol>
<li><a href="https://mp.weixin.qq.com/s/glPPSqHjsnDjC0DZSuuPzA">一文探秘LLM应用开发(13)-模型部署与推理(优化理论) </a> </li>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization </a>  lilianweng</li>
</ol>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a> ***<br>1xx. <a href="https://zhuanlan.zhihu.com/p/656485997">大语言模型推理性能优化综述</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Inference</category>
      </categories>
      <tags>
        <tag>Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA</title>
    <url>/www6vHomeAIGC/2023/01/01/gptLlama/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="llama">LLaMA</span><a href="#llama" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/LLaMA-2b55a2a573b549df9f6fc4cc26c2292f?pvs=4">LLaMA</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLaMA</category>
      </categories>
      <tags>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>LLMOps</title>
    <url>/www6vHomeAIGC/2022/12/28/gptLLMOps/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<ol>
<li><a href="https://drive.google.com/file/d/1LZXTrRdrloIqAJT6xaNTl4WQd6y95o7K/view">LLMOps: Deployment and Learning in Production</a><br><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/">LLMOps: Deployment and Learning in Production</a><br><a href="https://zhuanlan.zhihu.com/p/629589593">[必读] LLM 应用开发全栈指南</a> LLMOps</li>
<li><a href="https://zhuanlan.zhihu.com/p/632026876">了解一下新领域 LLMOps: 大模型运维</a><br><a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations">Understanding LLMOps: Large Language Model Operations</a></li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>LLMOps</category>
      </categories>
      <tags>
        <tag>LLMOps</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 系列</title>
    <url>/www6vHomeAIGC/2022/12/11/gptFamily/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E8%BF%9B%E5%8C%96%E6%97%B6%E9%97%B4%E7%BA%BF">进化时间线</a></li>
<li><a href="#gpt1-1">GPT1 [1]</a></li>
<li><a href="#gpt2-1">GPT2 [1]</a><ul>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">核心思想</a></li>
<li><a href="#gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</a></li>
</ul>
</li>
<li><a href="#gpt3-1">GPT3 [1]</a><ul>
<li><a href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">下游任务评估方法</a></li>
<li><a href="#few-shot-vs-fine-tuning">Few-shot vs fine-tuning</a></li>
<li><a href="#gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</a></li>
</ul>
</li>
<li><a href="#instructgpt-1">InstructGPT [1]</a><ul>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88">技术方案</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="#chatgpt-%E8%AE%AD%E7%BB%83-3">ChatGPT 训练  [3]</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="进化时间线">进化时间线</span><a href="#进化时间线" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/12/11/gptFamily/family.jpg" class>

<h1><span id="gpt1-1">GPT1 [1]</span><a href="#gpt1-1" class="header-anchor">#</a></h1><ol>
<li>它是最早一批提出在 NLP 任务上使用 <strong>pre-train + fine-tuning 范式</strong>的工作。</li>
<li>GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</li>
<li><strong>预训练模型具有 zero-shot 的能力</strong>，并且能随着预训练的进行不断增强</li>
</ol>
<h1><span id="gpt2-1">GPT2 [1]</span><a href="#gpt2-1" class="header-anchor">#</a></h1><h3><span id="核心思想">核心思想</span><a href="#核心思想" class="header-anchor">#</a></h3><p>当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，<strong>不需要在下游任务微调</strong>。</p>
<h3><span id="gpt-2-vs-gpt-1">GPT-2 vs. GPT-1</span><a href="#gpt-2-vs-gpt-1" class="header-anchor">#</a></h3><ol>
<li><strong>主推 zero-shot</strong>，而 GPT-1 为 pre-train + fine-tuning；</li>
<li>训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；</li>
<li>模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；</li>
<li>模型结构调整，层归一化和参数初始化方式；</li>
<li>训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；</li>
</ol>
<h1><span id="gpt3-1">GPT3 [1]</span><a href="#gpt3-1" class="header-anchor">#</a></h1><h3><span id="下游任务评估方法">下游任务评估方法</span><a href="#下游任务评估方法" class="header-anchor">#</a></h3><p>GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：<br><strong>Zero-shot</strong>：仅使用当前任务的自然语言描述，不进行任何梯度更新；<br><strong>One-shot</strong>：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；<br><strong>Few-shot</strong>：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；</p>
<ul>
<li>Shot[2]<ul>
<li>One-shot</li>
<li>Few-Shot</li>
<li>Zero-Shot</li>
</ul>
</li>
</ul>
<h3><span id="few-shot-vs-fine-tuning">Few-shot vs fine-tuning</span><a href="#few-shot-vs-fine-tuning" class="header-anchor">#</a></h3><p>其中 <strong>Few-shot</strong> 也被称为 <strong>in-context learning</strong>，虽然它与 fine-tuning 一样都需要一些<strong>有监督标注数据</strong>，但是两者的区别是：<br>【本质区别】<br><strong>fine-tuning</strong> 基于标注数据<strong>对模型参数进行更新</strong><br>而<strong>in-context learning</strong>使用标注数据时不做任何的梯度回传, <strong>模型参数不更新</strong></p>
<h3><span id="gpt-3-vs-gpt-2">GPT-3 vs. GPT-2</span><a href="#gpt-3-vs-gpt-2" class="header-anchor">#</a></h3><ol>
<li>效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章；</li>
<li><strong>主推 few-shot</strong>，相比于 GPT-2 的 zero-shot，具有很强的创新性；</li>
<li>模型结构略微变化，采用 <strong>sparse attention</strong> 模块；</li>
<li>海量训练语料 <strong>45TB</strong>（清洗后 570GB），相比于 GPT-2 的 40GB；</li>
<li>海量模型参数，最大模型为 <strong>1750 亿</strong>，GPT-2 最大为 15 亿参数；</li>
</ol>
<h1><span id="instructgpt-1">InstructGPT [1]</span><a href="#instructgpt-1" class="header-anchor">#</a></h1><h3><span id="步骤">步骤</span><a href="#步骤" class="header-anchor">#</a></h3><ul>
<li>有监督微调，</li>
<li>奖励模型训练，</li>
<li>强化学习训练</li>
</ul>
<h3><span id="技术方案">技术方案</span><a href="#技术方案" class="header-anchor">#</a></h3><ul>
<li><p>有监督微调（SFT）<br>本质上来说，<strong>SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3</strong>。但是值得一提的是，这里<strong>标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别</strong>。<br>InstructGPT 在 SFT 中标注的数据，正是为了<strong>消除这种模型预测与用户表达习惯之间的 gap</strong>。在标注过程中，他们<strong>从 GPT-3 的用户真实请求中采样</strong>大量下游任务的描述，然后让<strong>标注人员对任务描述进行续写</strong>，从而得到该问题的高质量回答。</p>
</li>
<li><p>基于人类反馈的强化学习（RLHF）</p>
<img src="/www6vHomeAIGC/2022/12/11/gptFamily/instructGPT.jpg" class></li>
</ul>
<h3><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h3><ol>
<li>解决 GPT-3 的<strong>输出与人类意图</strong>之间的<strong>Align问题</strong>；</li>
<li>让具备丰富世界知识的大模型，<strong>学习“人类偏好”</strong>；</li>
<li>标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠；</li>
<li>InstructGPT 在<strong>真实性</strong>，<strong>丰富度</strong>上表现更好；</li>
<li>InstructGPT 对有害结果的生成控制的更好，但是对于<strong>“偏见”没有明显改善</strong>；</li>
</ol>
<h1><span id="chatgpt-训练-3">ChatGPT 训练  [3]</span><a href="#chatgpt-训练-3" class="header-anchor">#</a></h1><ul>
<li>基于人类反馈的强化学习微调技术 RLHF<ul>
<li>使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型<ul>
<li>Supervised fine-tuning (SFT)<br>&#x3D; Instruction Tuning</li>
</ul>
</li>
<li>训练奖励模型 Reward Model（RM）</li>
<li>使用强化学习算法微调语言模型<ul>
<li>RLHF<br>[本质  基于强化学习, 强化学习算法]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/609716668">GPT &#x2F; GPT-2 &#x2F; GPT-3 &#x2F; InstructGPT 进化之路</a> ***</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/624793654">Few-Shot, Zero-Shot &amp; One-shot 的通俗理解</a></p>
</li>
<li><p><a href="https://shimo.im/docs/KlkKv4XQDouwWRqd/read">AI 大模型微调训练营大纲</a></p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s/VYv8BRgGnp9ZTuXxaSuFwg">万字拆解！追溯ChatGPT各项能力的起源 </a> 符尧</p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/642282717">[Transformer 101系列] ChatGPT是怎么炼成的?</a> 未</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Transformer</title>
    <url>/www6vHomeAIGC/2022/11/30/gptTransformer/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>


<h1><span id="原理transformer">(原理)Transformer</span><a href="#原理transformer" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Transformer-b1b9836f9c244db3acda7869f64ff860?pvs=4">(原理)Transformer</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>金融大模型</title>
    <url>/www6vHomeAIGC/2022/11/24/gptDomainFinance/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%8A%80%E6%9C%AF1">金融大模型 技术[1]</a></li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B">金融大模型</a><ul>
<li><a href="#fingpt-%E5%93%A5%E5%A4%A7-2">FinGPT 哥大 [2]</a></li>
<li><a href="#disc-finllm-4">DISC-FinLLM [4]</a></li>
<li><a href="#%E8%BD%A9%E8%BE%95%E9%87%91%E8%9E%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B-3">轩辕金融大模型 [3]</a></li>
<li><a href="#bloomberggpt">BloombergGPT</a></li>
<li><a href="#finbert">FinBERT</a></li>
</ul>
</li>
<li><a href="#%E9%87%91%E8%9E%8D%E5%9C%BA%E6%99%AF">金融场景</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="金融大模型-技术1">金融大模型 技术[1]</span><a href="#金融大模型-技术1" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/24/gptDomainFinance/finance.png" class>
<p>A,B  先忽略<br>C - BloombergGPT<br>D - FinGPT</p>
<h1><span id="金融大模型">金融大模型</span><a href="#金融大模型" class="header-anchor">#</a></h1><h3><span id="fingpt-哥大-2">FinGPT   哥大 [2]</span><a href="#fingpt-哥大-2" class="header-anchor">#</a></h3><ul>
<li><p>Resource</p>
<ul>
<li><a href="https://github.com/www6v/FinGPT">Github Repo</a></li>
<li><a href="https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster">FinGPT-Forecaster</a></li>
<li><a href="https://huggingface.co/FinGPT">huggingface</a></li>
<li>五篇paper</li>
</ul>
</li>
<li><p><a href="https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99">Training</a></p>
</li>
</ul>
<h3><span id="disc-finllm-4">DISC-FinLLM [4]</span><a href="#disc-finllm-4" class="header-anchor">#</a></h3><h3><span id="轩辕金融大模型-3">轩辕金融大模型 [3]</span><a href="#轩辕金融大模型-3" class="header-anchor">#</a></h3><ul>
<li>Resource<ul>
<li><a href="https://github.com/Duxiaoman-DI/XuanYuan">XuanYuan</a> git </li>
<li><a href="https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus">数据集</a></li>
</ul>
</li>
</ul>
<p>【大模型(基于BLOOM-176B)转向小模型（XuanYuan-13B-Chat）】</p>
<h3><span id="bloomberggpt">BloombergGPT</span><a href="#bloomberggpt" class="header-anchor">#</a></h3><p>未开源</p>
<h3><span id="finbert">FinBERT</span><a href="#finbert" class="header-anchor">#</a></h3><h1><span id="金融场景">金融场景</span><a href="#金融场景" class="header-anchor">#</a></h1><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li>《A Survey of Large Language Models in Finance (FinLLMs)》<br> <a href="https://github.com/adlnlp/FinLLMs">FinLLMs</a></li>
<li><a href="https://www.bilibili.com/video/BV1R64y1j76H/">FinGPT开源金融垂类大模型</a> V</li>
<li>&lt;&lt;06【脱敏版】金融行业实战：度小满轩辕金融大模型应用探索与开发实践&gt;&gt;<br> <a href="https://www.bilibili.com/video/BV1G64y1j7Zj/">金融行业实战：度小满轩辕金融大模型应用探索与开发实践</a> V</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648404800&idx=2&sn=9c1ad9d8aa8b0725dd6289bc15e177c9">本周大模型代表进展解析:ChatGLM3的特性认识及LoRA专家模组形式的金融领域微调模型实现策略</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400799&idx=1&sn=fb3778d1914849d3b41b047b33ce32a9">ChatGPT能否预测股价走势？大模型应用于金融预测与今日大模型前沿速递</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>金融大模型</category>
      </categories>
      <tags>
        <tag>金融大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>(原理)Training</title>
    <url>/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#training-pipeline0">Training Pipeline[0]</a><ul>
<li><a href="#%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0-2">设置训练参数 [2]</a></li>
<li><a href="#%E5%8F%82%E6%95%B0%E9%87%8F-vs-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%87%8F-2">参数量 vs 训练数据量 [2]</a></li>
</ul>
</li>
<li><a href="#pre-training">Pre-training</a><ul>
<li><a href="#pre-training-4">Pre-training [4]</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="training-pipeline0">Training Pipeline[0]</span><a href="#training-pipeline0" class="header-anchor">#</a></h1><img src="/www6vHomeAIGC/2022/11/19/gptLargeModelTraining/bigModelTrainingPipeline.jpg" class>

<p><strong>模型训练分为四个阶段</strong> [2]</p>
<ul>
<li>预训练（Pretraining） –&gt;Base model  <ul>
<li>预训练技术<br>预训练本质上是⼀个⽆监督学习过程</li>
</ul>
</li>
<li>监督微调（Supervised Finetuning） –&gt; SFT model<br>核⼼原因还是在于需要“赋予”⼤模型更加定制化的功能</li>
<li>奖励建模（Reward Modeling）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<p><strong>三个角度解析</strong> [2]</p>
<ul>
<li>数据量：<strong>预训练</strong>阶段所需的<strong>数据量很大</strong>，但<strong>质量要求不高</strong>；而<strong>后面的三个阶段</strong>恰恰相反，需要的<strong>数据质量较高</strong>。</li>
<li>训练方法：<strong>预训练和监督微调</strong>的训练方法相同，都是<strong>预测下一个单词</strong>。奖励模型和强化学习的训练方法则不同。<strong>奖励模型</strong>是<strong>二元分类学习</strong>，而<strong>强化学习</strong>则鼓励模型生成奖励模型评分较高的回答。</li>
<li>训练所需资源：预训练阶段的资源消耗巨大，使用数千颗GPU，花费<strong>数月</strong>时间，占总训练时间的99%。后面的三个阶段只需使用数十颗GPU，训练时间约<strong>数天</strong>。</li>
</ul>
<h3><span id="设置训练参数-2">设置训练参数 [2]</span><a href="#设置训练参数-2" class="header-anchor">#</a></h3><p>设置训练参数，如batch-size、learning rate等</p>
<ul>
<li>预训练阶段的<strong>Batch Size非常大</strong>，范围在0.5M到4M之间。</li>
<li><strong>Learning rate设定较小</strong>，且随着网络规模的增大，Learning rate越来越小。</li>
</ul>
<h3><span id="参数量-vs-训练数据量-2">参数量 vs 训练数据量 [2]</span><a href="#参数量-vs-训练数据量-2" class="header-anchor">#</a></h3><p><strong>参数量并不是衡量模型能力的唯一标准，训练数据量也是一个非常重要的因素。</strong><br>LLaMA模型，尽管它的参数量只有650亿，但其性能与参数量为1750亿的GPT-3模型相比也非常优秀。主要原因在于，LLaMA模型的训练数据量达到了1.4万亿，而GPT-3只有3000亿。</p>
<h1><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor">#</a></h1><h3><span id="pre-training-4">Pre-training [4]</span><a href="#pre-training-4" class="header-anchor">#</a></h3><ul>
<li><p>⾃回归与⽣成式</p>
<ul>
<li><strong>⾃回归模型</strong>是⼀种序列模型，它在预测下⼀个输出时，会将之前的所有输出作为输⼊，然后<strong>根据统计规律、结合已经输⼊的样本</strong>，预测下个位置各单词出现的概率，然后输出概率最⼤的单词，类似于完形填空；</li>
<li><strong>⽣成式模型</strong>的预测过程和⾃回归模型类似，都是根据统<br>计规律预测下个单词的概率，所不同的是，<strong>⽣成式模型可以根据之前的样本的<br>概率分布⽣成下⼀个词，⽣成式模型预测时会存在⼀定的随机性；</strong></li>
</ul>
</li>
<li><p>GPT来说，就是⼀个⾃回归⽣成式模型 [4]<br>⼀个⾃回归⽣成式模型在进⾏预测的时候，<strong>会⾸先根据⾃回归模型，在参考到⽬前为⽌<br>已经⽣成的词的情况下确定下⼀个词的概率分布，然后再根据⽣成式的⽅式来根据这个<br>分布⽣成下⼀个词</strong></p>
</li>
</ul>
<h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol start="0">
<li><p><a href="https://zhuanlan.zhihu.com/p/648050614">LLM学习系列1：大模型架构要点总结</a>  from ppt</p>
</li>
<li><p>xxx</p>
</li>
<li><p><a href="https://techdiylife.github.io/big-model-training/deepspeed/LLM-state-of-GPT.html">大模型训练入门实战</a>  ***<br><a href="https://karpathy.ai/stateofgpt.pdf">State of GPT</a><br><a href="https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg">State of GPT：大神Andrej揭秘OpenAI大模型原理和训练过程 </a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/2328541">主流大语言模型的技术原理细节</a> *** 腾讯     架构 + 训练 + 微调</p>
</li>
<li><p>大模型入门必看教程  九天Hector</p>
</li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648399532&idx=1&sn=31b7bc5a4f3114d8215da0edc2559e47">语言模型预训练基础知识总结：标准数据流pipleline、tokenizer的认识以及常见编码模型范式 </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/651316650">从头预训练大模型实践经验</a>  ***<br><a href="https://wandb.ai/site/wp-content/uploads/2023/09/Current-Best-Practices-for-Training-LLMs-from-Scratch-Final.pdf">Current Best Practices for Training LLMs from Scratch</a>  原文</p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>train</category>
      </categories>
      <tags>
        <tag>train</tag>
      </tags>
  </entry>
  <entry>
    <title>(综述)大模型</title>
    <url>/www6vHomeAIGC/2022/10/30/gptLargeModelSurvey/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#llms%E7%9A%84%E8%83%8C%E6%99%AF1">LLMs的背景[1]</a><ul>
<li><a href="#scaling-law-of-llms">Scaling law of LLMs</a></li>
<li><a href="#llms%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B">LLMs的涌现能力</a></li>
<li><a href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF">大语言模型的关键技术 ***</a></li>
</ul>
</li>
<li><a href="#pre-training1">Pre-training[1]</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86">数据收集</a></li>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">模型训练 ***</a></li>
</ul>
</li>
<li><a href="#adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</a><ul>
<li><a href="#%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98">指令调优 ***</a><ul>
<li><a href="#%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%9E%84%E5%BB%BA">格式化实例的构建</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%AD%96%E7%95%A5">指令微调策略</a></li>
<li><a href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%9A%84%E6%95%88%E6%9E%9C">指令微调的效果</a></li>
</ul>
</li>
<li><a href="#%E5%AF%B9%E9%BD%90%E8%B0%83%E4%BC%98">对齐调优</a></li>
<li><a href="#%E9%AB%98%E6%95%88%E8%B0%83%E4%BC%98">高效调优</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>


<h1><span id="llms的背景1">LLMs的背景[1]</span><a href="#llms的背景1" class="header-anchor">#</a></h1><h3><span id="scaling-law-of-llms">Scaling law of LLMs</span><a href="#scaling-law-of-llms" class="header-anchor">#</a></h3><ul>
<li>KM scaling law</li>
<li>Chinchilla Scaling law</li>
</ul>
<h3><span id="llms的涌现能力">LLMs的涌现能力</span><a href="#llms的涌现能力" class="header-anchor">#</a></h3><ul>
<li>in-context learning</li>
<li>instruction following</li>
<li>step-by-step reasoning</li>
</ul>
<h3><span id="大语言模型的关键技术">大语言模型的关键技术 ***</span><a href="#大语言模型的关键技术" class="header-anchor">#</a></h3><ul>
<li>Scaling</li>
<li>Training</li>
<li>Ability Eliciting</li>
<li>Alignment Tuning</li>
<li>Tool Manipulation</li>
</ul>
<h1><span id="pre-training1">Pre-training[1]</span><a href="#pre-training1" class="header-anchor">#</a></h1><h3><span id="数据收集">数据收集</span><a href="#数据收集" class="header-anchor">#</a></h3><h3><span id="架构">架构</span><a href="#架构" class="header-anchor">#</a></h3><h3><span id="模型训练">模型训练 ***</span><a href="#模型训练" class="header-anchor">#</a></h3><ul>
<li><p>优化设置</p>
<ul>
<li>Batch Training</li>
<li>Learning Rate</li>
<li>Optimizer</li>
<li>Stabilizing the Training</li>
</ul>
</li>
<li><p>可扩展的训练技巧</p>
<ul>
<li>3D并行<br>数据并行 +  流水线并行 + 张量并行</li>
<li>ZeRO</li>
<li>混合精度训练</li>
<li>总体训练建议</li>
</ul>
</li>
</ul>
<h1><span id="adaptation-tuning-of-llms1">Adaptation Tuning of LLMs[1]</span><a href="#adaptation-tuning-of-llms1" class="header-anchor">#</a></h1><h3><span id="指令调优">指令调优 ***</span><a href="#指令调优" class="header-anchor">#</a></h3><p>本质上，指令微调是在<strong>自然语言格式的实例（instance）集合上</strong>微调预训练后的 LLM 的方法 [62]。</p>
<p>指令微调后，LLM 可以展现出<strong>泛化到未见过任务</strong>的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。</p>
<h5><span id="格式化实例的构建">格式化实例的构建</span><a href="#格式化实例的构建" class="header-anchor">#</a></h5><ul>
<li>格式化已有数据集</li>
<li>格式化人类需求</li>
<li>构建实例的关键因素<ul>
<li><strong>增加指令</strong></li>
<li><strong>设计格式</strong></li>
</ul>
</li>
</ul>
<p>总的来说，指令<strong>多样性似乎比实例数量更重要</strong></p>
<h5><span id="指令微调策略">指令微调策略</span><a href="#指令微调策略" class="header-anchor">#</a></h5><ul>
<li><p><strong>平衡数据分布</strong><br>一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。<br>此外，根据最近的研究发现 [64, 99]，<strong>提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例</strong>通常可以带来<strong>性能提升</strong>。</p>
</li>
<li><p>结合指令微调和预训练<br>为了使微调过程更加有效和稳定，OPT-IML [99] 在<strong>指令微调期间加入了预训练数据</strong>，这可以看作是对模型的正则化（regularization）。</p>
</li>
</ul>
<p>具体而言，GLM-130B [97] 和 Galactica [34] 将<strong>指令格式数据集作为预训练语料库的一小部分来预训练 LLM</strong>，这有可能同时获得预训练和指令微调的优势。</p>
<h5><span id="指令微调的效果">指令微调的效果</span><a href="#指令微调的效果" class="header-anchor">#</a></h5><ul>
<li>性能改进<br>最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，**表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]**。 【普适性】</li>
</ul>
<p>此外，**经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]**。</p>
<ul>
<li>任务泛化性<br>todo</li>
</ul>
<h3><span id="对齐调优">对齐调优</span><a href="#对齐调优" class="header-anchor">#</a></h3><h3><span id="高效调优">高效调优</span><a href="#高效调优" class="header-anchor">#</a></h3><h1><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h1><ol>
<li><a href="http://aibox.ruc.edu.cn/docs/2023-08/cb9badcb213f4c8b89d00d579eed4a4c.pdf">大语言模型综述</a> 中文  v10<br>  <a href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf">大语言模型综述</a> 中文<br>  <a href="https://github.com/www6v/LLMSurvey">LLMSurvey Repo</a>  git<br>  <a href="https://zhuanlan.zhihu.com/p/630203554">[论文]大语言模型综述</a><br>  <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400817&idx=1&sn=c1ed1c9c87bf2526e02d21d84429c5cf">详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结</a><br>  <a href="https://zhuanlan.zhihu.com/p/662673023">大模型综述-A Survey of Large Language Models</a></li>
</ol>
<p>1xx. <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648408221&idx=1&sn=2874583ed668ae0b89889c81a4ab8d79">值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya </a></p>
<p>1xx. <a href="https://zhuanlan.zhihu.com/p/381282229">43页预训练模型综述（清华、复旦、人大）</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT  学习资源</title>
    <url>/www6vHomeAIGC/2022/08/01/gptStudy/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#%E5%B7%A5%E7%A8%8B">工程</a></li>
<li><a href="#%E8%AF%BE%E7%A8%8B">课程</a><ul>
<li><a href="#%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4">极客时间</a></li>
<li><a href="#%E7%9F%A5%E4%B9%8E">知乎</a></li>
<li><a href="#%E6%B8%85%E5%8D%8E">清华</a></li>
<li><a href="#%E7%99%BE%E5%BA%A6">百度</a></li>
<li><a href="#%E4%B9%9D%E5%A4%A9">九天</a></li>
</ul>
</li>
<li><a href="#%E5%B7%A5%E4%B8%9A%E7%95%8C">工业界</a></li>
</ul>
<!-- tocstop -->

</div>

<h1><span id="工程">工程</span><a href="#工程" class="header-anchor">#</a></h1><ul>
<li><a href="https://github.com/www6v/openai-cookbook">openai-cookbook</a><br><a href="https://cookbook.openai.com/">cookbook.openai</a></li>
</ul>
<h1><span id="课程">课程</span><a href="#课程" class="header-anchor">#</a></h1><h3><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h3><ul>
<li><a href="https://shimo.im/docs/47kgM6NewnSO613V">尚硅谷×极客时间《AI 大模型实战训练营》大纲</a> </li>
<li><a href="https://shimo.im/docs/XKq42v7061SxZ2AN/read">AI 大模型应用开发实战营1期大纲</a> </li>
<li><a href="https://w.1yb.co/KqBR58E">《AI 大模型微调训练营》大纲</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100540901">GitHub Copilot 实践课</a>  </li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541101">ChatGPT 从 0 到 1</a>  基础</li>
<li><a href="https://time.geekbang.org/opencourse/videointro/100541201">ChatGPT 和预训练模型实战课</a></li>
</ul>
<h3><span id="知乎">知乎</span><a href="#知乎" class="header-anchor">#</a></h3><ul>
<li><a href="https://agiclass.feishu.cn/docx/DDzxdQZBooXw9Jx4DdWcLZjLnHd">《AI 大模型全栈工程师》课程表（第 02 期） </a>  </li>
<li><a href="https://www.zhihu.com/people/dou-hong-jian-44/posts">AI Box专栏</a>  中国人大  AI ***<br>大模型survey</li>
</ul>
<h3><span id="清华">清华</span><a href="#清华" class="header-anchor">#</a></h3><ul>
<li><a href="https://www.zhihu.com/education/video-course/1545850719483392000">【清华 NLP X OpenBMB】大模型公开课｜带你从入门到实战</a>  V ***</li>
</ul>
<h3><span id="百度">百度</span><a href="#百度" class="header-anchor">#</a></h3><p><a href="https://cloud.baidu.com/qianfandev/topic/267956">《大模型应用实践》实训营</a></p>
<h3><span id="九天">九天</span><a href="#九天" class="header-anchor">#</a></h3><p><a href="https://appze9inzwc2314.pc.xiaoe-tech.com/p/t_pc/goods_pc_detail/goods_detail/p_64467371e4b0cf39e6c0c026?fromH5=true&entry_type=2002&share_type=5&type=3&entry=2">大模型技术实战课 </a></p>
<h1><span id="工业界">工业界</span><a href="#工业界" class="header-anchor">#</a></h1><p><a href="https://rocketmq-learning.com/">rocketmq-learning 社区</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>gpt</category>
        <category>study</category>
      </categories>
      <tags>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning</title>
    <url>/www6vHomeAIGC/2022/06/11/aiDeepLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>



<h1><span id="deep-learning">Deep Learning</span><a href="#deep-learning" class="header-anchor">#</a></h1><p><a href="https://candied-skunk-1ca.notion.site/Deep-Learning-10dbfe21108480b9affbf52a3b5bb13e?pvs=4">Deep Learning</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/www6vHomeAIGC/2022/06/07/aiMachineLearning/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="机器学习算法">机器学习算法</span><a href="#机器学习算法" class="header-anchor">#</a></h2><ul>
<li><p>监督式学习</p>
<ul>
<li><p>Linear Models</p>
<ul>
<li>逻辑回归 (Logistic Regression)<br><strong>离散</strong><br>逻辑回归其实是一个分类算法而不是回归算法。</li>
<li>线性回归 (Linear Regression)<br><strong>连续</strong></li>
</ul>
</li>
<li><p>Nearest Neighbors</p>
<ul>
<li>K邻近算法，KNN</li>
</ul>
</li>
<li><p>决策树 Decision Trees</p>
</li>
<li><p>Support Vector Machines, SVM [2]</p>
<ul>
<li>可分 <ul>
<li>线性可分</li>
<li>线性不可分</li>
</ul>
</li>
<li>超平面<ul>
<li>低纬升到高纬</li>
</ul>
</li>
</ul>
</li>
<li><p>Naive Bayes</p>
</li>
<li><p>随机森林</p>
</li>
</ul>
</li>
<li><p>无监督式学习</p>
<ul>
<li>关联规则 </li>
<li>K-means聚类算法<br>质心（centroids），距离</li>
</ul>
</li>
<li><p>强化学习</p>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>Classification<br>Identifying which category an object belongs to.</li>
<li>Regression<br>  Predicting a continuous-valued attribute associated with an object.</li>
<li>Clustering<br>  Automatic grouping of similar objects into sets.  </li>
<li>Dimensionality reduction<br>  Reducing the number of random variables to consider.</li>
</ul>
<img src="/www6vHomeAIGC/2022/06/07/aiMachineLearning/scikit-learn.png" class title="scikit-learn overview">



<h2><span id="按学习模型划分-4">按学习模型划分 [4]</span><a href="#按学习模型划分-4" class="header-anchor">#</a></h2><h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/479973669">【机器学习算法】10种常见机器学习算法+Python代码</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/b8227eac1fa6">机器学习–有监督–支持向量机SVM</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/supervised_learning.html">Supervised learning</a></p>
</li>
<li><p><a href="https://blog.csdn.net/hustlei/article/details/121803226">人工智能导论(6)——机器学习(Machine Learning)</a> ***</p>
</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能-学习资源</title>
    <url>/www6vHomeAIGC/2022/01/22/aiStudyResouce/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="书籍">书籍</span><a href="#书籍" class="header-anchor">#</a></h2><ul>
<li>理论<ul>
<li>《人工智能的数据基础》</li>
<li>《统计学习方法》 v2</li>
<li>The Hundred-Page Machine Learning Book - 入门</li>
<li>西瓜书 ***</li>
<li>花书</li>
</ul>
</li>
<li>框架<ul>
<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, v3 - 入门</li>
<li>Deep Learning with PyTorch - 入门</li>
<li>Deep Learning with Python - v2 - keras库</li>
</ul>
</li>
</ul>
<h2><span id="机器学习">机器学习</span><a href="#机器学习" class="header-anchor">#</a></h2><ul>
<li>浙大 - 吴浩基   ***</li>
<li>周志华 《机器学习初步》 *** </li>
<li>Coursera吴恩达- 《machine learning》+ 笔记  入门  *** </li>
<li><a href="https://www.bilibili.com/video/av79340208/">Machine Learning A-Z Hands-On Python &amp; R In Data Science</a>  Udemy 入门  ***</li>
<li><a href="https://www.bilibili.com/video/BV1KB4y1E73v">【人工智能系列】【中文】机器学习A-Z Machine Learning in Chinese(前7部分)</a></li>
<li><a href="https://www.bilibili.com/video/BV1jF411A7VF/">[Coursera公开课] [机器学习专项课程1&#x2F;4] 机器学习基础：案例研究</a>  ***</li>
<li><a href="https://www.bilibili.com/video/BV1Bg411Z77N">聚类算法：层次聚类、k-means 聚类、k-medoids 聚类、密度聚类</a>  ***</li>
</ul>
<h2><span id="深度学习">深度学习</span><a href="#深度学习" class="header-anchor">#</a></h2><ul>
<li>《动手学深度学习- 第二版 -pyTorch》  ***<br>b站有视频课<br><a href="http://zh.d2l.ai/index.html">《动手学深度学习》</a> 在线<br>电子书+jupternote代码</li>
<li>《神经网络和深度学习 》 复旦  </li>
<li>Coursera吴恩达《深度学习》 + 笔记  ***</li>
<li>老唐 ***</li>
<li>莫烦Python </li>
<li>北京大学 TensorFlow 2.0</li>
<li>算法可视化  ***</li>
<li>李宏毅 台湾</li>
</ul>
<h2><span id="nlp-amp-大模型">NLP &amp; 大模型</span><a href="#nlp-amp-大模型" class="header-anchor">#</a></h2><ul>
<li><p><a href="https://www.zhihu.com/education/video-course/1546509363711614976">沈向洋带你读论文——CV &amp; NLP 专题</a> V </p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1C14y147dp">2022年首发！B站讲的最好的【NLP自然语言处理】保姆级教程！</a>  V  有实践  *** </p>
</li>
<li><p>MLNLP第六期学术研讨会开始报名</p>
</li>
</ul>
<h2><span id="知识图谱">知识图谱</span><a href="#知识图谱" class="header-anchor">#</a></h2><ul>
<li><a href="https://www.bilibili.com/video/BV1VT411G7Y6?p=6">【国家级精品课】浙江大学教授（新全44集）知识图谱公开课分享</a>  ***</li>
</ul>
<h2><span id="极客时间">极客时间</span><a href="#极客时间" class="header-anchor">#</a></h2><ul>
<li>极客时间<ul>
<li>《AI 技术内参》  洪亮劼   全 ***</li>
<li>《机器学习 40 讲》  王天一 </li>
<li>《人工智能基础课》  王天一<br> 机器学习，深度学习</li>
<li>《成为AI产品经理》  刘海丰 京东   </li>
<li>《零基础实战机器学习》 黄佳  ***</li>
<li>《PyTorch深度学习实战》方远 大厂</li>
<li><a href="https://time.geekbang.org/course/intro/100023001?tab=catalog">TensorFlow 快速入门与实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/315">TensorFlow 2 项目进阶实战</a></li>
<li><a href="https://time.geekbang.org/course/intro/100046401">NLP 实战高手课</a></li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>深度学习在CTR预估的应用   张俊林</li>
<li>深度学习在图像理解中的应用  熊鹏飞</li>
</ul>
</li>
<li><a href="https://time.geekbang.org/course/detail/100005001-3090">深度学习应用实践 60 讲</a><ul>
<li>知识图谱技术实践  邵蓥侠</li>
</ul>
</li>
<li>极客训练营<br>-《机器学习训练营1期》  视频课</li>
</ul>
<h2><span id="中国大学mooc">中国大学MOOC</span><a href="#中国大学mooc" class="header-anchor">#</a></h2><ul>
<li>中国大学MOOC <a href="https://www.icourse163.org/learn/HIT-1206320802?tid=1468208513#/learn/announce">深度学习基础</a>   哈尔滨工业大学</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/FUDAN-1205806833">深度学习及其应用</a>   复旦</li>
<li>中国大学MOOC <a href="https://www.icourse163.org/course/ZUCC-1206146808">深度学习应用开发-TensorFlow实践</a>  浙大城市学院</li>
</ul>
<h2><span id="培训">培训</span><a href="#培训" class="header-anchor">#</a></h2><ul>
<li><a href="http://bit.baidu.com/">百度技术培训中心</a>  *** 认证， 自动驾驶，人工智能培训  </li>
<li><a href="http://bit.baidu.com/courseRouteDetail?id=111">人工智能学习路线</a>  百度技术培训中心</li>
</ul>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能 知识点</title>
    <url>/www6vHomeAIGC/2022/01/22/aiOverview/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="人工智能引论知识点">人工智能引论知识点</span><a href="#人工智能引论知识点" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2022/01/22/aiOverview/ai-overview.png" class>

<blockquote>
<p>62个知识点，9个高阶知识点(研究生课程)</p>
</blockquote>
<h2><span id="美国k12-ai知识点">美国K12 AI知识点</span><a href="#美国k12-ai知识点" class="header-anchor">#</a></h2><ul>
<li>智能感知</li>
<li>表示和推理</li>
<li>机器学习</li>
<li>自然交互能力</li>
<li>对社会的影响</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1Wa41157U4?spm_id_from=333.880.my_history.page.click&vd_source=f6e8c1128f9f264c5ab8d9411a644036">吴飞教授解读：人工智能知识点全景图：迈向智能+时代蓝皮书</a> video<br><a href="https://www.163.com/dy/article/HFAFUJPM051193U6.html">人工智能知识点全景图：迈向智能+时代蓝皮书</a></p>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>AI 应用场景</title>
    <url>/www6vHomeAIGC/2021/08/11/ai/</url>
    <content><![CDATA[<p></p>
<span id="more"></span>

<h2><span id="目录">目录</span><a href="#目录" class="header-anchor">#</a></h2><div class="toc">

<!-- toc -->

<ul>
<li><a href="#overview-1">Overview [1]</a></li>
<li><a href="#%E5%BA%94%E7%94%A8%E4%B8%8E%E8%A1%8C%E4%B8%9A-2">应用与行业 [2]</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-4">计算机视觉 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">应用场景</a></li>
<li><a href="#%E8%AF%AD%E9%9F%B3%E6%8A%80%E6%9C%AF-4">语音技术 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1">应用场景</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-4">自然语言处理 [4]</a><br>- <a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2">应用场景</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<!-- tocstop -->

</div>

<h2><span id="overview-1">Overview [1]</span><a href="#overview-1" class="header-anchor">#</a></h2><img src="/www6vHomeAIGC/2021/08/11/ai/ai.png" class title="AI">

<ul>
<li>人工智能的三个层面<ul>
<li>计算智能<br>能算能存</li>
<li>感知智能<br>能听会说， 能看会认</li>
<li>认知智能<br>能理解，会思考</li>
</ul>
</li>
</ul>
<h2><span id="应用与行业-2">应用与行业 [2]</span><a href="#应用与行业-2" class="header-anchor">#</a></h2><ul>
<li>健康码<img src="/www6vHomeAIGC/2021/08/11/ai/ai-hangye.png" class title="行业"></li>
</ul>
<h2><span id="计算机视觉-4">计算机视觉 [4]</span><a href="#计算机视觉-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>图像分类<ul>
<li>计算机视觉的核心问题<ul>
<li>细粒度图像分类</li>
</ul>
</li>
<li>人脸识别<ul>
<li>身份确认</li>
<li>身份查找</li>
</ul>
</li>
</ul>
</li>
<li>图像重建</li>
<li>目标检测<ul>
<li>物体定位</li>
<li>热门方向，领域<ul>
<li>在无人驾驶领域很重要</li>
<li>机器人导航</li>
<li>智能视频监控</li>
<li>工业检查</li>
</ul>
</li>
<li>关键问题<ul>
<li>小目标 高精度检测</li>
<li>多类别物体检测</li>
</ul>
</li>
</ul>
</li>
<li>图像搜索</li>
<li>图像分割<ul>
<li>核心问题</li>
<li>三类(逐层递进)<ul>
<li>语义分割</li>
<li>实例分割</li>
<li>全景分割</li>
</ul>
</li>
<li>应用场景</li>
</ul>
</li>
<li>目标跟踪</li>
</ul>
<h2><span id="语音技术-4">语音技术 [4]</span><a href="#语音技术-4" class="header-anchor">#</a></h2><h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>语音识别<ul>
<li>语音转文字</li>
<li>应用<ul>
<li>智能音响</li>
<li>语音输入发</li>
</ul>
</li>
</ul>
</li>
<li>语音合成 <ul>
<li>文字转语音 TTS</li>
<li>应用<ul>
<li>人机交互</li>
<li>语音客服</li>
<li>虚拟偶像-腾讯AI主播 艾灵</li>
</ul>
</li>
</ul>
</li>
<li>声纹识别<ul>
<li>微信的声音锁功能</li>
</ul>
</li>
</ul>
<h2><span id="自然语言处理-4">自然语言处理 [4]</span><a href="#自然语言处理-4" class="header-anchor">#</a></h2><ul>
<li>人工智能的最高境界</li>
</ul>
<h5><span id="应用场景">应用场景</span><a href="#应用场景" class="header-anchor">#</a></h5><ul>
<li>文本分类<ul>
<li>新闻分类  </li>
<li>邮件自动回复，垃圾邮件</li>
<li>客服聊天情感分析</li>
<li>内容审核</li>
</ul>
</li>
<li>机器翻译<ul>
<li>在线多语言翻译</li>
<li>会议中的语音同传</li>
<li>翻译机</li>
<li>跨语言检索</li>
</ul>
</li>
<li>知识图谱<ul>
<li>认知智能</li>
</ul>
</li>
<li>对话系统<ul>
<li>任务导向 - 问答系统</li>
<li>非任务导向 - 聊天机器人</li>
</ul>
</li>
<li>信息检索</li>
<li>文本生成<ul>
<li>写作机器人</li>
</ul>
</li>
</ul>
<h2><span id="总结">总结</span><a href="#总结" class="header-anchor">#</a></h2><ul>
<li>机器可以看   -  计算机视觉 </li>
<li>机器可以听   -  语音技术</li>
<li>机器可以理解 -  自然语言处理</li>
</ul>
<h2><span id="参考">参考</span><a href="#参考" class="header-anchor">#</a></h2><p><a href="https://cloud.tencent.com/edu/learning/course-3460-61199">腾讯云人工智能从业者认证线上培训课程</a> </p>
<ol>
<li>1.1  </li>
<li>1.2 </li>
<li>1.4 未</li>
<li>1.5</li>
</ol>
]]></content>
      <categories>
        <category>AIGC</category>
        <category>应用场景</category>
      </categories>
      <tags>
        <tag>应用场景</tag>
      </tags>
  </entry>
</search>
